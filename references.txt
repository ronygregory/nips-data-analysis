John Bohannon. Social science for pennies. Science, 334(6054):307307, 2011.
Andrew Carlson, Justin Betteridge, Richard C Wang, Estevam R Hruschka Jr, and Tom M Mitchell. Coupled semi-supervised learning for information extraction. In ACM WSDM, pages 101110, 2010.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition, pages 248255, 2009.
Double or Nothing. http://wikipedia.org/wiki/Double_or_nothing, 2014. Last accessed: July 31, 2014.
Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Association, 102(477):359378, 2007.
Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):8297, 2012. Panagiotis G Ipeirotis, Foster Provost, Victor S Sheng, and Jing Wang. Repeated labeling using multiple noisy labelers. Data Mining and Knowledge Discovery, 28(2):402441, 2014.  +
Srikanth Jagabathula, Lakshminarayanan Subramanian, and Ashwin Venkataraman. Reputation-based worker filtering in crowdsourcing. In Advances in Neural Information Processing Systems 27, pages 24922500, 2014.
Gabriella Kazai, Jaap Kamps, Marijn Koolen, and Natasa Milic-Frayling. Crowdsourcing for book search evaluation: impact of HIT design on comparative system ranking. In ACM SIGIR, pages 205214, 2011.
David R Karger, Sewoong Oh, and Devavrat Shah. Iterative learning for reliable crowdsourcing systems. In Advances in neural information processing systems, pages 19531961, 2011.
Qiang Liu, Jian Peng, and Alexander T Ihler. Variational inference for crowdsourcing. In NIPS, pages 701709, 2012.
Vikas C Raykar, Shipeng Yu, Linda H Zhao, Gerardo Hermosillo Valadez, Charles Florin, Luca Bogoni, and Linda Moy. Learning from crowds. The Journal of Machine Learning Research, 11:12971322, 2010.
Nihar B Shah and Dengyong Zhou. Double or nothing: Multiplicative incentive mechanisms for crowdsourcing. arXiv:1408.1387, 2014.
Nihar B Shah, Dengyong Zhou, and Yuval Peres. Approval voting and incentives in crowdsourcing. In International Conference on Machine Learning (ICML), 2015.
Jeroen Vuurens, Arjen P de Vries, and Carsten Eickhoff. How much spam can you take? An analysis of crowdsourcing results to increase accuracy. In ACM SIGIR Workshop on Crowdsourcing for Information Retrieval, pages 2126, 2011.
Paul Wais, Shivaram Lingamneni, Duncan Cook, Jason Fennell, Benjamin Goldenberg, Daniel Lubarov, David Marin, and Hari Simons. Towards building a highquality workforce with Mechanical Turk. NIPS workshop on computational social science and the wisdom of crowds, 2010.
Dengyong Zhou, Qiang Liu, John C Platt, Christopher Meek, and Nihar B Shah. Regularized minimax conditional entropy for crowdsourcing. arXiv preprint arXiv:1503.07240, 2015.  9
Dana Angluin and Philip Laird. Learning from noisy examples. Machine Learning, 2(4):343370, 1988.
Peter L. Bartlett, Michael I. Jordan, and Jon D. McAuliffe. Convexity, classification, and risk bounds. Journal
of the American Statistical Association, 101(473):138  156, 2006.
Christopher M Bishop. Pattern Recognition and Machine Learning. Springer-Verlag New York, Inc., 2006.
Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In Conference on
Computational Learning Theory (COLT), pages 92100, 1998.
Vasil Denchev, Nan Ding, Hartmut Neven, and S.V.N. Vishwanathan. Robust classification with adiabatic
quantum optimization. In International Conference on Machine Learning (ICML), pages 863870, 2012.
Luc Devroye, Laszlo Gyorfi, and Gabor Lugosi. A Probabilistic Theory of Pattern Recognition. Springer, 1996.
Nan Ding and S.V.N. Vishwanathan. t-logistic regression. In Advances in Neural Information Processing
Systems (NIPS), pages 514522. Curran Associates, Inc., 2010.
Thomas S. Ferguson. Mathematical Statistics: A Decision Theoretic Approach. Academic Press, 1967.
Aritra Ghosh, Naresh Manwani, and P. S. Sastry. Making risk minimization tolerant to label noise. Neurocomputing, 160:93  107, 2015.
Trevor Hastie, Saharon Rosset, Robert Tibshirani, and Ji Zhu. The entire regularization path for the support
vector machine. Journal of Machine Learning Research, 5:13911415, December 2004. ISSN 1532-4435.
Michael Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM, 5(6):392401,
November 1998.
Philip M. Long and Rocco A. Servedio. Random classification noise defeats all convex potential boosters.
Machine Learning, 78(3):287304, 2010. ISSN 0885-6125.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schutze. Introduction to Information Retrieval.
Cambridge University Press, New York, NY, USA, 2008. ISBN 0521865719, 9780521865715.
Naresh Manwani and P. S. Sastry. Noise tolerance under risk minimization. IEEE Transactions on Cybernetics,
43(3):11461151, June 2013.
Hamed Masnadi-Shirazi, Vijay Mahadevan, and Nuno Vasconcelos. On the design of robust classifiers for
computer vision. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2010.
Nagarajan Natarajan, Inderjit S. Dhillon, Pradeep D. Ravikumar, and Ambuj Tewari. Learning with noisy
labels. In Advances in Neural Information Processing Systems (NIPS), pages 11961204, 2013.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in Neural
Information Processing Systems (NIPS), pages 11771184, 2007.
Mark D. Reid and Robert C. Williamson. Composite binary losses. Journal of Machine Learning Research,
11:23872422, December 2010.
Mark D Reid and Robert C Williamson. Information, divergence and risk for binary experiments. Journal of
Machine Learning Research, 12:731817, Mar 2011.
Bernhard Scholkopf and Alexander J Smola. Learning with kernels, volume 129. MIT Press, 2002.
Rocco A. Servedio. On PAC learning using Winnow, Perceptron, and a Perceptron-like algorithm. In Conference on Computational Learning Theory (COLT), 1999.
John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge Uni. Press, 2004.
Alex Smola, Arthur Gretton, Le Song, and Bernhard Scholkopf. A Hilbert space embedding for distributions.
In Algorithmic Learning Theory (ALT), 2007.
Bharath K. Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Gert R. G. Lanckriet, and Bernhard Scholkopf.
Kernel choice and classifiability for RKHS embeddings of probability distributions. In Advances in Neural
Information Processing Systems (NIPS), 2009.
Guillaume Stempfel and Liva Ralaivola. Learning SVMs from sloppily labeled data. In Artificial Neural
Networks (ICANN), volume 5768, pages 884893. Springer Berlin Heidelberg, 2009.
Robert Tibshirani, Trevor Hastie, Balasubramanian Narasimhan, and Gilbert Chu. Diagnosis of multiple cancer
types by shrunken centroids of gene expression. Proceedings of the National Academy of Sciences, 99(10):
65676572, 2002.
Oscar Wilde. The Importance of Being Earnest, 1895.
V. N. Vapnik, An overview of statistical learning theory, Neural Networks, IEEE Transactions on, vol. 10, September 1999.
C. Cortes and V. Vapnik, Support-vector networks, Machine learning, vol. 20, pp. 273297, 1995.
A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth, Learnability and the VapnikChervonenkis dimension, Journal of the ACM (JACM), vol. 36, no. 4, pp. 929965, 1989.
M. Talagrand, Majorizing measures: the generic chaining, The Annals of Probability, vol. 24, no. 3, pp. 10491103, 1996.
D. A. McAllester, PAC-Bayesian stochastic model selection, Machine Learning, vol. 51, pp. 521, 2003.
O. Bousquet and A. Elisseeff, Stability and generalization, The Journal of Machine Learning Research (JMLR), vol. 2, pp. 499526, 2002.
P. L. Bartlett and S. Mendelson, Rademacher and gaussian complexities: Risk bounds and structural results, The Journal of Machine Learning Research (JMLR), vol. 3, pp. 463482, 2002.
J.-Y. Audibert and O. Bousquet, Combining PAC-Bayesian and generic chaining bounds, The Journal of Machine Learning Research (JMLR), vol. 8, pp. 863889, 2007.
H. Xu and S. Mannor, Robustness and generalization, Machine learning, vol. 86, no. 3, pp. 391423, 2012.
A. Elisseeff, M. Pontil, et al., Leave-one-out error and stability of learning algorithms with applications, NATO-ASI series on Learning Theory and Practice Science Series Sub Series III: Computer and Systems Sciences, 2002.
S. Kutin and P. Niyogi, Almost-everywhere algorithmic stability and generalization error, in Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence (UAI), 2002.
T. Poggio, R. Rifkin, S. Mukherjee, and P. Niyogi, General conditions for predictivity in learning theory, Nature, vol. 428, pp. 419422, 2004.
M. Kearns and D. Ron, Algorithmic stability and sanity-check bounds for leave-one-out crossvalidation, Neural Computation, vol. 11, no. 6, pp. 14271453, 1999.
S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan, Learnability, stability and uniform convergence, The Journal of Machine Learning Research (JMLR), vol. 11, pp. 2635 2670, 2010.
L. Devroye, L. Gyorfi, and G. Lugosi, A probabilistic theory of pattern recognition. Springer, 1996.
V. Vapnik and O. Chapelle, Bounds on error expectation for support vector machines, Neural Computation, vol. 12, no. 9, pp. 20132036, 2000.
H. Robbins, A remark on stirlings formula, American Mathematical Monthly, pp. 2629, 1955.
T. M. Cover and J. A. Thomas, Elements of information theory. Wiley & Sons, 1991.
T. Downs, K. E. Gates, and A. Masters, Exact simplification of support vector solutions, JMLR, vol. 2, pp. 293297, 2002.
S. Wager, S. Wang, and P. S. Liang, Dropout training as adaptive regularization, in NIPS, pp. 351359, 2013.
S. M. Stigler, The history of statistics: The measurement of uncertainty before 1900. Harvard University Press, 1986.
P. Diaconis and S. Zabell, Closed form summation for classical distributions: Variations on a theme of de moivre, Statlstlcal Science, vol. 6, no. 3, pp. 284302, 1991.
S. Shalev-Shwartz and S. Ben-David, Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press, 2014.  9
Antoniak, C. E. Mixtures of Dirichlet Processes with Applications to Bayesian Nonparametric
Problems. The Annals of Statistics, 2(6):11521174, 1974.
Batir, N. Inequalities for the Gamma Function. Archiv der Mathematik, 91(6):554563, 2008.
Blei, D. M. and Jordan, M. I. Variational Inference for Dirichlet Process Mixtures. Bayesian Analysis, 1(1):121144, 2006.
Daume, H. Fast Search for Dirichlet Process Mixture Models. In Conference on Artificial Intelligence and Statistics, 2007.
Escobar, M. D. and West, M. Bayesian Density Estimation and Inference using Mixtures. Journal
of the American Statistical Association, 90(430):577588, June 1995.
Fearnhead, P. Particle Filters for Mixture Models with an Uknown Number of Components. Statistics and Computing, 14:1121, 2004.
Kurihara, K., Welling, M., and Vlassis, N. Accelerated Variational Dirichlet Mixture Models. In
Advances in Neural Information Processing Systems (NIPS), 2006.
Lin, Dahua. Online learning of nonparametric mixture models via sequential variational approximation. In Burges, C.J.C., Bottou, L., Welling, M., Ghahramani, Z., and Weinberger, K.Q. (eds.),
Advances in Neural Information Processing Systems 26, pp. 395403. Curran Associates, Inc.,
Neal, R. M. Bayesian Mixture Modeling. In Proceedings of the Workshop on Maximum Entropy
and Bayesian Methods of Statistical Analysis, volume 11, pp. 197211, 1992.
Neal, R. M. Markov chain sampling methods for Dirichlet process mixture models. Journal of
Computational and Graphical Statistics, 9(2):249265, June 2000.
Rasmussen, C. E. The infinite gaussian mixture model. In Advances in Neural Information Processing Systems 12, pp. 554560. MIT Press, 2000.
Tsiligkaridis, T. and Forsythe, K. W. A Sequential Bayesian Inference Framework for Blind Frequency Offset Estimation. In Proceedings of IEEE International Workshop on Machine Learning
for Signal Processing, Boston, MA, September 2015.
Tzikas, D. G., Likas, A. C., and Galatsanos, N. P. The Variational Approximation for Bayesian
Inference. IEEE Signal Processing Magazine, pp. 131146, November 2008.
Wang, L. and Dunson, D. B. Fast Bayesian Inference in Dirichlet Process Mixture Models. Journal
of Computational and Graphical Statistics, 20(1):196216, 2011.
A. Abdulle, G. Vilmart, and K. C. Zygalakis. Long time accuracy of Lie-Trotter splitting methods for Langevin dynamics. SIAM Journal on Numerical Analysis, 53(1):116, 2015.
S. Ahn, A. Korattikara, and M. Welling. Bayesian posterior sampling via stochastic gradient Fisher scoring. In Proceedings of the 29th International Conference on Machine Learning, pages 15911598, 2012.
S. Brooks, A. Gelman, G. Jones, and X.-L. Meng. Handbook of Markov Chain Monte Carlo. CRC Press, 2011.
T. Chen, E. B. Fox, and C. Guestrin. Stochastic gradient Hamiltonian Monte Carlo. In Proceedings of the 31st International Conference on Machine Learning, pages 16831691, 2014.
N. Ding, Y. Fang, R. Babbush, C. Chen, R. D. Skeel, and H. Neven. Bayesian sampling using stochastic gradient thermostats. In Advances in Neural Information Processing Systems 27, pages 32033211, 2014.
S. Duane, A. D. Kennedy, B. J. Pendleton, and D. Roweth. Hybrid Monte Carlo. Physics Letters B, 195(2):216222, 1987.
D. Frenkel and B. Smit. Understanding Molecular Simulation: From Algorithms to Applications, Second Edition. Academic Press, 2001.
W. G. Hoover. Computational Statistical Mechanics, Studies in Modern Thermodynamics. Elsevier Science, 1991.
A. M. Horowitz. A generalized guided Monte Carlo algorithm. Physics Letters B, 268(2):247 252, 1991.
A. Jones and B. Leimkuhler. Adaptive stochastic methods for sampling driven molecular systems. The Journal of Chemical Physics, 135:084125, 2011.
H. Larochelle and Y. Bengio. Classification using discriminative restricted Boltzmann machines. In Proceedings of the 25th International Conference on Machine Learning, pages 536543, 2008.
B. Leimkuhler and C. Matthews. Rational construction of stochastic numerical methods for molecular sampling. Applied Mathematics Research eXpress, 2013(1):3456, 2013.
B. Leimkuhler and C. Matthews. Molecular Dynamics: With Deterministic and Stochastic Numerical Methods. Springer, 2015.
B. Leimkuhler, C. Matthews, and G. Stoltz. The computation of averages from equilibrium and nonequilibrium Langevin molecular dynamics. IMA Journal of Numerical Analysis, 2015.
B. Leimkuhler and X. Shang. Adaptive thermostats for noisy gradient systems. arXiv preprint arXiv:1505.06889, 2015.
N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller. Equation of state calculations by fast computing machines. The Journal of Chemical Physics, 21(6):1087, 1953.
S. Nose. A unified formulation of the constant temperature molecular dynamics methods. The Journal of Chemical Physics, 81:511, 1984.
H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical Statistics, 22(2):400407, 1951.
C. Robert and G. Casella. Monte Carlo Statistical Methods, Second Edition. Springer, 2004.
S. J. Vollmer, K. C. Zygalakis, and Y. W. Teh. (Non-) asymptotic properties of stochastic gradient Langevin dynamics. arXiv preprint arXiv:1501.00438, 2015.
M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient Langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning, pages 681688, 2011.  9
Harry Markowitz. Portfolio selection. The Journal of Finance, 7(1):7791, 1952.
Michael J Best and Robert R Grauer. On the sensitivity of mean-variance-efficient portfolios to changes in asset means: some analytical and computational results. Review of Financial Studies, 4(2):315342, 1991.
Vijay Kumar Chopra and William T Ziemba. The effect of errors in means, variances, and covariances on optimal portfolio choice. The Journal of Portfolio Management, 19(2):611, 1993.
Robert C Merton. On estimating the expected return on the market: An exploratory investigation. Journal of Financial Economics, 8(4):323361, 1980.
Jarl G Kallberg and William T Ziemba. Mis-specifications in portfolio selection problems. In Risk and Capital, pages 7487. Springer, 1984.
Jianqing Fan, Yingying Fan, and Jinchi Lv. High dimensional covariance matrix estimation using a factor model. Journal of Econometrics, 147(1):186197, 2008.
James H Stock and Mark W Watson. Forecasting using principal components from a large number of predictors. Journal of the American Statistical Association, 97(460):11671179, 2002.
Jushan Bai, Kunpeng Li, et al. Statistical analysis of factor models of high dimension. The Annals of Statistics, 40(1):436465, 2012.
Jianqing Fan, Yuan Liao, and Martina Mincheva. Large covariance estimation by thresholding principal orthogonal complements. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 75(4):603680, 2013.
Olivier Ledoit and Michael Wolf. Improved estimation of the covariance matrix of stock returns with an application to portfolio selection. Journal of Empirical Finance, 10(5):603621, 2003.
Olivier Ledoit and Michael Wolf. A well-conditioned estimator for large-dimensional covariance matrices. Journal of Multivariate Analysis, 88(2):365411, 2004.
Olivier Ledoit and Michael Wolf. Honey, I shrunk the sample covariance matrix. The Journal of Portfolio Management, 30(4):110119, 2004.
Peter J Huber. Robust Statistics. Wiley, 1981.
Ricardo A Maronna and Ruben H Zamar. Robust estimates of location and dispersion for highdimensional datasets. Technometrics, 44(4):307317, 2002.
Ramanathan Gnanadesikan and John R Kettenring. Robust estimates, residuals, and outlier detection with multiresponse data. Biometrics, 28(1):81124, 1972.
Yilun Chen, Ami Wiesel, and Alfred O Hero. Robust shrinkage estimation of high-dimensional covariance matrices. IEEE Transactions on Signal Processing, 59(9):40974107, 2011.
Romain Couillet and Matthew R McKay. Large dimensional analysis and optimization of robust shrinkage covariance matrix estimators. Journal of Multivariate Analysis, 131:99120, 2014.
Ravi Jagannathan and T Ma. Risk reduction in large portfolios: Why imposing the wrong constraints helps. The Journal of Finance, 58(4):16511683, 2003.
Jianqing Fan, Jingjin Zhang, and Ke Yu. Vast portfolio selection with gross-exposure constraints. Journal of the American Statistical Association, 107(498):592606, 2012.
Peter J Rousseeuw and Christophe Croux. Alternatives to the median absolute deviation. Journal of the American Statistical Association, 88(424):12731283, 1993.
M. H. Xu and H. Shao. Solving the matrix nearness problem in the maximum norm by applying a projection and contraction method. Advances in Operations Research, 2012:115, 2012.
Alexandre Belloni and Victor Chernozhukov. `1 -penalized quantile regression in high-dimensional sparse models. The Annals of Statistics, 39(1):82130, 2011.
Lan Wang, Yichao Wu, and Runze Li. Quantile regression for analyzing heterogeneity in ultra-high dimension. Journal of the American Statistical Association, 107(497):214222, 2012.
Peter J Bickel and Elizaveta Levina. Covariance regularization by thresholding. The Annals of Statistics, 36(6):25772604, 2008.
T Tony Cai, Cun-Hui Zhang, and Harrison H Zhou. Optimal rates of convergence for covariance matrix estimation. The Annals of Statistics, 38(4):21182144, 2010.
Kai-Tai Fang, Samuel Kotz, and Kai Wang Ng. Symmetric Multivariate and Related Distributions. Chapman and Hall, 1990.
Harry Joe. Multivariate Models and Dependence Concepts. Chapman and Hall, 1997.
Rafael Schmidt. Tail dependence for elliptically contoured distributions. Mathematical Methods of Operations Research, 55(2):301327, 2002.
Svetlozar Todorov Rachev. Handbook of Heavy Tailed Distributions in Finance. Elsevier, 2003.
Svetlozar T Rachev, Christian Menn, and Frank J Fabozzi. Fat-tailed and Skewed Asset Return Distributions: Implications for Risk Management, Portfolio Selection, and Option Pricing. Wiley, 2005.
Kevin Dowd. Measuring Market Risk. Wiley, 2007.
Torben Gustav Andersen. Handbook of Financial Time Series. Springer, 2009.
Jushan Bai and Shuzhong Shi. Estimating high dimensional covariance matrices and its applications. Annals of Economics and Finance, 12(2):199215, 2011.
Sara Van De Geer and SA Van De Geer. Empirical Processes in M -estimation. Cambridge University Press, Cambridge, 2000.
Alastair R Hall. Generalized Method of Moments. Oxford University Press, Oxford, 2005.
Peter Buhlmann and Sara Van De Geer. Statistics for High-dimensional Data: Methods, Theory and Applications. Springer, 2011.  9
R. Rifkin and A. Klautau. In defense of one-vs-all classification. J. Mach. Learn. Res., 5:101141, 2004.
T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley & Sons, Inc., 1991.
L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classification and Regression Trees. CRC Press LLC, Boca Raton, Florida, 1984.
M. Kearns and Y. Mansour. On the boosting ability of top-down decision tree learning algorithms. Journal of Computer and Systems Sciences, 58(1):109128, 1999 (also In STOC, 1996).
A. Beygelzimer, J. Langford, and P. D. Ravikumar. Error-correcting tournaments. In ALT, 2009.
A. Beygelzimer, J. Langford, Y. Lifshits, G. B. Sorkin, and A. L. Strehl. Conditional probability tree estimation analysis and algorithms. In UAI, 2009.
C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.
S. Bengio, J. Weston, and D. Grangier. Label embedding trees for large multi-class tasks. In NIPS, 2010.
G. Madzarov, D. Gjorgjevikj, and I. Chorbev. A multi-class svm classifier utilizing binary decision tree. Informatica, 33(2):225233, 2009.
J. Deng, S. Satheesh, A. C. Berg, and L. Fei-Fei. Fast and balanced: Efficient label tree learning for large scale object recognition. In NIPS, 2011.
J. Weston, A. Makadia, and H. Yee. Label partitioning for sublinear ranking. In ICML, 2013.
B. Zhao and E. P. Xing. Sparse output coding for large-scale visual recognition. In CVPR, 2013.
D. Hsu, S. Kakade, J. Langford, and T. Zhang. Multi-label prediction via compressed sensing. In NIPS, 2009.
A. Agarwal, S. M. Kakade, N. Karampatziakis, L. Song, and G. Valiant. Least squares revisited: Scalable approaches for multi-class prediction. In ICML, 2014.
O. Beijbom, M. Saberian, D. Kriegman, and N. Vasconcelos. Guess-averse loss functions for costsensitive multiclass boosting. In ICML, 2014.
R. Agarwal, A. Gupta, Y. Prabhu, and M. Varma. Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages. In WWW, 2013.
Y. Prabhu and M. Varma. Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning. In ACM SIGKDD, 2014.
H.-F. Yu, P. Jain, P. Kar, and I. S. Dhillon. Large-scale multi-label learning with missing labels. In ICML, 2014.
T.-Y. Liu, Y. Yang, H. Wan, H.-J. Zeng, Z. Chen, and W.-Y. Ma. Support vector machines classification with a very large-scale taxonomy. In SIGKDD Explorations, 2005.
P. N. Bennett and N. Nguyen. Refined experts: improving classification in large taxonomies. In SIGIR, 2009.
A. Montillo, J. Tu, J. Shotton, J. Winn, J.E. Iglesias, D.N. Metaxas, and A. Criminisi. Entanglement and differentiable information gain maximization. Decision Forests for Computer Vision and Medical Image Analysis, 2013.
K. Tentori, V. Crupi, N. Bonini, and D. Osherson. Comparison of confirmation measures. Cognition, 103(1):107  119, 2007.
R. Carnap. Logical Foundations of Probability. 2nd ed. Chicago: University of Chicago Press. Par. 87 (pp. 468-478), 1962.
S. Shalev-Shwartz. Online learning and online convex optimization. Found. Trends Mach. Learn., 4(2):107194, 2012.
J. Langford, L. Li, and A. Strehl. http://hunch.net/vw, 2007.
Y. Nesterov. Introductory lectures on convex optimization : a basic course. Applied optimization, Kluwer Academic Publ., 2004.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.  9  Logarithmic Time Online Multiclass prediction (Supplementary Material) 6  Bottom-up partitions do not work  The most natural bottom-up construction for creating partitions is not viable as will be now shown by an example. Bottom-up construction techniques start by pairing labels, either randomly or arbitrarily, and then building a predictor of whether the class label is left or right conditioned on the class label being one of the paired labels. In order to construct a full tree, this operation must compose, pairing trees with size 2 to create trees of size 4. Here, we show that the straightforward approach to composition fails. Suppose we have a one dimensional feature space with examples of class label i having feature value i and we work with threshold predictors. Suppose we have 4 classes 1, 2, 3, 4, and we happen to pair (1, 3) and (2, 4). It is easy to build a linear predictor for each of these splits. The next step is building a predictor for (1, 3) vs (2, 4) which is impossible because all thresholds in (, 1), (2, 3), and (4, ) err on two labels while thresholds on (1, 2) and (3, 4) err on one label.  7  Proof of Lemma 1  We start from deriving an upper-bound on J(h). For the ease of notation let Pi = P (h(x) > 0|i). Thus     k k k X X X   J(h) = 2 i |P (h(x) > 0|i)  P (h(x) > 0)| = 2 i Pi  j Pj  ,   i=1 i=1 j=1 where i={1,2,...,k} 0  Pi  1. Let i = min(Pi , 1  Pi ) and recall the purity factor  = Pk 1 i=1 i i and the balancing factor  = P (h(x) > 0). Without loss of generality let   2 . Furthermore, let 1 1 }, L2 = {i : i  {1, 2, . . . , k}, Pi  [, )} 2 2  L1 = {i : i  {1, 2, . . . , k}, Pi   L3 = {i : i  {1, 2, . . . , k}, Pi < }.  and First notice that =  k X  i Pi =  i=1  X  X  i (1  i ) +  i i =  iL2 L3  iL1  X  i  2  iL1  X  i i +   (3)  iL1  Therefore J(h) 2  k X  =  i |Pi  | =  i=1  X  =  J(h) 2  P  iL3  =  X  i (1  )   i = 1   X iL1  =  X iL1  i (1  i  ) +  iL1  iL1  Note that  X  i i +  iL1  i   X  i i   iL2  P  iL2  X  i (i  ) +  iL2  iL1  P  X  X  i (  i )  iL3  X  i  +  iL2  i    iL3  X  i i  iL3  i and therefore  X X X X X X i (1) i i + i i  i  + (1 i  i ) i i iL1  i (1  2)   X iL1  iL2  i i +  iL2  X iL2  10  i i + (1  2  iL1  X iL2  iL2  i )   X iL3  iL3  i i  Furthermore, since  write that J(h) 2  P  iL1  i i + X  =  P  iL2  i i   P  iL3  i (1  2) + (1  2  i i =  + 2  X  i )   + 2  iL2  X  i i we further  i i  iL2  iL2  iL1  P  By Equation 3, it can be further rewritten as X X X J(h) = (1  2)( + 2 i i  ) + (1  2 i )   + 2 i  i 2 iL1 iL2 iL2 X X = 2(1  )(  ) + 2(1  2) i i + 2 i (i  ) iL1  iL2  Since i s are bounded by 0.5 we obtain X X 1 J(h) i (  ) i i + 2  2(1  )(  ) + 2(1  2) 2 2 iL2  iL1   2(1  )(  ) + 2(1  2) + 1  2 = 2(1  )  2(1  ) + 2(1  2) + 1  2 =  1  2 2  2  Thus:   8  2  J(h)  . 4  Proof of Lemma 2  Proof. We first show that J(h) 
. We start from deriving an upper-bound on J(h), where h  H is some hypothesis in the hypothesis class. For the ease of notation let Pi = P (h(x) > 0|i). Thus k X J(h) = 2 i |P (h(x) > 0|i)  P (h(x) > 0)| (4) i=1      k X    2  i  Pi  j Pj  ,   i=1 j=1 k X  =  where i={1,2,...,k} 0  Pi  1. The objective J(h) is certainly maximized on the extremes of the
interval. The upper-bound on J(h) can be thus obtained by setting some of the Pi s to 1s and remaining ones to 0s. To be more precise, let L1 = {i : i  {1, 2, . . . , k}, Pi = 1} and L2 = {i : i  {1, 2, . . . , k}, Pi = 0}. Therefore it follows that   X X X X J(h)  2  i (1  j ) + i j  iL1  jL1  iL2  jL1  " =  # X  2  i  (  X  iL1  iL1  X  X  2  i ) + (1   iL1  " =  4  Let b =  iL1  i )  X  i  iL1  # iL1  P  X  i  (  i )2  iL1  i thus J(h)  4b(1  b) = 4b2 + 4b  (5)  2  Since b 
, it is straightforward that 4b + 4b 
and thus J(h) 
. We now proceed to prove the main statement of Lemma 2, if h induces a maximally pure and balanced partition then J(h) = 1. Since h is maximally balanced, P (h(x) > 0) = 0.5. Simultaneously, since h is maximally pure i={1,2,...,k} (P (h(x) > 0|i) = 0 or P (h(x) > 0|i) = 1). Substituting that into Equation 5 yields that J(h) = 1. 11  9  Proof of Theorem 1  Proof. The analysis studies a tree construction algorithm where we recursively find the leaf node with the highest weight, and choose to split it into two children. Consider the tree constructed over t steps where in each step we take one leaf node and split it into two. Let n be the heaviest node at time t and its weight wn be denoted by w for brevity. Consider splitting this leaf to two children n0 and n1 . For the ease of notation let w0 = wn0 and w1 = wn1 . Also for the ease of notation let  = P (hn (x) > 0) and Pi = P (hn (x) > 0|i). Let i be the shorthand for Pk Pk n,i and h be the shorthand for hn . Recall that  = i=1 i Pi and i=1 i = 1. Also notice that th w0 = w(1) and w1 =  w.  Let  be the k-element vector with i entry equal to i . Furthermore Pk let G() = i=1 i ln 1i . i) Before the split the contribution of node n to Gt was wG(). Let n0 ,i = i (1P and n1 ,i = iPi 1 be the probabilities that a randomly chosen x drawn from P has label i given that x reaches nodes n0 and n1 respectively. For brevity, let n0 ,i be denoted by 0,i and n1 ,i be denoted by 1,i . Furthermore let 0 be the k-element vector with ith entry equal to 0,i and let 1 be the k-element vector with ith entry equal to 1,i . Notice that  = (1)0 +1 . After the split the contribution of the same, now internal, node n changes to w((1  )G(0 ) +  G(1 )). We denote the difference between them as t and thus h i t := Gt  Gt+1 = w G()  (1  )G(0 )   G(1 ) . (6)  We aim to lower-bound t . The entropy reduction of Equation 6
corresponds to a gap in the Jensens inequality applied to the concave function G(). This leads to the lower-bound on t given in Lemma 4 (the lemma is proven in Section 10 in the Supplementary material). Lemma 4. The entropy reduction t of Equation 6 can be lower-bounded as follows t   J(h)2 Gt 8(1  )t ln k  Lemma 4 implies that the larger the objective J(h) is at time t, the larger the entropy reduction ends up being, which further reinforces intuitions to maximize J. In general, it might not be possible to find any hypothesis with a large enough objective J(h) to guarantee sufficient progress at this point so we appeal to a weak learning assumption. This assumption can be used to further lower-bound t . The lower-bound can then be used (details are in Section 9 in the Supplementary material) to obtain the main theoretical statement of the paper captured in Theorem 1. From the definition of  it follows that 1      . Also note that the weak hypothesis assumption guarantees J(h)  2, which applied to the lower-bound on t captured in Lemma 4 yields  2 Gt . t  2(1  )2 t ln k q 2 Gt Let  = (1)82 ln k . Then t > 16t . Thus we obtain the recurrence inequality    2 Gt 2 Gt+1  Gt  t < Gt  = Gt 1  16t 16t One can now compute the minimum number of splits required to reduce Gt below , where  
. Applying the proof technique from
(the proof of Theorem 10) gives the final statement of Theorem 1.  10  Proof of Lemma 4  Proof. Without loss of generality assume that P1  P2      Pk . As mentioned before, the entropy reduction t corresponds to a gap in the Jensens inequality applied to the concave function G(). Also recall that Shannon entropy is strongly concave with respect to `1 -norm (see e.g., Example 2.5 in Shalev-Shwartz
). As a specific consequence (see e.g. Theorem 2.1.9 in Nesterov
) we obtain !2 k X w wJ(h)2 2 t  w(1  )k0  1 k1 = |i (Pi  )| = , (7) (1  ) i=1 4(1  ) 12  where the last equality results from the definition of J(h) = 2  Pk  i=1  i |Pi  |.  Note that the following holds w  2tGlnt k , where recall that w is the weight of the heaviest leaf in the tree, i.e. the leaf with the highest weight, at round t. This leaf is selected to the currently considered split
. In particular, the lower-bound on w is the consequence of the following   X k X X 1 Gt = wl l,i ln  wl ln k  2tw ln k, l,i i=1 lL  lL  where w = maxlL wl . Thus w  statement of the lemma.  11  Gt 2t ln k  which when substituted to Equation 7 gives the final  Proof of Lemma 3  Proof. We bound the number of swaps that any node makes. Consider RS = 4 and let j be the node that is about to split and s be the orphan node that will be recycled (thus Cr = Cs ). The condition in Equation 2 implies that the swap is done if Cj > 4(Cr + 1) = 4(Cs + 1). Algorithm 1 makes s a child of j during the swap and sets its counter to Csnew = bCj /2c  2(Cr + 1) = 2(Cs + 1). Then Cr gets updated. Since the value of Csnew at least doubles after a swap and all counters are bounded by the number of examples n, the node can be involved in at most log2 n swaps.  12  Equivalent forms of the objective function  Consider the objective function as given in Equation 1 J(h) = 2  k X  i |P (h(x) > 0)  P (h(x) > 0|i)| .  i=1  Recall that X denotes the set of all examples and let Xi denote the set of examples in class i. Also let |X | denote the cardinality of set X and let |Xi | denote the cardinality of set Xi . Then we can re-write the objective as P P  k X  xX 1(h(x) > 0)  xXi 1(h(x) > 0)   i  J(h) = 2   |X | |Xi | i=1 =  2  k X  i |Ex
|  i=1  =  2Ei
|].  13  13  Toy example of the behavior of LOMtree algorithm  Figure 4 shows the toy example of the behavior of LOMtree algorithm for the first few data points. Without loss of generality we consider the root node (exactly the same actions would be performed in any other tree node). Notice that the algorithm achieves simultaneously balanced and pure split of classes reaching the considered node. e denotes the expectation Ex
, and e1, e2, e3, e4 denote the expectations Ex
. For simplicity we assume score h(x) can only be either 1 (if the example is sent to the right) or 1 (if the example is sent to the left). The figure should be read as follows (we explain how to read first few illustrations): a) Root is initialized. Expectation e is initialized to 0. b) The first example x1 comes with label 1 (we denote it as (x1, 1)). e1 is initialized to 0. The difference between e and e1 is computed: e  e1 = 0. The difference is non-positive thus the example is sent to the right child of the root, which is now being created (the left child is created along with the right child as we always create both children of any node simultaneously). c) Expectations e and e1 get updated. It is shown that root and its right child saw an example of class 1. d) The second example x2 comes with label 2 (we denote it as (x2, 2)). e2 is initialized to 0. The difference between e and e2 is computed: e  e2 = 1. The difference is positive thus the example is sent to the left child of the root. e) Expectations e and e2 get updated. It is shown that root saw examples of class 1 and 2, whereas its resp. left and right child saw example of class resp. 2 and 1. f) . . .  a)  b)  c)  d)  e)  f)  g)  h)  i)  j)  k)  Figure 4: Toy example of the behavior of LOMtree algorithm in the tree root.  14  14  Experiments - dataset details  Below we provide the details of the datasets that we were using for the experiments in Section 4:  Isolet: downloaded from http://www.cs.huji.ac.il/shais/datasets/ ClassificationDatasets.html  Sector and Aloi: downloaded from http://www.csie.ntu.edu.tw/cjlin/ libsvmtools/datasets/multiclass.html  ImageNet
: features extracted according to http://www.di.ens.fr/willow/ research/cnn/, dataset obtained from the authors.  ODP
: obtained from Paul Bennett. Our version has significantly more classes than reported in the cited paper because we use the entire dataset.  15
Nir Ailon and Moses Charikar. Fitting tree metrics: Hierarchical clustering and phylogeny. In Foundations of Computer Science, 2005., pages 7382, 2005.
Bjoern Andres, Joerg H. Kappes, Thorsten Beier, Ullrich Kothe, and Fred A. Hamprecht. Probabilistic image segmentation with closedness constraints. In Proc. of ICCV, pages 26112618, 2011.
Bjoern Andres, Thorben Kroger, Kevin L. Briggman, Winfried Denk, Natalya Korogod, Graham Knott, Ullrich Kothe, and Fred. A. Hamprecht. Globally optimal closed-surface segmentation for connectomics. In Proc. of ECCV, 2012.
Bjoern Andres, Julian Yarkony, B. S. Manjunath, Stephen Kirchhoff, Engin Turetken, Charless Fowlkes, and Hanspeter Pfister. Segmenting planar superpixel adjacency graphs w.r.t. nonplanar superpixel affinity graphs. In Proc. of EMMCVPR, 2013.
Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Jitendra Malik. Contour detection and hierarchical image segmentation. IEEE Trans. Pattern Anal. Mach. Intell., 33(5):898916, May 2011.
Yoram Bachrach, Pushmeet Kohli, Vladimir Kolmogorov, and Morteza Zadimoghaddam. Optimal coalition structure generation in cooperative graph games. In Proc. of AAAI, 2013.
Shai Bagon and Meirav Galun. Large scale correlation clustering. In CoRR, abs/1112.2903, 2011.
F Barahona. On the computational complexity of ising spin glass models. Journal of Physics A: Mathematical, Nuclear and General, 15(10):32413253, april 1982.
F Barahona. On cuts and matchings in planar graphs. Mathematical Programming, 36(2):53 68, november 1991.
F Barahona and A Mahjoub. On the cut polytope. Mathematical Programming, 60(1-3):157 173, September 1986.
Thorsten Beier, Thorben Kroeger, Jorg H Kappes, Ullrich Kothe, and Fred A Hamprecht. Cut, glue, and cut: A fast, approximate solver for multicut partitioning. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 7380, 2014.
Michel Deza and Monique Laurent. Geometry of cuts and metrics, volume 15. Springer Science & Business Media, 1997.
Michael Fisher. On the dimer solution of planar ising models. Journal of Mathematical Physics, 7(10):17761781, 1966.
Sungwoong Kim, Sebastian Nowozin, Pushmeet Kohli, and Chang Dong Yoo. Higher-order correlation clustering for image segmentation. In Advances in Neural Information Processing Systems,25, pages 15301538, 2011.
Vladimir Kolmogorov. Blossom v: a new implementation of a minimum cost perfect matching algorithm. Mathematical Programming Computation, 1(1):4367, 2009.
David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proc. of ICCV, pages 416423, 2001.
David Martin, Charless C. Fowlkes, and Jitendra Malik. Learning to detect natural image boundaries using local brightness, color, and texture cues. IEEE Trans. Pattern Anal. Mach. Intell., 26(5):530549, May 2004.
Julian Yarkony. Analyzing PlanarCC. NIPS 2014 workshop, 2014.
Julian Yarkony, Thorsten Beier, Pierre Baldi, and Fred A Hamprecht. Parallel multicut segmentation via dual decomposition. In New Frontiers in Mining Complex Patterns, 2014.
Julian Yarkony, Alexander Ihler, and Charless Fowlkes. Fast planar correlation clustering for image segmentation. In Proc. of ECCV, 2012.
Chong Zhang, Julian Yarkony, and Fred A. Hamprecht. Cell detection and segmentation using correlation clustering. In MICCAI, volume 8673, pages 916, 2014.  9
R. Barzilay and M. Lapata. Modeling Local Coherence: An Entity-Based Approach. In ACL, 2008.
S. Bird, E. Loper, and E. Klein. Natural Language Processing with Python. OReilly Media Inc., 2009.
X. Chen and C. L. Zitnick. Minds Eye: A Recurrent Visual Representation for Image Caption Generation. In CVPR, 2015.
F. Y. Y. Choi, P. Wiemer-Hastings, and J. Moore. Latent Semantic Analysis for Text Segmentation. In EMNLP, 2001.
J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell. Long-term Recurrent Convolutional Networks for Visual Recognition and Description. In CVPR, 2015.
Y. Gong, L. Wang, M. Hodosh, J. Hockenmaier, and S. Lazebnik. Improving Image-Sentence Embeddings Using Large Weakly Annotated Photo Collections. In ECCV, 2014.
K. He, X. Zhang, S. Ren, and J. Sun. Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. In arXiv, 2015.
M. Hodosh, P. Young, and J. Hockenmaier. Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics. JAIR, 47:853899, 2013.
A. Karpathy and L. Fei-Fei. Deep Visual-Semantic Alignments for Generating Image Descriptions. In CVPR, 2015.
G. Kim, S. Moon, and L. Sigal. Joint Photo Stream and Blog Post Summarization and Exploration. In CVPR, 2015.
G. Kim, S. Moon, and L. Sigal. Ranking and Retrieval of Image Sequences from Multiple Paragraph Queries. In CVPR, 2015.
R. Kiros, R. Salakhutdinov, and R. Zemel. Multimodal Neural Language Models. In ICML, 2014.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet Classification with Deep Convolutional Neural Networks. In NIPS, 2012.
G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C. Berg, and T. L. Berg. Baby Talk: Understanding and Generating Image Descriptions. In CVPR, 2011.
P. Kuznetsova, V. Ordonez, T. L. Berg, and Y. Choi. TreeTalk: Composition and Compression of Trees for Image Descriptions. In TACL, 2014.
S. B. A. Lavie. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In ACL, 2005.
Q. Le and T. Mikolov. Distributed Representations of Sentences and Documents. In ICML, 2014.
C. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J. Bethard, and D. McClosky. The Stanford CoreNLP Natural Language Processing Toolkit. In ACL, 2014.
J. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and A. L. Yuille. Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN). In ICLR, 2015.
T. Mikolov. Statistical Language Models based on Neural Networks. In Ph. D. Thesis, Brno University of Technology, 2012.
V. Ordonez, G. Kulkarni, and T. L. Berg. Im2Text: Describing Images Using 1 Million Captioned Photographs. In NIPS, 2011.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. BLEU: A Method for Automatic Evaluation of Machine Translation. In ACL, 2002.
M. Rohrbach, W. Qiu, I. Titov, S. Thater, M. Pinkal, and B. Schiele. Translating Video Content to Natural Language Descriptions. In ICCV, 2013.
M. Schuster and K. K. Paliwal. Bidirectional Recurrent Neural Networks. In IEEE TSP, 1997.
K. Simonyan and A. Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. In ICLR, 2015.
R. Socher, A. Karpathy, Q. V. Le, C. D. Manning, and A. Y. Ng. Grounded Compositional Semantics for Finding and Describing Images with Sentences. In TACL, 2013.
N. Srivastava and R. Salakhutdinov. Multimodal Learning with Deep Boltzmann Machines. In NIPS, 2012.
T. Tieleman and G. E. Hinton. Lecture 6.5  RMSProp. In Coursera, 2012.
R. Vedantam, C. L. Zitnick, and D. Parikh. CIDEr: Consensus-based Image Description Evaluation. In arXiv:1411.5726, 2014.
O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and Tell: A Neural Image Caption Generator. In CVPR, 2015.
P. J. Werbos. Generalization of Backpropagation with Application to a Recurrent Gas Market Model. Neural Networks, 1:339356, 1988.
R. Xu, C. Xiong, W. Chen, and J. J. Corso. Jointly Modeling Deep Video and Compositional Text to Bridge Vision and Language in a Unified Framework. In AAAI, 2015.  9
Ahmed K Elmagarmid, Panagiotis G Ipeirotis, and Vassilios S Verykios. Duplicate record detection: A survey. Knowledge and Data Engineering, IEEE Transactions on, 19(1):116, 2007.
Arvind Arasu, Christopher Re, and Dan Suciu. Large-scale deduplication with constraints using dedupalog. In Data Engineering, 2009. ICDE09. IEEE 25th International Conference on, pages 952963. IEEE, 2009.
Micha Elsner and Warren Schudy. Bounding and comparing methods for correlation clustering beyond ilp. In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, pages 1927. Association for Computational Linguistics, 2009.
Bilal Hussain, Oktie Hassanzadeh, Fei Chiang, Hyun Chul Lee, and Renee J Miller. An evaluation of clustering algorithms in duplicate detection. Technical report, 2013.
Francesco Bonchi, David Garcia-Soriano, and Edo Liberty. Correlation clustering: from theory to practice. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 19721972. ACM, 2014.
Flavio Chierichetti, Nilesh Dalvi, and Ravi Kumar. Correlation clustering in mapreduce. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 641650. ACM, 2014.
Bo Yang, William K Cheung, and Jiming Liu. Community mining from signed social networks. Knowledge and Data Engineering, IEEE Transactions on, 19(10):13331348, 2007.
N Cesa-Bianchi, C Gentile, F Vitale, G Zappella, et al. A correlation clustering approach to link classification in signed networks. In Annual Conference on Learning Theory, pages 341. Microtome, 2012.
Amir Ben-Dor, Ron Shamir, and Zohar Yakhini. Clustering gene expression patterns. Journal of computational biology, 6(3-4):281297, 1999.
Nir Ailon, Moses Charikar, and Alantha Newman. Aggregating inconsistent information: ranking and clustering. Journal of the ACM (JACM), 55(5):23, 2008.
Xinghao Pan, Joseph E Gonzalez, Stefanie Jegelka, Tamara Broderick, and Michael Jordan. Optimistic concurrency control for distributed unsupervised learning. In Advances in Neural Information Processing Systems, pages 14031411, 2013.
Guy E Blelloch, Jeremy T Fineman, and Julian Shun. Greedy sequential maximal independent set and matching are parallel on average. In Proceedings of the twenty-fourth annual ACM symposium on Parallelism in algorithms and architectures, pages 308317. ACM, 2012.
Michael Krivelevich. The phase transition in site percolation on pseudo-random graphs. arXiv preprint arXiv:1404.5731, 2014.
Nikhil Bansal, Avrim Blum, and Shuchi Chawla. Correlation clustering. In 2013 IEEE 54th Annual Symposium on Foundations of Computer Science, pages 238238. IEEE Computer Society, 2002.
Moses Charikar, Venkatesan Guruswami, and Anthony Wirth. Clustering with qualitative information. In Foundations of Computer Science, 2003. Proceedings. 44th Annual IEEE Symposium on, pages 524533. IEEE, 2003.
Erik D Demaine, Dotan Emanuel, Amos Fiat, and Nicole Immorlica. Correlation clustering in general weighted graphs. Theoretical Computer Science, 361(2):172187, 2006.
Shuchi Chawla, Konstantin Makarychev, Tselil Schramm, and Grigory Yaroslavtsev. Near optimal LP rounding algorithm for correlation clustering on complete and complete k-partite graphs. In Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC 15, pages 219228, 2015.
Chaitanya Swamy. Correlation clustering: maximizing agreements via semidefinite programming. In Proceedings of the fifteenth annual ACM-SIAM symposium on Discrete algorithms, pages 526527. Society for Industrial and Applied Mathematics, 2004.
Ioannis Giotis and Venkatesan Guruswami. Correlation clustering with a fixed number of clusters. In Proceedings of the seventeenth annual ACM-SIAM symposium on Discrete algorithm, pages 11671176. ACM, 2006.
Moses Charikar and Anthony Wirth. Maximizing quadratic programs: extending grothendiecks inequality. In Foundations of Computer Science, 2004. Proceedings. 45th Annual IEEE Symposium on, pages 5460. IEEE, 2004.
Noga Alon, Konstantin Makarychev, Yury Makarychev, and Assaf Naor. Quadratic forms on graphs. Inventiones mathematicae, 163(3): 499522, 2006.
Francesco Bonchi, Aristides Gionis, Francesco Gullo, and Antti Ukkonen. Chromatic correlation clustering. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 13211329. ACM, 2012.
Francesco Bonchi, Aristides Gionis, and Antti Ukkonen. Overlapping correlation clustering. In Data Mining (ICDM), 2011 IEEE 11th International Conference on, pages 5160. IEEE, 2011.
Gregory J Puleo and Olgica Milenkovic. Correlation clustering with constrained cluster sizes and extended weights bounds. arXiv preprint arXiv:1411.0547, 2014.
P. Boldi and S. Vigna. The WebGraph framework I: Compression techniques. In WWW, 2004.
P. Boldi, M. Rosa, M. Santini, and S. Vigna. Layered label propagation: A multiresolution coordinate-free ordering for compressing social networks. In WWW. ACM Press, 2011.
P. Boldi, B. Codenotti, M. Santini, and S. Vigna. Ubicrawler: A scalable fully distributed web crawler. Software: Practice & Experience, 34(8):711726, 2004.  9
N. Chavali, H. Agrawal, A. Mahendru, and D. Batra. Object-Proposal Evaluation Protocol is Gameable. arXiv: 1505.05836, 2015.
J. Dai, K. He, and J. Sun. Convolutional feature masking for joint object and stuff segmentation. In CVPR, 2015.
D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable object detection using deep neural networks. In CVPR, 2014.
M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results, 2007.
R. Girshick. Fast R-CNN. arXiv:1504.08083, 2015.
R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.
K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV. 2014.
J. Hosang, R. Benenson, P. Dollar, and B. Schiele. What makes for effective detection proposals? arXiv:1502.05082, 2015.
J. Hosang, R. Benenson, and B. Schiele. How good are detection proposals, really? In BMVC, 2014.
Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv:1408.5093, 2014.
A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.
Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1989.
K. Lenc and A. Vedaldi. R-CNN minus R. arXiv:1506.06981, 2015.
J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.
V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In ICML, 2010.
S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun. Object detection networks on convolutional feature maps. arXiv:1504.06066, 2015.
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. arXiv:1409.0575, 2014.
P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014.
K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.
C. Szegedy, S. Reed, D. Erhan, and D. Anguelov. Scalable, high-quality object detection. arXiv:1412.1441v2, 2015.
C. Szegedy, A. Toshev, and D. Erhan. Deep neural networks for object detection. In NIPS, 2013.
J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders. Selective search for object recognition. IJCV, 2013.
M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional neural networks. In ECCV, 2014.
C. L. Zitnick and P. Dollar. Edge boxes: Locating object proposals from edges. In ECCV, 2014.  9
K. Zeger and A. Gersho. How many points in Euclidean space can have a common nearest neighbor? In International Symposium on Information Theory, page 109, 1994.
L. van der Maaten and G. E. Hinton. Visualizing non-metric similarities in multiple maps. Machine Learning, 87(1):3355, 2012.
J. Laub and K. R. Muller. Feature discovery in non-metric pairwise data. JMLR, 5(Jul):801 818, 2004.
G. E. Hinton and S. T. Roweis. Stochastic neighbor embedding. In NIPS 15, pages 833840. MIT Press, 2003.
J. Cook, I. Sutskever, A. Mnih, and G. E. Hinton. Visualizing similarity data with a mixture of maps. In AISTATS07, pages 6774, 2007.
J. Jost. Riemannian Geometry and Geometric Analysis. Universitext. Springer, 6th edition, 2011.
R. C. Wilson, E. R. Hancock, E. Pekalska, and R. P. W. Duin. Spherical embeddings for non-Euclidean dissimilarities. In CVPR10, pages 19031910, 2010.
D. Lunga and O. Ersoy. Spherical stochastic neighbor embedding of hyperspectral data. Geoscience and Remote Sensing, IEEE Transactions on, 51(2):857871, 2013.
B. ONeill. Semi-Riemannian Geometry With Applications to Relativity. Number 103 in Series: Pure and Applied Mathematics. Academic Press, 1983.
L. Goldfarb. A unified approach to pattern recognition. Pattern Recognition, 17(5):575582, 1984.
E. Pekalska and R. P. W. Duin. The Dissimilarity Representation for Pattern Recognition: Foundations and Applications. World Scientific, 2005.
J. Laub, J. Macke, K. R. Muller, and F. A. Wichmann. Inducing metric violations in human similarity judgements. In NIPS 19, pages 777784. MIT Press, 2007.
L. van der Maaten and G. E. Hinton. Visualizing data using t-SNE. JMLR, 9(Nov):25792605, 2008.
N. D. Lawrence. Spectral dimensionality reduction via maximum entropy. In AISTATS11, JMLR W&CP 15, pages 5159, 2011.
K. Q. Weinberger, F. Sha, and L. K. Saul. Learning a kernel matrix for nonlinear dimensionality reduction. In ICML04, pages 839846, 2004.
J. Leskovec, J. Kleinberg, and C. Faloutsos. Graph evolution: Densification and shrinking diameters. ACM Transactions on Knowledge Discovery from Data, 1(1), 2007.
D. L. Nelson, C. L. McEvoy, and T. A Schreiber. The university of South Florida word association, rhyme, and word fragment norms. 1998. http://www.usf.edu/ FreeAssociation.  9
Arash A. Amini and Martin J. Wainwright. High-dimensional analysis of semidefinite relaxations for sparse principal components. The Annals of Statistics, 37(5):28772921, 2009.
Francis Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic regression. The Journal of Machine Learning Research, 15(1):595627, 2014.
Francis Bach and Eric Moulines. Non-asymptotic analysis of stochastic approximation algorithms for machine learning. In Advances in Neural Information Processing Systems (NIPS), 2011.
Samuel Burer and Renato DC Monteiro. A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization. Mathematical Programming, 95(2):329 357, 2003.
Jian-Feng Cai, Emmanuel J Candes, and Zuowei Shen. A singular value thresholding algorithm for matrix completion. SIAM Journal on Optimization, 20(4):19561982, 2010.
Emmanuel Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via wirtinger flow: Theory and algorithms. arXiv preprint arXiv:1407.1065, 2014.
A. dAspremont, L. El Ghaoui, M. I. Jordan, and G. Lanckriet. A direct formulation for sparse PCA using semidefinite programming. In S. Thrun, L. Saul, and B. Schoelkopf (Eds.), Advances in Neural Information Processing Systems (NIPS), 2004.
Michel X. Goemans and David P. Williamson. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. Journal of the ACM, 42 (6):11151145, November 1995. ISSN 0004-5411.
Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM review, 53(2):217288, 2011.
Matt Hoffman, David M. Blei, Chong Wang, and John Paisley. Stochastic variational inference. The Journal of Machine Learning Research, 14, 2013.
Prateek Jain, Raghu Meka, and Inderjit S Dhillon. Guaranteed rank minimization via singular value projection. In Advances in Neural Information Processing Systems, pages 937945, 2010.
Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternating minimization. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 665674. ACM, 2013.
Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection. Annals of Statistics, pages 13021338, 2000.
Michel Ledoux and Brian Rider. Small deviations for beta ensembles. Electron. J. Probab., 15:no. 41, 13191343, 2010. ISSN 1083-6489. doi: 10.1214/EJP.v15-798. URL http: //ejp.ejpecp.org/article/view/798.
Raghu Meka, Prateek Jain, Constantine Caramanis, and Inderjit S Dhillon. Rank minimization via online learning. In Proceedings of the 25th International Conference on Machine learning, pages 656663. ACM, 2008.
Yurii Nesterov. Introductory lectures on convex optimization, volume 87. Springer Science & Business Media, 2004.
Praneeth Netrapalli, Prateek Jain, and Sujay Sanghavi. Phase retrieval using alternating minimization. In Advances in Neural Information Processing Systems, pages 27962804, 2013.
Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM review, 52(3):471501, 2010.
Ryota Tomioka, Kohei Hayashi, and Hisashi Kashima. Estimation of low-rank tensors via convex optimization. arXiv preprint arXiv:1010.0789, 2010.
Joel A Tropp. An introduction to matrix concentration inequalities. arXiv preprint arXiv:1501.01571, 2015.
Stephen Tu, Ross Boczar, Mahdi Soltanolkotabi, and Benjamin Recht. Low-rank solutions of linear matrix equations via procrustes flow. arXiv preprint arXiv:1507.03566, 2015. 9
Dhruv Batra, Payman Yadollahpour, Abner Guzman-Rivera, and Gregory Shakhnarovich. Diverse m-best solutions in markov random fields. In European Conference on Computer Vision (ECCV), 2012.
Yuxin Chen and Andreas Krause. Near-optimal batch mode active learning and adaptive submodular optimization. In International Conference on Machine Learning (ICML), 2013.
Debadeepta Dey, Tommy Liu, Martial Hebert, and J. Andrew Bagnell. Contextual sequence prediction via submodular function optimization. In Robotics: Science and Systems Conference (RSS), 2012.
Khalid El-Arini and Carlos Guestrin. Beyond keyword search: discovering relevant scientific literature. In ACM Conference on Knowledge Discovery and Data Mining (KDD), 2011.
Khalid El-Arini, Gaurav Veda, Dafna Shahaf, and Carlos Guestrin. Turning down the noise in the blogosphere. In ACM Conference on Knowledge Discovery and Data Mining (KDD), 2009.
Victor Gabillon, Branislav Kveton, Zheng Wen, Brian Eriksson, and S. Muthukrishnan. Adaptive submodular maximization in bandit setting. In Neural Information Processing Systems (NIPS), 2013.
Daniel Golovin and Andreas Krause. Adaptive submodularity: A new approach to active learning and stochastic optimization. In Conference on Learning Theory (COLT), 2010.
Manuel Gomez Rodriguez, Jure Leskovec, and Andreas Krause. Inferring networks of diffusion and influence. In ACM Conference on Knowledge Discovery and Data Mining (KDD), 2010.
Andrew Guillory. Active Learning and Submodular Functions. PhD thesis, University of Washington, 2012.
Andrew Guillory and Jeff Bilmes. Interactive submodular set cover. In International Conference on Machine Learning (ICML), 2010.
Andrew Guillory and Jeff Bilmes. Simultaneous learning and covering with adversarial noise. In International Conference on Machine Learning (ICML), 2011.
Steve Hanneke. The complexity of interactive machine learning. Masters thesis, Carnegie Mellon University, 2007.
Shervin Javdani, Yuxin Chen, Amin Karbasi, Andreas Krause, J. Andrew Bagnell, and Siddhartha Srinivasa. Near optimal bayesian active learning for decision making. In Conference on Artificial Intelligence and Statistics (AISTATS), 2014.
Shervin Javdani, Matthew Klingensmith, J. Andrew Bagnell, Nancy Pollard, and Siddhartha Srinivasa. Efficient touch based localization through submodularity. In IEEE International Conference on Robotics and Automation (ICRA), 2013.
Andreas Krause, Ajit Singh, and Carlos Guestrin. Near-optimal sensor placements in gaussian processes. In International Conference on Machine Learning (ICML), 2005.
Jure Leskovec, Andreas Krause, Carlos Guestrin, Christos Faloutsos, Jeanne VanBriesen, and Natalie Glance. Cost-effective outbreak detection in networks. In ACM Conference on Knowledge Discovery and Data Mining (KDD), 2007.
Hui Lin and Jeff Bilmes. Learning mixtures of submodular shells with application to document summarization. In Conference on Uncertainty in Artificial Intelligence (UAI), 2012.
George Nemhauser, Laurence Wolsey, and Marshall Fisher. An analysis of approximations for maximizing submodular set functions. Mathematical Programming, 14(1):265294, 1978.
Adarsh Prasad, Stefanie Jegelka, and Dhruv Batra. Submodular meets structured: Finding diverse subsets in exponentially-large structured item sets. In Neural Information Processing Systems (NIPS), 2014.
Filip Radlinski, Robert Kleinberg, and Thorsten Joachims. Learning diverse rankings with multi-armed bandits. In International Conference on Machine Learning (ICML), 2008.
Karthik Raman, Pannaga Shivaswamy, and Thorsten Joachims. Online learning to diversify from implicit feedback. In ACM Conference on Knowledge Discovery and Data Mining (KDD), 2012.
Stephane Ross, Jiaji Zhou, Yisong Yue, Debadeepta Dey, and J. Andrew Bagnell. Learning policies for contextual submodular prediction. In International Conference on Machine Learning (ICML), 2013.
Sebastian Tschiatschek, Rishabh Iyer, Haochen Wei, and Jeff Bilmes. Learning mixtures of submodular functions for image collection summarization. In Neural Information Processing Systems (NIPS), 2014.
Laurence A Wolsey. An analysis of the greedy algorithm for the submodular set covering problem. Combinatorica, 2(4):385393, 1982.
Yisong Yue and Carlos Guestrin. Linear submodular bandits and their application to diversified retrieval. In Neural Information Processing Systems (NIPS), 2011.
Yisong Yue and Thorsten Joachims. Predicting diverse subsets using structural svms. In International Conference on Machine Learning (ICML), 2008.  9
Renee Baillargeon. Infants physical world. Current directions in psychological science, 13(3):8994, 2004.
Peter W Battaglia, Jessica B Hamrick, and Joshua B Tenenbaum. Simulation as an engine of physical scene understanding. PNAS, 110(45):1832718332, 2013.
Susan Carey. The origin of concepts. Oxford University Press, 2009.
Erwin Coumans. Bullet physics engine. Open Source Software: http://bulletphysics. org, 2010.
Peter Dayan, Geoffrey E Hinton, Radford M Neal, and Richard S Zemel. The helmholtz machine. Neural computation, 7(5):889904, 1995.
Zhaoyin Jia, Andy Gallagher, Ashutosh Saxena, and Tsuhan Chen. 3d reasoning from blocks to stability. IEEE TPAMI, 2014.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, 1998.
Adam N Sanborn, Vikash K Mansinghka, and Thomas L Griffiths. Reconciling intuitive physics and newtonian mechanics for colliding objects. Psychological review, 120(2):411, 2013.
John Schulman, Alex Lee, Jonathan Ho, and Pieter Abbeel. Tracking deformable objects with point clouds. In Robotics and Automation (ICRA), 2013 IEEE International Conference on, pages 11301137. IEEE, 2013.
Carlo Tomasi and Takeo Kanade. Detection and tracking of point features. International Journal of Computer Vision, 1991.
Tomer Ullman, Andreas Stuhlmuller, Noah Goodman, and Josh Tenenbaum. Learning physics from dynamical scenes. In CogSci, 2014.
Ilker Yildirim, Tejas D Kulkarni, Winrich A Freiwald, and Joshua B Tenenbaum. Efficient analysis-by-synthesis in vision: A computational framework, behavioral tests, and modeling neuronal representations. In Thirty-Seventh Annual Conference of the Cognitive Science Society, 2015.
Bo Zheng, Yibiao Zhao, Joey C Yu, Katsushi Ikeuchi, and Song-Chun Zhu. Detecting potential falling objects by inferring human action and natural disturbance. In ICRA, 2014.  9
Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University Press, NY, NY, USA, 1999.
Moshe Babaioff, Nicole Immorlica, Brendan Lucier, and S. Matthew Weinberg. A simple and approximately optimal mechanism for an additive buyer. SIGecom Exch., 13(2):3135, January 2015.
Maria-Florina Balcan, Avrim Blum, and Yishay Mansour. Single price mechanisms for revenue maximization in unlimited supply combinatorial auctions. Technical report, Carnegie Mellon University, 2007.
Maria-Florina Balcan, Avrim Blum, Jason D Hartline, and Yishay Mansour. Reducing mechanism design to algorithm design via machine learning. Jour. of Comp. and System Sciences, 74(8):12451270, 2008.
Yang Cai and Constantinos Daskalakis. Extreme-value theorems for optimal multidimensional pricing. In Foundations of Computer Science (FOCS), 2011 IEEE 52nd Annual Symposium on, pages 522531, Palm Springs, CA, USA., Oct 2011. IEEE.
Shuchi Chawla, Jason Hartline, and Robert Kleinberg. Algorithmic pricing via virtual valuations. In Proceedings of the 8th ACM Conf. on Electronic Commerce, pages 243251, NY, NY, USA, 2007. ACM.
Shuchi Chawla, Jason D. Hartline, David L. Malec, and Balasubramanian Sivan. Multi-parameter mechanism design and sequential posted pricing. In Proceedings of the Forty-second ACM Symposium on Theory of Computing, pages 311320, NY, NY, USA, 2010. ACM.
Richard Cole and Tim Roughgarden. The sample complexity of revenue maximization. In Proceedings of the 46th Annual ACM Symposium on Theory of Computing, pages 243252, NY, NY, USA, 2014. SIAM.
Nikhil Devanur, Jason Hartline, Anna Karlin, and Thach Nguyen. Prior-independent multi-parameter mechanism design. In Internet and Network Economics, pages 122133. Springer, Singapore, 2011.
Peerapong Dhangwatnotai, Tim Roughgarden, and Qiqi Yan. Revenue maximization with a single sample. In Proceedings of the 11th ACM Conf. on Electronic Commerce, pages 129138, NY, NY, USA, 2010. ACM.
Shaddin Dughmi, Li Han, and Noam Nisan. Sampling and representation complexity of revenue maximization. In Web and Internet Economics, volume 8877 of Lecture Notes in Computer Science, pages 277291. Springer Intl. Publishing, Beijing, China, 2014.
Edith Elkind. Designing and learning optimal finite support auctions. In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages 736745. SIAM, 2007.
Jason Hartline. Mechanism design and approximation. Jason Hartline, Chicago, Illinois, 2015.
Jason D. Hartline and Tim Roughgarden. Simple versus optimal mechanisms. In ACM Conf. on Electronic Commerce, Stanford, CA, USA., 2009. ACM.
Zhiyi Huang, Yishay Mansour, and Tim Roughgarden. Making the most of your samples. abs/1407.2479: 13, 2014. URL http://arxiv.org/abs/1407.2479.
Michael J. Kearns and Umesh V. Vazirani. An Introduction to Computational Learning Theory. MIT Press, Cambridge, MA, 1994.
Andres Munoz Medina and Mehryar Mohri. Learning theory and algorithms for revenue optimization in second price auctions with reserve. In Proceedings of The 31st Intl. Conf. on Machine Learning, pages 262270, 2014.
Roger B Myerson. Optimal auction design. Mathematics of operations research, 6(1):5873, 1981.
David Pollard. Convergence of stochastic processes. David Pollard, New Haven, Connecticut, 1984.
T. Roughgarden and O. Schrijvers. Ironing in the dark. Submitted, 2015.
Tim Roughgarden, Inbal Talgam-Cohen, and Qiqi Yan. Supply-limiting mechanisms. In Proceedings of the 13th ACM Conf. on Electronic Commerce, pages 844861, NY, NY, USA, 2012. ACM.
Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):11341142, 1984.
Vladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability & Its Applications, 16(2):264280, 1971.
Andrew Chi-Chih Yao. An n-to-1 bidder reduction for multi-item auctions and its applications. In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 92109, San Diego, CA, USA., 2015. ACM.  9
A. Renart and C. K. Machens. Variability in neural activity and behavior. Curr Opin Neurobiol, 25:211 20, 2014.
A. Destexhe. Intracellular and computational evidence for a dominant role of internal network activity in cortical computations. Curr Opin Neurobiol, 21(5):717725, 2011.
G. Maimon. Modulation of visual physiology by behavioral state in monkeys, mice, and flies. Curr Opin Neurobiol, 21(4):55964, 2011.
K. D. Harris and A. Thiele. Cortical state and attention. Nat Rev Neurosci, 12(9):509523, 2011.
Ecker et al. State dependence of noise correlations in macaque primary visual cortex. Neuron, 82(1):235 48, 2014.
Ralf M Haefner, Pietro Berkes, and Jozsef Fiser. Perceptual decision-making as probabilistic inference by neural sampling. arXiv preprint arXiv:1409.0257, 2014.
Alexander S Ecker, George H Denfield, Matthias Bethge, and Andreas S Tolias. On the structure of population activity under fluctuations in attentional state. bioRxiv, page 018226, 2015.
A. C. Smith and E. N. Brown. Estimating a state-space model from point process observations. Neural Comput, 15(5):96591, 2003.
U. T. Eden, L. M. Frank, R. Barbieri, V. Solo, and E. N. Brown. Dynamic analysis of neural encoding by point process adaptive filtering. Neural Comput, 16(5):97198, 2004.
B. M. Yu, A. Afshar, G. Santhanam, S. I. Ryu, K. Shenoy, and M. Sahani. Extracting dynamical structure embedded in neural activity. In NIPS 18, pages 15451552. MIT Press, Cambridge, MA, 2006.
J. E. Kulkarni and L. Paninski. Common-input models for multiple neural spike-train data. Network, 18(4):375407, 2007.
W. Truccolo, L. R. Hochberg, and J. P. Donoghue. Collective dynamics in human and monkey sensorimotor cortex: predicting single neuron spikes. Nat Neurosci, 13(1):105111, 2010.
J. H. Macke, L. Buesing, J. P. Cunningham, B. M. Yu, K. V. Shenoy, and M. Sahani. Empirical models of spiking in neural populations. In NIPS, pages 13501358, 2011.
C. van Vreeswijk and H. Sompolinsky. Chaos in neuronal networks with balanced excitatory and inhibitory activity. Science, 274(5293):17246, 1996.
G. J. Tomko and D. R. Crapper. Neuronal variability: non-stationary responses to identical visual stimuli. Brain Res, 79(3):40518, 1974.
C. D. Brody. Correlations without synchrony. Neural Comput, 11(7):153751, 1999.
R. L. T. Goris, J. A. Movshon, and E. P. Simoncelli. Partitioning neuronal variability. Nat Neurosci, 17(6):85865, 2014.
C. D. Gilbert and W. Li. Adult visual cortical plasticity. Neuron, 75(2):25064, 2012.
E. N. Brown, D. P. Nguyen, L. M. Frank, M. A. Wilson, and V. Solo. An analysis of neural receptive field plasticity by point process adaptive filtering. Proc Natl Acad Sci U S A, 98(21):122616, 2001.
Frank et al. Contrasting patterns of receptive field plasticity in the hippocampus and the entorhinal cortex: an adaptive filtering approach. J Neurosci, 22(9):381730, 2002.
N. A. Lesica and G. B. Stanley. Improved tracking of time-varying encoding properties of visual neurons by extended recursive least-squares. IEEE Trans Neural Syst Rehabil Eng, 13(2):194200, 2005.
V. Ventura, C. Cai, and R.E. Kass. Trial-to-Trial Variability and Its Effect on Time-Varying Dependency Between Two Neurons, 2005.
C. S. Quiroga-Lombard, J. Hass, and D. Durstewitz. Method for stationarity-segmentation of spike train data with application to the pearson cross-correlation. J Neurophysiol, 110(2):56272, 2013.
Scholvinck et al. Cortical state determines global variability and correlations in visual cortex. J Neurosci, 35(1):1708, 2015.
Gabriela C., Uri T. E., Sylvia W., Marianna Y., Wendy A. S., and Emery N. B. Analysis of between-trial and within-trial neural spiking dynamics. Journal of Neurophysiology, 99(5):26722693, 2008.
Mangion et al. Online variational inference for state-space models with point-process observations. Neural Comput, 23(8):19671999, 2011.
Neil C Rabinowitz, Robbe LT Goris, Johannes Balle, and Eero P Simoncelli. A model of sensory neural responses in the presence of unknown modulatory inputs. arXiv preprint arXiv:1507.01497, 2015.
C.E. Rasmussen and C.K.I. Williams. Gaussian processes for machine learning. MIT Press Cambridge, MA, USA, 2006.
M. J. Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis, Gatsby Unit, University College London, 2003.
Yu et al. Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity. 102(1):614635, 2009.
A. J. Smola, V. Vishwanathan, and E. Eskin. Laplace propagation. In Sebastian Thrun, Lawrence K. Saul, and Bernhard Scholkopf, editors, NIPS, pages 441448. MIT Press, 2003.
A. Ypma and T. Heskes. Novel approximations for inference in nonlinear dynamical systems using expectation propagation. Neurocomput., 69(1-3):8599, 2005.
K. V. Shenoy B. M. Yu and M. Sahani. Expectation propagation for inference in non-linear dynamical models with poisson observations. In Proc IEEE Nonlinear Statistical Signal Processing Workshop, 2006.
I. Murray and R. P. Adams. Slice sampling covariance hyperparameters of latent Gaussian models. In NIPS 23, pages 17231731. 2010.  9
S. T. Roweis and L. K. Saul. Nonlinear Dimensionality Reduction by Locally Linear Embedding. Science, 290(5500):23232326, 2000.
M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, pages 585591, 2002.
J. B. Tenenbaum, V. Silva, and J. C. Langford. A Global Geometric Framework for Nonlinear Dimensionality Reduction. Science, 290(5500):23192323, 2000.
L.J.P. van der Maaten, E. O. Postma, and H. J. van den Herik. Dimensionality reduction: A comparative review, 2008. http://www.iai.uni-bonn.de/jz/ dimensionality_reduction_a_comparative_review.pdf.
L. Cayton. Algorithms for manifold learning. Univ. of California at San Diego Tech. Rep, pages 117, 2005. http://www.lcayton.com/resexam.pdf.
J. Platt. Fastmap, metricmap, and landmark MDS are all Nystrom algorithms. In Proceedings of 10th International Workshop on Artificial Intelligence and Statistics, pages 261268, 2005.
N. Lawrence. Gaussian process latent variable models for visualisation of high dimensional data. In NIPS, pages 329336, 2003.
M. K. Titsias and N. D. Lawrence. Bayesian Gaussian process latent variable model. In AISTATS, pages 844851, 2010.
S. Roweis, L. Saul, and G. Hinton. Global coordination of local linear models. In NIPS, pages 889896, 2002.
M. Brand. Charting a manifold. In NIPS, pages 961968, 2003.
Y. Zhan and J. Yin. Robust local tangent space alignment. In NIPS, pages 293301. 2009.
J. Verbeek. Learning nonlinear image manifolds by global alignment of local linear models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(8):12361250, 2006.
C. Bishop. Pattern recognition and machine learning. Springer New York, 2006.
M. J. Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis, Gatsby Unit, University College London, 2003.
M. Menne, C. Williams, and R. Vose. The U.S. historical climatology network monthly temperature data, version 2.5. Bulletin of the American Meteorological Society, 90(7):9931007, July 2009.
Mukund Balasubramanian and Eric L. Schwartz. The isomap algorithm and topological stability. Science, 295(5552):77, January 2002.
N. Lawrence. Spectral dimensionality reduction via maximum entropy. In AISTATS, pages 5159, 2011.  9
D.H. Brainard and A Radonjic. Color constancy. In The new visual neurosciences, 2014.
G. Buchsbaum. A spatial processor model for object colour perception. J. Franklin Inst, 1980.
E.H. Land. The retinex theory of color vision. Scientific American, 1971.
A. Gijsenij, T. Gevers, and J. van de Weijer. Generalized gamut mapping using image derivative structures for color constancy. IJCV, 2010.
J. van de Weijer, T. Gevers, and A. Gijsenij. Edge-Based Color Constancy. IEEE Trans. Image Proc., 2007.
A. Chakrabarti, K. Hirakawa, and T. Zickler. Color Constancy with Spatio-spectral Statistics. PAMI, 2012.
H. R. V. Joze and M. S. Drew. Exemplar-based color constancy and multiple illumination. PAMI, 2014.
B. Li, W. Xiong, and D. Xu. A supervised combination strategy for illumination chromaticity estimation. ACM Trans. Appl. Percept., 2010.
R. Lu, A. Gijsenij, T. Gevers, V. Nedovic, and D. Xu. Color constancy using 3D scene geometry. In ICCV, 2009.
S. Bianco, F. Gasparini, and R. Schettini. Consensus-based framework for illuminant chromaticity estimation. J. Electron. Imag., 2008.
S. Bianco, G. Ciocca, C. Cusano, and R. Schettini. Automatic color constancy algorithm selection and combination. Pattern recognition, 2010.
N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. JMLR, 2014.
H. Chong, S. Gortler, and T. Zickler. The von Kries hypothesis and a basis for color constancy. In Proc. ICCV, 2007.
P.V. Gehler, C. Rother, A. Blake, T. Minka, and T. Sharp. Bayesian Color Constancy Revisited. In CVPR, 2008.
L. Shi and B. Funt. Re-processed version of the gehler color constancy dataset of 568 images. Accessed from http://www.cs.sfu.ca/~colour/data/.
D.B. Judd, D.L. MacAdam, G. Wyszecki, H.W. Budde, H.R. Condit, S.T. Henderson, and J.L. Simonds. Spectral distribution of typical daylight as a function of correlated color temperature. JOSA, 1964.
A. Chakrabarti and T. Zickler. Statistics of Real-World Hyperspectral Images. In Proc. CVPR, 2011.
B. Li, W. Xiong, W. Hu, and B. Funt. Evaluating combinational illumination estimation methods on real-world images. IEEE Trans. Imag. Proc., 2014. Data at http://www.escience.cn/people/ BingLi/Data_TIP14.html.
S. Bianco, C. Cusano, and R. Schettini. Color constancy using cnns. arXiv:1504.04548
D. Forsyth. A novel algorithm for color constancy. IJCV, 1990.
W. Xiong and B. Funt. J. Imag. Sci. Technol., 2006.  Estimating illumination chromaticity via support vector regression.
A. Gijsenij, T. Gevers, and J. van de Weijer. Computational color constancy: Survey and experiments. IEEE Trans. Image Proc., 2011. Data at http://www.colorconstancy.com/.  9
D. McFadden. Conditional logit analysis of qualitative choice behavior. In P. Zarembka, editor, Frontiers in Econometrics, pages 105142. Academic Press, 1973.
L. Thurstone. The method of paired comparisons for social values. Journal of Abnormal and Social Psychology, 21(4):384400, 1927.
R. A. Bradley and M. E. Terry. Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons. Biometrika, 39(3/4):324345, 1952.
R. L. Plackett. The Analysis of Permutations. Journal of the Royal Statistical Society, Series C (Applied Statistics), 24(2):193202, 1975.
A. Elo. The Rating Of Chess Players, Past & Present. Arco, 1978.
T. Hastie and R. Tibshirani. Classification by pairwise coupling. The Annals of Statistics, 26(2):451471, 1998.
R. D. Luce. Individual Choice behavior: A Theoretical Analysis. Wiley, 1959.
C. Dwork, R. Kumar, M. Naor, and D. Sivakumar. Rank Aggregation Methods for the Web. In Proceedings of the 10th international conference on World Wide Web (WWW 2001), Hong Kong, China, 2001.
S. Negahban, S. Oh, and D. Shah. Iterative Ranking from Pair-wise Comparisons. In Advances in Neural Information Processing Systems 25 (NIPS 2012), Lake Tahoe, CA, 2012.
H. Azari Soufiani, W. Z. Chen, D. C. Parkes, and L. Xia. Generalized Method-of-Moments for Rank Aggregation. In Advances in Neural Information Processing Systems 26 (NIPS 2013), Lake Tahoe, CA, 2013.
B. Hajek, S. Oh, and J. Xu. Minimax-optimal Inference from Partial Rankings. In Advances in Neural Information Processing Systems 27 (NIPS 2014), Montreal, QC, Canada, 2014.
D. A. Levin, Y. Peres, and E. L. Wilmer. Markov Chains and Mixing Times. American Mathematical Society, 2008.
T. L. Saaty. The Analytic Hierarchy Process: Planning, Priority Setting, Resource Allocation. McGraw-Hill, 1980.
L. Page, S. Brin, R. Motwani, and T. Winograd. The PageRank Citation Ranking: Bringing Order to the Web. Technical report, Stanford University, 1998.
E. Zermelo. Die Berechnung der Turnier-Ergebnisse als ein Maximumproblem der Wahrscheinlichkeitsrechnung. Mathematische Zeitschrift, 29(1):436460, 1928.
L. R. Ford, Jr. Solution of a Ranking Problem from Binary Comparisons. The American Mathematical Monthly, 64(8):2833, 1957.
O. Dykstra, Jr. Rank Analysis of Incomplete Block Designs: A Method of Paired Comparisons Employing Unequal Repetitions on Pairs. Biometrics, 16(2):176188, 1960.
D. R. Hunter. MM algorithms for generalized BradleyTerry models. The Annals of Statistics, 32(1): 384406, 2004.
R. Kumar, A. Tomkins, S. Vassilvitskii, and E. Vee. Inverting a Steady-State. In Proceedings of the 8th International Conference on Web Search and Data Mining (WSDM 2015), pages 359368, 2015.
A. Rajkumar and S. Agarwal. A Statistical Convergence Perspective of Algorithms for Rank Aggregation from Pairwise Data. In Proceedings of the 31st International Conference on Machine Learning (ICML 2014), Beijing, China, 2014.
F. Caron and A. Doucet. Efficient Bayesian Inference for Generalized BradleyTerry models. Journal of Computational and Graphical Statistics, 21(1):174196, 2012.
J. Guiver and E. Snelson. Bayesian inference for PlackettLuce ranking models. In Proceedings of the 26th International Conference on Machine Learning (ICML 2009), Montreal, Canada, 2009.
P. V. Rao and L. L. Kupper. Ties in Paired-Comparison Experiments: A Generalization of the Bradley-Terry Model. Journal of the American Statistical Association, 62(317):194204, 1967.
T. Kamishima and S. Akaho. Efficient Clustering for Orders. In Mining Complex Data, pages 261279. Springer, 2009.  9
H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics, 22(3):400407, Sep. 1951.
T. Zhang. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In Twenty-first International Conference on Machine Learning (ICML 2004), 2004.
L. Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of the 19th Int. Conf. on Computational Statistic (COMPSTAT), pages 177186. Springer, 2010.
M.D. Hoffman, D.M. Blei, C. Wang, and J. Paisley. Stochastic variational inference. Journal of Machine Learning Research, 14(1):13031347, 2013.
J. Hensman, M. Rattray, and N.D. Lawrence. Fast variational inference in the conjugate exponential family. In Advances in Neural Information Processing Systems (NIPS 25), pages 28882896, 2012.
T. Broderick, N. Boyd, A. Wibisono, A.C. Wilson, and M.I. Jordan. Streaming variational Bayes. In Advances in Neural Information Processing Systems (NIPS 26), pages 17271735, 2013.
A.P. George and W.B. Powell. Adaptive stepsizes for recursive estimation with applications in approximate dynamic programming. Machine Learning, 65(1):167198, 2006.
J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:21212159, 2011.
N.N. Schraudolph. Local gain adaptation in stochastic gradient descent. In Ninth International Conference on Artificial Neural Networks (ICANN) 99, volume 2, pages 569574, 1999.
S.-I. Amari, H. Park, and K. Fukumizu. Adaptive method of realizing natural gradient learning for multilayer perceptrons. Neural Computation, 12(6):13991409, 2000.
N.L. Roux and A.W. Fitzgibbon. A fast natural Newton method. In 27th International Conference on Machine Learning (ICML), pages 623630, 2010.
R. Rajesh, W. Chong, D. Blei, and E. Xing. An adaptive learning rate for stochastic variational inference. In 30th International Conference on Machine Learning (ICML), pages 298306, 2013.
P. Hennig. Fast Probabilistic Optimization from Noisy Gradients. In 30th International Conference on Machine Learning (ICML), 2013.
T. Schaul, S. Zhang, and Y. LeCun. No more pesky learning rates. In 30th International Conference on Machine Learning (ICML-13), pages 343351, 2013.
R. Fletcher and C.M. Reeves. Function minimization by conjugate gradients. The Computer Journal, 7(2):149154, 1964.
C.G. Broyden. A new double-rank minimization algorithm. Notices of the AMS, 16:670, 1969.
R. Fletcher. A new approach to variable metric algorithms. The Computer Journal, 13(3):317, 1970.
D. Goldfarb. A family of variable metric updates derived by variational means. Math. Comp., 24(109):23 26, 1970.
D.F. Shanno. Conditioning of quasi-Newton methods for function minimization. Math. Comp., 24(111):647 656, 1970.
J. Nocedal and S.J. Wright. Numerical Optimization. Springer Verlag, 1999.
P. Wolfe. Convergence conditions for ascent methods. SIAM Review, pages 226235, 1969.
L. Armijo. Minimization of functions having Lipschitz continuous first partial derivatives. Pacific Journal of Mathematics, 16(1):13, 1966.
D.R. Jones, M. Schonlau, and W.J. Welch. Efficient global optimization of expensive black-box functions. Journal of Global Optimization, 13(4):455492, 1998.
C.E. Rasmussen and C.K.I. Williams. Gaussian Processes for Machine Learning. MIT, 2006.
G. Wahba. Spline models for observational data. Number 59 in CBMS-NSF Regional Conferences series in applied mathematics. SIAM, 1990.
S. Sarkka. Bayesian filtering and smoothing. Cambridge University Press, 2013.
A. Papoulis. Probability, Random Variables, and Stochastic Processes. McGraw-Hill, New York, 3rd ed. edition, 1991.
R.J. Adler. The Geometry of Random Fields. Wiley, 1981.
Z. Drezner and G.O. Wesolowsky. On the computation of the bivariate normal integral. Journal of Statistical Computation and Simulation, 35(1-2):101107, 1990.  9
Y. Bengio and Y. LeCun. Scaling learning algorithms towards ai. Large-scale kernel machines, 2007.
C. M. Bishop. Pattern recognition and machine learning. springer New York, 2006.
M. Boden and J. Wiles. Context-free and context-sensitive dynamics in recurrent neural networks. Connection Science, 2000. The code is available at https://github.com/facebook/Stack-RNN  8
L. Bottou. Large-scale machine learning with stochastic gradient descent. In COMPSTAT. Springer, 2010.
L. Breiman. Random forests. Machine learning, 45(1):532, 2001.
J. S. Bridle. Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. In Neurocomputing, pages 227236. Springer, 1990.
M. H. Christiansen and N. Chater. Toward a connectionist model of recursion in human linguistic performance. Cognitive Science, 23(2):157205, 1999.
J. Chung, C. Gulcehre, K Cho, and Y. Bengio. Gated feedback recurrent neural networks. arXiv, 2015.
D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, and J. Schmidhuber. High-performance neural networks for visual object classification. arXiv preprint, 2011.
M. W. Crocker. Mechanisms for sentence processing. University of Edinburgh, 1996.
G. E. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing, 20(1):3042, 2012.
S. Das, C. Giles, and G. Sun. Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory. In ACCSS, 1992.
S. Das, C. Giles, and G. Sun. Using prior knowledge in a nnpda to learn context-free languages. NIPS, 1993.
J. L. Elman. Finding structure in time. Cognitive science, 14(2):179211, 1990.
M. Fanty. Context-free parsing in connectionist networks. Parallel natural language processing, 1994.
F. A. Gers and J. Schmidhuber. Lstm recurrent networks learn simple context-free and context-sensitive languages. Transactions on Neural Networks, 12(6):13331340, 2001.
A. Graves, G. Wayne, and I. Danihelka. Neural turing machines. arXiv preprint, 2014.
P. Grunwald. A recurrent network that performs a context-sensitive prediction task. In ACCSS, 1996.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):17351780, 1997.
S. Holldobler, Y. Kalinke, and H. Lehmann. Designing a counter: Another case study of dynamics and activation landscapes in recurrent networks. In Advances in Artificial Intelligence, 1997.
A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. 1998.
T. Mikolov. Statistical language models based on neural networks. PhD thesis, Brno University of Technology, 2012.
T. Mikolov, A. Joulin, S. Chopra, M. Mathieu, and M. A. Ranzato. Learning longer memory in recurrent neural networks. arXiv preprint, 2014.
M. Minsky and S. Papert. Perceptrons. MIT press, 1969.
M. C. Mozer and S. Das. A connectionist symbol manipulator that discovers the structure of context-free languages. NIPS, 1993.
J. B. Pollack. The induction of dynamical recognizers. Machine Learning, 7(2-3):227252, 1991.
B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In NIPS, 2011.
P. Rodriguez, J. Wiles, and J. L. Elman. A recurrent neural network that learns to count. Connection Science, 1999.
D. E Rumelhart, G. Hinton, and R. J. Williams. Learning internal representations by error propagation. Technical report, DTIC Document, 1985.
W. Tabor. Fractal encoding of context-free grammars in connectionist networks. Expert Systems, 2000.
P. Werbos. Generalization of backpropagation with application to a recurrent gas market model. Neural Networks, 1(4):339356, 1988.
J. Weston, S. Chopra, and A. Bordes. Memory networks. In ICLR, 2015.
J. Wiles and J. Elman. Learning to count without a counter: A case study of dynamics and activation landscapes in recurrent networks. In ACCSS, 1995.
R. J. Williams and D. Zipser. Gradient-based learning algorithms for recurrent networks and their computational complexity. Back-propagation: Theory, architectures and applications, pages 433486, 1995.
W. Zaremba and I. Sutskever. Learning to execute. arXiv preprint, 2014.
Z. Zeng, R. M. Goodman, and P. Smyth. Discrete recurrent neural networks for grammatical inference. Transactions on Neural Networks, 5(2):320330, 1994.  9
A. Borji, D. Parks, and L. Itti. Complementary effects of gaze direction and early saliency in guiding fixations during free viewing. Journal of vision, 14(13):3, 2014.
A. Borji, D. N. Sihite, and L. Itti. Salient object detection: A benchmark. In ECCV. 2012.
N. Emery. The eyes have it: the neuroethology, function and evolution of social gaze. Neuroscience & Biobehavioral Reviews, 2000.
M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes (VOC) Challenge. IJCV, 2010.
A. Fathi, J. K. Hodgins, and J. M. Rehg. Social interactions: A first-person perspective. In CVPR, 2012.
A. Fathi, Y. Li, and J. M. Rehg. Learning to recognize daily actions using gaze. In ECCV. 2012.
M. W. Hoffman, D. B. Grimes, A. P. Shon, and R. P. Rao. A probabilistic model of gaze imitation and shared attention. Neural Networks, 2006.
L. Itti and C. Koch. Computational modelling of visual attention. Nature Reviews Neuroscience, 2001.
H. Jasso, J. Triesch, and G. Deak. Using eye direction cues for gaze followinga developmental model. In ICDL, 2006.
Y. Jia. Caffe: An open source convolutional architecture for fast feature embedding. http://caffe.berkeleyvision.org, 2013.
T. Judd, K. Ehinger, F. Durand, and A. Torralba. Learning to predict where humans look. In CVPR, 2009.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick. Microsoft coco: Common objects in context. In ECCV. 2014.
M. J. Marin-Jimenez, A. Zisserman, M. Eichner, and V. Ferrari. Detecting people looking at each other in videos. IJCV, 2014.
H. Park, E. Jain, and Y. Sheikh. Predicting primary gaze behavior using social saliency fields. In ICCV, 2013.
D. Parks, A. Borji, and L. Itti. Augmented saliency model using automatic 3d head pose detection and learned gaze following in natural scenes. Vision Research, 2014.
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 2015.
H. Soo Park and J. Shi. Social saliency prediction. In CVPR, 2015.
J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. SUN database: Large-scale scene recognition from abbey to zoo. In CVPR, 2010.
B. Yao, X. Jiang, A. Khosla, A. L. Lin, L. Guibas, and L. Fei-Fei. Human action recognition by learning bases of action attributes and parts. In ICCV, 2011.
B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Object detectors emerge in deep scene cnns. In ICLR, 2015.
B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva. Learning deep features for scene recognition using places database. In NIPS, 2014.
X. Zhu and D. Ramanan. Face detection, pose estimation, and landmark localization in the wild. In CVPR, 2012.  9
Shipra Agrawal and Navin Goyal. Further optimal regret bounds for thompson sampling. In Proceedings of International Conference on Artificial Intelligence and Statistics (AISTATS), 2012a.
Shipra Agrawal and Navin Goyal. Analysis of thompson sampling for the multi-armed bandit problem. In Proceedings of Conference on Learning Theory (COLT), 2012b.
Jean-Yves Audibert and Sebastien Bubeck. Minimax policies for adversarial and stochastic bandits.
In COLT, pages 217226, 2009.
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. Gambling in a rigged
casino: The adversarial multi-armed bandit problem. In Foundations of Computer Science, 1995.
Proceedings., 36th Annual Symposium on, pages 322331. IEEE, 1995.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
problem. Machine Learning, 47:235256, 2002.
Sebastien Bubeck and Nicolo Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multiarmed Bandit Problems. Foundations and Trends in Machine Learning. Now Publishers Incorporated, 2012. ISBN 9781601986269.
Olivier Cappe, Aurelien Garivier, Odalric-Ambrym Maillard, Remi Munos, and Gilles Stoltz.
KullbackLeibler upper confidence bounds for optimal sequential allocation. The Annals of
Statistics, 41(3):15161541, 2013.
Nicolo Cesa-Bianchi. Prediction, learning, and games. Cambridge University Press, 2006.
Eyal Even-Dar, Michael Kearns, Yishay Mansour, and Jennifer Wortman. Regret to the best vs.
regret to the average. Machine Learning, 72(1-2):2137, 2008.
Marcus Hutter and Jan Poland. Adaptive online prediction by following the perturbed leader. The
Journal of Machine Learning Research, 6:639660, 2005.
Michael Kapralov and Rina Panigrahy. Prediction strategies without loss. In Advances in Neural
Information Processing Systems, pages 828836, 2011.
Wouter M Koolen. The pareto regret frontier. In Advances in Neural Information Processing Systems, pages 863871, 2013.
Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances
in applied mathematics, 6(1):422, 1985.
Tor Lattimore. Optimally confident UCB : Improved regret for finite-armed bandits. Technical
report, 2015. URL http://arxiv.org/abs/1507.07880.
Che-Yu Liu and Lihong Li.
arXiv:1506.03378, 2015.
On the prior sensitivity of thompson sampling.
arXiv preprint
Amir Sani, Gergely Neu, and Alessandro Lazaric. Exploiting easy data in online optimization. In
Advances in Neural Information Processing Systems, pages 810818, 2014.
William Thompson. On the likelihood that one unknown probability exceeds another in view of the
evidence of two samples. Biometrika, 25(3/4):285294, 1933.
L. Addario-Berry, N. Broutin, L. Devroye, G. Lugosi. On combinatorial testing problems. Annals of Statistics 38(5) (2011), 30633092.
N. Alon, M. Krivelevich and B. Sudakov. Finding a large hidden clique in a random graph. Random Structures and Algorithms 13 (1998), 457466.
G. W. Anderson, A. Guionnet and O. Zeitouni. An introduction to random matrices. Cambridge University Press (2010).
E. Arias-Castro, E. J., Candes, H. Helgason and O. Zeitouni. Searching for a trail of evidence in a maze. Annals of Statistics 36 (2008), 17261757.
A. Auffinger, G. Ben Arous, and J. Cerny. Random matrices and complexity of spin glasses. Communications on Pure and Applied Mathematics 66(2) (2013), 165201.
S. Balakrishnan, M. Kolar, A. Rinaldo, A. Singh, and L. Wasserman. Statistical and computational tradeoffs in biclustering. NIPS Workshop on Computational Trade-offs in Statistical Learning (2011).
S. Bhamidi, P.S. Dey, and A.B. Nobel. Energy landscape for large average submatrix detection problems in Gaussian random matrices. arXiv:1211.2284. p
Y. Deshpande and A. Montanari. Finding hidden cliques of size N/e in nearly linear time. Foundations of Computational Mathematics (2014), 160
A. Dembo and O. Zeitouni. Matrix optimization under random external fields. arXiv:1409.4606
U. Feige and R. Krauthgamer. Finding and certifying a large hidden clique in a semi-random graph. Random Struct. Algorithms 162(2) (1999), 195208.
D. Feral and S. Peche. The largest eigenvalue of rank one deformation of large Wigner matrices. Comm. Math. Phys. 272 (2007), 185228.
Z. Furedi and J. Komlos, The eigenvalues of random symmetric matrices. Combinatorica 1 (1981), 233241.
A. Guionnet and M. Maida. A Fourier view on R-transform and related asymptotics of spherical integrals. Journal of Functional Analysis 222 (2005), 435490.
G. R. Grimmett and C. J. H. McDiarmid. On colouring random graphs. Math. proc. Cambridge Philos. Soc. 77 (1975), 313324.
D. Hsu, S. M. Kakade, and T. Zhang. A spectral algorithm for learning hidden Markov models. Journal of Computer and System Sciences 78.5 (2012): 1460-1480.
M. Jerrum. Large cliques elude the Metropolis process. Random Struct. Algorithms 3(4) (1992), 347360.
A. Knowles and J. Yin, The isotropic semicircle law and deformation of Wigner matrices. Communications on Pure and Applied Mathematics 66(11) (2013), 16631749.
M. Kolar, S. Balakrishnan, A. Rinaldo, and A. Singh. Minimax localization of structural information in large noisy matrices. Neural Information Processing Systems (NIPS), (2011), 909917.
M. Talagrand. Free energy of the spherical mean field model. Probability theory and related fields 134(3) (2006), 339382.
Z Ma and Y Wu. Computational barriers in minimax submatrix detection. arXiv:1309.5914.
A. Montanari and E. Richard. A Statistical Model for Tensor PCA. Neural Information Processing Systems (NIPS) (2014), 28972905.
A. Onatski, M. J. Moreira, M. Hallin, et al. Asymptotic power of sphericity tests for high-dimensional data. The Annals of Statistics 41(3) (2013), 12041231.
W. C. Waterhouse. The absolute-value estimate for symmetric multilinear forms. Linear Algebra and its Applications 128 (1990), 97105.  9
S. Brooks, A. Gelman, G. Jones, and X.-L. Meng. Handbook of Markov chain monte carlo. CRC press, 2011.
C. J. Geyer. Markov chain monte carlo maximum likelihood. Computer Science and Statistics: Proc. 23rd Symp. Interface, pages 156163, 1991.
M. Welling and Y.-W. Teh. Bayesian learning via stochastic gradient Langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning, pages 681688, 2011.
S. Ahn, A. Korattikara, and M. Welling. Bayesian posterior sampling via stochastic gradient fisher scoring. In Proceeding of 29th International Conference on Machine Learning (ICML12), 2012.
A. Korattikara, Y. Chen, and M. Welling. Austerity in MCMC land: Cutting the Metropolis-Hastings budget. In Proceeding of 31th International Conference on Machine Learning (ICML14), 2014.
A. Muller. Integral probability metrics and their generating classes of functions. Advances in Applied Probability, 29(2):pp. 429443, 1997.
C. Stein. A bound for the error in the normal approximation to the distribution of a sum of dependent random variables. In Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability, Volume 2: Probability Theory, pages 583602, Berkeley, CA, 1972. University of California Press.
A. D. Barbour. Steins method and Poisson process convergence. J. Appl. Probab., (Special Vol. 25A): 175184, 1988. A celebration of applied probability.
L. HY. Chen, L. Goldstein, and Q.-M. Shao. Normal approximation by Steins method. Springer Science & Business Media, 2010.
S. Chatterjee and Q.-M. Shao. Nonnormal approximation by Steins method of exchangeable pairs with application to the CurieWeiss model. Annals of Applied Probability, 21(2):464483, 2011.
G. Reinert and A. Rollin. Multivariate normal approximation with Steins method of exchangeable pairs under a general linearity condition. Annals of Probability, 37(6):21502173, 2009.
S. Chatterjee and E. Meckes. Multivariate normal approximation using exchange-able pairs. Alea, 4: 257283, 2008.
E. Meckes. On Steins method for multivariate normal approximation. In High dimensional probability V: The Luminy volume, pages 153178. Institute of Mathematical Statistics, 2009.
G. Glaeser. Etude de quelques algebres tayloriennes. J. Analyse Math., 6:1124; erratum, insert to 6 (1958), no. 2, 1958.
P. Shvartsman. The Whitney extension problem and Lipschitz selections of set-valued mappings in jetspaces. Transactions of the American Mathematical Society, 360(10):55295550, 2008.
P. Chew. There is a planar graph almost as good as the complete graph. In Proceedings of the Second Annual Symposium on Computational Geometry, SCG 86, pages 169177, New York, NY, 1986. ACM.
D. Peleg and A. A. Schaffer. Graph spanners. Journal of Graph Theory, 13(1):99116, 1989.
S. Har-Peled and M. Mendel. Fast construction of nets in low-dimensional metrics and their applications. SIAM Journal on Computing, 35(5):11481184, 2006.
Q. W. Bouts, A. P. ten Brink, and K. Buchin. A framework for computing the greedy spanner. In Proceedings of the Thirtieth Annual Symposium on Computational Geometry, SOCG14, pages 11:11 11:19, New York, NY, 2014. ACM.
M. Lubin and I. Dunning. Computing in operations research using Julia. INFORMS Journal on Computing, 27(2):238248, 2015.
Gurobi Optimization. Gurobi optimizer reference manual, 2015. URL http://www.gurobi.com.
S. S. Vallender. Calculation of the Wasserstein distance between probability distributions on the line. Theory of Probability & Its Applications, 18(4):784786, 1974.
C. Dobler. Steins method of exchangeable pairs for the Beta distribution and generalizations. arXiv:1411.4477, 2014.
A. Canty and B. D. Ripley. boot: Bootstrap R (S-Plus) Functions, 2015. R package version 1.3-15.
G. O. Roberts and R. L. Tweedie. Exponential convergence of Langevin distributions and their discrete approximations. Bernoulli, pages 341363, 1996.
R. E. Caflisch. Monte carlo and quasi-monte carlo methods. Acta numerica, 7:149, 1998.
Y. Chen, M. Welling, and A. Smola. Super-samples from kernel herding. In Proceeding of 26th Uncertainty in Artificial Intelligence (UAI10), 2010.
E. del Barrio, E. Gin, and C. Matrn. Central limit theorems for the Wasserstein distance between the empirical and the true distributions. Ann. Probab., 27(2):10091071, 04 1999.
F. Bach, S. Lacoste-Julien, and G. Obozinski. On the equivalence between herding and conditional gradient algorithms. In Proceeding of 29th International Conference on Machine Learning (ICML12), 2012.
A. Zellner and C.-K. Min. Gibbs sampler convergence criteria. Journal of the American Statistical Association, 90(431):921927, 1995.
Y. Fan, S. P. Brooks, and A. Gelman. Output assessment for monte carlo simulations via the score statistic. Journal of Computational and Graphical Statistics, 15(1), 2006.
A. Gretton, K. M Borgwardt, M. Rasch, B. Scholkopf, and A. J. Smola. A kernel method for the twosample-problem. In Advances in Neural Information Processing Systems, pages 513520, 2006.  9
Video enhancer. http://www.infognition.com/videoenhancer/, version 1.9.10. 2014.
S. Baker and T. Kanade. Super-resolution optical flow. Technical report, CMU, 1999.
B. Bascle, A. Blake, and A. Zisserman. Motion deblurring and super-resolution from an image sequence. European Conference on Computer Vision, pages 571582, 1996.
M. Bevilacqua, A. Roumy, C. Guillemot, and M.-L. A. Morel. Low-complexity single-image superresolution based on nonnegative neighbor embedding. British Machine Vision Conference, 2012.
H. Chang, D.-Y. Yeung, and Y. Xiong. Super-resolution through neighbor embedding. IEEE Conference on Computer Vision and Pattern Recognition, page I, 2004.
C. Dong, C. C. Loy, K. He, and X. Tang. Learning a deep convolutional network for image superresolution. European Conference on Computer Vision, pages 184199, 2014.
D. Eigen, D. Krishnan, and R. Fergus. Restoring an image taken through a window covered with dirt or rain. IEEE International Conference on Computer Vision, pages 633640, 2013.
W. T. Freeman, E. C. Pasztor, and O. T. Carmichael. Learning low-level vision. International Journal of Computer Vision, pages 2547, 2000.
D. Glasner, S. Bagon, and M. Irani. Super-resolution from a single image. IEEE International Conference on Computer Vision, pages 349356, 2009.
M. Irani and S. Peleg. Improving resolution by image registration. CVGIP: Graphical Models and Image Processing, pages 231239, 1991.
V. Jain and S. Seung. Natural image denoising with convolutional networks. Advances in Neural Information Processing Systems, pages 769776, 2008.
K. Jia, X. Wang, and X. Tang. Image transformation based on learning dictionaries across image spaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 367380, 2013.
C. Liu and D. Sun. On bayesian adaptive video super resolution. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 346360, 2014.
D. Mitzel, T. Pock, T. Schoenemann, and D. Cremers. Video super resolution using duality based tv-l 1 optical flow. Pattern Recognition, pages 432441, 2009.
V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. International Conference on Machine Learning, pages 807814, 2010.
M. Protter, M. Elad, H. Takeda, and P. Milanfar. Generalizing the nonlocal-means to super-resolution reconstruction. IEEE Transactions on Image Processing, pages 3651, 2009.
R. R. Schultz and R. L. Stevenson. Extraction of high-resolution frames from video sequences. IEEE Transactions on Image Processing, pages 9961011, 1996.
M. Schusterand and K. K. Paliwal. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, pages 26732681, 1997.
O. Shahar, A. Faktor, and M. Irani. Space-time super-resolution from a single video. IEEE Conference on Computer Vision and Pattern Recognition, pages 33533360, 2011.
I. Sutskever and G. E. Hinton. Learning multilevel distributed representations for high-dimensional sequences. In International Conference on Artificial Intelligence and Statistics, pages 548555, 2007.
H. Takeda, P. Milanfar, M. Protter, and M. Elad. Super-resolution without explicit subpixel motion estimation. IEEE Transactions on Image Processing, pages 19581975, 2009.
G. Taylor, G. Hinton, and S. Roweis. Modeling human motion using binary latent variables. Advances in Neural Information Processing Systems, pages 448455, 2006.
R. Timofte, V. De, and L. V. Gool. Anchored neighborhood regression for fast example-based superresolution. IEEE International Conference on Computer Vision, pages 19201927, 2013.
L. Xu, J. S. Ren, C. Liu, and J. Jia. Deep convolutional neural network for image deconvolution. In Advances in Neural Information Processing Systems, pages 17901798, 2014.
J. Yang, J. Wright, T. S. Huang, and Y. Ma. Image super-resolution via sparse representation. IEEE Transactions on Image Processing, pages 28612873, 2010.
R. Zeyde, M. Elad, and M. Protte. On single image scale-up using sparse-representations. Curves and Surfaces, pages 711730, 2012.  9
Thomas P. Minka. Expectation Propagation for approximate Bayesian inference. In UAI 01: Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence, pages 362369, San Francisco, CA, USA, 2001. Morgan Kaufmann Publishers Inc. ISBN 1-55860-800-1. URL http://portal.acm.org/citation.cfm?id=720257. 8
Malte Kuss and Carl E. Rasmussen. Assessing Approximate Inference for Binary Gaussian Process Classification. J. Mach. Learn. Res., 6:16791704, December 2005. ISSN 1532-4435. URL http://portal.acm.org/citation.cfm?id=1194901.
Hannes Nickisch and Carl E. Rasmussen. Approximations for Binary Gaussian Process Classification. Journal of Machine Learning Research, 9:20352078, October 2008. URL http://www.jmlr.org/papers/volume9/nickisch08a/nickisch08a.pdf.
Guillaume Dehaene and Simon Barthelm. Expectation propagation in the large-data limit. Technical report, March 2015. URL http://arxiv.org/abs/1503.08060.
T. Minka. Divergence Measures and Message Passing. Technical report, 2005. URL http://research.microsoft.com/en-us/um/people/minka/papers/ message-passing/minka-divergence.pdf.
M. Seeger. Expectation Propagation for Exponential Families. Technical report, 2005. URL http://people.mmci.uni-saarland.de/~{}mseeger/papers/ epexpfam.pdf.
Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer, 1st ed. 2006. corr. 2nd printing 2011 edition, October 2007. ISBN 0387310738. URL http://www.amazon.com/exec/obidos/redirect? tag=citeulike07-20&path=ASIN/0387310738.
Jack Raymond, Andre Manoel, and Manfred Opper. Expectation propagation, September 2014. URL http://arxiv.org/abs/1409.6179.
Anirban DasGupta. Asymptotic Theory of Statistics and Probability (Springer Texts in Statistics). Springer, 1 edition, March 2008. ISBN 0387759700. URL http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20& path=ASIN/0387759700.
Adrien Saumard and Jon A. Wellner. Log-concavity and strong log-concavity: A review. Statist. Surv., 8:45114, 2014. doi: 10.1214/14-SS107. URL http://dx.doi.org/10. 1214/14-SS107.
Herm J. Brascamp and Elliott H. Lieb. Best constants in youngs inequality, its converse, and its generalization to more than three functions. Advances in Mathematics, 20(2):151173, May 1976. ISSN 00018708. doi: 10.1016/0001-8708(76)90184-5. URL http://dx.doi. org/10.1016/0001-8708(76)90184-5.  9
J. Barnes and P. Hut. A hierarchical O(N log N ) force-calculation algorithm. Nature, 324, 1986.
M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation, 15:13731396, 2003.
Y. Bengio, O. Delalleau, N. Le Roux, J.-F. Paiement, P. Vincent, and M. Ouimet. Learning eigenfunctions links spectral embedding and kernel PCA. Neural Computation, 16:21972219, 2004.
J. Bromley, J. W. Bentz, L. Bottou, I. Guyon, Y. LeCun, C. Moore, E. Sackinger, and R. Shah. Signature verification using a siamese time delay neural network. Int. J. Pattern Recognition and Artificial Intelligence, 5:669688, 1993.
M. Carreira-Perpinan. The elastic embedding algorithm for dimensionality reduction. ICML, 2010.
M. Carreira-Perpinan and Z. Lu. The Laplacian Eigenmaps Latent Variable Model. AISTATS, 2007.
M. Carreira-Perpinan and W. Wang. Distributed optimization of deeply nested systems. arXiv:1212.5921
, Dec. 24 2012.
M. Carreira-Perpinan and W. Wang. Distributed optimization of deeply nested systems. AISTATS, 2014.
A. Globerson and S. Roweis. Metric learning by collapsing classes. NIPS, 2006.
J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov. Neighbourhood components analysis. NIPS, 2005.
L. Greengard and V. Rokhlin. A fast algorithm for particle simulations. J. Comp. Phys., 73, 1987.
A. Griewank and A. Walther. Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation. SIAM Publ., second edition, 2008.
R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduction by learning an invariant mapping. CVPR, 2006.
X. He and P. Niyogi. Locality preserving projections. NIPS, 2004.
G. Hinton and S. T. Roweis. Stochastic neighbor embedding. NIPS, 2003.
D. Lowe and M. E. Tipping. Feed-forward neural networks and topographic mappings for exploratory data analysis. Neural Computing & Applications, 4:8395, 1996.
J. Mao and A. K. Jain. Artificial neural networks for feature extraction and multivariate data projection. IEEE Trans. Neural Networks, 6:296317, 1995.
R. Min, Z. Yuan, L. van der Maaten, A. Bonner, and Z. Zhang. Deep supervised t-distributed embedding. ICML, 2010.
J. Nocedal and S. J. Wright. Numerical Optimization. Springer, second edition, 2006.
J. Peltonen and S. Kaski. Discriminative components of data. IEEE Trans. Neural Networks, 16, 2005.
R. Raziperchikolaei and M. Carreira-Perpinan. Learning hashing with affinity-based loss functions using auxiliary coordinates. arXiv:1501.05352
, Jan. 21 2015.
R. Salakhutdinov and G. Hinton. Learning a nonlinear embedding by preserving class neighbourhood structure. AISTATS, 2007.
J. W. Sammon, Jr. A nonlinear mapping for data structure analysis. IEEE Trans. Computers, 18, 1969.
Y. W. Teh and S. Roweis. Automatic alignment of local representations. NIPS, 2003.
L. J. P. van der Maaten. Learning a parametric embedding by preserving local structure. AISTATS, 2009.
L. J. P. van der Maaten. Barnes-Hut-SNE. Int. Conf. Learning Representations (ICLR), 2013.
L. J. P. van der Maaten and G. E. Hinton. Visualizing data using t-SNE. JMLR, 9:25792605, 2008.
J. Venna, J. Peltonen, K. Nybo, H. Aidos, and S. Kaski. Information retrieval perspective to nonlinear dimensionality reduction for data visualization. JMLR, 11:451490, 2010.
M. Vladymyrov and M. Carreira-Perpinan. Partial-Hessian strategies for fast learning of nonlinear embeddings. ICML, 2012.
M. Vladymyrov and M. Carreira-Perpinan. Entropic affinities: Properties and efficient numerical computation. ICML, 2013.
M. Vladymyrov and M. Carreira-Perpinan. Linear-time training of nonlinear low-dimensional embeddings. AISTATS, 2014.
A. R. Webb. Multidimensional scaling by iterative majorization using radial basis functions. Pattern Recognition, 28:753759, 1995.
J. Weston, F. Ratle, and R. Collobert. Deep learning via semi-supervised embedding. ICML, 2008.
Z. Yang, J. Peltonen, and S. Kaski. Scalable optimization for neighbor embedding for visualization. ICML, 2013.  9
B. Balas, L. Nakano, and R. Rosenholtz. A summary-statistic representation in peripheral vision explains visual crowding. Journal of vision, 9(12):13, 2009.  8
C. F. Cadieu, H. Hong, D. L. K. Yamins, N. Pinto, D. Ardila, E. A. Solomon, N. J. Majaj, and J. J. DiCarlo. Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object Recognition. PLoS Comput Biol, 10(12):e1003963, December 2014.
M. Cimpoi, S. Maji, and A. Vedaldi. Deep convolutional filter banks for texture recognition and segmentation. arXiv:1411.6836
, November 2014. arXiv: 1411.6836.
E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus. Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation. In NIPS, 2014.
A. Efros and T. K. Leung. Texture synthesis by non-parametric sampling. In Computer Vision, 1999. The Proceedings of the Seventh IEEE International Conference on, volume 2, pages 10331038. IEEE, 1999.
A. A. Efros and W. T. Freeman. Image quilting for texture synthesis and transfer. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques, pages 341346. ACM, 2001.
J. Freeman and E. P. Simoncelli. Metamers of the ventral stream. Nature Neuroscience, 14(9):11951201, September 2011.
J. Freeman, C. M. Ziemba, D. J. Heeger, E. P. Simoncelli, and A. J. Movshon. A functional and perceptual signature of the second visual area in primates. Nature Neuroscience, 16(7):974981, July 2013.
K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. arXiv preprint arXiv:1406.4729, 2014.
D. J. Heeger and J. R. Bergen. Pyramid-based Texture Analysis/Synthesis. In Proceedings of the 22Nd Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 95, pages 229238, New York, NY, USA, 1995. ACM.
M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up Convolutional Neural Networks with Low Rank Expansions. In BMVC 2014, 2014.
Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the ACM International Conference on Multimedia, pages 675678. ACM, 2014.
B. Julesz. Visual Pattern Discrimination. IRE Transactions on Information Theory, 8(2), February 1962.
S. Khaligh-Razavi and N. Kriegeskorte. Deep Supervised, but Not Unsupervised, Models May Explain IT Cortical Representation. PLoS Comput Biol, 10(11):e1003915, November 2014.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 27, pages 10971105, 2012.
V. Kwatra, A. Schodl, I. Essa, G. Turk, and A. Bobick. Graphcut textures: image and video synthesis using graph cuts. In ACM Transactions on Graphics (ToG), volume 22, pages 277286. ACM, 2003.
V. Lebedev, Y. Ganin, M. Rakhuba, I. Oseledets, and V. Lempitsky. Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition. arXiv preprint arXiv:1412.6553, 2014.
Y. A. LeCun, L. Bottou, G. B. Orr, and K. R. Muller. Efficient backprop. In Neural networks: Tricks of the trade, pages 948. Springer, 2012.
A. J. Movshon and E. P. Simoncelli. Representation of naturalistic image structure in the primate visual cortex. Cold Spring Harbor Symposia on Quantitative Biology: Cognition, 2015.
G. Okazawa, S. Tajima, and H. Komatsu. Image statistics underlying natural texture selectivity of neurons in macaque V4. PNAS, 112(4):E351E360, January 2015.
J. Portilla and E. P. Simoncelli. A Parametric Texture Model Based on Joint Statistics of Complex Wavelet Coefficients. International Journal of Computer Vision, 40(1):4970, October 2000.
R. Rosenholtz, J. Huang, A. Raj, B. J. Balas, and L. Ilie. A summary statistic representation in peripheral vision explains visual search. Journal of vision, 12(4):14, 2012.
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. arXiv:1409.0575
, September 2014. arXiv: 1409.0575.
E. P. Simoncelli and W. T. Freeman. The steerable pyramid: A flexible architecture for multi-scale derivative computation. In Image Processing, International Conference on, volume 3, pages 34443444. IEEE Computer Society, 1995.
K. Simonyan and A. Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv:1409.1556
, September 2014. arXiv: 1409.1556.
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going Deeper with Convolutions. arXiv:1409.4842
, September 2014. arXiv: 1409.4842.
L. Wei, S. Lefebvre, V. Kwatra, and G. Turk. State of the art in example-based texture synthesis. In Eurographics 2009, State of the Art Report, EG-STAR, pages 93117. Eurographics Association, 2009.
L. Wei and M. Levoy. Fast texture synthesis using tree-structured vector quantization. In Proceedings of the 27th annual conference on Computer graphics and interactive techniques, pages 479488. ACM Press/Addison-Wesley Publishing Co., 2000.
D. L. K. Yamins, H. Hong, C. F. Cadieu, E. A. Solomon, D. Seibert, and J. J. DiCarlo. Performanceoptimized hierarchical models predict neural responses in higher visual cortex. PNAS, page 201403112, May 2014.
C. Zhu, R. H. Byrd, P. Lu, and J. Nocedal. Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization. ACM Transactions on Mathematical Software (TOMS), 23(4):550560, 1997.  9
Bela Bollobas. Modern Graph Theory, volume 184. Springer, 1998.
Stephen P. Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah. Randomized gossip algorithms. IEEE Transactions on Information Theory, 52(6):25082530, 2006.
Fan R. K. Chung. Spectral Graph Theory, volume 92. American Mathematical Society, 1997.
Stephan Clemencon. On U-processes and clustering performance. In Advances in Neural Information Processing Systems 24, pages 3745, 2011.
Alexandros G. Dimakis, Soummya Kar, Jose M. F. Moura, Michael G. Rabbat, and Anna Scaglione. Gossip Algorithms for Distributed Signal Processing. Proceedings of the IEEE, 98(11):18471864, 2010.
Alexandros G. Dimakis, Anand D. Sarwate, and Martin J. Wainwright. Geographic Gossip: Efficient Averaging for Sensor Networks. IEEE Transactions on Signal Processing, 56(3):1205 1216, 2008.
John C. Duchi, Alekh Agarwal, and Martin J. Wainwright. Dual Averaging for Distributed Optimization: Convergence Analysis and Network Scaling. IEEE Transactions on Automatic Control, 57(3):592606, 2012.
James A. Hanley and Barbara J. McNeil. The meaning and use of the area under a receiver operating characteristic (ROC) curve. Radiology, 143(1):2936, 1982.
Richard Karp, Christian Schindelhauer, Scott Shenker, and Berthold Vocking. Randomized rumor spreading. In Symposium on Foundations of Computer Science, pages 565574. IEEE, 2000.
David Kempe, Alin Dobra, and Johannes Gehrke. Gossip-Based Computation of Aggregate Information. In Symposium on Foundations of Computer Science, pages 482491. IEEE, 2003.
Wojtek Kowalczyk and Nikos A. Vlassis. Newscast EM. In Advances in Neural Information Processing Systems, pages 713720, 2004.
Alan J. Lee. U-Statistics: Theory and Practice. Marcel Dekker, New York, 1990.
Wenjun Li, Huaiyu Dai, and Yanbing Zhang. Location-Aided Fast Distributed Consensus in Wireless Networks. IEEE Transactions on Information Theory, 56(12):62086227, 2010.
Henry B. Mann and Donald R. Whitney. On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other. Annals of Mathematical Statistics, 18(1):5060, 1947.
Damon Mosk-Aoyama and Devavrat Shah. Fast distributed algorithms for computing separable functions. IEEE Transactions on Information Theory, 54(7):29973007, 2008.
Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimization. IEEE Transactions on Automatic Control, 54(1):4861, 2009.
Kristiaan Pelckmans and Johan Suykens. Gossip Algorithms for Computing U-Statistics. In IFAC Workshop on Estimation and Control of Networked Systems, pages 4853, 2009.
Devavrat Shah. Gossip Algorithms. Foundations and Trends in Networking, 3(1):1125, 2009.
John N. Tsitsiklis. Problems in decentralized decision making and computation. PhD thesis, Massachusetts Institute of Technology, 1984.
Duncan J Watts and Steven H Strogatz. Collective dynamics of small-worldnetworks. Nature, 393(6684):440442, 1998.  9
Agostino Nobile. Bayesian Analysis of Finite Mixture Distributions. PhD thesis, Carnegie Mellon University, 1994.
Jeffrey W. Miller and Matthew T. Harrison. A simple example of Dirichlet process mixture inconsistency for the number of components. In Advances in Neural Information Processing Systems 26, 2013.
Yee Whye Teh. Dirichlet processes. In Encyclopedia of Machine Learning. Springer, New York, 2010.
Thomas L. Griffiths and Zoubin Ghahramani. Infinite latent feature models and the Indian buffet process. In Advances in Neural Information Processing Systems 22, 2005.
Tamara Broderick, Nicholas Boyd, Andre Wibisono, Ashia C. Wilson, and Michael I. Jordan. Streaming variational Bayes. In Advances in Neural Information Procesing Systems 26, 2013.
Trevor Campbell and Jonathan P. How. Approximate decentralized Bayesian inference. In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence, 2014.
Dahua Lin. Online learning of nonparametric mixture models via sequential variational approximation. In Advances in Neural Information Processing Systems 26, 2013.
Xiaole Zhang, David J. Nott, Christopher Yau, and Ajay Jasra. A sequential algorithm for fast fitting of Dirichlet process mixture models. Journal of Computational and Graphical Statistics, 23(4):11431162, 2014.
Matt Hoffman, David Blei, Chong Wang, and John Paisley. Stochastic variational inference. Journal of Machine Learning Research, 14:13031347, 2013.
Chong Wang, John Paisley, and David M. Blei. Online variational inference for the hierarchical Dirichlet process. In Proceedings of the 11th International Conference on Artificial Intelligence and Statistics, 2011.
Michael Bryant and Erik Sudderth. Truly nonparametric online variational inference for hierarchical Dirichlet processes. In Advances in Neural Information Proecssing Systems 23, 2009.
Chong Wang and David Blei. Truncation-free stochastic variational inference for Bayesian nonparametric models. In Advances in Neural Information Processing Systems 25, 2012.
Michael Hughes and Erik Sudderth. Memoized online variational inference for Dirichlet process mixture models. In Advances in Neural Information Processing Systems 26, 2013.
Jason Chang and John Fisher III. Parallel sampling of DP mixture models using sub-clusters splits. In Advances in Neural Information Procesing Systems 26, 2013.
Willie Neiswanger, Chong Wang, and Eric P. Xing. Asymptotically exact, embarassingly parallel MCMC. In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence, 2014.
Carlos M. Carvalho, Hedibert F. Lopes, Nicholas G. Polson, and Matt A. Taddy. Particle learning for general mixtures. Bayesian Analysis, 5(4):709740, 2010.
Matthew Stephens. Dealing with label switching in mixture models. Journal of the Royal Statistical Society: Series B, 62(4):795809, 2000.
Ajay Jasra, Chris Holmes, and David Stephens. Markov chain Monte Carlo methods and the label switching problem in Bayesian mixture modeling. Statistical Science, 20(1):5067, 2005.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):15661581, 2006.
Finale Doshi-Velez and Zoubin Ghahramani. Accelerated sampling for the Indian buffet process. In Proceedings of the International Conference on Machine Learning, 2009.
Avinava Dubey, Sinead Williamson, and Eric Xing. Parallel Markov chain Monte Carlo for Pitman-Yor mixture models. In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence, 2014.
Jack Edmonds and Richard Karp. Theoretical improvements in algorithmic efficiency for network flow problems. Journal of the Association for Computing Machinery, 19:248264, 1972.
Jim Pitman. Exchangeable and partially exchangeable random partitions. Probability Theory and Related Fields, 102(2):145158, 1995.
David M. Blei and Michael I. Jordan. Variational inference for Dirichlet process mixtures. Bayesian Analysis, 1(1):121144, 2006.
Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. MNIST database of handwritten digits. Online: yann.lecun.com/exdb/mnist.
Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. SUN 397 image database. Online: vision.cs.princeton.edu/projects/2010/SUN.  9
The MOSEK Optimization Software. http://mosek.com/. A. Ahumada Jr. Perceptual classification images from vernier acuity masked by noise. 1996. Y. Aytar and A. Zisserman. Tabula rasa: Model transfer for object category detection. In ICCV, 2011. B. L. Beard and A. J. Ahumada Jr. A technique to extract relevant image features for visual tasks. In SPIE, 1998.
C. Blais, R. E. Jack, C. Scheepers, D. Fiset, and R. Caldara. Culture shapes how we look at faces. PLoS One, 2008.  8
S. Branson, C. Wah, F. Schroff, B. Babenko, P. Welinder, P. Perona, and S. Belongie. Visual recognition with humans in the loop. 2010.
H. F. Chua, J. E. Boland, and R. E. Nisbett. Cultural variation in eye movements during scene perception. Proceedings of the National Academy of Sciences of the United States of America, 2005.
N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.
M. P. Eckstein and A. J. Ahumada. Classification images: A tool to analyze visual strategies. Journal of Vision, 2002.
W. D. Ellis. A source book of Gestalt psychology. Psychology Press, 1999.
A. Epshteyn and G. DeJong. Rotational prior knowledge for svms. In ECML. 2005.
M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes challenge. IJCV, 2010.
L. Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object categories. PAMI, 2006.
M. Ferecatu and D. Geman. A statistical framework for image category search from a mental picture. PAMI, 2009.
F. Gosselin and P. G. Schyns. Superstitious perceptions reveal properties of internal representations. Psychological Science, 2003.
M. R. Greene, A. P. Botros, D. M. Beck, and L. Fei-Fei. Visual noise from natural scene statistics reveals human scene category representations. arXiv, 2014.
A. A. Jr and J. Lovell. Stimulus features in signal detection. The Journal of the Acoustical Society of America, 1971.
A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.
B. Kulis, K. Saenko, and T. Darrell. What you saw is not what you get: Domain adaptation using asymmetric kernel transforms. In CVPR, pages 17851792, 2011.
S. Li. Concise formulas for the area and volume of a hyperspherical cap. Asian Journal of Mathematics and Statistics, 2011.
A. Mahendran and A. Vedaldi. Understanding deep image representations by inverting them. CVPR, 2015.
M. C. Mangini and I. Biederman. Making the ineffable explicit: Estimating the information employed for face classifications. Cognitive Science, 2004.
E. Mezuman and Y. Weiss. Learning about canonical views from internet image collections. In NIPS, 2012.
R. F. Murray. Classification images: A review. Journal of Vision, 2011.
S. Palmer, E. Rosch, and P. Chase. Canonical perspective and the perception of objects. Attention and performance IX, 1981.
D. Parikh and C. Zitnick. Human-debugging of machines. In NIPS WCSSWC, 2011.
J. Ponce, T. L. Berg, M. Everingham, D. A. Forsyth, M. Hebert, S. Lazebnik, M. Marszalek, C. Schmid, B. C. Russell, A. Torralba, et al. Dataset issues in object recognition. In Toward category-level object recognition. 2006.
R. Salakhutdinov, A. Torralba, and J. Tenenbaum. Learning to share visual appearance for multiclass object detection. In CVPR, 2011.
A. B. Sekuler, C. M. Gaspar, J. M. Gold, and P. J. Bennett. Inversion leads to quantitative, not qualitative, changes in face processing. Current Biology, 2004.
A. Sorokin and D. Forsyth. Utility data annotation with amazon mechanical turk. In CVPR Workshops, 2008.
A. Torralba and A. Efros. Unbiased look at dataset bias. In CVPR.
S. Vijayanarasimhan and K. Grauman. Large-scale live active learning: Training object detectors with crawled data and crowds. In CVPR, 2011.
L. Von Ahn, R. Liu, and M. Blum. Peekaboom: a game for locating objects in images. In SIGCHI Human Factors, 2006.
C. Vondrick, A. Khosla, T. Malisiewicz, and A. Torralba. HOGgles: Visualizing Object Detection Features. ICCV, 2013.
P. Weinzaepfel, H. Jegou, and P. Perez. Reconstructing an image from its local descriptors. In CVPR, 2011.
J. Yang, R. Yan, and A. G. Hauptmann. Adapting svm classifiers to data with shifted distributions. In ICDM Workshops, 2007.  9
D. Belanger, A. Passos, S. Riedel, and A. McCallum. Message passing for soft constraint dual decomposition. In UAI, 2014.  8
E. Borenstein, E. Sharon, and S. Ullman. Combining top-down and bottom-up segmentation. In CVPR, 2004.
J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. Efficient projections onto the l 1-ball for learning in high dimensions. In ICML, pages 272279, 2008.
M. Frank and P. Wolfe. An algorithm for quadratic programming, volume 3, pages 95110. 1956.
D. Garber and E. Hazan. A linearly convergent conditional gradient algorithm with applications to online and stochastic optimization. arXiv preprint arXiv:1301.4666, 2013.
A. Globerson and T. Jaakkola. Fixing max-product: Convergent message passing algorithms for MAP LP-relaxations. In NIPS. MIT Press, 2008.
T. Hazan and A. Shashua. Norm-product belief propagation: Primal-dual message-passing for approximate inference. IEEE Transactions on Information Theory, 56(12):62946316, 2010.
J. Johnson. Convex Relaxation Methods for Graphical Models: Lagrangian and Maximum Entropy Approaches. PhD thesis, EECS, MIT, 2008.
J. H. Kappes, B. Savchynskyy, and C. Schnorr. A bundle approach to efficient map-inference by lagrangian relaxation. In CVPR, 2012.
V. Kolmogorov. Convergent tree-reweighted message passing for energy minimization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(10):15681583, 2006.
N. Komodakis, N. Paragios, and G. Tziritas. MRF energy minimization and beyond via dual decomposition. IEEE PAMI, 2010.
M. P. Kumar, V. Kolmogorov, and P. H. S.Torr. An analysis of convex relaxations for map estimation of discrete mrfs. JMLR, 10:71106, 2009.
S. Lacoste-Julien, M. Jaggi, M. Schmidt, and P. Pletscher. Block-coordinate Frank-Wolfe optimization for structural SVMs. In ICML, pages 5361, 2013.
Y. Li and R. Zemel. High order regularization for semi-supervised learning of structured output problems. In ICML, pages 13681376, 2014.
A. L. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar, N. A. Smith, and E. P. Xing. An augmented lagrangian approach to constrained map inference. In ICML, pages 169176, 2011.
O. Meshi and A. Globerson. An alternating direction method for dual map lp relaxation. In ECML, 2011.
O. Meshi, T. Jaakkola, and A. Globerson. Convergence rate analysis of map coordinate minimization algorithms. In NIPS, pages 30233031, 2012.
O. Meshi, N. Srebro, and T. Hazan. Efficient training of structured svms via soft constraints. In AISTATS, 2015.
A. Nemirovski and D. Yudin. Problem complexity and method efficiency in optimization. Wiley, 1983.
Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course, volume 87. Kluwer Academic Publishers, 2004.
Y. Nesterov. Smooth minimization of non-smooth functions. Math. Prog., 103(1):127152, 2005.
D. Prusa and T. Werner. Universality of the local marginal polytope. In CVPR, pages 17381743. IEEE, 2013.
P. Ravikumar, A. Agarwal, and M. J. Wainwright. Message-passing for graph-structured linear programs: Proximal methods and rounding schemes. JMLR, 11:10431080, 2010.
B. Savchynskyy, S. Schmidt, J. Kappes, and C. Schnorr. A study of Nesterovs scheme for lagrangian decomposition and map labeling. CVPR, 2011.
A. G. Schwing, T. Hazan, M. Pollefeys, and R. Urtasun. Globally Convergent Dual MAP LP Relaxation Solvers using Fenchel-Young Margins. In Proc. NIPS, 2012.
A. G. Schwing, T. Hazan, M. Pollefeys, and R. Urtasun. Globally Convergent Parallel MAP LP Relaxation Solver using the Frank-Wolfe Algorithm. In Proc. ICML, 2014.
S. Shalev-Shwartz and T. Zhang. Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization. In ICML, 2014.
D. Sontag, A. Globerson, and T. Jaakkola. Introduction to dual decomposition for inference. In Optimization for Machine Learning, pages 219254. MIT Press, 2011.
D. Tarlow, I. Givoni, and R. Zemel. Hop-map: Efficient message passing with high order potentials. In AISTATS, volume 9, pages 812819. JMLR: W&CP, 2010.
M. Wainwright, T. Jaakkola, and A. Willsky. MAP estimation via agreement on trees: message-passing and linear programming. IEEE Transactions on Information Theory, 51(11):36973717, 2005.
T. Werner. A linear programming approach to max-sum problem: A review. IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(7):11651179, 2007.
T. Werner. Revisiting the linear programming relaxation approach to gibbs energy minimization and weighted constraint satisfaction. IEEE PAMI, 32(8):14741488, 2010.
C. Yanover, T. Meltzer, and Y. Weiss. Linear programming relaxations and belief propagation  an empirical study. Journal of Machine Learning Research, 7:18871907, 2006.  9
Y. Yue, J. Broder, R. Kleinberg, and T. Joachims. The K-armed dueling bandits problem. Journal of Computer and System Sciences, 78(5), 2012.
T. Joachims. Optimizing search engines using clickthrough data. In KDD, 2002.
Y. Yue and T. Joachims. Beat the mean bandit. In ICML, 2011.
K. Hofmann, S. Whiteson, and M. de Rijke. Balancing exploration and exploitation in listwise and pairwise online learning to rank for information retrieval. Information Retrieval, 16, 2013.
J. Furnkranz and E. Hullermeier, editors. Preference Learning. Springer-Verlag, 2010.
A. Schuth, F. Sietsma, S. Whiteson, D. Lefortier, and M. de Rijke. Multileaved comparisons for fast online evaluation. In CIKM, 2014.
L. Li, J. Kim, and I. Zitouni. Toward predicting the outcome of an A/B experiment for search relevance. In WSDM, 2015.
M. Zoghi, S. Whiteson, M. de Rijke, and R. Munos. Relative confidence sampling for efficient on-line ranker evaluation. In WSDM, 2014.
M. Schulze. A new monotonic, clone-independent, reversal symmetric, and Condorcetconsistent single-winner election method. Social Choice and Welfare, 36(2):267303, 2011.
T. Urvoy, F. Clerot, R. Feraud, and S. Naamane. Generic exploration and k-armed voting bandits. In ICML, 2013.
R. Busa-Fekete, B. Szorenyi, P. Weng, W. Cheng, and E. Hullermeier. Top-k selection based on adaptive sampling of noisy preferences. In ICML, 2013.
R. Busa-Fekete, B. Szorenyi, and E. Hullermeier. PAC rank elicitation through adaptive sampling of stochastic pairwise preferences. In AAAI, 2014.
M. Zoghi, S. Whiteson, R. Munos, and M. de Rijke. Relative upper confidence bound for the K-armed dueling bandits problem. In ICML, 2014.
R. Kohavi, A. Deng, B. Frasca, T. Walker, Y. Xu, and N. Pohlmann. Online controlled experiments at large scale. In KDD, 2013.
W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, pages 285294, 1933.
N. Ailon, Z. Karnin, and T. Joachims. Reducing dueling bandits to cardinal bandits. In ICML, 2014.
M. Zoghi, S. Whiteson, and M. de Rijke. MergeRUCB: A method for large-scale online ranker evaluation. In WSDM, 2015.
S. Negahban, S. Oh, and D. Shah. Iterative ranking from pair-wise comparisons. In NIPS, 2012.
M. Dudk, K. Hofmann, R. E. Schapire, A. Slivkins, and M. Zoghi. Contextual dueling bandits. In COLT, 2015.
A. Piccolboni and C. Schindelhauer. Discrete prediction games with arbitrary feedback and loss. In COLT, 2001.
G. Bartok, N. Zolghadr, and C. Szepesvari. An adaptive algorithm for finite stochastic partial monitoring. In ICML, 2012.
O. Cappe, A. Garivier, O. Maillard, R. Munos, G. Stoltz, et al. Kullbackleibler upper confidence bounds for optimal sequential allocation. The Annals of Statistics, 41(3), 2013.
C. Manning, P. Raghavan, and H. Schutze. Introduction to Information Retrieval. Cambridge University Press, 2008.
R. Kleinberg, A. Slivkins, and E. Upfa. Multi-armed bandits in metric space. In STOC, 2008.
S. Bubeck, R. Munos, G. Stoltz, and C. Szepesvari. X-armed bandits. JMLR, 12, 2011.
N. Srinivas, A. Krause, S. M. Kakade, and M. Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. In ICML, 2010.
R. Munos. Optimistic optimization of a deterministic function without the knowledge of its smoothness. In NIPS, 2011.
A. D. Bull. Convergence rates of efficient global optimization algorithms. JMLR, 12, 2011.
N. de Freitas, A. Smola, and M. Zoghi. Exponential regret bounds for Gaussian process bandits with deterministic observations. In ICML, 2012.
M. Valko, A. Carpentier, and R. Munos. Stochastic simultaneous optimistic optimization. In ICML, 2013.
Y. Yue and T. Joachims. Interactively optimizing information retrieval systems as a dueling bandits problem. In ICML, 2009.
A. Altman and M. Tennenholtz. Axiomatic foundations for ranking systems. JAIR, 2008. 9
E. Bas, N. Ghadarghadar, and D. Erdogmus. Automated extraction of blood vessel networks from 3d microscopy image stacks via multi-scale principal curve tracing. In Biomedical Imaging: From Nano to Macro, 2011 IEEE International Symposium on, pages 13581361. IEEE, 2011.
E. Bas, D. Erdogmus, R. Draft, and J. W. Lichtman. Local tracing of curvilinear structures in volumetric color images: application to the brainbow analysis. Journal of Visual Communication and Image Representation, 23(8):12601271, 2012.
B. Cadre. Kernel estimation of density level sets. Journal of multivariate analysis, 2006.
Y.-C. Chen, C. R. Genovese, R. J. Tibshirani, and L. Wasserman. Nonparametric modal regression. arXiv preprint arXiv:1412.1716, 2014.
Y.-C. Chen, C. R. Genovese, and L. Wasserman. Generalized mode and ridge estimation. arXiv: 1406.1803, June 2014.
Y.-C. Chen, C. R. Genovese, and L. Wasserman. Asymptotic theory for density ridges. arXiv preprint arXiv:1406.5663, 2014.
Y.-C. Chen, S. Ho, P. E. Freeman, C. R. Genovese, and L. Wasserman. Cosmic web reconstruction through density ridges: Method and algorithm. arXiv preprint arXiv:1501.05303, 2015.
Y. Cheng. Mean shift, mode seeking, and clustering. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 17(8):790799, 1995.
A. Cuevas, W. Gonzalez-Manteiga, and A. Rodriguez-Casal. Plug-in estimation of general level sets. Aust. N. Z. J. Stat., 2006.
D. Eberly. Ridges in Image and Data Analysis. Springer, 1996.
J. Einbeck. Bandwidth selection for mean-shift based unsupervised learning techniques: a unified approach via self-coverage. Journal of pattern recognition research., 6(2):175192, 2011.
U. Einmahl and D. M. Mason. Uniform in bandwidth consistency for kernel-type function estimators. The Annals of Statistics, 2005.
L. C. Evans and R. F. Gariepy. Measure theory and fine properties of functions, volume 5. CRC press, 1991.
K. Fukunaga and L. Hostetler. The estimation of the gradient of a density function, with applications in pattern recognition. Information Theory, IEEE Transactions on, 21(1):3240, 1975.
C. R. Genovese, M. Perone-Pacifico, I. Verdinelli, and L. Wasserman. Nonparametric ridge estimation. The Annals of Statistics, 42(4):15111545, 2014.
E. Gine and A. Guillou. Rates of strong uniform consistency for multivariate kernel density estimators. In Annales de lInstitut Henri Poincare (B) Probability and Statistics, 2002.
T. Hastie. Principal curves and surfaces. Technical report, DTIC Document, 1984.
T. Hastie and W. Stuetzle. Principal curves. Journal of the American Statistical Association, 84(406): 502516, 1989.
M. C. Jones, J. S. Marron, and S. J. Sheather. A brief survey of bandwidth selection for density estimation. Journal of the American Statistical Association, 91(433):401407, 1996.
D. M. Mason, W. Polonik, et al. Asymptotic normality of plug-in level set estimates. The Annals of Applied Probability, 19(3):11081142, 2009.
Z. Miao, B. Wang, W. Shi, and H. Wu. A method for accurate road centerline extraction from a classified image. 2014.
U. Ozertem and D. Erdogmus. Locally defined principal curves and surfaces. Journal of Machine Learning Research, 2011.
A. Rinaldo and L. Wasserman. Generalized density clustering. The Annals of Statistics, 2010.
D. W. Scott. Multivariate density estimation: theory, practice, and visualization, volume 383. John Wiley & Sons, 2009.
B. Silverman and G. Young. The bootstrap: To smooth or not to smooth? Biometrika, 74(3):469479, 1987.
B. W. Silverman. Density Estimation for Statistics and Data Analysis. Chapman and Hall, 1986.
R. Tibshirani. Principal curves revisited. Statistics and Computing, 2(4):183190, 1992.
L. Wasserman. All of Nonparametric Statistics. Springer-Verlag New York, Inc., 2006.   9
A. Bordes, L. Bottou, P. Gallinari, and J. Weston. Solving multiclass support vector machines with LaRank. In ICML, pages 8996, 2007.
O. Bousquet and L. Bottou. The tradeoffs of large scale learning. In NIPS, pages 161168, 2008.
S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
S. Bu, Z. Liu, J. Han, and J. Wu. Superpixel segmentation based structural scene recognition. In MM, pages 681684. ACM, 2013.
K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector machines. The Journal of Machine Learning Research, 2:265292, 2001.
C. Doersch, A. Gupta, and A. A. Efros. Mid-level visual element discovery as discriminative mode seeking. In NIPS, pages 494502, 2013.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:18711874, 2008.
Y. Gong, L. Wang, R. Guo, and S. Lazebnik. Multi-scale orderless pooling of deep convolutional activation features. In ECCV, 2014.
M. R. Gupta, S. Bengio, and J. Weston. Training highly multiclass classifiers. JMLR, 15:14611492, 2014.
Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.
T. Joachims. A support vector method for multivariate performance measures. In ICML, pages 377384, 2005.
M. Juneja, A. Vedaldi, C. Jawahar, and A. Zisserman. Blocks that shout: distinctive parts for scene classification. In CVPR, 2013.
K. Kiwiel. Variable fixing algorithms for the continuous quadratic knapsack problem. Journal of Optimization Theory and Applications, 136(3):445458, 2008.
M. Koskela and J. Laaksonen. Convolutional network features for scene recognition. In Proceedings of the ACM International Conference on Multimedia, pages 11691172. ACM, 2014.
M. Lapin, B. Schiele, and M. Hein. Scalable multitask representation learning for scene classification. In CVPR, 2014.
N. Li, R. Jin, and Z.-H. Zhou. Top rank optimization in linear time. In NIPS, pages 15021510, 2014.
W. Ogryczak and A. Tamir. Minimizing the sum of the k largest functions in linear time. Information Processing Letters, 85(3):117122, 2003.
M. Patriksson. A survey on the continuous nonlinear resource allocation problem. European Journal of Operational Research, 185(1):146, 2008.
M. Patriksson and C. Strmberg. Algorithms for the continuous nonlinear resource allocation problem  new implementations and numerical studies. European Journal of Operational Research, 243(3):703 722, 2015.
A. Quattoni and A. Torralba. Recognizing indoor scenes. In CVPR, 2009.
A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carlsson. Cnn features off-the-shelf: an astounding baseline for recognition. arXiv preprint arXiv:1403.6382, 2014.
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge, 2014.
J. Snchez, F. Perronnin, T. Mensink, and J. Verbeek. Image classification with the Fisher vector: theory and practice. IJCV, pages 124, 2013.
S. Shalev-Shwartz and T. Zhang. Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization. Mathematical Programming, pages 141, 2014.
J. Sun and J. Ponce. Learning discriminative part detectors for image classification and cosegmentation. In ICCV, pages 34003407, 2013.
K. Swersky, B. J. Frey, D. Tarlow, R. S. Zemel, and R. P. Adams. Probabilistic n-choose-k models for classification and ranking. In NIPS, pages 30503058, 2012.
N. Usunier, D. Buffoni, and P. Gallinari. Ranking with ordered weighted pairwise classification. In ICML, pages 10571064, 2009.
J. Weston, S. Bengio, and N. Usunier. Wsabie: scaling up to large vocabulary image annotation. IJCAI, pages 27642770, 2011.
J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. SUN database: Large-scale scene recognition from abbey to zoo. In CVPR, 2010.
B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva. Learning deep features for scene recognition using places database. In NIPS, 2014.  9
R.S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3(1):944, 1988.
S.J. Bradtke and A.G. Barto. Linear least-squares algorithms for temporal difference learning. Machine Learning, 22(1-3):3357, March 1996.
C. Downey and S. Sanner. Temporal difference Bayesian model averaging: A Bayesian perspective on adapting lambda. In Proceedings of the 27th International Conference on Machine Learning, pages 311 318, 2010.
G.D. Konidaris, S. Niekum, and P.S. Thomas. TD : Re-evaluating complex backups in temporal difference learning. In Advances in Neural Information Processing Systems 24, pages 24022410, 2011.
R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 1998.
T. Kariya and H. Kurata. Generalized Least Squares. Wiley, 2004.
D. Precup, R. S. Sutton, and S. Singh. Eligibility traces for off-policy policy evaluation. In Proceedings of the 17th International Conference on Machine Learning, pages 759766, 2000.
A. R. Mahmood, H. Hasselt, and R. S. Sutton. Weighted importance sampling for off-policy learning with linear function approximation. In Advances in Neural Information Processing Systems 27, 2014.
J. R. Tetreault and D. J. Litman. Comparing the utility of state features in spoken dialogue using reinforcement learning. In Proceedings of the Human Language Technology/North American Association for Computational Linguistics, 2006.
G. D. Konidaris, S. Osentoski, and P. S. Thomas. Value function approximation in reinforcement learning using the Fourier basis. In Proceedings of the Twenty-Fifth Conference on Artificial Intelligence, pages 380395, 2011.
G. Theocharous and A. Hallak. Lifetime value marketing using reinforcement learning. In The 1st Multidisciplinary Conference on Reinforcement Learning and Decision Making, 2013.
P. S. Thomas, G. Theocharous, and M. Ghavamzadeh. High confidence off-policy evaluation. In Proceedings of the Twenty-Ninth Conference on Artificial Intelligence, 2015.
D. Blana, R. F. Kirsch, and E. K. Chadwick. Combined feedforward and feedback control of a redundant, nonlinear, dynamic musculoskeletal system. Medical and Biological Engineering and Computing, 47: 533542, 2009.
P. S. Thomas, M. S. Branicky, A. J. van den Bogert, and K. M. Jagodnik. Application of the actor-critic architecture to functional electrical stimulation control of a human arm. In Proceedings of the TwentyFirst Innovative Applications of Artificial Intelligence, pages 165172, 2009.
P. M. Pilarski, M. R. Dawson, T. Degris, F. Fahimi, J. P. Carey, and R. S. Sutton. Online human training of a myoelectric prosthesis controller via actor-critic reinforcement learning. In Proceedings of the 2011 IEEE International Conference on Rehabilitation Robotics, pages 134140, 2011.
K. Jagodnik and A. van den Bogert. A proportional derivative FES controller for planar arm movement. In 12th Annual Conference International FES Society, Philadelphia, PA, 2007.
N. Hansen and A. Ostermeier. Completely derandomized self-adaptation in evolution strategies. Evolutionary Computation, 9(2):159195, 2001.  9
Daniel D Lee and H Sebastian Seung. Algorithms for non-negative matrix factorization. In Advances in neural information processing systems, pages 556562, 2000.
Gershon Buchsbaum and Orin Bloch. Color categories revealed by non-negative matrix factorization of munsell color spectra. Vision research, 42(5):559563, 2002.
Farial Shahnaz, Michael W Berry, V Paul Pauca, and Robert J Plemmons. Document clustering using nonnegative matrix factorization. Information Processing & Management, 42(2):373386, 2006.
Chih-Jen Lin. Projected gradient methods for nonnegative matrix factorization. Neural computation, 19(10):27562779, 2007.
Andrzej Cichocki, Rafal Zdunek, Anh Huy Phan, and Shun-ichi Amari. Nonnegative matrix and tensor factorizations: applications to exploratory multi-way data analysis and blind source separation. John Wiley & Sons, 2009.
Victor Bittorf, Benjamin Recht, Christopher Re, and Joel A Tropp. Factoring nonnegative matrices with linear programs. Advances in Neural Information Processing Systems, 25:12231231, 2012.
Nicolas Gillis and Stephen A Vavasis. Fast and robust recursive algorithms for separable nonnegative matrix factorization. arXiv preprint arXiv:1208.1237, 2012.
K Huang, ND Sidiropoulos, and A Swamiy. Nmf revisited: New uniqueness results and algorithms. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 45244528. IEEE, 2013.
Chris Ding, Tao Li, Wei Peng, and Haesun Park. Orthogonal nonnegative matrix t-factorizations for clustering. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 126135. ACM, 2006.
Tao Li and Chris Ding. The relationships among various nonnegative matrix factorization methods for clustering. In Data Mining, 2006. ICDM06. Sixth International Conference on, pages 362371. IEEE, 2006.
Filippo Pompili, Nicolas Gillis, P-A Absil, and Francois Glineur. Two algorithms for orthogonal nonnegative matrix factorization with application to clustering. arXiv preprint arXiv:1201.0901, 2012.
Seungjin Choi. Algorithms for orthogonal nonnegative matrix factorization. In Neural Networks, 2008. IJCNN 2008.(IEEE World Congress on Computational Intelligence). IEEE International Joint Conference on, pages 18281832. IEEE, 2008.
Zhirong Yang and Erkki Oja. Linear and nonlinear projective nonnegative matrix factorization. Neural Networks, IEEE Transactions on, 21(5):734749, 2010.
Da Kuang, Haesun Park, and Chris HQ Ding. Symmetric nonnegative matrix factorization for graph clustering. In SDM, volume 12, pages 106117. SIAM, 2012.
Zhirong Yang, Tele Hao, Onur Dikmen, Xi Chen, and Erkki Oja. Clustering by nonnegative matrix factorization using graph random walk. In Advances in Neural Information Processing Systems, pages 10881096, 2012.
Ron Zass and Amnon Shashua. Nonnegative sparse pca. In Advances in Neural Information Processing Systems 19, pages 15611568, Cambridge, MA, 2007. MIT Press.
Megasthenis Asteris, Dimitris Papailiopoulos, and Alexandros Dimakis. Nonnegative sparse pca with provable guarantees. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 17281736, 2014.
Zhijian Yuan and Erkki Oja. Projective nonnegative matrix factorization for image compression and feature extraction. In Image Analysis, pages 333342. Springer, 2005.
Hualiang Li, Tulay Adal, Wei Wang, Darren Emge, and Andrzej Cichocki. Non-negative matrix factorization with orthogonality constraints and its application to raman spectroscopy. The Journal of VLSI Signal Processing Systems for Signal, Image, and Video Technology, 48(1-2):8397, 2007.
Bin Cao, Dou Shen, Jian-Tao Sun, Xuanhui Wang, Qiang Yang, and Zheng Chen. Detect and track latent factors with online nonnegative matrix factorization. In IJCAI, volume 7, pages 26892694, 2007.
Xin Li, William KW Cheung, Jiming Liu, and Zhili Wu. A novel orthogonal nmf-based belief compression for pomdps. In Proceedings of the 24th international conference on Machine learning, pages 537544. ACM, 2007.
Gang Chen, Fei Wang, and Changshui Zhang. Collaborative filtering using orthogonal nonnegative matrix tri-factorization. Information Processing & Management, 45(3):368379, 2009.
NA Gillis and S Vavasis. Fast and robust recursive algorithms for separable nonnegative matrix factorization. IEEE transactions on pattern analysis and machine intelligence, 2013.
Sanjeev Arora, Rong Ge, Yoni Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu, and Michael Zhu. A practical algorithm for topic modeling with provable guarantees. arXiv preprint arXiv:1212.4777, 2012.
Sanjeev Arora, Rong Ge, Ravindran Kannan, and Ankur Moitra. Computing a nonnegative matrix factorizationprovably. In Proceedings of the 44th symposium on Theory of Computing, pages 145162, 2012.
Abhishek Kumar, Vikas Sindhwani, and Prabhanjan Kambadur. Fast conical hull algorithms for near-separable non-negative matrix factorization. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 231239, 2013.
Christian D. Sigg and Joachim M. Buhmann. Expectation-maximization for sparse and non-negative pca. In Proceedings of the 25th International Conference on Machine Learning, ICML 08, pages 960967, New York, NY, USA, 2008. ACM.
Liming Cai, Michael Fellows, David Juedes, and Frances Rosamond. On efficient polynomial-time approximation schemes for problems on planar structures. Journal of Computer and System Sciences, 2003.
Marco Cesati and Luca Trevisan. On the efficiency of polynomial time approximation schemes. Information Processing Letters, 64(4):165171, 1997.
Kah-Kay Sung. Learning and example selection for object and pattern recognition. PhD thesis, PhD thesis, MIT, Artificial Intelligence Laboratory and Center for Biological and Computational Learning, Cambridge, MA, 1996.
M. Lichman. UCI machine learning repository, 2013.
Filippo Pompili, Nicolas Gillis, Pierre-Antoine Absil, and Francois Glineur. Onp-mf: An orthogonal nonnegative matrix factorization algorithm with application to clustering. In ESANN 2013, 2013.
K. Bache and M. Lichman. UCI machine learning repository, 2013.  9
J.-Y. Audibert and S. Bubeck. Best arm identification in multi-armed bandits. In COLT, 2010.
J.-Y. Audibert, S. Bubeck, and G. Lugosi. Minimax policies for combinatorial prediction games. arXiv preprint arXiv:1105.4871, 2011.
P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2-3):235256, 2002.
P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. SIAM Journal on Computing, 32(1):4877, 2002.
A. Bjorner and G. M. Ziegler. Introduction to greedoids. Matroid applications, 40:284357, 1992.
S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. arXiv preprint arXiv:1204.5721, 2012.
S. Bubeck, R. Munos, and G. Stoltz. Pure exploration in finitely-armed and continuous-armed bandits. Theoretical Computer Science, 412(19):18321852, 2011.
N. Cesa-Bianchi and G. Lugosi. Combinatorial bandits. Journal of Computer and System Sciences, 78 (5):14041422, 2012.
S. Chen, T. Lin, I. King, M. R. Lyu, and W. Chen. Combinatorial pure exploration of multi-armed bandits. In NIPS, 2014.
W. Chen, Y. Wang, and Y. Yuan. Combinatorial multi-armed bandit: General framework and applications. In ICML, 2013.
V. Chvatal. A greedy heuristic for the set-covering problem. Mathematics of operations research, 4(3): 233235, 1979.
V. Gabillon, B. Kveton, Z. Wen, B. Eriksson, and S. Muthukrishnan. Adaptive submodular maximization in bandit setting. In NIPS. 2013.
Y. Gai, B. Krishnamachari, and R. Jain. Learning multiuser channel allocations in cognitive radio networks: A combinatorial multi-armed bandit formulation. In DySPAN. IEEE, 2010.
A. Garivier and O. Cappe. The kl-ucb algorithm for bounded stochastic bandits and beyond. arXiv preprint arXiv:1102.2490, 2011.
P. Helman, B. M. Moret, and H. D. Shapiro. An exact characterization of greedy structures. SIAM Journal on Discrete Mathematics, 6(2):274283, 1993.
S. Kalyanakrishnan, A. Tewari, P. Auer, and P. Stone. Pac subset selection in stochastic multi-armed bandits. In ICML, 2012.
D. Kempe, J. Kleinberg, and E. Tardos. Maximizing the spread of influence through a social network. In SIGKDD, 2003.
B. Korte and L. Lovasz. Greedoids and linear objective functions. SIAM Journal on Algebraic Discrete Methods, 5(2):229238, 1984.
J. B. Kruskal. On the shortest spanning subtree of a graph and the traveling salesman problem. Proceedings of the American Mathematical society, 7(1):4850, 1956.
B. Kveton, Z. Wen, A. Ashkan, H. Eydgahi, and B. Eriksson. Matroid bandits: Fast combinatorial optimization with learning. arXiv preprint arXiv:1403.5045, 2014.
B. Kveton, Z. Wen, A. Ashkan, and C. Szepesvari. Tight regret bounds for stochastic combinatorial semi-bandits. arXiv preprint arXiv:1410.0949, 2014.
T. L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances in applied mathematics, 6(1):422, 1985.
T. Lin, B. Abrahao, R. Kleinberg, J. Lui, and W. Chen. Combinatorial partial monitoring game with linear feedback and its applications. In ICML, 2014.
B. Mirzasoleiman, A. Badanidiyuru, A. Karbasi, J. Vondrak, and A. Krause. Lazier than lazy greedy. In Proc. Conference on Artificial Intelligence (AAAI), 2015.
R. C. Prim. Shortest connection networks and some generalizations. Bell system technical journal, 36(6): 13891401, 1957.
M. Streeter and D. Golovin. An online algorithm for maximizing submodular functions. In NIPS, 2009.  9
S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du, C. Huang, and P. Torr, Conditional random fields as recurrent neural networks, 2015.
. Available: http://arxiv.org/abs/1502.03240
A. Schwing and R. Urtasun, Fully connected deep structured networks, 2015.
. Available: http://arxiv.org/abs/1503.02351
G. Lin, C. Shen, I. Reid, and A. van den Hengel, Efficient piecewise training of deep structured models for semantic segmentation, 2015.
. Available: http://arxiv.org/abs/1504.01013
F. Liu, C. Shen, and G. Lin, Deep convolutional neural fields for depth estimation from a single image, in Proc. IEEE Conf. Comp. Vis. Pattern Recogn., 2015.
L. Chen, A. Schwing, A. Yuille, and R. Urtasun, Learning deep structured models, 2014.
. Available: http://arxiv.org/abs/1407.2538
J. Besag, Efficiency of pseudolikelihood estimation for simple Gaussian fields, Biometrika, 1977.
C. Sutton and A. McCallum, Piecewise training for undirected models, in Proc. Conf. Uncertainty Artificial Intelli, 2005.
S. Ross, D. Munoz, M. Hebert, and J. Bagnell, Learning message-passing inference machines for structured prediction, in Proc. IEEE Conf. Comp. Vis. Pattern Recogn., 2011.
L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. Yuille, Semantic image segmentation with deep convolutional nets and fully connected CRFs, 2014.
. Available: http://arxiv.org/abs/1412.7062
P. Krahenbuhl and V. Koltun, Efficient inference in fully connected CRFs with Gaussian edge potentials, in Proc. Adv. Neural Info. Process. Syst., 2012.
F. Liu, C. Shen, G. Lin, and I. Reid, Learning depth from single monocular images using deep convolutional neural fields, 2015.
. Available: http://arxiv.org/abs/1502.07411
J. Tompson, A. Jain, Y. LeCun, and C. Bregler, Joint training of a convolutional network and a graphical model for human pose estimation, in Proc. Adv. Neural Info. Process. Syst., 2014.
S. Nowozin and C. Lampert, Structured learning and prediction in computer vision, Found. Trends. Comput. Graph. Vis., 2011.
V. Kolmogorov, Convergent tree-reweighted message passing for energy minimization, IEEE T. Pattern Analysis & Machine Intelligence, 2006.
J. S. Yedidia, W. T. Freeman, Y. Weiss et al., Generalized belief propagation, in Proc. Adv. Neural Info. Process. Syst., 2000.
J. Long, E. Shelhamer, and T. Darrell, Fully convolutional networks for semantic segmentation, in Proc. IEEE Conf. Comp. Vis. Pattern Recogn., 2015.
M. Mostajabi, P. Yadollahpour, and G. Shakhnarovich, Feedforward semantic segmentation with zoom-out features, 2014.
. Available: http://arxiv.org/abs/1412.0774
J. Dai, K. He, and J. Sun, BoxSup: exploiting bounding boxes to supervise convolutional networks for semantic segmentation, 2015.
. Available: http://arxiv.org/abs/1503.01640
M. Everingham, L. V. Gool, C. Williams, J. Winn, and A. Zisserman, The pascal visual object classes (VOC) challenge, Int. J. Comp. Vis., 2010.
B. Hariharan, P. Arbelaez, R. Girshick, and J. Malik, Simultaneous detection and segmentation, in Proc. European Conf. Computer Vision, 2014.
B. Hariharan, P. Arbelaez, L. Bourdev, S. Maji, and J. Malik, Semantic contours from inverse detectors, in Proc. Int. Conf. Comp. Vis., 2011.
K. Simonyan and A. Zisserman, Very deep convolutional networks for large-scale image recognition, 2014.
. Available: http://arxiv.org/abs/1409.1556
A. Vedaldi and K. Lenc, Matconvnet  convolutional neural networks for matlab, in Proceeding of the ACM Int. Conf. on Multimedia, 2015.
H. Noh, S. Hong, and B. Han, Learning deconvolution network for semantic segmentation, in Proc. IEEE Conf. Comp. Vis. Pattern Recogn., 2015.
G. Papandreou, L. Chen, K. Murphy, and A. Yuille, Weakly-and semi-supervised learning of a DCNN for semantic image segmentation, 2015.
. Available: http://arxiv.org/abs/1502.02734
Hatfield G. Perception as Unconscious Inference. In: Perception and the Physical World: Psychological and Philosophical Issues in Perception. Wiley; 2002. p. 115143.
Pouget A, Beck JM, Ma WJ, Latham PE. Probabilistic brains: knowns and unknowns. Nature Neuroscience. 2013;16(9):11701178.
Winkler I, Denham S, Mill R, Bohm TM, Bendixen A. Multistability in auditory stream segregation: a predictive coding view. Phil Trans R Soc B: Biol Sci. 2012;367(1591):10011012.
Brea J, Senn W, Pfister JP. Sequence learning with hidden units in spiking neural networks. In: NIPS. vol. 24; 2011. p. 14221430.
Rezende DJ, Gerstner W. Stochastic variational learning in recurrent spiking networks. Front Comput Neur. 2014;8:38.
Nessler B, Pfeiffer M, Maass W. STDP enables spiking neurons to detect hidden causes of their inputs. In: NIPS. vol. 22; 2009. p. 13571365.
MacKay DJ. Bayesian interpolation. Neural Computation. 1992;4(3):415447.
Bishop CM. Pattern Recognition and Machine Learning. New York: Springer; 2006.
Holtmaat AJ, Trachtenberg JT, Wilbrecht L, Shepherd GM, Zhang X, Knott GW, et al. Transient and Persistent Dendritic Spines in the Neocortex In Vivo. Neuron. 2005;45:279291.
Lowenstein Y, Kuras A, Rumpel S. Multiplicative dynamics underlie the emergence of the log-normal distribution of spine sizes in the neocortex in vivo. J Neurosci. 2011;31(26):94819488.
Marder E. Variability, compensation and modulation in neurons and circuits. PNAS. 2011;108(3):15542 15548.
Gardiner CW. Handbook of Stochastic Methods. 3rd ed. Springer; 2004.
Welling M, Teh YW. Bayesian learning via stochastic gradient Langevin dynamics. In: Proceedings of the 28th International Conference on Machine Learning (ICML-11); 2011. p. 681688.
Sato I, Nakagawa H. Approximation analysis of stochastic gradient langevin dynamics by using fokkerplanck equation and ito process. In: NIPS; 2014. p. 982990.
Kappel D, Nessler B, Maass W. STDP installs in winner-take-all circuits an online approximation to hidden Markov model learning. PLoS Comp Biol. 2014;10(3):e1003511.
Jolivet R, Rauch A, Luscher H, Gerstner W. Predicting spike timing of neocortical pyramidal neurons by simple threshold models. J Comp Neurosci. 2006;21:3549.
Mensi S, Naud R, Gerstner W. From stochastic nonlinear integrate-and-fire to generalized linear models. In: NIPS. vol. 24; 2011. p. 13771385.
Gerstner W, Kistler WM. Spiking Neuron Models. Cambridge University Press; 2002.
Carandini M. From circuits to behavior: a bridge too far? Nature Neurosci. 2012;15(4):507509.
Habenschuss S, Bill J, Nessler B. Homeostatic plasticity in Bayesian spiking networks as Expectation Maximization with posterior constraints. In: NIPS. vol. 25; 2012. p. 782790.
Habenschuss S, Puhr H, Maass W. Emergence of optimal decoding of population codes through STDP. Neural Computation. 2013;25:137.
Kappel D, Habenschuss S, Legenstein R, Maass W. Network Plasticity as Bayesian Inference. PLoS Comp Biol. 2015;in press.
Xiong H, Szedmak S, Rodrguez-Sanchez A, Piater J. Towards sparsity and selectivity: Bayesian learning of restricted Boltzmann machine for early visual features. In: ICANN; 2014. p. 419426.
Bi GQ, Poo MM. Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type. J Neurosci. 1998;18(24):1046410472.
Sjostrom PJ, Turrigiano GG, Nelson SB. Rate, timing, and cooperativity jointly determine cortical synaptic plasticity. Neuron. 2001;32(6):11491164.
Montgomery JM, Pavlidis P, Madison DV. Pair recordings reveal all-silent synaptic connections and the postsynaptic expression of long-term potentiation. Neuron. 2001;29(3):691701.
Kennedy AD. The Hybrid Monte Carlo algorithm on parallel computers. Parallel Computing. 1999;25(10):13111339.
Johannes Schemmel KMEM Andreas Gruebl. Implementing Synaptic Plasticity in a VLSI Spiking Neural Network Model. In: IJCNN; 2006. p. 16.
Bill J, Legenstein R. A compound memristive synapse model for statistical learning through STDP in spiking neural networks. Frontiers in Neuroscience. 2014;8.  9
F. Yun, editor. Low-rank and sparse modeling for visual analysis. Springer, 2014. 1
E.J. Candes, M.B. Wakin, and S.P. Boyd. Enhancing sparsity by reweighted l1 minimization. Journal of Fourier Analysis and Applications, 14(5):877905, 2008. 1
T. Zhang. Analysis of multi-stage convex relaxation for sparse regularization. The Journal of Machine Learning Rearch, 11:10811107, 2010. 1, 7
S. Foucart and M.J. Lai. Sparsest solutions of underdeterminied linear systems via lq minimization for 0 < q  1. Applied and Computational Harmonic Analysis, 26(3):395407, 2009. 1
C.H. Zhang. Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics, 38(2):894942, 2010. 1
D. Geman and C. Yang. Nonlinear image recovery with half-quadratic regularization. IEEE Transactions on Image Processing, 4(7):932946, 1995. 1
J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96(456):13481360, 2001. 1
K. Mohan and M. Fazel. Iterative reweighted algorithms for matrix rank minimization. The Journal of Machine Learning Research, 13(1):34413473, 2012. 1, 2
Y.E. Nesterov. A method for unconstrained convex minimization problem with the rate of convergence O(1/k2 ). Soviet Mathematics Doklady, 27(2):372376, 1983. 1
Y.E. Nesterov. Smooth minimization of nonsmooth functions. Mathematical programming, 103(1):127 152, 2005. 1
Y.E. Nesterov. Gradient methods for minimizing composite objective functions. Technical report, Center for Operations Research and Econometrics(CORE), Catholie University of Louvain, 2007. 1
A. Beck and M. Teboulle. Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems. IEEE Transactions on Image Processing, 18(11):24192434, 2009. 1, 2, 3, 4, 5
A. Beck and M. Teboulle. A fast iterative shrinkage thresholding algorithm for linear inverse problems. SIAM J. Imaging Sciences, 2(1):183202, 2009. 1, 2, 3, 4, 5
P. Tseng. On accelerated proximal gradient methods for convex-concave optimization. Technical report, University of Washington, Seattle, 2008. 1
S. Ghadimi and G. Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic programming. arXiv preprint arXiv:1310.3787, 2013. 1, 2
H. Attouch, J. Bolte, and B.F. Svaier. Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward-backward splitting, and regularized Gauss-Seidel methods. Mathematical Programming, 137:91129, 2013. 2, 4, 5
P. Frankel, G. Garrigos, and J. Peypouquet. Splitting methods with variable metric for Kurdykaojasiewicz functions and general convergence rates. Journal of Optimization Theory and Applications, 165:874900, 2014. 2, 5
P. Ochs, Y. Chen, T. Brox, and T. Pock. IPiano: Inertial proximal algorithms for nonconvex optimization. SIAM J. Image Sciences, 7(2):13881419, 2014. 2
P. Gong, C. Zhang, Z. Lu, J. Huang, and J. Ye. A general iterative shrinkage and thresholding algorithm for nonconvex regularized optimization problems. In ICML, pages 3745, 2013. 2, 7
W. Zhong and J. Kwok. Gradient descent with proximal average for nonconvex and composite regularization. In AAAI, 2014. 2
P. Ochs, A. Dosovitskiy, T. Brox, and T. Pock. On iteratively reweighted algorithms for non-smooth non-convex optimization in computer vision. SIAM J. Imaging Sciences, 2014. 2
R.L. Bot, E.R. Csetnek, and S. Laszlo. An inertial forward-backward algorithm for the minimization of the sum of two nonconvex functions. Preprint, 2014. 2, 7
J. Bolte, S. Sabach, and M. Teboulle. Proximal alternating linearized minimization for nonconvex and nonsmooth problems. Mathematical Programming, 146(1-2):459494, 2014. 3, 5
H. Zhang and W.W. Hager. A nonmonotone line search technique and its application to unconstrained optimization. SIAM J. Optimization, 14:10431056, 2004. 5, 6
S.K. Shevade and S.S. Keerthi. A simple and efficient algorithm for gene selection using sparse logistic regression. Bioinformatics, 19(17):22462253, 2003. 7
A. Genkin, D.D. Lewis, and D. Madigan. Large-scale bayesian logistic regression for text categorization. Technometrics, 49(14):291304, 2007. 7  9
M. Asteris, D. Papailiopoulos, and A. Dimakis. Non-negative sparse PCA with provable guarantees. In Proc. ICML, 2014.
J. Cadima and I. Jolliffe. Loadings and correlations in the interpretation of principal components. Applied Statistics, 22:203214, 1995.
T. T. Cai, Z. Ma, and Y. Wu. Sparse pca: Optimal rates and adaptive estimation. The Annals of Statistics, 41(6):30743110, 2013.
Alexandre dAspremont, Francis Bach, and Laurent El Ghaoui. Optimal solutions for sparse principal component analysis. Journal of Machine Learning Research, 9:12691294, June 2008.
Alexandre dAspremont, Laurent El Ghaoui, Michael I. Jordan, and Gert R. G. Lanckriet. A direct formulation for sparse PCA using semidefinite programming. SIAM Review, 49(3):434 448, 2007.
E. Gabrilovich and S. Markovitch. Text categorization with many redundant features: using aggressive feature selection to make SVMs competitive with C4.5. In Proceedings of International Conference on Machine Learning, 2004.
J. J. Hull. A database for handwritten text recognition research. In IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 550554, 16(5), 1994.
A. Kundu, P. Drineas, and M. Magdon-Ismail. Recovering PCA from Hybrid-(`1 , `2 ) Sparse Sampling of Data Elements. In http://arxiv.org/pdf/1503.00547v1.pdf, 2015.
J. Lei and V. Q. Vu. Sparsistency and agnostic inference in sparse pca. The Annals of Statistics, 43(1):299322, 2015.
Karim Lounici. Sparse principal component analysis with missing observations. arxiv report: http://arxiv.org/abs/1205.7060, 2012.
Z. Ma. Sparse principal component analysis and iterative thresholding. The Annals of Statistics, 41(2):772801, 2013.
M. Magdon-Ismail. NP-hardness and inapproximability of sparse pca. http://arxiv.org/abs/1502.05675, 2015.  arxiv report:
M. Magdon-Ismail and C. Boutsidis. arxiv report: http://arxiv.org/abs/1502.06626, 2015.
B. Moghaddam, Y. Weiss, and S. Avidan. Generalized spectral bounds for sparse LDA. In Proc. ICML, 2006.
K. Pearson. On lines and planes of closest fit to systems of points in space. Philosophical Magazine, 2:559572, 1901.
Haipeng Shen and Jianhua Z. Huang. Sparse principal component analysis via regularized low rank matrix approximation. Journal of Multivariate Analysis, 99:10151034, July 2008.
K. Sjstrand, L.H. Clemmensen, R. Larsen, and B. Ersbll. Spasm: A matlab toolbox for sparse statistical modeling. In Journal of Statistical Software (Accepted for publication), 2012.
N. Trendafilov, I. T. Jolliffe, and M. Uddin. A modified principal component technique based on the lasso. Journal of Computational and Graphical Statistics, 12:531547, 2003.
Z. Wang, H. Lu, and H. Liu. Nonconvex statistical optimization: Minimax-optimal sparse pca in polynomial time. http://arxiv.org/abs/1408.5352?context=cs.LG, 2014.
H. Zou, T. Hastie, and R. Tibshirani. Sparse principal component analysis. Journal of Computational & Graphical Statistics, 15(2):265286, 2006.  9
Jan Beirlant, Edward J. Dudewicz, Laszlo Gyorfi, and Edward C. Van der Meulen. Nonparametric entropy estimation: An overview. International Journal of Mathematical and Statistical Sciences, 1997.  8
Peter J. Bickel and Yaacov Ritov. Estimating integrated squared density derivatives: sharp best order of convergence estimates. Sankhya: The Indian Journal of Statistics, 1988.
Lucien Birge and Pascal Massart. Estimation of integral functionals of a density. Ann. of Stat., 1995.
Kevin M. Carter, Raviv Raich, and Alfred O. Hero. On local intrinsic dimension estimation and its applications. IEEE Transactions on Signal Processing, 2010.
Inderjit S. Dhillon, Subramanyam Mallela, and Rahul Kumar. A Divisive Information Theoretic Feature Clustering Algorithm for Text Classification. J. Mach. Learn. Res., 2003.
M Emery, A Nemirovski, and D Voiculescu. Lectures on Prob. Theory and Stat. Springer, 1998.
Luisa Fernholz. Von Mises calculus for statistical functionals. Lecture notes in statistics. Springer, 1983.
Mohammed Nawaz Goria, Nikolai N Leonenko, Victor V Mergel, and Pier Luigi Novi Inverardi. A new class of random vector entropy estimators and its applications. Nonparametric Statistics, 2005.
Hero, Bing Ma, O. J. J. Michel, and J. Gorman. Applications of entropic spanning graphs. IEEE Signal Processing Magazine, 19, 2002.
David Kallberg and Oleg Seleznjev. Estimation of entropy-type integral functionals. arXiv, 2012.
Gerard Kerkyacharian and Dominique Picard. Estimating nonquadratic functionals of a density using haar wavelets. Annals of Stat., 1996.
Akshay Krishnamurthy, Kirthevasan Kandasamy, Barnabas Poczos, and Larry Wasserman. Nonparametric Estimation of Renyi Divergence and Friends. In ICML, 2014.
Akshay Krishnamurthy, Kirthevasan Kandasamy, Barnabas Poczos, and Larry Wasserman. On Estimating L22 Divergence. In Artificial Intelligence and Statistics, 2015.
Beatrice Laurent. Efficient estimation of integral functionals of a density. Ann. of Stat., 1996.
Erik Learned-Miller and Fisher John. ICA using spacings estimates of entropy. Mach. Learn. Res., 2003.
Bastian Leibe and Bernt Schiele. Analyzing Appearance and Contour Based Methods for Object Categorization. In CVPR, 2003.
Nikolai Leonenko and Oleg Seleznjev. Statistical inference for the epsilon-entropy and the quadratic Renyi entropy. Journal of Multivariate Analysis, 2010.
Jeremy Lewi, Robert Butera, and Liam Paninski. Real-time adaptive information-theoretic optimization of neurophysiology experiments. In NIPS, 2006.
Han Liu, Larry Wasserman, and John D Lafferty. Exponential concentration for mutual information estimation with application to forests. In NIPS, 2012.
Erik G Miller. A new class of Entropy Estimators for Multi-dimensional Densities. In ICASSP, 2003.
Kevin Moon and Alfred Hero. Multivariate f-divergence Estimation With Confidence. In NIPS, 2014.
XuanLong Nguyen, Martin J. Wainwright, and Michael I. Jordan. Estimating divergence functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory, 2010.
Havva Alizadeh Noughabi and Reza Alizadeh Noughabi. On the Entropy Estimators. Journal of Statistical Computation and Simulation, 2013.
David Pal, Barnabas Poczos, and Csaba Szepesvari. Estimation of Renyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs. In NIPS, 2010.
Hanchuan Peng, Fulmi Long, and Chris Ding. Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy. IEEE PAMI, 2005.
Fernando Perez-Cruz. KL divergence estimation of continuous distributions. In IEEE ISIT, 2008.
Barnabas Poczos and Jeff Schneider. On the estimation of alpha-divergences. In AISTATS, 2011.
Barnabas Poczos, Liang Xiong, and Jeff G. Schneider. Nonparametric Divergence Estimation with Applications to Machine Learning on Distributions. In UAI, 2011.
David Ramrez, Javier Va, Ignacio Santamara, and Pedro Crespo. Entropy and Kullback-Leibler Divergence Estimation based on Szegos Theorem. In EUSIPCO, 2009.
James Robins, Lingling Li, Eric Tchetgen, and Aad W. van der Vaart. Quadratic semiparametric Von Mises Calculus. Metrika, 2009.
Elad Schneidman, William Bialek, and Michael J. Berry II. An Information Theoretic Approach to the Functional Classification of Neurons. In NIPS, 2002.
Shashank Singh and Barnabas Poczos. Exponential Concentration of a Density Functional Estimator. In NIPS, 2014.
Dan Stowell and Mark D Plumbley. Fast Multidimensional Entropy Estimation by k-d Partitioning. IEEE Signal Process. Lett., 2009.
Zoltan Szabo. Information Theoretical Estimators Toolbox. J. Mach. Learn. Res., 2014.
Alexandre B. Tsybakov. Introduction to Nonparametric Estimation. Springer, 2008.
Aad W. van der Vaart. Asymptotic Statistics. Cambridge University Press, 1998.
Qing Wang, Sanjeev R. Kulkarni, and Sergio Verdu. Divergence estimation for multidimensional densities via k-nearest-neighbor distances. IEEE Transactions on Information Theory, 2009.  9
Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near optimal coresets for least-squares regression. IEEE Transactions on Information Theory, 59(10), October 2013.
C. Boutsidis and M. Magdon-Ismail. A note on sparse least-squares regression. Information Processing Letters, 115(5):273276, 2014.
Christos Boutsidis and Malik Magdon-Ismail. Deterministic feature selection for k-means clustering. IEEE Transactions on Information Theory, 59(9), September 2013.
Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Sparse features for pca-like regression. In Proc. 25th Annual Conference on Neural Information Processing Systems (NIPS), 2011. to appear.
Malik Magdon-Ismail and Christos Boutsidis. Optimal sparse linear auto-encoders and sparse pca. arXiv:1502.06626, 2015.
T. F. Chan and P. C. Hansen. Some applications of the rank revealing QR factorization. SIAM J. Sci. Stat. Comput., 13(3):727741, 1992.
A. Deshpande and L. Rademacher. Efficient volume sampling for row/column subset selection. In Proceedings of the IEEE 51st FOCS, pages 329338, 2010.
A. Deshpande and S. Vempala. Adaptive sampling and fast low-rank matrix approximation. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 292303. Springer, 2006.
A. Deshpande, L. Rademacher, S. Vempala, and G. Wang. Matrix approximation and projective clustering via volume sampling. Theory of Computing, 2(1):225247, 2006.
P. Drineas, I. Kerenidis, and P. Raghavan. Competitive recommendation systems. In Proceedings of the 34th STOC, pages 8290, 2002.
A. Frieze, R. Kannan, and S. Vempala. Fast monte-carlo algorithms for finding low-rank approximations. Journal of the ACM (JACM), 51(6):10251041, 2004.
N. Halko, P. G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Rev., 53(2):217288, May 2011.
E. Liberty, F. Woolfe, P.G. Martinsson, V. Rokhlin, and M. Tygert. Randomized algorithms for the lowrank approximation of matrices. PNAS, 104(51):2016720172, 2007.
Michael W Mahoney and Petros Drineas. CUR matrix decompositions for improved data analysis. PNAS, 106(3):697702, 2009.
C. Boutsidis, P. Drineas, and M. Magdon-Ismail. Near-optimal column-based matrix reconstruction. SIAM Journal of Computing, 43(2):687717, 2014.
P. Drineas, M. W Mahoney, and S Muthukrishnan. Subspace sampling and relative-error matrix approximation: Column-based methods. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 316326. Springer, 2006.
Venkatesan Guruswami and Ali Kemal Sinop. Optimal column-based low-rank matrix reconstruction. In Proceedings of the 23rd Annual ACM-SIAM Symposium on Discrete Algorithms, pages 12071214, 2012.
C. Boutsidis, P. Drineas, and M. Magdon-Ismail. Near optimal column-based matrix reconstruction. In IEEE 54th Annual Symposium on FOCS, pages 305314, 2011.
Petros Drineas, Michael W Mahoney, and S Muthukrishnan. Relative-error cur matrix decompositions. SIAM Journal on Matrix Analysis and Applications, 30(2):844881, 2008.
T.F. Chan. Rank revealing QR factorizations. Linear Algebra and its Applications, 8889(0):67  82, 1987.
Crystal Maung and Haim Schweitzer. Pass-efficient unsupervised feature selection. In Advances in Neural Information Processing Systems, pages 16281636, 2013.
C. Boutsidis, M. W Mahoney, and P. Drineas. An improved approximation algorithm for the column subset selection problem. In Proceedings of the 20th SODA, pages 968977, 2009.
D. Papailiopoulos, A. Kyrillidis, and C. Boutsidis. Provable deterministic leverage score sampling. In Proc. SIGKDD, pages 9971006, 2014.
A. Deshpande, L. Rademacher, S. Vempala, and G. Wang. Matrix approximation and projective clustering via volume sampling. In Proc. SODA, pages 11171126, 2006.
P. Drineas and M. W Mahoney. A randomized algorithm for a tensor-based generalization of the singular value decomposition. Linear algebra and its applications, 420(2):553571, 2007.
P. Paschou, J. Lewis, A. Javed, and P. Drineas. Ancestry informative markers for fine-scale individual assignment to worldwide populations. Journal of Medical Genetics, 47(12):83547, 2010.
D. Davidov, E. Gabrilovich, and S. Markovitch. Parameterized generation of labeled datasets for text categorization based on a hierarchical directory. In Proc. SIGIR, pages 250257, 2004.  9
G. Andrew and J. Gao. Scalable training of 1 -regularized log-linear models. In ICML, pages 3340, 2007.
J. Bioucas-Dias and M. Figueiredo. A new TwIST: two-step iterative shrinkage/thresholding algorithms for image restoration. IEEE Transactions on Image Processing, 16(12):29923004, 2007.
R. H. Byrd, G. M. Chin, J. Nocedal, and F. Oztoprak. A family of second-order methods for convex 1 -regularized optimization. Technical report, Industrial Engineering and Management Sciences, Northwestern University, Evanston, IL, 2012.
R. H. Byrd, G. M. Chin, J. Nocedal, and Y. Wu. Sample size selection in optimization methods for machine learning. Mathematical Programming, 134(1):127155, 2012.
R. H. Byrd, P. Lu, J. Nocedal, and C. Zhu. A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientific Computing, 16(5):11901208, 1995.
E. Candes, M. Wakin, and S. Boyd. Enhancing sparsity by reweighted 1 minimization. Journal of Fourier Analysis and Applications, 14(5):877905, 2008.
F. Clarke. Optimization and Nonsmooth Analysis. John Wiley&Sons, New York, 1983.
J. Dutta. Generalized derivatives and nonsmooth optimization, a finite dimensional tour. Top, 13(2):185 279, 2005.
L. El Ghaoui, G. Li, V. Duong, V. Pham, A. Srivastava, and K. Bhaduri. Sparse machine learning methods for understanding large text corpora. In CIDU, pages 159173, 2011.
J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96(456):13481360, 2001.
J. Fan, L. Xue, and H. Zou. Strong oracle optimality of folded concave penalized estimation. Annals of Statistics, 42(3):819, 2014.
G. Gasso, A. Rakotomamonjy, and S. Canu. Recovering sparse signals with a certain family of nonconvex penalties and dc programming. IEEE Transactions on Signal Processing, 57(12):46864698, 2009.
P. Gong and J. Ye. A modified orthant-wise limited memory quasi-newton method with convergence analysis. In ICML, 2015.
P. Gong, C. Zhang, Z. Lu, J. Huang, and J. Ye. A general iterative shrinkage and thresholding algorithm for non-convex regularized optimization problems. In ICML, volume 28, pages 3745, 2013.
N. Jorge and J. Stephen. Numerical Optimization. Springer, 1999.
R. Mazumder, J. Friedman, and T. Hastie. Sparsenet: Coordinate descent with nonconvex penalties. Journal of the American Statistical Association, 106(495), 2011.
P. Olsen, F. Oztoprak, J. Nocedal, and S. Rennie. Newton-like methods for sparse inverse covariance estimation. In Advances in Neural Information Processing Systems (NIPS), pages 764772, 2012.
A. Rakotomamonjy, R. Flamary, and G. Gasso. Dc proximal newton for non-convex optimization problems. 2014.
S. Shevade and S. Keerthi. A simple and efficient algorithm for gene selection using sparse logistic regression. Bioinformatics, 19(17):2246, 2003.
X. Tan, W. Roberts, J. Li, and P. Stoica. Sparse learning via iterative minimization with application to mimo radar imaging. IEEE Transactions on Signal Processing, 59(3):10881101, 2011.
P. Tao and L. An. The dc (difference of convex functions) programming and dca revisited with dc models of real world nonconvex optimization problems. Annals of Operations Research, 133(1-4):2346, 2005.
J. Wright, A. Yang, A. Ganesh, S. Sastry, and Y. Ma. Robust face recognition via sparse representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(2):210227, 2008.
C. Zhang. Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics, 38(2):894942, 2010.
C. Zhang and T. Zhang. A general theory of concave regularization for high-dimensional sparse estimation problems. Statistical Science, 27(4):576593, 2012.
T. Zhang. Analysis of multi-stage convex relaxation for sparse regularization. JMLR, 11:10811107, 2010.
T. Zhang. Multi-stage convex relaxation for feature selection. Bernoulli, 2012.
H. Zou and R. Li. One-step sparse estimates in nonconcave penalized likelihood models. Annals of Statistics, 36(4):1509, 2008.  9
P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part based models. PAMI, 2010.
A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.
K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In arXiv:1409.1556, 2014.
K. Van de Sande, J. Uijlings, T. Gevers, and A. Smeulders. Segmentation as selective search for object recognition. In ICCV, 2011.
P. Arbelaez, J. Pont-Tusetand, J. Barron, F. Marques, and J. Malik. Multiscale combinatorial grouping. In CVPR. 2014.
R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. arXiv preprint arXiv:1311.2524, 2013.
Y. Zhu, R. Urtasun, R. Salakhutdinov, and S. Fidler. SegDeepM: Exploiting segmentation and context in deep neural networks for object detection. In CVPR, 2015.
M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2010 (VOC2010) Results.
L. Zitnick and P. Dollar. Edge boxes: Locating object proposals from edges. In ECCV. 2014.
J. Carreira and C. Sminchisescu. Cpmc: Automatic object segmentation using constrained parametric min-cuts. PAMI, 34(7):13121328, 2012.
A. Geiger, P. Lenz, and R. Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In CVPR, 2012.
Y. Xiang, W. Choi, Y. Lin, and S. Savarese. Data-driven 3d voxel patterns for object category recognition. In CVPR, 2015.
C. Long, X. Wang, G. Hua, M. Yang, and Y. Lin. Accurate object detection with location relaxation and regionlets relocalization. In ACCV, 2014.
S. Gupta, R. Girshick, P. Arbelaez, and J. Malik. Learning rich features from RGB-D images for object detection and segmentation. In ECCV. 2014.  8
M. Cheng, Z. Zhang, M. Lin, and P. Torr. BING: Binarized normed gradients for objectness estimation at 300fps. In CVPR, 2014.
T. Lee, S. Fidler, and S. Dickinson. A learning framework for generating region proposals with mid-level cues. In ICCV, 2015.
D. Banica and C Sminchisescu. Cpmc-3d-o2p: Semantic segmentation of rgb-d images using cpmc and second order pooling. In CoRR abs/1312.7715, 2013.
D. Lin, S. Fidler, and R. Urtasun. Holistic scene understanding for 3d object detection with rgbd cameras. In ICCV, 2013.
A. Karpathy, S. Miller, and Li Fei-Fei. Object discovery in 3d scenes via shape analysis. In ICRA, 2013.
D. Oneata, J. Revaud, J. Verbeek, and C. Schmid. Spatio-temporal object detection proposals. In ECCV, 2014.
J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Semantic segmentation with second-order pooling. In ECCV. 2012.
S. Fidler, R. Mottaghi, A. Yuille, and R. Urtasun. Bottom-up segmentation for top-down detection. In CVPR, 2013.
B. Alexe, T. Deselares, and V. Ferrari. Measuring the objectness of image windows. PAMI, 2012.
J. Hosang, R. Benenson, P. Dollar, and B. Schiele. What makes for effective detection proposals? arXiv:1502.05082, 2015.
S. Song and J. Xiao. Sliding shapes for 3d object detection in depth images. In ECCV. 2014.
M. Zia, M. Stark, and K. Schindler. Towards scene understanding with detailed 3d object representations. IJCV, 2015.
B. Pepik, M. Stark, P. Gehler, and B. Schiele. Multi-view and 3d deformable part models. PAMI, 2015.
E. Ohn-Bar and M. M. Trivedi. Learning to detect vehicles by clustering appearance patterns. IEEE Transactions on Intelligent Transportation Systems, 2015.
S. Wang, S. Fidler, and R. Urtasun. Holistic 3d scene understanding from a single geo-tagged image. In CVPR, 2015.
P. Dollar, R. Appel, S. Belongie, and P. Perona. Fast feature pyramids for object detection. PAMI, 2014.
K. Yamaguchi, D. McAllester, and R. Urtasun. Efficient joint segmentation, occlusion labeling, stereo and flow estimation. In ECCV, 2014.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support Vector Learning for Interdependent and Structured Output Spaces. In ICML, 2004.
A. Schwing, S. Fidler, M. Pollefeys, and R. Urtasun. Box in the box: Joint 3d layout and object reasoning from single images. In ICCV, 2013.
Ross Girshick. Fast R-CNN. In ICCV, 2015.
A. Geiger, C. Wojek, and R. Urtasun. Joint 3d estimation of objects and scene layout. In NIPS, 2011.
R. Benenson, M. Mathias, T. Tuytelaars, and L. Van Gool. Seeking the strongest rigid detector. In CVPR, 2013.
J. Yebes, L. Bergasa, R. Arroyo, and A. Lzaro. Supervised learning and evaluation of KITTIs cars detector with DPM. In IV, 2014.
B. Pepik, M. Stark, P. Gehler, and B. Schiele. Occlusion patterns for object class detection. In CVPR, 2013.
B. Li, T. Wu, and S. Zhu. Integrating context and occlusion for car detection by hierarchical and-or model. In ECCV, 2014.
J. Xu, S. Ramos, D. Vozquez, and A. Lopez. Hierarchical Adaptive Structural SVM for Domain Adaptation. In arXiv:1408.5400, 2014.
C. Premebida, J. Carreira, J. Batista, and U. Nunes. Pedestrian detection combining rgb and dense lidar data. In IROS, 2014.
J. Hosang, M. Omran, R. Benenson, and B. Schiele. Taking a deeper look at pedestrians. In arXiv, 2015.
S. Zhang, R. Benenson, and B. Schiele. Filtered channel features for pedestrian detection. In arXiv:1501.05759, 2015.
S. Paisitkriangkrai, C. Shen, and A. van den Hengel. Pedestrian detection with spatially pooled features and structured ensemble learning. In arXiv:1409.5209, 2014.
A. Gonzalez, G. Villalonga, J. Xu, D. Vazquez, J. Amores, and A. Lopez. Multiview random forest of local experts combining rgb and lidar data for pedestrian detection. In IV, 2015.  9
J. Langford and T. Zhang. The epoch-greedy algorithm for contextual multi-armed bandits. In Advances in Neural Information Processing Systems (NIPS), pages 817824, 2007.
T. Lu, D. Pal, and M. Pal. Contextual multi-armed bandits. In International Conference on Artificial Intelligence and Statistics, pages 485492, 2010.
L. Zhou. A survey on contextual multi-armed bandits. arXiv preprint arXiv:1508.03326, 2015.
P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2-3):235256, 2002.
L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to personalized news article recommendation. In ACM International Conference on World Wide Web (WWW), pages 661670, 2010.
A. Slivkins. Contextual bandits with similarity information. The Journal of Machine Learning Research, 15(1):25332568, 2014.
A. Agarwal, D. Hsu, S. Kale, J. Langford, L. Li, and R. E. Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine Learning (ICML), 2014.
P. Auer and R. Ortner. Logarithmic online regret bounds for undiscounted reinforcement learning. In Advances in Neural Information Processing Systems (NIPS), pages 4956, 2007.
A. Badanidiyuru, R. Kleinberg, and Y. Singer. Learning on a budget: posted price mechanisms for online procurement. In ACM Conference on Electronic Commerce, pages 128145, 2012.
T. L. Lai and O. Y.-W. Liao. Efficient adaptive randomization and stopping rules in multi-arm clinical trials for testing a new treatment. Sequential Analysis, 31(4):441457, 2012.
L. Tran-Thanh, A. C. Chapman, A. Rogers, and N. R. Jennings. Knapsack based optimal policies for budget-limited multi-armed bandits. In AAAI Conference on Artificial Intelligence, 2012.
A. Badanidiyuru, R. Kleinberg, and A. Slivkins. Bandits with knapsacks. In IEEE 54th Annual Symposium on Foundations of Computer Science (FOCS), pages 207216, 2013.
C. Jiang and R. Srikant. Bandits with budgets. In IEEE 52nd Annual Conference on Decision and Control (CDC), pages 53455350, 2013.
A. Slivkins. Dynamic ad allocation: Bandits with budgets. arXiv preprint arXiv:1306.0155, 2013.
Y. Xia, H. Li, T. Qin, N. Yu, and T.-Y. Liu. Thompson sampling for budgeted multi-armed bandits. In International Joint Conference on Artificial Intelligence, 2015.
R. Combes, C. Jiang, and R. Srikant. Bandits with budgets: Regret lower bounds and optimal algorithms. In ACM Sigmetrics, 2015.
A. Badanidiyuru, J. Langford, and A. Slivkins. Resourceful contextual bandits. In Conference on Learning Theory (COLT), 2014.
R. Combes, A. Proutiere, D. Yun, J. Ok, and Y. Yi. Optimal rate sampling in 802.11 systems. In IEEE INFOCOM, pages 27602767, 2014.
M. H. Veatch. Approximate linear programming for average cost MDPs. Mathematics of Operations Research, 38(3):535544, 2013.
S. Agrawal and N. R. Devanur. Bandits with concave rewards and convex knapsacks. In ACM Conference on Economics and Computation, pages 9891006. ACM, 2014.
S. Agrawal, N. R. Devanur, and L. Li. Contextual bandits with global constraints and objective. arXiv preprint arXiv:1506.03374, 2015.
S. Agrawal and N. R. Devanur. Linear contextual bandits with global constraints and objective. arXiv preprint arXiv:1507.06738, 2015.
D. P. Dubhashi and A. Panconesi. Concentration of measure for the analysis of randomized algorithms. Cambridge University Press, 2009.
A. Garivier and O. Cappe. The KL-UCB algorithm for bounded stochastic bandits and beyond. In Conference on Learning Theory (COLT), pages 359376, 2011.
D. Golovin and A. Krause. Dealing with partial feedback, 2009.
T. L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics, 6(1):422, 1985.  9
K. Asanovi and N. Morgan, Experimental determination of precision requirements for back-propagation training of artificial neural networks, International Computer Science Institute, Tech. Rep., 1991.
J. Ba and R. Caruana, Do deep nets really need to be deep? in Advances in Neural Information Processing Systems 27 (NIPS), 2014, pp. 26542662.
J. D. Caroll and J. J. Chang, Analysis of individual differences in multidimensional scaling via n-way generalization of Eckart-Young decomposition, Psychometrika, vol. 35, pp. 283319, 1970.
W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and Y. Chen, Compressing neural networks with the hashing trick, in International Conference on Machine Learning (ICML), 2015, pp. 22852294.
G. Cybenko, Approximation by superpositions of a sigmoidal function, Mathematics of control, signals and systems, pp. 303314, 1989.
M. Denil, B. Shakibi, L. Dinh, M. Ranzato, and N. de Freitas, Predicting parameters in deep learning, in Advances in Neural Information Processing Systems 26 (NIPS), 2013, pp. 21482156.
E. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus, Exploiting linear structure within convolutional networks for efficient evaluation, in Advances in Neural Information Processing Systems 27 (NIPS), 2014, pp. 12691277.
E. Gilboa, Y. Saati, and J. P. Cunningham, Scaling multidimensional inference for structured gaussian processes, arXiv preprint, no. 1209.4120, 2012.
Y. Gong, L. Liu, M. Yang, and L. Bourdev, Compressing deep convolutional networks using vector quantization, arXiv preprint, no. 1412.6115, 2014.
I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio, Maxout networks, in International Conference on Machine Learning (ICML), 2013, pp. 13191327.
W. Hackbusch and S. Kuhn, A new scheme for the tensor representation, J. Fourier Anal. Appl., vol. 15, pp. 706722, 2009.
A. Krizhevsky, Learning multiple layers of features from tiny images, Masters thesis, Computer Science Department, University of Toronto, 2009.
A. Krizhevsky, I. Sutskever, and G. E. Hinton, Imagenet classification with deep convolutional neural networks, in Advances in Neural Information Processing Systems 25 (NIPS), 2012, pp. 10971105.
V. Lebedev, Y. Ganin, M. Rakhuba, I. Oseledets, and V. Lempitsky, Speeding-up convolutional neural networks using fine-tuned CP-decomposition, in International Conference on Learning Representations (ICLR), 2014.
Y. LeCun, C. Cortes, and C. J. C. Burges, The MNIST database of handwritten digits, 1998.
A. Novikov, A. Rodomanov, A. Osokin, and D. Vetrov, Putting MRFs on a Tensor Train, in International Conference on Machine Learning (ICML), 2014, pp. 811819.
I. V. Oseledets, Tensor-Train decomposition, SIAM J. Scientific Computing, vol. 33, no. 5, pp. 2295 2317, 2011.
D. E. Rumelhart, G. E. Hinton, and R. J. Williams, Learning representations by back-propagating errors, Nature, vol. 323, no. 6088, pp. 533536, 1986.
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, Imagenet large scale visual recognition challenge, International Journal of Computer Vision (IJCV), 2015.
T. N. Sainath, B. Kingsbury, V. Sindhwani, E. Arisoy, and B. Ramabhadran, Low-rank matrix factorization for deep neural network training with high-dimensional output targets, in International Conference of Acoustics, Speech, and Signal Processing (ICASSP), 2013, pp. 66556659.
K. Simonyan and A. Zisserman, Very deep convolutional networks for large-scale image recognition, in International Conference on Learning Representations (ICLR), 2015.
J. Snoek, H. Larochelle, and R. P. Adams, Practical bayesian optimization of machine learning algorithms, in Advances in Neural Information Processing Systems 25 (NIPS), 2012, pp. 29512959.
L. R. Tucker, Some mathematical notes on three-mode factor analysis, Psychometrika, vol. 31, no. 3, pp. 279311, 1966.
A. Vedaldi and K. Lenc, Matconvnet  convolutional neural networks for MATLAB, in Proceeding of the ACM Int. Conf. on Multimedia.
J. Xue, J. Li, and Y. Gong, Restructuring of deep neural network acoustic models with singular value decomposition, in Interspeech, 2013, pp. 23652369.
Z. Yang, M. Moczulski, M. Denil, N. de Freitas, A. Smola, L. Song, and Z. Wang, Deep fried convnets, arXiv preprint, no. 1412.7149, 2014.
Z. Zhang, X. Yang, I. V. Oseledets, G. E. Karniadakis, and L. Daniel, Enabling high-dimensional hierarchical uncertainty quantification by ANOVA and tensor-train decomposition, Computer-Aided Design of Integrated Circuits and Systems, IEEE Transactions on, pp. 6376, 2014.  9
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), 2011.
Dougal Maclaurin and Ryan P Adams. Firefly Monte Carlo: Exact MCMC with subsets of data. Proceedings of the conference on Uncertainty in Artificial Intelligence (UAI), 2014.
Steven L Scott, Alexander W Blocker, Fernando V Bonassi, Hugh A Chipman, Edward I George, and Robert E McCulloch. Bayes and big data: The consensus Monte Carlo algorithm. In EFaBBayes 250 conference, volume 16, 2013.
Stanislav Minsker, Sanvesh Srivastava, Lizhen Lin, and David Dunson. Scalable and robust bayesian inference via the median posterior. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), 2014.
Sanvesh Srivastava, Volkan Cevher, Quoc Tran-Dinh, and David B Dunson. WASP: Scalable Bayes via barycenters of subset posteriors. In Proceedings of the 18th International Conference on Artificial Intelligence and Statistics (AISTATS), volume 38, 2015.
Willie Neiswanger, Chong Wang, and Eric Xing. Asymptotically exact, embarrassingly parallel MCMC. In Proceedings of the Thirtieth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-14), pages 623632, Corvallis, Oregon, 2014. AUAI Press.
Xiangyu Wang and David B Dunson. Parallel MCMC via Weierstrass sampler. arXiv preprint arXiv:1312.4605, 2013.
Linxi Liu and Wing Hung Wong. Multivariate density estimation based on adaptive partitioning: Convergence rate, variable selection and spatial adaptation. arXiv preprint arXiv:1401.2597, 2014.
Manuel Blum, Robert W Floyd, Vaughan Pratt, Ronald L Rivest, and Robert E Tarjan. Time bounds for selection. Journal of Computer and System Sciences, 7(4):448461, 1973.
Jon Louis Bentley. Multidimensional binary search trees used for associative searching. Communications of the ACM, 18(9):509517, 1975.
Leo Breiman. Random forests. Machine Learning, 45(1):532, 2001.
Leo Breiman. Bagging predictors. Machine Learning, 24(2):123140, 1996.
Xiaotong Shen and Wing Hung Wong. Convergence rate of sieve estimates. The Annals of Statistics, pages 580615, 1994.
Nils Lid Hjort and Ingrid K Glad. Nonparametric density estimation with a parametric start. The Annals of Statistics, pages 882904, 1995.
Heikki Haario, Marko Laine, Antonietta Mira, and Eero Saksman. DRAM: efficient adaptive MCMC. Statistics and Computing, 16(4):339354, 2006.
Heikki Haario, Eero Saksman, and Johanna Tamminen. An adaptive Metropolis algorithm. Bernoulli, pages 223242, 2001.
Jock A Blackard and Denis J Dean. Comparative accuracies of neural networks and discriminant analysis in predicting forest cover types from cartographic variables. In Proc. Second Southern Forestry GIS Conf, pages 189199, 1998.
Byron P Roe, Hai-Jun Yang, Ji Zhu, Yong Liu, Ion Stancu, and Gordon McGregor. Boosted decision trees as an alternative to artificial neural networks for particle identification. Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment, 543(2):577584, 2005.
M. Lichman. UCI machine learning repository, 2013.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), pages 267288, 1996.  9
J. Talairach and P. Tournoux. Co-planar stereotaxic atlas of the human brain. 3-Dimensional proportional system: an approach to cerebral imaging. Thieme, 1988.
J.D.G. Watson, R. Myers, et al. Area V5 of the human brain: evidence from a combined study using positron emission tomography and magnetic resonance imaging. Cereb. Cortex, 3:7994, 1993.
R. B. H. Tootell, J. B. Reppas, et al. Visual motion aftereffect in human cortical area MT revealed by functional magnetic resonance imaging. Nature, 375(6527):139141, 05 1995.
J. Mazziotta, A. Toga, et al. A probabilistic atlas and reference system for the human brain. Philosophical Transactions of the Royal Society B: Biological Sciences, 356(1412):12931322, 2001.
B. Fischl, M. I. Sereno, R.B.H. Tootell, and A. M. Dale. High-resolution intersubject averaging and a coordinate system for the cortical surface. Human brain mapping, 8(4):272284, 1999.
M. Brett, I. S. Johnsrude, and A. M. Owen. The problem of functional localization in the human brain. Nat Rev Neurosci, 3(3):243249, 03 2002.
M. R. Sabuncu, B. D. Singer, B. Conroy, R. E. Bryan, P. J. Ramadge, and J. V. Haxby. Function-based intersubject alignment of human cortical anatomy. Cerebral Cortex, 20(1):130140, 2010.
B. R. Conroy, B. D. Singer, J. V. Haxby, and P. J. Ramadge. fMRI-based inter-subject cortical alignment using functional connectivity. In Advances in Neural Information Processing Systems, 2009.
B. R. Conroy, B. D. Singer, J. S. Guntupalli, P. J. Ramadge, and J. V. Haxby. Inter-subject alignment of human cortical anatomy using functional connectivity. NeuroImage, 2013.
J. V. Haxby, J. S. Guntupalli, et al. A common, high-dimensional model of the representational space in human ventral temporal cortex. Neuron, 72(2):404416, 2011.
A. Lorbert and P. J. Ramadge. Kernel hyperalignment. In Adv. in Neural Inform. Proc. Systems, 2012.
A. G. Huth, T. L. Griffiths, F. E. Theunissen, and J. L. Gallant. PrAGMATiC: a Probabilistic and Generative Model of Areas Tiling the Cortex. ArXiv 1504.03622, 2015.
R. A. Horn and C. R. Johnson. Matrix analysis. Cambridge university press, 2012.
A. Edelman, T. A. Arias, and S. T Smith. The geometry of algorithms with orthogonality constraints. SIAM journal on Matrix Analysis and Applications, 20(2):303353, 1998.
J.-H. Ahn and J.-H. Oh. A constrained EM algorithm for principal component analysis. Neural Computation, 15(1):5765, 2003.
J. R. Manning, R. Ranganath, K. A. Norman, and D. M. Blei. Topographic factor analysis: a bayesian model for inferring brain networks from neural data. PLoS One, 9(5):e94914, 2014.
A. M. Michael, M. Anderson, et al. Preserving subject variability in group fMRI analysis: performance evaluation of GICA vs. IVA. Frontiers in systems neuroscience, 8, 2014.
H. Xu, A. Lorbert, P. J. Ramadge, J. S. Guntupalli, and J. V. Haxby. Regularized hyperalignment of multi-set fMRI data. In Proc. Statistical Signal Processing Workshop, pages 229232. IEEE, 2012.
H. Hotelling. Relations between two sets of variates. Biometrika, 28(3-4):321377, 1936.
A. Hyvrinen, J. Karhunen, and E. Oja. Independent component analysis. John Wiley & Sons, 2004.
J. Chen, Y. C. Leong, K. A. Norman, and U. Hasson. Reinstatement of neural patterns during narrative free recall. Abstracts of the Cognitive Neuroscience Society, 2014.
D. S. Margulies, J. L. Vincent, and et al. Precuneus shares intrinsic functional architecture in humans and monkeys. Proceedings of the National Academy of Sciences, 106(47):2006920074, 2009.
J. V. Haxby, M. I. Gobbini, M. L. Furey, A. Ishai, J. L. Schouten, and P. Pietrini. Distributed and overlapping representations of faces and objects in ventral temporal cortex. Science, 293(5539), 2001.
M. Hanke, F. J. Baumgartner, et al. A high-resolution 7-Tesla fMRI dataset from complex natural stimulation with an audio movie. Scientific Data, 1, 2014.
T. D. Griffiths and J. D. Warren. The planum temporale as a computational hub. Trends in neurosciences, 25(7):348353, 2002.
Y. Yeshurun, S. Swanson, J. Chen, E. Simony, C. Honey, P. C. Lazaridi, and U. Hasson. How does the brain represent different ways of understanding the same story? Society for Neuroscience Abstracts, 2014.
M. E. Raichle. The brains default mode network. Annual Review of Neuroscience, 38(1), 2015.
D. L. Ames, C. J. Honey, M. Chow, A. Todorov, and U. Hasson. Contextual alignment of cognitive and neural dynamics. Journal of cognitive neuroscience, 2014.
M.T. deBettencourt, J. D. Cohen, R. F. Lee, K. A. Norman, and N. B. Turk-Browne. Closed-loop training of attention with real-time brain imaging. Nature neuroscience, 18(3):470475, 2015.  9
Anima Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. CoRR, abs/1210.7559, 2012.
Animashree Anandkumar, Daniel Hsu, and Sham M. Kakade. A method of moments for mixture models and hidden Markov models. CoRR, abs/1203.0683, 2012.
B. Balle, X. Carreras, F. Luque, and A. Quattoni. Spectral learning of weighted automata - A forward-backward perspective. Machine Learning, 96(1-2), 2014.
B. Balle, W. L. Hamilton, and J. Pineau. Methods of moments for learning stochastic languages: Unied presentation and empirical comparison. In ICML, pages 13861394, 2014.
Jacob Biesinger, Yuanfeng Wang, and Xiaohui Xie. Discovering and mapping chromatin states using a tree hidden Markov model. BMC Bioinformatics, 14(Suppl 5):S4, 2013.
A. Chaganty and P. Liang. Estimating latent-variable graphical models using moments and likelihoods. In ICML, 2014.
ENCODE Project Consortium. An integrated encyclopedia of DNA elements in the human genome. Nature, 489:5774, 2012.
Jason Ernst and Manolis Kellis. Discovery and characterization of chromatin states for systematic annotation of the human genome. Nature Biotechnology, 28(8):817825, 2010.
Bernstein et. al. The NIH Roadmap Epigenomics Mapping Consortium. Nature Biotechnology, 28:10451048, 2010.
Creyghton et. al. Histone H3K27ac separates active from poised enhancers and predicts developmental state. Proc Natl Acad Sci, 107(50):2193121936, 2010.
Ernst et. al. Mapping and analysis of chromatin state dynamics in nine human cell types. Nature, 473:4349, 2011.
Jun Zhu et al. Characterizing dynamic changes in the human blood transcriptional network. PLoS Comput Biol, 6:e1000671, 2010.
M. Hoffman et al. Unsupervised pattern discovery in human chromatin structure through genomic segmentation. Nature Methods, 9(5):473476, 2012.
S. Djebali et al. Landscape of transcription in human cells. Nature, 2012.
D. Foster, J. Rodu, and L. Ungar. Spectral dimensionality reduction for HMMs. In CoRR, 2012.
N. Foti, J. Xu, D. Laird, and E. Fox. Stochastic variational inference for hidden markov models. In NIPS, 2014.
N. Halko, P. Martinsson, and J. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Review, 53, 2011.
D. Hsu, S. Kakade, and T. Zhang. A spectral algorithm for learning hidden Markov models. In COLT, 2009.
I. Melnyk and A. Banerjee. A spectral algorithm for inference in hidden semi-Markov models. In AISTATS, 2015.
E. Mossel and S Roch. Learning non-singular phylogenies and hidden Markov models. Ann. Appl. Probab., 16(2), 05 2006.
A. Parikh, L. Song, and E. P. Xing. A spectral algorithm for latent tree graphical models. In ICML, pages 10651072, 2011.
A. P. Parikh, L. Song, M. Ishteva, G. Teodoru, and E. P. Xing. A spectral algorithm for latent junction trees. In UAI, 2012.
S. Siddiqi, B. Boots, and G. Gordon. Reduced-rank hidden Markov models. In AISTATS, 2010.
J. Song and K. C. Chen. Spectacle: fast chromatin state annotation using spectral learning. Genome Biology, 16:33, 2015.
L. Song, M. Ishteva, A. P. Parikh, E. P. Xing, and H. Park. Hierarchical tensor decomposition of latent tree graphical models. In ICML, 2013.
J. Zou, D. Hsu, D. Parkes, and R. Adams. Contrastive learning using spectral methods. In NIPS, 2013. 9
Ekhlas Sonu and Prashant Doshi. Scalable solutions of interactive POMDPs using generalized and bounded policy iteration. Journal of Autonomous Agents and Multi-Agent Systems, pages DOI: 10.1007/s1045801492615, in press, 2014.
Hagai Attias. Planning by probabilistic inference. In Ninth International Workshop on AI and Statistics (AISTATS), 2003.
Marc Toussaint and Amos J. Storkey. Probabilistic inference for solving discrete and continuous state markov decision processes. In International Conference on Machine Learning (ICML), pages 945952, 2006.
Jeffrey A. Fessler and Alfred O. Hero. Space-alternating generalized expectationmaximization algorithm. IEEE Transactions on Signal Processing, 42:26642677, 1994.
P. Tseng. Convergence of block coordinate descent method for nondifferentiable minimization. Journal of Optimization Theory and Applications, 109:475494, 2001.
Feng Wu, Shlomo Zilberstein, and Nicholas R. Jennings. Monte-carlo expectation maximization for decentralized POMDPs. In Twenty-Third International Joint Conference on Articial Intelligence (IJCAI), pages 397403, 2013.
Piotr J. Gmytrasiewicz and Prashant Doshi. A framework for sequential planning in multiagent settings. Journal of Articial Intelligence Research, 24:4979, 2005.
Yifeng Zeng and Prashant Doshi. Exploiting model equivalences for solving interactive dynamic inuence diagrams. Journal of Articial Intelligence Research, 43:211255, 2012.
Akshat Kumar and Shlomo Zilberstein. Anytime planning for decentralized pomdps using expectation maximization. In Conference on Uncertainty in AI (UAI), pages 294301, 2010.
Ankan Saha and Ambuj Tewari. On the nonasymptotic convergence of cyclic coordinate descent methods. SIAM Journal on Optimization, 23(1):576601, 2013.
C. K. Carter and R. Kohn. Markov chainmonte carlo in conditionally gaussian state space models. Biometrika, 83:589601, 1996.
Marc Toussaint, Laurent Charlin, and Pascal Poupart. Hierarchical POMDP controller optimization by likelihood maximization. In Twenty-Fourth Conference on Uncertainty in Articial Intelligence (UAI), pages 562570, 2008.
Prashant Doshi and Piotr J. Gmytrasiewicz. Monte Carlo sampling methods for approximating interactive POMDPs. Journal of Articial Intelligence Research, 34:297337, 2009.
Brenda Ng, Carol Meyers, Ko Boakye, and John Nitao. Towards applying interactive POMDPs to real-world adversary modeling. In Innovative Applications in Articial Intelligence (IAAI), pages 18141820, 2010.
Ekhlas Sonu, Yingke Chen, and Prashant Doshi. Individual planning in agent populations: Anonymity and frame-action hypergraphs. In International Conference on Automated Planning and Scheduling (ICAPS), pages 202211, 2015.
C. F. Jeff Wu. On the convergence properties of the em algorithm. Annals of Statistics, 11(1):95103, 1983.
Akshat Kumar, Shlomo Zilberstein, and Marc Toussaint. Scalable multiagent planning using probabilistic inference. In International Joint Conference on Articial Intelligence (IJCAI), pages 21402146, 2011.
S. Arimoto. An algorithm for computing the capacity of arbitrary discrete memoryless channels. IEEE Transactions on Information Theory, 18(1):1420, 1972.
Jeffrey A. Fessler and Donghwan Kim. Axial block coordinate descent (ABCD) algorithm for X-ray CT image reconstruction. In International Meeting on Fully Three-dimensional Image Reconstruction in Radiology and Nuclear Medicine, volume 11, pages 262265, 2011.
Olivier Cappe and Eric Moulines. Online expectation-maximization algorithm for latent data models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71(3):593613, 2009.  9
D. M. Titterington, A. F. Smith, and U. E. Makov. Statistical analysis of finite mixture distributions, volume 7. Wiley New York, 1985.
G. McLachlan and D. Peel. Finite mixture models. John Wiley & Sons, 2004.
K. Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions of the Royal Society of London. A, 185:71110, 1894.
A. Anandkumar, D. Hsu, and S. M. Kakade. A method of moments for mixture models and hidden Markov models. In Conference on Learning Theory (COLT), 2012.
A. Anandkumar, D. P. Foster, D. Hsu, S. M. Kakade, and Y. Liu. Two SVDs suffice: Spectral decompositions for probabilistic topic modeling and latent Dirichlet allocation. In Advances in Neural Information Processing Systems (NIPS), 2012.
A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky. Tensor decompositions for learning latent variable models. arXiv, 2013.
D. Hsu, S. M. Kakade, and P. Liang. Identifiability and unmixing of latent parse trees. In Advances in Neural Information Processing Systems (NIPS), 2012.
D. Hsu and S. M. Kakade. Learning mixtures of spherical Gaussians: Moment methods and spectral decompositions. In Innovations in Theoretical Computer Science (ITCS), 2013.
A. Chaganty and P. Liang. Spectral experts for estimating mixtures of linear regressions. In International Conference on Machine Learning (ICML), 2013.
A. T. Kalai, A. Moitra, and G. Valiant. Efficiently learning mixtures of two Gaussians. In Symposium on Theory of Computing (STOC), pages 553562, 2010.
M. Hardt and E. Price. Sharp bounds for learning a mixture of two Gaussians. arXiv preprint arXiv:1404.4997, 2014.
R. Ge, Q. Huang, and S. M. Kakade. Learning mixtures of Gaussians in high dimensions. arXiv preprint arXiv:1503.00424, 2015.
B. Balle, X. Carreras, F. M. Luque, and A. Quattoni. Spectral learning of weighted automata - A forwardbackward perspective. Machine Learning, 96(1):3363, 2014.
J. B. Lasserre. Moments, Positive Polynomials and Their Applications. Imperial College Press, 2011.
J. B. Lasserre. A semidefinite programming approach to the generalized problem of moments. Mathematical Programming, 112(1):6592, 2008.
H. J. Stetter. Multivariate polynomial equations as matrix eigenproblems. WSSIA, 2:355371, 1993.
H. M. Moller and H. J. Stetter. Multivariate polynomial equations with multiple zeros solved by matrix eigenproblems. Numerische Mathematik, 70(3):311329, 1995.
B. Sturmfels. Solving systems of polynomial equations. American Mathematical Society, 2002.
D. Henrion and J. Lasserre. Detecting global optimality and extracting solutions in GloptiPoly. In Positive polynomials in control, pages 293310, 2005.
A. Anandkumar, R. Ge, D. Hsu, and S. Kakade. A tensor spectral approach to learning mixed membership community models. In Conference on Learning Theory (COLT), pages 867881, 2013.
A. Anandkumar, R. Ge, and M. Janzamin. Provable learning of overcomplete latent variable models: Semi-supervised and unsupervised settings. arXiv preprint arXiv:1408.0553, 2014.
K. Viele and B. Tong. Modeling with mixtures of linear regressions. Statistics and Computing, 12(4):315 330, 2002.
B. Sturmfels. Algorithms in invariant theory. Springer Science & Business Media, 2008.
R. M. Corless, K. Gatermann, and I. S. Kotsireas. Using symmetries in the eigenvalue method for polynomial systems. Journal of Symbolic Computation, 44(11):15361550, 2009.
R. E. Curto and L. A. Fialkow. Solution of the truncated complex moment problem for flat data, volume 568. American Mathematical Society, 1996 1996.
J. B. Lasserre. Global optimization with polynomials and the problem of moments. SIAM Journal on Optimization, 11(3):796817, 2001.
M. Laurent. Sums of squares, moment matrices and optimization over polynomials. In Emerging applications of algebraic geometry, pages 157270, 2009.
P. A. Parrilo and B. Sturmfels. Minimizing polynomial functions. Algorithmic and quantitative real algebraic geometry, DIMACS Series in Discrete Mathematics and Theoretical Computer Science, 60:83 99, 2003.
P. A. Parrilo. Semidefinite programming relaxations for semialgebraic problems. Mathematical programming, 96(2):293320, 2003.
N. Ozay, M. Sznaier, C. M. Lagoa, and O. I. Camps. GPCA with denoising: A moments-based convex approach. In Computer Vision and Pattern Recognition (CVPR), pages 32093216, 2010.  9
S. D. Ahipaaoglu, P. Sun, and M. Todd. Linear convergence of a modified Frank-Wolfe algorithm for computing minimum-volume enclosing ellipsoids. Optimization Methods and Software, 23(1):519, 2008.
R. Alexander. The width and diameter of a simplex. Geometriae Dedicata, 6(1):8794, 1977.
F. Bach. Learning with submodular functions: A convex optimization perspective. Foundations and Trends in Machine Learning, 6(2-3):145373, 2013.
A. Beck and S. Shtern. Linearly convergent away-step conditional gradient for non-strongly convex functions. arXiv:1504.05002v1, 2015.
A. Beck and M. Teboulle. A conditional gradient method with linear rate of convergence for solving convex linear systems. Mathematical Methods of Operations Research (ZOR), 59(2):235247, 2004.
M. D. Canon and C. D. Cullum. A tight upper bound on the rate of convergence of Frank-Wolfe algorithm. SIAM Journal on Control, 6(4):509516, 1968.
V. Chari et al. On pairwise costs for network flow multi-object tracking. In CVPR, 2015.
J. C. Dunn. Rates of convergence for conditional gradient algorithms near singular and nonsingular extremals. SIAM Journal on Control and Optimization, 17(2):187211, 1979.
M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval Research Logistics Quarterly, 3:95110, 1956.
D. Garber and E. Hazan. A linearly convergent conditional gradient algorithm with applications to online and stochastic optimization. arXiv:1301.4666v5, 2013.
D. Garber and E. Hazan. Faster rates for the Frank-Wolfe method over strongly-convex sets. In ICML, 2015.
J. Guelat and P. Marcotte. Some comments on Wolfes away step. Mathematical Programming, 1986.
D. Hearn, S. Lawphongpanich, and J. Ventura. Restricted simplicial decomposition: Computation and extensions. In Computation Mathematical Programming, volume 31, pages 99118. Springer, 1987.
C. A. Holloway. An extension of the Frank and Wolfe method of feasible directions. Mathematical Programming, 6(1):1427, 1974.
M. Jaggi. Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In ICML, 2013.
A. Joulin, K. Tang, and L. Fei-Fei. Efficient image and video co-localization with Frank-Wolfe algorithm. In ECCV, 2014.
V. Kolmogorov and R. Zabin. What energy functions can be minimized via graph cuts? IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(2):147159, 2004.
R. G. Krishnan, S. Lacoste-Julien, and D. Sontag. Barrier Frank-Wolfe for marginal inference. In NIPS, 2015.
P. Kumar and E. A. Yildirim. A linearly convergent linear-time first-order algorithm for support vector classification with a core set result. INFORMS Journal on Computing, 2010.
S. Lacoste-Julien and M. Jaggi. An affine invariant linear convergence analysis for Frank-Wolfe algorithms. arXiv:1312.7864v2, 2013.
S. Lacoste-Julien, M. Jaggi, M. Schmidt, and P. Pletscher. Block-coordinate Frank-Wolfe optimization for structural SVMs. In ICML, 2013.
G. Lan. The complexity of large-scale convex programming under a linear optimization oracle. arXiv:1309.5550v2, 2013.
E. S. Levitin and B. T. Polyak. Constrained minimization methods. USSR Computational Mathematics and Mathematical Physics, 6(5):787823, Jan. 1966.
H. Lin, J. Mairal, and Z. Harchaoui. A universal catalyst for first-order optimization. In NIPS, 2015.
B. Mitchell, V. F. Demyanov, and V. Malozemov. Finding the point of a polyhedron closest to the origin. SIAM Journal on Control, 12(1), 1974.
R. Nanculef, E. Frandi, C. Sartori, and H. Allende. A novel Frank-Wolfe algorithm. Analysis and applications to large-scale SVM training. Information Sciences, 2014.
Y. Nesterov. Introductory Lectures on Convex Optimization. Kluwer Academic Publishers, 2004.
J. Pena, D. Rodriguez, and N. Soheili. On the von Neumann and Frank-Wolfe algorithms with away steps. arXiv:1507.04073v2, 2015.
J. C. Platt. Fast training of support vector machines using sequential minimal optimization. In Advances in kernel methods: support vector learning, pages 185208. 1999.
S. M. Robinson. Generalized Equations and their Solutions, Part II: Applications to Nonlinear Programming. Springer, 1982.
B. Von Hohenbalken. Simplicial decomposition in nonlinear programming algorithms. Mathematical Programming, 13(1):4968, 1977.
M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1(12):1305, 2008.
P.-W. Wang and C.-J. Lin. Iteration complexity of feasible descent methods for convex optimization. Journal of Machine Learning Research, 15:15231548, 2014.
P. Wolfe. Convergence theory in nonlinear programming. In Integer and Nonlinear Programming. 1970.
P. Wolfe. Finding the nearest point in a polytope. Mathematical Programming, 11(1):128149, 1976.
G. M. Ziegler. Lectures on 0/1-polytopes. arXiv:math/9909177v1, 1999.  9
Khan academy privacy notice https://www.khanacademy.org/about/privacy-policy, 2015.
BARANIUK , R. Compressive sensing. IEEE signal processing magazine 24, 4 (2007).
C EN , H., KOEDINGER , K., AND J UNKER , B. Learning factors analysisa general method for cognitive model evaluation and improvement. In Intelligent tutoring systems (2006), Springer, pp. 164175.
C OHEN , G. L., AND G ARCIA , J. Identity, belonging, and achievement a model, interventions, implications. Current Directions in Psychological Science 17, 6 (2008), 365369.
C ORBETT, A. Cognitive computer tutors: Solving the two-sigma problem. In User Modeling 2001. Springer, 2001, pp. 137147.
C ORBETT, A. T., AND A NDERSON , J. R. Knowledge tracing: Modeling the acquisition of procedural knowledge. User modeling and user-adapted interaction 4, 4 (1994), 253278.
D BAKER , R. S. J., C ORBETT, A. T., AND A LEVEN , V. More accurate student modeling through contextual estimation of slip and guess probabilities in bayesian knowledge tracing. In Intelligent Tutoring Systems (2008), Springer, pp. 406415.  8
D BAKER , R. S. J., PARDOS , Z. A., G OWDA , S. M., N OORAEI , B. B., AND H EFFERNAN , N. T. Ensembling predictions of student knowledge within intelligent tutoring systems. In User Modeling, Adaption and Personalization. Springer, 2011, pp. 1324. D RASGOW, F., AND H ULIN , C. L. Item response theory. Handbook of industrial and organizational psychology 1 (1990), 577636. E LLIOT, A. J., AND DWECK , C. S. Handbook of competence and motivation. Guilford Publications, 2013. F ENG , M., H EFFERNAN , N., AND KOEDINGER , K. Addressing the assessment challenge with an online system that tutors as it assesses. User Modeling and User-Adapted Interaction 19, 3 (2009), 243266. F ITCH , W. T., H AUSER , M. D., AND C HOMSKY, N. The evolution of the language faculty: clarifications and implications. Cognition 97, 2 (2005), 179210. G ENTNER , D. Structure-mapping: A theoretical framework for analogy. Cognitive science 7, 2. G ONG , Y., B ECK , J. E., AND H EFFERNAN , N. T. In Intelligent Tutoring Systems, Springer. G RAVES , A., M OHAMED , A.-R., AND H INTON , G. Speech recognition with deep recurrent neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on (2013), IEEE, pp. 66456649. H OCHREITER , S., AND S CHMIDHUBER , J. Long short-term memory. Neural computation 9, 8. K ARPATHY, A., AND F EI -F EI , L. Deep visual-semantic alignments for generating image descriptions. arXiv preprint arXiv:1412.2306 (2014). K HAJAH , M., W ING , R. M., L INDSEY, R. V., AND M OZER , M. C. Incorporating latent factors into knowledge tracing to predict individual differences in learning. Proceedings of the 7th International Conference on Educational Data Mining (2014). K HAJAH , M. M., H UANG , Y., G ONZ ALEZ -B RENES , J. P., M OZER , M. C., AND B RUSILOVSKY, P. Integrating knowledge tracing and item response theory: A tale of two frameworks. Proceedings of the 4th International Workshop on Personalization Approaches in Learning Environments (2014). L AN , A. S., S TUDER , C., AND BARANIUK , R. G. Time-varying learning and content analytics via sparse factor analysis. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining (2014), ACM, pp. 452461. L INNENBRINK , E. A., AND P INTRICH , P. R. Role of affect in cognitive processing in academic contexts. Motivation, emotion, and cognition: Integrative perspectives on intellectual functioning and development (2004), 5787. M IKOLOV, T., K ARAFI AT, M., B URGET, L., C ERNOCK Y , J., AND K HUDANPUR , S. Recurrent neural network based language model. In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, 2010 (2010), pp. 10451048. PARDOS , Z. A., AND H EFFERNAN , N. T. Kt-idem: Introducing item difficulty to the knowledge tracing model. In User Modeling, Adaption and Personalization. Springer, 2011, pp. 243254. PAVLIK J R , P. I., C EN , H., AND KOEDINGER , K. R. Performance Factors AnalysisA New Alternative to Knowledge Tracing. Online Submission (2009). P IECH , C., H UANG , J., N GUYEN , A., P HULSUKSOMBATI , M., S AHAMI , M., AND G UIBAS , L. J. Learning program embeddings to propagate feedback on student code. CoRR abs/1505.05969 (2015). P IECH , C., S AHAMI , M., H UANG , J., AND G UIBAS , L. Autonomously generating hints by inferring problem solving policies. In Proceedings of the Second (2015) ACM Conference on Learning @ Scale (New York, NY, USA, 2015), L@S 15, ACM, pp. 195204. P IECH , C., S AHAMI , M., KOLLER , D., C OOPER , S., AND B LIKSTEIN , P. Modeling how students learn to program. In Proceedings of the 43rd ACM symposium on Computer Science Education. P OLSON , M. C., AND R ICHARDSON , J. J. Foundations of intelligent tutoring systems. Psychology Press, 2013. R AFFERTY, A. N., B RUNSKILL , E., G RIFFITHS , T. L., AND S HAFTO , P. Faster teaching by POMDP planning. In Artificial intelligence in education (2011), Springer, pp. 280287. ROHRER , D. The effects of spacing and mixing practice problems. Journal for Research in Mathematics Education (2009), 417. S CHRAAGEN , J. M., C HIPMAN , S. F., AND S HALIN , V. L. Cognitive task analysis. Psychology Press, 2000. W ILLIAMS , R. J., AND Z IPSER , D. A learning algorithm for continually running fully recurrent neural networks. Neural computation 1, 2 (1989), 270280. Y UDELSON , M. V., KOEDINGER , K. R., AND G ORDON , G. J. Individualized bayesian knowledge tracing models. In Artificial Intelligence in Education (2013), Springer, pp. 171180.  9
D.M. Blei, A.Y. Ng, and M.I. Jordan. Latent Dirichlet allocation. J. Mach. Learn. Res., 3:9031022, 2003.
T. Griffiths. Gibbs sampling in the generative model of latent Dirichlet allocation. Technical report, Stanford University, 2002.
A. Anandkumar, D.P. Foster, D. Hsu, S.M. Kakade, and Y.-K. Liu. A spectral algorithm for latent Dirichlet allocation. In NIPS, 2012.
A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky. Tensor decompositions for learning latent variable models. J. Mach. Learn. Res., 15:27732832, 2014.
P. Comon and C. Jutten, editors. Handbook of blind sourse separation: independent component analysis and applications. Academic Press, 2010.
C. Jutten. Calcul neuromimetique et traitement du signal: analyse en composantes independantes. PhD thesis, INP-USM Grenoble, 1987.
C. Jutten and J. Herault. Blind separation of sources, part I: an adaptive algorithm based on neuromimetric architecture. Signal Process., 24:110, 1991.
P. Comon. Independent component analysis, a new concept? Signal Process., 36:287314, 1994.
W.L. Buntine. Variational extensions to EM and multinomial PCA. In ECML, 2002.
M.E. Tipping and C.M. Bishop. Probabilistic principal component analysis. J. R. Stat. Soc., 61:611622, 1999.
S. Roweis. EM algorithms for PCA and SPCA. In NIPS, 1998.
J. Canny. GaP: a factor model for discrete data. In SIGIR, 2004.
W.L. Buntine and A. Jakulin. Applying discrete PCA in data analysis. In UAI, 2004.
S. Boucheron, G. Lugosi, and P. Massart. Concentration inequalities: a nonasymptotic theory of independence. Oxford University Press, 2013.
A. Anandkumar, D.P. Foster, D. Hsu, S.M. Kakade, and Y.-K. Liu. A spectral algorithm for latent Dirichlet allocation. CoRR, abs:1204.6703, 2013.
H.M. Wallach, I. Murray, R. Salakhutdinov, and D. Mimno. Evaluation methods for topic models. In ICML, 2009.
J.-F. Cardoso. Source separation using higher order moments. In ICASSP, 1989.
J.-F. Cardoso. Eigen-structure of the fourth-order cumulant tensor with application to the blind source separation problem. In ICASSP, 1990.
J.-F. Cardoso and P. Comon. Independent component analysis, a survey of some algebraic methods. In ISCAS, 1996.
A. Hyvarinen. Fast and robust fixed-point algorithms for independent component analysis. IEEE Trans. Neural Netw., 10(3):626634, 1999.
J.-F. Cardoso and A. Souloumiac. Blind beamforming for non Gaussian signals. In IEE Proceedings-F, 1993.
J.-F. Cardoso. High-order contrasts for independent component analysis. Neural Comput., 11:157192, 1999.
J.-F. Cardoso and A. Souloumiac. Jacobi angles for simultaneous diagonalization. SIAM J. Mat. Anal. Appl., 17(1):161164, 1996.
A. Bunse-Gerstner, R. Byers, and V. Mehrmann. Numerical methods for simulataneous diagonalization. SIAM J. Matrix Anal. Appl., 14(4):927949, 1993.
J. Nocedal and S.J. Wright. Numerical optimization. Springer, 2nd edition, 2006.
F.R. Bach and M.I. Jordan. Kernel independent component analysis. J. Mach. Learn. Res., 3:148, 2002.
V. Kuleshov, A.T. Chaganty, and P. Liang. Tensor factorization via matrix factorization. In AISTATS, 2015.
A. Globerson, G. Chechik, F. Pereira, and N. Tishby. Euclidean embedding of co-occurrence data. J. Mach. Learn. Res., 8:22652295, 2007.
S. Arora, R. Ge, Y. Halpern, D. Mimno, A. Moitra, D. Sontag, Y. Wu, and M. Zhu. A practical algorithm for topic modeling with provable guarantees. In ICML, 2013.
S. Cohen and M. Collins. A provably correct learning algorithm for latent-variable PCFGs. In ACL, 2014.  9
Jacopo Bertolotti, Elbert G. van Putten, Christian Blum, Ad Lagendijk, Willem L. Vos, and Allard P. Mosk. Non-invasive imaging through opaque scattering layers. Nature, 491(7423):232234, Nov. 2012.
Antoine Liutkus, David Martina, Sbastien Popoff, Gilles Chardon, Ori Katz, Geoffroy Lerosey, Sylvain Gigan, Laurent Daudet, and Igor Carron. Imaging with nature: Compressive imaging using a multiply scattering medium. Scientific Reports, volume 4, article no. 5552, Jul. 2014.
Emmanuel J. Cands, Thomas Strohmer, and Vladislav Voroninski. PhaseLift: Exact and stable signal recovery from magnitude measurements via convex programming. Communications on Pure and Applied Mathematics, 66(8):12411274, 2013.
Emmanuel J. Cands and Xiaodong Li. Solving quadratic equations via PhaseLift when there are about as many equations as unknowns. Foundations of Computational Mathematics, 14(5):10171026, 2014.
R. Kueng, H. Rauhut, and U. Terstiege. Low rank matrix recovery from rank one measurements. Applied and Computational Harmonic Analysis, 2015. In press. Preprint arXiv:1410.6913
Joel A. Tropp. Convex recovery of a structured signal from independent random linear measurements. Preprint arXiv:1405.1102
Irne Waldspurger, Alexandre dAspremont, and Stphane Mallat. Phase recovery, MaxCut and complex semidefinite programming. Mathematical Programming, 149(1-2):4781, 2015.
Matthew L. Moravec, Justin K. Romberg, and Richard G. Baraniuk. Compressive phase retrieval. In Proceedings of SPIE Wavelets XII, volume 6701, pages 670120 111, 2007.
Yoav Shechtman, Yonina C. Eldar, Alexander Szameit, and Mordechai Segev. Sparsity based subwavelength imaging with partially incoherent light via quadratic compressed sensing. Optics Express, 19(16):1480714822, Aug. 2011.
Yoav Shechtman, Amir Beck, and Yonina C. Eldar. GESPAR: Efficient phase retrieval of sparse signals. Signal Processing, IEEE Transactions on, 62(4):928938, Feb. 2014.
Praneeth Netrapalli, Prateek Jain, and Sujay Sanghavi. Phase retrieval using alternating minimization. In Advances in Neural Information Processing Systems 26 (NIPS 2013), pages 27962804, 2013.
Xiaodong Li and Vladislav Voroninski. Sparse signal recovery from quadratic measurements via convex programming. SIAM Journal on Mathematical Analysis, 45(5):30193033, 2013.
Henrik Ohlsson, Allen Yang, Roy Dong, and Shankar Sastry. CPRLan extension of compressive sensing to the phase retrieval problem. In Advances in Neural Information Processing Systems 25 (NIPS 2012), pages 13671375, 2012.
David Gross. Recovering low-rank matrices from few coefficients in any basis. Information Theory, IEEE Transactions on, 57(3):15481566, Mar. 2011.
Samet Oymak, Amin Jalali, Maryam Fazel, Yonina Eldar, and Babak Hassibi. Simultaneously structured models with application to sparse and low-rank matrices. Information Theory, IEEE Transactions on, 61(5):28862908, 2015.
P. Schniter and S. Rangan. Compressive phase retrieval via generalized approximate message passing. Signal Processing, IEEE Transactions on, 63(4):10431055, February 2015.
Ramtin Pedarsani, Kangwook Lee, and Kannan Ramchandran. Phasecode: Fast and efficient compressive phase retrieval based on sparse-graph codes. In Communication, Control, and Computing (Allerton), 52nd Annual Allerton Conference on, pages 842849, Sep. 2014. Extended preprint arXiv:1408.0034
Mark Iwen, Aditya Viswanathan, and Yang Wang. Robust sparse phase retrieval made easy. Applied and Computational Harmonic Analysis, 2015. In press. Preprint arXiv:1410.5295
Emmanuel J. Cands, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via Wirtinger flow: Theory and algorithms. Information Theory, IEEE Transactions on, 61(4):19852007, Apr. 2015.
Thomas Blumensath and Mike E. Davies. Iterative hard thresholding for compressed sensing. Applied and Computational Harmonic Analysis, 27(3):265274, 2009.
Deanna Needell and Joel A. Tropp. CoSaMP: Iterative signal recovery from incomplete and inaccurate samples. Applied and Computational Harmonic Analysis, 26(3):301321, 2009.
Emmanuel J. Cands. The restricted isometry property and its implications for compressed sensing. Comptes Rendus Mathematique, 346(9-10):589592, 2008.
Richard Baraniuk, Mark Davenport, Ronald DeVore, and Michael Wakin. A simple proof of the restricted isometry property for random matrices. Constructive Approximation, 28(3):253263, 2008.
Stephen R. Becker, Emmanuel J. Cands, and Michael C. Grant. Templates for convex cone problems with applications to sparse signal recovery. Mathematical Programming Computation, 3(3):165218, 2011.  9
D. Allouche, S. de Givry, and T. Schiex. Toulbar2, an open source exact cost function network solver, 2010.
B. Andres, B. T., and J. H. Kappes. OpenGM: A C++ library for discrete graphical models, June 2012.
D. Belanger, D. Sheldon, and A. McCallum. Marginal inference in MRFs using Frank-Wolfe. NIPS Workshop
on Greedy Optimization, Frank-Wolfe and Friends, 2013.
J. Besag. On the statistical analysis of dirty pictures. J R Stat Soc Series B, 1986.
E. Borenstein and S. Ullman. Class-specific, top-down segmentation. In ECCV, 2002.
Y. Boykov and V. Kolmogorov. An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision. TPAMI, 2004.
J. Domke. Learning graphical model parameters with approximate marginal inference. TPAMI, 2013.
S. Ermon, C. P. Gomes, A. Sabharwal, and B. Selman. Taming the curse of dimensionality: Discrete integration
by hashing and optimization. In ICML, 2013.
D. Garber and E. Hazan. A linearly convergent conditional gradient algorithm with applications to online and
stochastic optimization. arXiv preprint arXiv:1301.4666, 2013.
A. Globerson and T. Jaakkola. Convergent propagation algorithms via oriented trees. In UAI, 2007.
I. Gurobi Optimization. Gurobi optimizer reference manual, 2015.
T. Hazan and T. Jaakkola. On the partition function and random maximum a-posteriori perturbations. In ICML,
M. Jaggi. Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In ICML, 2013.
J. Jancsary and G. Matz. Convergent decomposition solvers for tree-reweighted free energies. In AISTATS,
J. Kappes et al. A comparative study of modern inference techniques for discrete energy minimization problems. In CVPR, 2013.
V. Kolmogorov. Convergent tree-reweighted message passing for energy minimization. TPAMI, 2006.
V. Kolmogorov and C. Rother. Minimizing nonsubmodular functions with graph cuts-A Review. TPAMI, 2007.
T. Koo, A. Globerson, X. Carreras, and M. Collins. Structured prediction models via the matrix-tree theorem.
In EMNLP-CoNLL, 2007.
S. Lacoste-Julien and M. Jaggi. On the global linear convergence of Frank-Wolfe optimization variants. In
NIPS, 2015.
S. Lacoste-Julien, M. Jaggi, M. Schmidt, and P. Pletscher. Block-coordinate Frank-Wolfe optimization for
structural SVMs. In ICML, 2013.
B. London, B. Huang, and L. Getoor. The benefits of learning with strongly convex approximate inference. In
ICML, 2015.
J. M. Mooij. libDAI: A free and open source C++ library for discrete approximate inference in graphical
models. JMLR, 2010.
S. Nowozin, C. Rother, S. Bagon, T. Sharp, B. Yao, and P. Kohli. Decision tree fields. In ICCV, 2011.
G. Papandreou and A. Yuille. Perturb-and-map random fields: Using discrete optimization to learn and sample
from energy models. In ICCV, 2011.
R. Salakhutdinov. Learning and evaluating Boltzmann machines. Technical report, 2008.
S. Shimony. Finding MAPs for belief networks is NP-hard. Artificial Intelligence, 1994.
D. Sontag and T. Jaakkola. New outer bounds on the marginal polytope. In NIPS, 2007.
D. Sontag, T. Meltzer, A. Globerson, Y. Weiss, and T. Jaakkola. Tightening LP relaxations for MAP using
message-passing. In UAI, 2008.
M. J. Wainwright, T. S. Jaakkola, and A. S. Willsky. A new class of upper bounds on the log partition function.
IEEE Transactions on Information Theory, 2005.
S. Wang, R. Frostig, P. Liang, and C. Manning. Relaxations for inference in restricted Boltzmann machines. In
ICLR Workshop, 2014.
T. M. Adams and A. B. Nobel. Uniform convergence of Vapnik-Chervonenkis classes under ergodic sampling.
The Annals of Probability, 38(4):13451367, 2010.
A. Agarwal and J. Duchi. The generalization ability of online algorithms for dependent data. Information
Theory, IEEE Transactions on, 59(1):573587, 2013.
P. Alquier and O. Wintenberger. Model selection for weakly dependent time series forecasting. Technical
Report 2010-39, Centre de Recherche en Economie et Statistique, 2010.
P. Alquier, X. Li, and O. Wintenberger. Prediction of time series by statistical learning: general losses and fast
rates. Dependence Modelling, 1:6593, 2014.
D. Andrews. First order autoregressive processes and strong mixing. Cowles Foundation Discussion Papers
664, Cowles Foundation for Research in Economics, Yale University, 1983.
R. Baillie. Long memory processes and fractional integration in econometrics. Journal of Econometrics, 73
(1):559, 1996.
R. D. Barve and P. M. Long. On the complexity of learning from drifting distributions. In COLT, 1996.
P. Berti and P. Rigo. A Glivenko-Cantelli theorem for exchangeable random variables. Statistics & Probability
Letters, 32(4):385  391, 1997.
T. Bollerslev. Generalized autoregressive conditional heteroskedasticity. J Econometrics, 1986.
G. E. P. Box and G. Jenkins. Time Series Analysis, Forecasting and Control. Holden-Day, Incorporated, 1990.
P. J. Brockwell and R. A. Davis. Time Series: Theory and Methods. Springer-Verlag, New York, 1986.
V. H. De la Pena and E. Gine. Decoupling: from dependence to independence: randomly stopped processes,
U-statistics and processes, martingales and beyond. Probability and its applications. Springer, NY, 1999.
P. Doukhan. Mixing: properties and examples. Lecture notes in statistics. Springer-Verlag, New York, 1994.
R. Engle. Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom
inflation. Econometrica, 50(4):9871007, 1982.
J. D. Hamilton. Time series analysis. Princeton, 1994.
V. Kuznetsov and M. Mohri. Generalization bounds for time series prediction with non-stationary processes.
In ALT, 2014.
A. C. Lozano, S. R. Kulkarni, and R. E. Schapire. Convergence and consistency of regularized boosting
algorithms with stationary -mixing observations. In NIPS, pages 819826, 2006.
R. Meir. Nonparametric time series prediction through adaptive model selection. Machine Learning, pages
534, 2000.
D. Modha and E. Masry. Memory-universal prediction of stationary random processes. Information Theory,
IEEE Transactions on, 44(1):117133, Jan 1998.
M. Mohri and A. Munoz Medina. New analysis and algorithm for learning with drifting distributions. In ALT,
M. Mohri and A. Rostamizadeh. Rademacher complexity bounds for non-i.i.d. processes. In NIPS, 2009.
M. Mohri and A. Rostamizadeh. Stability bounds for stationary -mixing and -mixing processes. Journal of
Machine Learning Research, 11:789814, 2010.
V. Pestov. Predictive PAC learnability: A paradigm for learning from exchangeable input data. In GRC, 2010.
A. Rakhlin, K. Sridharan, and A. Tewari. Online learning: Random averages, combinatorial parameters, and
learnability. In NIPS, 2010.
A. Rakhlin, K. Sridharan, and A. Tewari. Online learning: Stochastic, constrained, and smoothed adversaries.
In NIPS, 2011.
A. Rakhlin, K. Sridharan, and A. Tewari. Sequential complexities and uniform martingale laws of large numbers. Probability Theory and Related Fields, 2015.
C. Shalizi and A. Kontorovitch. Predictive PAC learning and process decompositions. In NIPS, 2013.
I. Steinwart and A. Christmann. Fast learning from non-i.i.d. observations. In NIPS, 2009.
P. D. Tao and L. T. H. An. A D.C. optimization algorithm for solving the trust-region subproblem. SIAM
Journal on Optimization, 8(2):476505, 1998.
M. Vidyasagar. A Theory of Learning and Generalization: With Applications to Neural Networks and Control
Systems. Springer-Verlag New York, Inc., 1997.
B. Yu. Rates of convergence for empirical processes of stationary mixing sequences. The Annals of Probability,
22(1):94116, 1994.
B. Scholkopf, A. Smola, and K.-R. Muller, Kernel principal component analysis, in Artificial Neural Networks ICANN97, ser. Lecture Notes in Computer Science, W. Gerstner, A. Germond, M. Hasler, and J.-D. Nicoud, Eds. Springer Berlin Heidelberg, 1997, pp. 583588.  8
S. Mika, B. Scholkopf, A. J. Smola, K.-R. Muller, M. Scholz, and G. Ratsch, Kernel PCA and de-noising in feature spaces, in Advances in Neural Information Processing Systems, 1999.
S. White and P. Smyth, A spectral clustering approach to finding communities in graph. in SDM, vol. 5. SIAM, 2005.
F. Gobel and A. A. Jagers, Random walks on graphs, Stochastic Processes and their Applications, 1974.
R. R. Nadakuditi and M. E. J. Newman, Graph spectra and the detectability of community structure in networks, Physical Review Letters, 2012.
C. Fowlkes, S. Belongie, F. Chung, and J. Malik, Spectral grouping using the Nystrom method, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 26, no. 2, 2004.
P. Drineas and M. W. Mahoney, On the Nystrom Method for Approximating a Gram Matrix for Improved Kernel-Based Learning, Journal on Machine Learning Resources, 2005.
N. Halko, P. G. Martinsson, and J. A. Tropp, Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions, SIAM Rev., 2011.
Python implementation of FastEmbed.
. Available: https://bitbucket.org/dineshkr/fastembed/src/NIPS2015
D. Achlioptas, Database-friendly random projections, in Proceedings of the Twentieth ACM SIGMODSIGACT-SIGART Symposium on Principles of Database Systems, ser. PODS 01, 2001.
E. Candes and M. Wakin, An introduction to compressive sampling, Signal Processing Magazine, IEEE, March 2008.
L. N. Trefethen and D. Bau, Numerical Linear Algebra. SIAM, 1997.
S. F. McCormick and T. Noe, Simultaneous iteration for the matrix eigenvalue problem, Linear Algebra and its Applications, vol. 16, no. 1, pp. 4356, 1977.
K. Zhang, I. W. Tsang, and J. T. Kwok, Improved Nystrom Low-rank Approximation and Error Analysis, in Proceedings of the 25th International Conference on Machine Learning, ser. ICML 08. ACM, 2008.
D. Yan, L. Huang, and M. I. Jordan, Fast Approximate Spectral Clustering, in Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD 09. ACM, 2009.
M. Li, J. T. Kwok, and B.-L. Lu, Making Large-Scale Nystrom Approximation Possible. in ICML, 2010.
S. Kumar, M. Mohri, and A. Talwalkar, Ensemble Nystrom method, in Advances in Neural Information Processing Systems, 2009.
F. Lin and W. W. Cohen, Power iteration clustering, in Proceedings of the 27th International Conference on Machine Learning (ICML-10), 2010.
F. Lin, Scalable methods for graph-based unsupervised and semi-supervised learning, Ph.D. dissertation, Carnegie Mellon University, 2012.
W. Yan, U. Brahmakshatriya, Y. Xue, M. Gilder, and B. Wise, PIC: Parallel power iteration clustering for big data, Journal of Parallel and Distributed Computing, 2013.
L. Lovasz, Random walks on graphs: A survey, Combinatorics, Paul erdos is eighty, vol. 2, no. 1, pp. 146, 1993.
D. A. Spielman and S.-H. Teng, Nearly-linear time algorithms for graph partitioning, graph sparsification, and solving linear systems, in Proceedings of the Thirty-sixth Annual ACM Symposium on Theory of Computing, ser. STOC 04. New York, NY, USA: ACM, 2004.
D. Spielman and S. Teng, Nearly linear time algorithms for preconditioning and solving symmetric, diagonally dominant linear systems, SIAM Journal on Matrix Analysis and Applications, vol. 35, Jan. 2014.
D. Spielman and N. Srivastava, Graph sparsification by effective resistances, SIAM Journal on Computing, 2011.
R. N. Silver, H. Roeder, A. F. Voter, and J. D. Kress, Kernel polynomial approximations for densities of states and spectral functions, Journal of Computational Physics, vol. 124, no. 1, pp. 115130, Mar. 1996.
E. Di Napoli, E. Polizzi, and Y. Saad, Efficient estimation of eigenvalue counts in an interval, arXiv:1308.4275
, Aug. 2013.
J. Yang and J. Leskovec, Defining and evaluating network communities based on ground-truth, in 2012 IEEE 12th International Conference on Data Mining (ICDM), Dec. 2012.
S. Fortunato, Community detection in graphs, Physics Reports, vol. 486, no. 3-5, Feb. 2010.  9
Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms for sparse coding. arXiv preprint arXiv:1503.00778, 2015.
Sivaraman Balakrishnan, Martin J Wainwright, and Bin Yu. Statistical guarantees for the EM algorithm: From population to sample-based analysis. arXiv preprint arXiv:1408.2156, 2014.
Emmanuel J Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via wirtinger flow: Theory and algorithms. IEEE Transactions on Information Theory, 61(4):19852007, 2015.
Emmanuel J Candes and Benjamin Recht. Exact matrix completion via convex optimization. Foundations of Computational Mathematics, 9(6):717772, 2009.
Emmanuel J Candes and Terence Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE Transactions on Information Theory, 56(5):20532080, 2010.
Yudong Chen. Incoherence-optimal matrix completion. arXiv preprint arXiv:1310.0154, 2013.
David Gross. Recovering low-rank matrices from few coefficients in any basis. IEEE Transactions on Information Theory, 57(3):15481566, 2011.
Moritz Hardt. Understanding alternating minimization for matrix completion. In Symposium on Foundations of Computer Science, pages 651660, 2014.
Moritz Hardt, Raghu Meka, Prasad Raghavendra, and Benjamin Weitz. Computational limits for matrix completion. arXiv preprint arXiv:1402.2331, 2014.
Moritz Hardt and Mary Wootters. Fast matrix completion without the condition number. arXiv preprint arXiv:1407.4070, 2014.
Trevor Hastie, Rahul Mazumder, Jason Lee, and Reza Zadeh. Matrix completion and low-rank SVD via fast alternating least squares. arXiv preprint arXiv:1410.2596, 2014.
Prateek Jain, Raghu Meka, and Inderjit S Dhillon. Guaranteed rank minimization via singular value projection. In Advances in Neural Information Processing Systems, pages 937945, 2010.
Prateek Jain and Praneeth Netrapalli. Fast exact matrix completion with finite samples. arXiv preprint arXiv:1411.1087, 2014.
Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternating minimization. In Symposium on Theory of Computing, pages 665674, 2013.
Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few entries. IEEE Transactions on Information Theory, 56(6):29802998, 2010.
Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from noisy entries. Journal of Machine Learning Research, 11:20572078, 2010.
Yehuda Koren. The Bellkor solution to the Netflix grand prize. Netflix Prize Documentation, 81, 2009.
Kiryung Lee and Yoram Bresler. Admira: Atomic decomposition for minimum rank approximation. IEEE Transactions on Information Theory, 56(9):44024416, 2010.
Sahand Negahban and Martin J Wainwright. Estimation of (near) low-rank matrices with noise and high-dimensional scaling. The Annals of Statistics, 39(2):10691097, 2011.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer, 2004.
Arkadiusz Paterek. Improving regularized singular value decomposition for collaborative filtering. In Proceedings of KDD Cup and workshop, volume 2007, pages 58, 2007.
Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning Research, 12:34133430, 2011.
Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM Review, 52(3):471501, 2010.
Benjamin Recht and Christopher Re. Parallel stochastic gradient algorithms for large-scale matrix completion. Mathematical Programming Computation, 5(2):201226, 2013.
Angelika Rohde and Alexandre B Tsybakov. Estimation of high-dimensional low-rank matrices. The Annals of Statistics, 39(2):887930, 2011.
Gilbert W Stewart, Ji-guang Sun, and Harcourt B Jovanovich. Matrix perturbation theory, volume 175. Academic press New York, 1990.
Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via non-convex factorization. arXiv preprint arXiv:1411.8003, 2014.
Gabor Takacs, Istvan Pilaszy, Bottyan Nemeth, and Domonkos Tikk. Major components of the gravity recommendation system. ACM SIGKDD Explorations Newsletter, 9(2):8083, 2007.  9
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to variational methods for graphical models. Machine Learning, 37(2):183233, 1999.
Martin J Wainwright and Michael I Jordan. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1(1-2):1305, 2008.
Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. The Journal of Machine Learning Research, 14(1):13031347, 2013.
Stan Development Team. Stan Modeling Language Users Guide and Reference Manual, 2015.
Matthew D Hoffman and Andrew Gelman. The No-U-Turn sampler. The Journal of Machine Learning Research, 15(1):15931623, 2014.
Diederik Kingma and Max Welling. Auto-encoding variational Bayes. arXiv:1312.6114, 2013.
Danilo J Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In ICML, pages 12781286, 2014.
Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In AISTATS, pages 814822, 2014.
Tim Salimans and David Knowles. On using control variates with stochastic approximation for variational Bayes. arXiv preprint arXiv:1401.1022, 2014.
Michalis Titsias and Miguel Lzaro-Gredilla. Doubly stochastic variational Bayes for nonconjugate inference. In ICML, pages 19711979, 2014.
David Wingate and Theophane Weber. Automated variational inference in probabilistic programming. arXiv preprint arXiv:1301.1299, 2013.
Noah D Goodman, Vikash K Mansinghka, Daniel Roy, Keith Bonawitz, and Joshua B Tenenbaum. Church: A language for generative models. In UAI, pages 220229, 2008.
Vikash Mansinghka, Daniel Selsam, and Yura Perov. Venture: a higher-order probabilistic programming platform with programmable inference. arXiv:1404.0099, 2014.
Frank Wood, Jan Willem van de Meent, and Vikash Mansinghka. A new approach to probabilistic programming inference. In AISTATS, pages 246, 2014.
John M Winn and Christopher M Bishop. Variational message passing. In Journal of Machine Learning Research, pages 661694, 2005.
Christopher M Bishop. Pattern Recognition and Machine Learning. Springer New York, 2006.
David J Olive. Statistical Theory and Inference. Springer, 2014.
Manfred Opper and Cdric Archambeau. The variational Gaussian approximation revisited. Neural computation, 21(3):786792, 2009.
Wolfgang Hrdle and Lopold Simar. Applied multivariate statistical analysis. Springer, 2012.
Christian P Robert and George Casella. Monte Carlo statistical methods. Springer, 1999.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:21212159, 2011.
Mark Girolami and Ben Calderhead. Riemann manifold langevin and hamiltonian monte carlo methods. Journal of the Royal Statistical Society: Series B, 73(2):123214, 2011.
Andrew Gelman and Jennifer Hill. Data analysis using regression and multilevel/hierarchical models. Cambridge University Press, 2006.
John Canny. GaP: a factor model for discrete data. In ACM SIGIR, pages 122129. ACM, 2004.
Mauricio Villegas, Roberto Paredes, and Bart Thomee. Overview of the ImageCLEF 2013 Scalable Concept Image Annotation Subtask. In CLEF Evaluation Labs and Workshop, 2013. 9
A. Graves. Generating sequences with recurrent neural networks. arXiv:1308.0850, August 2013.
D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. In Proc. of the 3rd ICLR, 2015. arXiv:1409.0473.
K. Xu, J. Ba, R. Kiros, et al. Show, attend and tell: Neural image caption generation with visual attention. In Proc. of the 32nd ICML, 2015. arXiv:1502.03044.
V. Mnih, N. Heess, A. Graves, et al. Recurrent models of visual attention. In Proc. of the 27th NIPS, 2014. arXiv:1406.6247.
J. Chorowski, D. Bahdanau, K. Cho, and Y. Bengio. End-to-end continuous speech recognition using attention-based recurrent NN: First results. arXiv:1412.1602
, December 2014.
A. Graves, G. Wayne, and I. Danihelka. Neural turing machines. arXiv:1410.5401, 2014.
J. Weston, S. Chopra, and A. Bordes. Memory networks. arXiv:1410.3916, 2014.
M. Gales and S. Young. The application of hidden markov models in speech recognition. Found. Trends Signal Process., 1(3):195304, January 2007.
G. Hinton, L. Deng, D. Yu, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):8297, November 2012.
A. Hannun, C. Case, J. Casper, et al. Deepspeech: Scaling up end-to-end speech recognition. arXiv:1412.5567, 2014.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural. Comput., 9(8):17351780, 1997.
K. Cho, B. van Merrienboer, C. Gulcehre, et al. Learning phrase representations using RNN encoderdecoder for statistical machine translation. In EMNLP, October 2014. to appear.
A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber. Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. In Proc. of the 23rd ICML-06, 2006.
A. Graves. Sequence transduction with recurrent neural networks. In Proc. of the 29th ICML, 2012.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient based learning applied to document recognition. Proc. IEEE, 1998.
A. Graves, A.-r. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In ICASSP 2013, pages 66456649. IEEE, 2013.
A. Graves and N. Jaitly. Towards end-to-end speech recognition with recurrent neural networks. In Proc. of the 31st ICML, 2014.
S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus. Weakly supervised memory networks. arXiv:1503.08895, 2015.
J. S. Garofolo, L. F. Lamel, W. M. Fisher, et al. DARPA TIMIT acoustic phonetic continuous speech corpus, 1993.
D. Povey, A. Ghoshal, G. Boulianne, et al. The kaldi speech recognition toolkit. In Proc. ASRU, 2011.
M. D. Zeiler. ADADELTA: An adaptive learning rate method. arXiv:1212.5701, 2012.
A. Graves. Practical variational inference for neural networks. In Proc of the 24th NIPS, 2011.
G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv:1207.0580, 2012.
I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In Proc. of the 27th NIPS, 2014. arXiv:1409.3215.
L. Toth. Combining time-and frequency-domain convolution in convolutional neural network-based phone recognition. In Proc. ICASSP, 2014.
C. Gulcehre, O. Firat, K. Xu, et al. On using monolingual corpora in neural machine translation. arXiv:1503.03535, 2015.
J. Bergstra, O. Breuleux, F. Bastien, et al. Theano: a CPU and GPU math expression compiler. In Proc. SciPy, 2010.
F. Bastien, P. Lamblin, R. Pascanu, et al. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.
I. J. Goodfellow, D. Warde-Farley, P. Lamblin, et al. Pylearn2: a machine learning research library. arXiv preprint arXiv:1308.4214, 2013.
B. van Merrienboer, D. Bahdanau, V. Dumoulin, et al. Blocks and fuel: Frameworks for deep learning. arXiv:1506.00619
, June 2015.  9
P. McCullagh and J.A. Nelder. Generalized linear models. Monographs on statistics and applied probability 37. Chapman and Hall/CRC, 1989.
G. E. Hoffman, B. A. Logsdon, and J. G. Mezey. Puma: A unified framework for penalized multiple regression analysis of gwas data. Plos computational Biology, 2013.
D. Witten and R. Tibshirani. Survival analysis with high-dimensional covariates. Stat Methods Med Res., 19:2951, 2010.
E. Yang, P. Ravikumar, G. I. Allen, and Z. Liu. Graphical models via generalized linear models. In Neur. Info. Proc. Sys. (NIPS), 25, 2012.
S. Van de Geer. High-dimensional generalized linear models and the lasso. Annals of Statistics, 36(2): 614645, 2008.
F. Bach. Self-concordant analysis for logistic regression. Electron. J. Stat., 4:384414, 2010.
S. M. Kakade, O. Shamir, K. Sridharan, and A. Tewari. Learning exponential families in high-dimensions: Strong convexity and sparsity. In Inter. Conf. on AI and Statistics (AISTATS), 2010.
S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A unified framework for high-dimensional analysis of M-estimators with decomposable regularizers. Arxiv preprint arXiv:1010.2731v1, 2010.
F. Bunea. Honest variable selection in linear and logistic regression models via l1 and l1 + l2 penalization. Electron. J. Stat., 2:11531194, 2008.
L. Meier, S. Van de Geer, and P. Buhlmann. The group lasso for logistic regression. Journal of the Royal Statistical Society, Series B, 70:5371, 2008.
Y. Kim, J. Kim, and Y. Kim. Blockwise sparse regression. Statistica Sinica, 16:375390, 2006.
J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized linear models via coordinate descent. Journal of Statistical Software, 33(1):122, 2010.
K. Koh, S. J. Kim, and S. Boyd. An interior-point method for large-scale `1 -regularized logistic regression. Jour. Mach. Learning Res., 3:15191555, 2007.
E. Yang, A. C. Lozano, and P. Ravikumar. Elementary estimators for high-dimensional linear regression. In International Conference on Machine learning (ICML), 31, 2014.
A. J. Rothman, E. Levina, and J. Zhu. Generalized thresholding of large covariance matrices. Journal of the American Statistical Association (Theory and Methods), 104:177186, 2009.
M. J. Wainwright and M. I. Jordan. Graphical models, exponential families and variational inference. Foundations and Trends in Machine Learning, 1(12):1305, December 2008.
P. J. Bickel and E. Levina. Covariance regularization by thresholding. Annals of Statistics, 36(6):2577 2604, 2008.
M. J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using `1 -constrained quadratic programming (Lasso). IEEE Trans. Information Theory, 55:21832202, May 2009.
Daniel A. Spielman and Shang-Hua Teng. Solving sparse, symmetric, diagonally-dominant linear systems in time 0(m1.31 ). In 44th Symposium on Foundations of Computer Science (FOCS 2003), 11-14 October 2003, Cambridge, MA, USA, Proceedings, pages 416427, 2003.
Michael B. Cohen, Rasmus Kyng, Gary L. Miller, Jakub W. Pachocki, Richard Peng, Anup B. Rao, and Shen Chen Xu. Solving sdd linear systems in nearly mlog1/2n time. In Proceedings of the 46th Annual ACM Symposium on Theory of Computing, STOC 14, pages 343352. ACM, 2014.
Daniel A. Spielman and Shang-Hua Teng. Nearly linear time algorithms for preconditioning and solving symmetric, diagonally dominant linear systems. SIAM J. Matrix Analysis Applications, 35(3):835885, 2014.
P. Ravikumar, M. J. Wainwright, G. Raskutti, and B. Yu. High-dimensional covariance estimation by minimizing `1 -penalized log-determinant divergence. Electronic Journal of Statistics, 5:935980, 2011.
E. Yang, A. C. Lozano, and P. Ravikumar. Elementary estimators for sparse covariance matrices and other structured moments. In International Conference on Machine learning (ICML), 31, 2014.
E. Yang, A. C. Lozano, and P. Ravikumar. Elementary estimators for graphical models. In Neur. Info. Proc. Sys. (NIPS), 27, 2014.  9
E. Amigo, J. C. de Albornoz, I. Chugur, A. Corujo, J. Gonzalo, T. Martn-Wanton, E. Meij, M. de Rijke, and D. Spina. Overview of RepLab 2013: Evaluating online reputation monitoring systems. In CLEF, volume 8138, pages 333352, 2013.
S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1122, 2012.
R. Busa-Fekete, B. Kegl, T. Elteto, and Gy. Szarvas. Tune and mix: Learning to rank using ensembles of calibrated multi-class classifiers. Machine Learning, 93(23):261292, 2013.
N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006.
L. Devroye and L. Gyorfi. Nonparametric Density Estimation: The L1 View. Wiley, NY, 1985.
L. Devroye, L. Gyorfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer, NY, 1996.
W. Gao, R. Jin, S. Zhu, and Z.-H. Zhou. One-pass AUC optimization. In ICML, volume 30:3, pages 906914, 2013.
V. Hangya and R. Farkas. Filtering and polarity detection for reputation management on tweets. In Working Notes of CLEF 2013 Evaluation Labs and Workshop, 2013.
P. Kar, H. Narasimhan, and P. Jain. Online and stochastic gradient methods for nondecomposable loss functions. In NIPS, 2014.
N. Nagarajan, S. Koyejo, R. Ravikumar, and I. Dhillon. Consistent binary classification with generalized performance metrics. In NIPS, pages 27442752, 2014.
H. Narasimhan, R. Vaish, and Agarwal S. On the statistical consistency of plug-in classifiers for non-decomposable performance measures. In NIPS, 2014.
H. Robbins and S. Monro. A stochastic approximation method. Ann. Math. Statist., 22(3):400 407, 1951.
F. Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65(6):386408, 1958.
S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal estimated sub-gradient solver for SVM. In ICML, pages 807814, 2007.
Y. Tsuruoka, J. Tsujii, and S. Ananiadou. Stochastic gradient descent training for L1regularized log-linear models with cumulative penalty. In ACL, pages 477485, 2009.
C.J. van Rijsbergen. Foundation and evalaution. Journal of Documentation, 30(4):365373, 1974.
S. R. S. Varadhan. Probability Theory. New York University, 2000.
W. Waegeman, K. Dembczynski, A. Jachnik, W. Cheng, and E. Hullermeier. On the Bayesoptimality of F-measure maximizers. Journal of Machine Learning Research, 15(1):3333 3388, 2014.
N. Ye, K. M. A. Chai, W. S. Lee, and H. L. Chieu. Optimizing F-measure: A tale of two approaches. In ICML, 2012.
M. Zhao, N. Edakunni, A. Pocock, and G. Brown. Beyond Fanos inequality: Bounds on the optimal F-score, BER, and cost-sensitive risk and their implications. JMLR, pages 10331090, 2013.
P. Zhao, S. C. H. Hoi, R. Jin, and T. Yang. Online AUC maximization. In ICML, pages 233240, 2011.  9
Nir Ailon. Reconciling real scores with binary comparisons: A new logistic based model for ranking. In Advances in Neural Information Processing Systems 21, pages 2532, 2008.
M. Braverman and E. Mossel. Noisy sorting without resampling. In Proceedings of the nineteenth annual ACM-SIAM Symposium on Discrete algorithms, pages 268276, 2008.
M. Braverman and E. Mossel. Sorting from noisy information. CoRR, abs/0910.1191, 2009.
S. Bubeck, R. Munos, and G. Stoltz. Pure exploration in multi-armed bandits problems. In Proceedings of the 20th ALT, ALT09, pages 2337, Berlin, Heidelberg, 2009. Springer-Verlag.
S. Bubeck, T. Wang, and N. Viswanathan. Multiple identifications in multi-armed bandits. In Proceedings of The 30th ICML, pages 258265, 2013.
R. Busa-Fekete and E. Hullermeier. A survey of preference-based online learning with bandit algorithms. In Algorithmic Learning Theory (ALT), volume 8776, pages 1839, 2014.
R. Busa-Fekete, E. Hullermeier, and B. Szorenyi. Preference-based rank elicitation using statistical models: The case of Mallows. In (ICML), volume 32 (2), pages 10711079, 2014.
R. Busa-Fekete, B. Szorenyi, and E. Hullermeier. Pac rank elicitation through adaptive sampling of stochastic pairwise preferences. In AAAI, pages 17011707, 2014.
R. Busa-Fekete, B. Szorenyi, P. Weng, W. Cheng, and E. Hullermeier. Top-k selection based on adaptive sampling of noisy preferences. In Proceedings of the 30th ICML, JMLR W&CP, volume 28, 2013.
C. J. Clopper and E. S. Pearson. The Use of Confidence or Fiducial Limits Illustrated in the Case of the Binomial. Biometrika, 26(4):404413, 1934.
E. Even-Dar, S. Mannor, and Y. Mansour. PAC bounds for multi-armed bandit and markov decision processes. In Proceedings of the 15th COLT, pages 255270, 2002.
Uriel Feige, Prabhakar Raghavan, David Peleg, and Eli Upfal. Computing with noisy information. SIAM J. Comput., 23(5):10011018, October 1994.
V. Gabillon, M. Ghavamzadeh, A. Lazaric, and S. Bubeck. Multi-bandit best arm identification. In NIPS 24, pages 22222230, 2011.
J. Guiver and E. Snelson. Bayesian inference for plackett-luce ranking models. In Proceedings of the 26th ICML, pages 377384, 2009.
C. A. R. Hoare. Quicksort. Comput. J., 5(1):1015, 1962.
W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58:1330, 1963.
D.R. Hunter. MM algorithms for generalized bradley-terry models. The Annals of Statistics, 32(1):384 406, 2004.
R. Luce and P. Suppes. Handbook of Mathematical Psychology, chapter Preference, Utility and Subjective Probability, pages 249410. Wiley, 1965.
R. D. Luce. Individual choice behavior: A theoretical analysis. Wiley, 1959.
C. Mallows. Non-null ranking models. Biometrika, 44(1):114130, 1957.
John I. Marden. Analyzing and Modeling Rank Data. Chapman & Hall, 1995.
C.J.H. McDiarmid and R.B. Hayward. Large deviations for quicksort. Journal of Algorithms, 21(3):476 507, 1996.
S. Negahban, S. Oh, and D. Shah. Iterative ranking from pairwise comparisons. In Advances in Neural Information Processing Systems, pages 24832491, 2012.
R. Plackett. The analysis of permutations. Applied Statistics, 24:193202, 1975.
Arun Rajkumar and Shivani Agarwal. A statistical convergence perspective of algorithms for rank aggregation from pairwise data. In ICML, pages 118126, 2014.
H. A. Soufiani, W. Z. Chen, D. C. Parkes, and L. Xia. Generalized method-of-moments for rank aggregation. In Advances in Neural Information Processing Systems (NIPS), pages 27062714, 2013.
T. Urvoy, F. Clerot, R. Feraud, and S. Naamane. Generic exploration and k-armed voting bandits. In Proceedings of the 30th ICML, JMLR W&CP, volume 28, pages 9199, 2013.
Y. Yue, J. Broder, R. Kleinberg, and T. Joachims. The k-armed dueling bandits problem. Journal of Computer and System Sciences, 78(5):15381556, 2012.
Y. Yue and T. Joachims. Beat the mean bandit. In Proceedings of the ICML, pages 241248, 2011.
M. Zoghi, S. Whiteson, R. Munos, and M. Rijke. Relative upper confidence bound for the k-armed dueling bandit problem. In ICML, pages 1018, 2014.  9
C. Arora, S. Banerjee, P. Kalra, and S. Maheshwari. Generalized flows for optimal inference in higher order MRF-MAP. TPAMI, 2015.
D. Batra. An efficient message-passing algorithm for the M-best MAP problem. arXiv:1210.4841, 2012.
D. Batra, P. Yadollahpour, A. Guzman-Rivera, and G. Shakhnarovich. Diverse M-best solutions in markov random fields. In ECCV. Springer Berlin/Heidelberg, 2012.
Y. Boykov and M.-P. Jolly. Interactive graph cuts for optimal boundary & region segmentation of objects in N-D images. In ICCV, 2001.
Y. Boykov and V. Kolmogorov. An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision. TPAMI, 26(9):11241137, 2004.
Y. Boykov, O. Veksler, and R. Zabih. Fast approximate energy minimization via graph cuts. TPAMI, 23(11):12221239, 2001.
C. Chen, V. Kolmogorov, Y. Zhu, D. N. Metaxas, and C. H. Lampert. Computing the M most probable modes of a graphical model. In AISTATS, 2013.
G. Elidan and A. Globerson. The probabilistic inference challenge (PIC2011).
M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results.
A. Fix, A. Gruber, E. Boros, and R. Zabih. A graph cut algorithm for higher-order Markov random fields. In ICCV, 2011.
V. Franc and B. Savchynskyy. Discriminative learning of max-sum classifiers. JMLR, 9:67104, 2008.
M. Fromer and A. Globerson. An lp view of the m-best map problem. In NIPS 22, 2009.
A. Guzman-Rivera, D. Batra, and P. Kohli. Multiple choice learning: Learning to produce multiple structured outputs. In NIPS 25, 2012.
A. Guzman-Rivera, P. Kohli, and D. Batra. DivMCuts: Faster training of structural SVMs with diverse M-best cutting-planes. In AISTATS, 2013.
A. Guzman-Rivera, P. Kohli, D. Batra, and R. A. Rutenbar. Efficiently enforcing diversity in multi-output structured prediction. In AISTATS, 2014.
H. Ishikawa. Exact optimization for Markov random fields with convex priors. TPAMI, 2003.
S. Jegelka and J. Bilmes. Submodularity beyond submodular energies: coupling edges in graph cuts. In CVPR, 2011.
J. H. Kappes, B. Andres, F. A. Hamprecht, C. Schnorr, S. Nowozin, D. Batra, S. Kim, B. X. Kausler, T. Kroger, J. Lellmann, N. Komodakis, B. Savchynskyy, and C. Rother. A comparative study of modern inference techniques for structured discrete energy minimization problems. IJCV, pages 130, 2015.
A. Kirillov, B. Savchynskyy, D. Schlesinger, D. Vetrov, and C. Rother. Inferring M-best diverse labelings in a single one. In ICCV, 2015.
V. Kolmogorov. Minimizing a sum of submodular functions. Discrete Applied Mathematics, 2012.
V. Kolmogorov and R. Zabin. What energy functions can be minimized via graph cuts? TPAMI, 2004.
A. Kulesza and B. Taskar. Structured determinantal point processes. In NIPS 23, 2010.
E. L. Lawler. A procedure for computing the K best solutions to discrete optimization problems and its application to the shortest path problem. Management Science, 18(7), 1972.
D. Nilsson. An efficient algorithm for finding the m most probable configurationsin probabilistic expert systems. Statistics and Computing, 8(2):159173, 1998.
A. Prasad, S. Jegelka, and D. Batra. Submodular meets structured: Finding diverse subsets in exponentially-large structured item sets. In NIPS 27, 2014.
V. Premachandran, D. Tarlow, and D. Batra. Empirical minimum bayes risk prediction: How to extract an extra few % performance from vision models with just three more parameters. In CVPR, 2014.
V. Ramakrishna and D. Batra. Mode-marginals: Expressing uncertainty via diverse M-best solutions. In NIPS Workshop on Perturbations, Optimization, and Statistics, 2012.
D. Schlesinger and B. Flach. Transforming an arbitrary minsum problem into a binary one. TU Dresden, Fak. Informatik, 2006.
M. I. Schlesinger and V. Hlavac. Ten lectures on statistical and structural pattern recognition, volume 24. Springer Science & Business Media, 2002.
D. Tarlow, I. E. Givoni, and R. S. Zemel. Hop-map: Efficient message passing with high order potentials. In AISTATS, 2010.
T. Werner. A linear programming approach to max-sum problem: A review. TPAMI, 29(7), 2007.
P. Yadollahpour, D. Batra, and G. Shakhnarovich. Discriminative re-ranking of diverse segmentations. In CVPR, 2013.
C. Yanover and Y. Weiss. Finding the M most probable configurations using loopy belief propagation. In NIPS 17, 2004.  9
Mark Bartlett and James Cussens. Advances in Bayesian network learning using integer programming. In 29th Conference on Uncertainty in Artificial Intelligence (UAI), 2013.
Jeremias Berg, Matti Jrvisalo, and Brandon Malone. Learning Optimal Bounded Treewidth Bayesian Networks via Maximum Satisfiability. In 17th International Conference on Artificial Intelligence and Statistics (AISTATS), 2014.
David M. Chickering. Learning Bayesian networks is NP-Complete. In Learning from Data: Artificial Intelligence and Statistics V, pages 121130. Springer-Verlag, 1996.
David M. Chickering, David Heckerman, and Chris Meek. Large-sample learning of Bayesian networks is NP-Hard. Journal of Machine Learning Research, 5:12871330, 2004.
C. K. Chow and C. N. Liu. Approximating discrete probability distributions with dependence trees. IEEE Transactions on Information Theory, 14(3):462467, 1968.
Gregory. F. Cooper. The computational complexity of probabilistic inference using Bayesian belief networks. Artificial Intelligence, 42:393405, 1990.
Gregory F. Cooper and Edward Herskovits. A Bayesian method for the induction of probabilistic networks from data. Machine Learning, 9:309347, 1992.
James Cussens. Bayesian network learning with cutting planes. In 27th Conference on Uncertainty in Artificial Intelligence (UAI), 2011.
Sanjoy Dasgupta. Learning polytrees. In 15th Conference on Uncertainty in Artificial Intelligence (UAI), 1999.
Rodney G. Downey and Michael R. Fellows. Parameterized computational feasibility. In Feasible Mathematics II, pages 219244. Birkhauser, 1994.
Rodney G. Downey and Michael R. Fellows. Parameterized complexity. Springer-Verlag, 1999.
Gal Elidan and Stephen Gould. Learning bounded treewidth Bayesian networks. Journal of Machine Learning Research, 9:26992731, 2008.
Jrg Flum and Martin Grohe. Parameterized complexity theory. Springer-Verlag, 2006.
David Heckerman, Dan Geiger, and David M. Chickering. Learning Bayesian networks: The combination of knowledge and statistical data. Machine Learning, 20(3):197243, 1995.
Tommi Jaakkola, David Sontag, Amir Globerson, and Marina Meila. Learning bayesian network structure using LP relaxations. In 13th International Conference on Artificial Intelligence and Statistics (AISTATS), 2010.
Janne H. Korhonen and Pekka Parviainen. Learning bounded tree-width Bayesian networks. In 16th International Conference on Artificial Intelligence and Statistics (AISTATS), 2013.
Johan H. P. Kwisthout, Hans L. Bodlaender, and L. C. van der Gaag. The necessity of bounded treewidth for efficient inference in Bayesian networks. In 19th European Conference on Artificial Intelligence (ECAI), 2010.
Chris Meek. Finding a path is harder than finding a tree. Journal of Artificial Intelligence Research, 15: 383389, 2001.
Siqi Nie, Denis Deratani Maua, Cassio Polpo de Campos, and Qiang Ji. Advances in Learning Bayesian Networks of Bounded Treewidth. In Advances in Neural Information Processing Systems 27 (NIPS), 2014.
Rolf Niedermeier. Invitation to fixed-parameter algorithms. Oxford University Press, 2006.
Sascha Ott and Satoru Miyano. Finding optimal gene networks using biological constraints. Genome Informatics, 14:124133, 2003.
Pekka Parviainen, Hossein Shahrabi Farahani, and Jens Lagergren. Learning Bounded Tree-width Bayesian Networks using Integer Linear Programming. In 17th International Conference on Artificial Intelligence and Statistics (AISTATS), 2014.
Tomi Silander and Petri Myllymki. A simple approach for finding the globally optimal Bayesian network structure. In 22nd Conference on Uncertainty in Artificial Intelligence (UAI), 2006.  9
E. Yang, G. Allen, Z. Liu, and P. K. Ravikumar, Graphical models via generalized linear models, in Advances in Neural Information Processing Systems, 2012, pp. 13581366.
P. Bonissone, M. Henrion, L. Kanal, and J. Lemmer, Equivalence and synthesis of causal models, in Uncertainty in artificial intelligence, vol. 6, 1991, p. 255.
P. Spirtes, C. Glymour, and R. Scheines, Causation, Prediction and Search. MIT Press, 2000.
S. L. Lauritzen, Graphical models.  Oxford University Press, 1996.
D. M. Chickering, Learning Bayesian networks is NP-complete, in Learning from data. Springer, 1996, pp. 121130.
C. Uhler, G. Raskutti, P. Buhlmann, B. Yu et al., Geometry of the faithfulness assumption in causal inference, The Annals of Statistics, vol. 41, no. 2, pp. 436463, 2013.
C. B. Dean, Testing for overdispersion in Poisson and binomial regression models, Journal of the American Statistical Association, vol. 87, no. 418, pp. 451457, 1992.
T. Zheng, M. J. Salganik, and A. Gelman, How many people do you know in prison? Using overdispersion in count data to estimate social structure in networks, Journal of the American Statistical Association, vol. 101, no. 474, pp. 409423, 2006.
J. Peters and P. Buhlmann, Identifiability of Gaussian structural equation models with equal error variances, Biometrika, p. ast043, 2013.
S. Shimizu, P. O. Hoyer, A. Hyvarinen, and A. Kerminen, A linear non-Gaussian acyclic model for causal discovery, The Journal of Machine Learning Research, vol. 7, pp. 2003 2030, 2006.
J. Peters, J. Mooij, D. Janzing et al., Identifiability of causal graphs using functional models, arXiv preprint arXiv:1202.3757, 2012.
I. Tsamardinos, L. E. Brown, and C. F. Aliferis, The max-min hill-climbing Bayesian network structure learning algorithm, Machine learning, vol. 65, no. 1, pp. 3178, 2006.
C. F. Aliferis, I. Tsamardinos, and A. Statnikov, HITON: a novel Markov Blanket algorithm for optimal variable selection, in AMIA Annual Symposium Proceedings, vol. 2003. American Medical Informatics Association, 2003, p. 21.
R. G. Cowell, P. A. Dawid, S. L. Lauritzen, and D. J. Spiegelhalter, Probabilistic Networks and Expert Systems. Springer-Verlag, 1999.
I. Tsamardinos and C. F. Aliferis, Towards principled feature selection: Relevancy, filters and wrappers, in Proceedings of the ninth international workshop on Artificial Intelligence and Statistics. Morgan Kaufmann Publishers: Key West, FL, USA, 2003.
N. Friedman, I. Nachman, and D. Peer, Learning bayesian network structure from massive datasets: the sparse candidate algorithm, in Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence. Morgan Kaufmann Publishers Inc., 1999, pp. 206215.
J. Friedman, T. Hastie, and R. Tibshirani, glmnet: Lasso and elastic-net regularized generalized linear models, R package version, vol. 1, 2009.
D. M. Chickering, Optimal structure identification with greedy search, The Journal of Machine Learning Research, vol. 3, pp. 507554, 2003.
D. Heckerman, D. Geiger, and D. M. Chickering, Learning Bayesian networks: The combination of knowledge and statistical data, Machine learning, vol. 20, no. 3, pp. 197243, 1995.  9
P. Smolensky. Chapter 6: Information Processing in Dynamical Systems: Foundations of Harmony Theory. Processing of the Parallel Distributed: Explorations in the Microstructure of Cognition, Volume 1: Foundations, 1986.
G. Hinton. Training products of experts by minimizing contrastive divergence. Neural Comp., 14:1771 1800, 2002.
G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504507, 2006.
H. Larochelle and Y. Bengio. Classification using discriminative restricted Boltzmann machines. In ICML, pages 536543, 2008.
R. Salakhutdinov, A. Mnih, and G. Hinton. Restricted Boltzmann machines for collaborative filtering. In ICML, pages 791798, 2007.
A. Coates, A. Y. Ng, and H. Lee. An analysis of single-layer networks in unsupervised feature learning. In Intl. Conf. on Artificial Intelligence and Statistics, pages 215223, 2011.
G. Hinton and R. Salakhutdinov. Replicated softmax: an undirected topic model. In NIPS, pages 1607 1614, 2009.
R. Salakhutdinov and G. Hinton. Deep Boltzmann machines. In Intl. Conf. on Artificial Intelligence and Statistics, pages 448455, 2009.
G. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural Comp., 18(7):15271554, 2006.
Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, (521):436444, May 2015.
R. M. Neal. Connectionist learning of deep belief networks. Artificial Int., 56(1):71113, 1992.
T. Tieleman. Training restricted Boltzmann machines using approximations to the likelihood gradient. In ICML, pages 10641071, 2008.
C. Peterson and J. R. Anderson. A mean field theory learning algorithm for neural networks. Complex Systems, 1:9951019, 1987.
G. Hinton. Deterministic Boltzmann learning performs steepest descent in weight-space. Neural Comp., 1(1):143150, 1989.
C. C. Galland. The limitations of deterministic Boltzmann machine learning. Network, 4:355379, 1993.
H. J. Kappen and F. B. Rodrguez. Boltzmann machine learning using mean field theory and linear response correction. In NIPS, pages 280286, 1998.
M. Welling and G. Hinton. A new learning algorithm for mean field Boltzmann machines. In Intl. Conf. on Artificial Neural Networks, pages 351357, 2002.
S. Cocco, S. Leibler, and R. Monasson. Neuronal couplings between retinal ganglion cells inferred by efficient inverse statistical physics methods. PNAS, 106(33):1405814062, 2009.
S. Cocco and R. Monasson. Adaptive cluster expansion for inferring Boltzmann machines with noisy data. Physical Review Letters, 106(9):90601, 2011.
D. J. Thouless, P. W. Anderson, and R. G. Palmer. Solution of Solvable model of a spin glass. Philosophical Magazine, 35(3):593601, 1977.
A. Georges and J. S. Yedidia. How to expand around mean-field theory using high-temperature expansions. Journal of Physics A: Mathematical and General, 24(9):21732192, 1999.
T. Plefka. Convergence condition of the TAP equation for the infinite-ranged Ising spin glass model. Journal of Physics A: Mathematical and General, 15(6):19711978, 1982.
M. Opper and D. Saad. Advanced mean field methods: Theory and practice. MIT press, 2001.
E. Bolthausen. An iterative construction of solutions of the TAP equations for the SherringtonKirkpatrick model. Communications in Mathematical Physics, 325(1):333366, 2014.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proc. of the IEEE, 86(11):22782323, 1998.
B. M. Marlin, K. Swersky, B. Chen, and N. de Freitas. Inductive principles for restricted Boltzmann machine learning. In Intl. Conf. on Artificial Intelligence and Statistics, pages 509516, 2010.
G. Hinton. A practical guide to training restricted Boltzmann machines. Computer, 9:1, 2010.
F. Pedregosa et al. Scikit-learn: Machine learning in Python. JMLR, 12:28252830, 2011.
I. J. Goodfellow, A. Courville, and Y. Bengio. Joint training deep Boltzmann machines for classification. arXiv preprint: 1301.3568, 2013.  9
L. Bottou, F. Fogelman Soulie, P. Blanchet, and J. Lienard. Experiments with time delay networks and dynamic time warping for speaker independent isolated digit recognition. In Proceedings of EuroSpeech 89, volume 2, pages 537540, Paris, France, 1989.
Y.-L. Boureau, F. Bach, Y. LeCun, and J. Ponce. Learning mid-level features for recognition. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 25592566. IEEE, 2010.
Y.-L. Boureau, J. Ponce, and Y. LeCun. A theoretical analysis of feature pooling in visual recognition. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 111118, 2010.
R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A matlab-like environment for machine learning. In BigLearn, NIPS Workshop, number EPFL-CONF-192376, 2011.
R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:24932537, Nov. 2011.
C. dos Santos and M. Gatti. Deep convolutional neural networks for sentiment analysis of short texts. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 6978, Dublin, Ireland, August 2014. Dublin City University and Association for Computational Linguistics.
C. Fellbaum. Wordnet and wordnets. In K. Brown, editor, Encyclopedia of Language and Linguistics, pages 665670, Oxford, 2005. Elsevier.
A. Graves and J. Schmidhuber. Framewise phoneme classification with bidirectional lstm and other neural network architectures. Neural Networks, 18(5):602610, 2005.
K. Greff, R. K. Srivastava, J. Koutnk, B. R. Steunebrink, and J. Schmidhuber. LSTM: A search space odyssey. CoRR, abs/1503.04069, 2015.
G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.  8
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Comput., 9(8):17351780, Nov. 1997.
T. Joachims. Text categorization with suport vector machines: Learning with many relevant features. In Proceedings of the 10th European Conference on Machine Learning, pages 137142. Springer-Verlag, 1998.
R. Johnson and T. Zhang. Effective use of word order for text categorization with convolutional neural networks. CoRR, abs/1412.1058, 2014.
K. S. Jones. A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation, 28(1):1121, 1972.
I. Kanaris, K. Kanaris, I. Houvardas, and E. Stamatatos. Words versus character n-grams for anti-spam filtering. International Journal on Artificial Intelligence Tools, 16(06):10471067, 2007.
Y. Kim. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 17461751, Doha, Qatar, October 2014. Association for Computational Linguistics.
Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541551, Winter 1989.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, November 1998.
J. Lehmann, R. Isele, M. Jakob, A. Jentzsch, D. Kontokostas, P. N. Mendes, S. Hellmann, M. Morsey, P. van Kleef, S. Auer, and C. Bizer. DBpedia - a large-scale, multilingual knowledge base extracted from wikipedia. Semantic Web Journal, 2014.
G. Lev, B. Klein, and L. Wolf. In defense of word embedding for generic text representation. In C. Biemann, S. Handschuh, A. Freitas, F. Meziane, and E. Mtais, editors, Natural Language Processing and Information Systems, volume 9103 of Lecture Notes in Computer Science, pages 3550. Springer International Publishing, 2015.
D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. Rcv1: A new benchmark collection for text categorization research. The Journal of Machine Learning Research, 5:361397, 2004.
J. McAuley and J. Leskovec. Hidden factors and hidden topics: Understanding rating dimensions with review text. In Proceedings of the 7th ACM Conference on Recommender Systems, RecSys 13, pages 165172, New York, NY, USA, 2013. ACM.
T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 31113119. 2013.
V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807814, 2010.
R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. In ICML 2013, volume 28 of JMLR Proceedings, pages 13101318. JMLR.org, 2013.
B. Polyak. Some methods of speeding up the convergence of iteration methods. {USSR} Computational Mathematics and Mathematical Physics, 4(5):1  17, 1964.
D. Rumelhart, G. Hintont, and R. Williams. Learning representations by back-propagating errors. Nature, 323(6088):533536, 1986.
C. D. Santos and B. Zadrozny. Learning character-level representations for part-of-speech tagging. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 18181826, 2014.
Y. Shen, X. He, J. Gao, L. Deng, and G. Mesnil. A latent semantic model with convolutional-pooling structure for information retrieval. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, pages 101110. ACM, 2014.
I. Sutskever, J. Martens, G. E. Dahl, and G. E. Hinton. On the importance of initialization and momentum in deep learning. In S. Dasgupta and D. Mcallester, editors, Proceedings of the 30th International Conference on Machine Learning (ICML-13), volume 28, pages 11391147. JMLR Workshop and Conference Proceedings, May 2013.
A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. J. Lang. Phoneme recognition using time-delay neural networks. Acoustics, Speech and Signal Processing, IEEE Transactions on, 37(3):328339, 1989.
C. Wang, M. Zhang, S. Ma, and L. Ru. Automatic online news issue construction in web environment. In Proceedings of the 17th International Conference on World Wide Web, WWW 08, pages 457466, New York, NY, USA, 2008. ACM.  9
E. Adeli-Mosabbeb and M. Fathy. Non-negative matrix completion for action detection. Image Vision Comput., 39:38  51, 2015.
N. Bissantz, L. Dumbgen, A. Munk, and B. Stratmann. Convergence analysis of generalized iteratively reweighted least squares algorithms on convex function spaces. SIAM Optimiz., 19(4):18281845, 2009.
S. Boyd and et al.. Distributed optimization and statistical learning via the alternating direction method of multipliers. Found. Trends Mach. Learn., 3(1):1122, 2011.
Heiko Braak, Kelly Tredici, Udo Rub, Rob de Vos, Ernst Jansen Steur, and Eva Braak. Staging of brain pathology related to sporadic parkinsons disease. Neurobio. of Aging, 24(2):197  211, 2003.
D. Cai, X. He, and J. Han. Semi-supervised discriminant analysis. In CVPR, 2007.
J.-F. Cai, E. Candes, and Z. Shen. A singular value thresholding algorithm for matrix completion. SIAM Optimiz., 20(4):19561982, 2010.
E. Candes, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? J. ACM, 58(3), 2011.
O. Chapelle, B. Scholkopf, and A. Zien, editors. Semi-Supervised Learning. MIT Press, 2006.
C. Croux and C. Dehon. Robust linear discriminant analysis using s-estimators. Canadian J. of Statistics, 29(3):473493, 2001.
F. De la Torre. A least-squares framework for component analysis. IEEE TPAMI, 34(6):10411055, 2012.
E. Elhamifar and R. Vidal. Robust classification using structured sparse representation. In CVPR, 2011.
S. Fidler, D. Skocaj, and A. Leonardis. Combining reconstructive and discriminative subspace methods for robust classification and regression by subsampling. IEEE TPAMI, 28(3):337350, 2006.
V. Fritsch, G. Varoquaux, B. Thyreau, J.-B. Poline, and B. Thirion. Detecting outliers in high-dimensional neuroimaging datasets with robust covariance estimators. Med. Image Anal., 16(7):1359  1370, 2012.
A. Goldberg, X. Zhu, B. Recht, J.-M. Xu, and R. Nowak. Transduction with matrix completion: Three birds with one stone. In NIPS, pages 757765, 2010.
D. Huang, R. Cabral, and F. De la Torre. Robust regression. In ECCV, pages 616630, 2012.
A. Joulin and F. Bach. A convex relaxation for weakly supervised classifiers. In ICML, 2012.
S. Kim, A. Magnani, and S. Boyd. Robust Fisher discriminant analysis. In NIPS, pages 659666, 2005.
H. Li, T. Jiang, and K. Zhang. Efficient and robust feature extraction by maximum margin criterion. In NIPS, pages 97104, 2003.
H. Li, C. Shen, A. van den Hengel, and Q. Shi. Worst-case linear discriminant analysis as scalable semidefinite feasibility problems. IEEE TIP, 24(8), 2015.
K.O. Lim and A. Pfefferbaum. Segmentation of MR brain images into cerebrospinal fluid spaces, white and gray matter. J. of Computer Assisted Tomography, 13:588593, 1989.
G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma. Robust recovery of subspace structures by low-rank representation. IEEE TPAMI, 35(1):171184, 2013.
C. Lu, J. Shi, and J. Jia. Online robust dictionary learning. In CVPR, pages 415422, June 2013.
K. Marek and et al.. The parkinson progression marker initiative (PPMI). Prog. Neurobiol., 95(4):629  635, 2011.
B. Pearce, A. Palmer, D. Bowen, G. Wilcock, M. Esiri, and A. Davison. Neurotransmitter dysfunction and atrophy of the caudate nucleus in alzheimers disease. Neurochem Pathol., 2(4):22132, 1985.
D. Shen and C. Davatzikos. HAMMER: Hierarchical attribute matching mechanism for elastic registration. IEEE TMI, 21:14211439, 2002.
K.-H. Thung, C.-Y. Wee, P.-T. Yap, and D. Shen. Neurodegenerative disease diagnosis using incomplete multi-modality data via matrix shrinkage and completion. NeuroImage, 91:386400, 2014.
N. Tzourio-Mazoyer and et al.. Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain. NeuroImage, 15(1):273289, 2002.
A. Wagner, J. Wright, A. Ganesh, Zihan Zhou, and Yi Ma. Towards a practical face recognition system: Robust registration and illumination by sparse representation. In CVPR, pages 597604, 2009.
Y. Wang, J. Nie, P.-T. Yap, G. Li, F. Shi, X. Geng, L. Guo, D. Shen, ADNI, et al. Knowledge-guided robust MRI brain extraction for diverse large-scale neuroimaging studies on humans and non-human primates. PLOS ONE, 9(1):e77810, 2014.
Y. Wang, J. Nie, P.-T. Yap, F. Shi, L. Guo, and D. Shen. Robust deformable-surface-based skull-stripping for large-scale studies. In MICCAI, volume 6893, pages 635642, 2011.
A. Worker and et al.. Cortical thickness, surface area and volume measures in parkinsons disease, multiple system atrophy and progressive supranuclear palsy. PLOS ONE, 9(12), 2014.
D. Zhang, Y. Wang, L. Zhou, H. Yuan, D. Shen, ADNI, et al. Multimodal classification of Alzheimers disease and mild cognitive impairment. NeuroImage, 55(3):856867, 2011.
Y. Zhang, M. Brady, and S. Smith. Segmentation of brain MR images through a hidden Markov random field model and the expectation-maximization algorithm. IEEE TMI, 20(1):4557, 2001.
Xiaojin Zhu. Semi-supervised learning literature survey. Technical Report 1530, Computer Sciences, University of Wisconsin-Madison, 2005.
D. Ziegler and J. Augustinack. Harnessing advances in structural MRI to enhance research on Parkinsons disease. Imaging Med., 5(2):9194, 2013.  9
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time Analysis of the Multiarmed Bandit Problem. Machine Learning, 47(2-3):235256, 2002.
Mohammad Gheshlaghi Azar, Alessandro Lazaric, and Emma Brunskill. Online Stochastic Optimization under Correlated Bandit Feedback. In International Conference on Machine Learning, 2014.
Sebastien Bubeck, Remi Munos, and Gilles Stoltz. Pure Exploration in Finitely-Armed and Continuously-Armed Bandits. Theoretical Computer Science, 412:18321852, 2011.
Sebastien Bubeck, Remi Munos, Gilles Stoltz, and Csaba Szepesvari. X-armed Bandits. Journal of Machine Learning Research, 12:15871627, 2011.
Sebastien Bubeck, Gilles Stoltz, and Jia Yuan Yu. Lipschitz Bandits without the Lipschitz Constant. In Algorithmic Learning Theory, 2011.
Adam D. Bull. Adaptive-treed bandits. Bernoulli, 21(4):22892307, 2015.
Richard Combes and Alexandre Proutiere. Unimodal Bandits without Smoothness. ArXiv e-prints: http://arxiv.org/abs/1406.7447, 2015.
Pierre-Arnaud Coquelin and Remi Munos. Bandit Algorithms for Tree Search. In Uncertainty in Artificial Intelligence, 2007.
Robert Kleinberg, Alexander Slivkins, and Eli Upfal. Multi-armed Bandit Problems in Metric Spaces. In Symposium on Theory Of Computing, 2008.
Levente Kocsis and Csaba Szepesvari. Bandit based Monte-Carlo Planning. In European Conference on Machine Learning, 2006.
Remi Munos. Optimistic Optimization of Deterministic Functions without the Knowledge of its Smoothness. In Neural Information Processing Systems, 2011.
Remi Munos. From Bandits to Monte-Carlo Tree Search: The Optimistic Principle Applied to Optimization and Planning. Foundations and Trends in Machine Learning, 7(1):1130, 2014.
Philippe Preux, Remi Munos, and Michal Valko. Bandits Attack Function Optimization. In Congress on Evolutionary Computation, 2014.
Aleksandrs Slivkins. Multi-armed Bandits on Implicit Metric Spaces. In Neural Information Processing Systems, 2011.
Michal Valko, Alexandra Carpentier, and Remi Munos. Stochastic Simultaneous Optimistic Optimization. In International Conference on Machine Learning, 2013.  9
E. Abbe and C. Sandon. Community detection in general stochastic block models: fundamental limits and efficient recovery algorithms. arXiv:1503.00609. To appear in FOCS15., March 2015.
A. Decelle, F. Krzakala, C. Moore, and L. Zdeborova. Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications. Phys. Rev. E, 84:066106, December 2011.
L. Massoulie. Community detection thresholds and the weak Ramanujan property. In STOC 2014: 46th Annual Symposium on the Theory of Computing, pages 110, New York, United States, June 2014.  8
E. Mossel, J. Neeman, and A. Sly. A proof of the block model threshold conjecture. Available online at arXiv:1311.4115
, January 2014.
E. Abbe, A. S. Bandeira, and G. Hall. Exact recovery in the stochastic block model. To appear in IEEE Transactions on Information Theory. Available at ArXiv:1405.3267, May 2014.
E. Mossel, J. Neeman, and A. Sly. Consistency thresholds for binary symmetric block models. Arxiv:arXiv:1407.1591. To appear in STOC15., July 2014.
J. Xu Y. Chen. Statistical-computational tradeoffs in planted problems and submatrix localization with a growing number of clusters and submatrices. arXiv:1402.1267, February 2014.
P. K. Gopalan and D. M. Blei. Efficient discovery of overlapping communities in massive networks. Proceedings of the National Academy of Sciences, 2013.
P. W. Holland, K. Laskey, and S. Leinhardt. Stochastic blockmodels: First steps. Social Networks, 5(2):109137, 1983.
T.N. Bui, S. Chaudhuri, F.T. Leighton, and M. Sipser. Graph bisection algorithms with good average case behavior. Combinatorica, 7(2):171191, 1987.
M.E. Dyer and A.M. Frieze. The solution of some random NP-hard problems in polynomial expected time. Journal of Algorithms, 10(4):451  489, 1989.
Mark Jerrum and Gregory B. Sorkin. The metropolis algorithm for graph bisection. Discrete Applied Mathematics, 82(13):155  175, 1998.
A. Condon and R. M. Karp. Algorithms for graph partitioning on the planted partition model. Lecture Notes in Computer Science, 1671:221232, 1999.
T. A. B. Snijders and K. Nowicki. Estimation and Prediction for Stochastic Blockmodels for Graphs with Latent Block Structure. Journal of Classification, 14(1):75100, January 1997.
F. McSherry. Spectral partitioning of random graphs. In Foundations of Computer Science, 2001. Proceedings. 42nd IEEE Symposium on, pages 529537, 2001.
P. J. Bickel and A. Chen. A nonparametric view of network models and newmangirvan and other modularities. Proceedings of the National Academy of Sciences, 2009.
K. Rohe, S. Chatterjee, and B. Yu. Spectral clustering and the high-dimensional stochastic blockmodel. The Annals of Statistics, 39(4):18781915, 08 2011.
D. S. Choi, P. J. Wolfe, and E. M. Airoldi. Stochastic blockmodels with a growing number of classes. Biometrika, pages 112, 2012.
V. Vu. A simple svd algorithm for finding hidden partitions. Available online at arXiv:1404.3918, 2014.
J. Xu B. Hajek, Y. Wu. Achieving exact cluster recovery threshold via semidefinite programming. arXiv:1412.6156, November 2014.
A. S. Bandeira. Random laplacian matrices and convex relaxations. arXiv:1504.03987, 2015.
S. Yun and A. Proutiere. Accurate community detection in the stochastic block model via spectral algorithms. arXiv:1412.7335, December 2014.
E. Mossel, J. Neeman, and A. Sly. Belief propagation, robust reconstruction, and optimal recovery of block models. Arxiv:arXiv:1309.1380, 2013.
O. Guedon and R. Vershynin. Community detection in sparse networks via Grothendiecks inequality. ArXiv:1411.4686, November 2014.
P. Chin, A. Rao, and V. Vu. Stochastic block model and community detection in the sparse graphs: A spectral algorithm with optimal rate of recovery. arXiv:1501.05021, January 2015.
E. Mossel, J. Neeman, and A. Sly. Stochastic block models and reconstruction. Available online at arXiv:1202.1499
C. Borgs, J. Chayes, and A. Smith. Private graphon estimation for sparse graphs. In preparation, 2015.
E. Abbe and C. Sandon. Recovering communities in the general stochastic block model without knowing the parameters. arXiv:1506.03729, June 2015.
C. Bordenave, M. Lelarge, and L. Massoulie. Non-backtracking spectrum of random graphs: community detection and non-regular ramanujan graphs. Available at arXiv:1501.06087, 2015.
S. Bhattacharyya and P. J. Bickel. Community Detection in Networks using Graph Distance. ArXiv e-prints, January 2014.
N. Alon and N. Kahale. A spectral technique for coloring random 3-colorable graphs. In SIAM Journal on Computing, pages 346355, 1994.
A. Y. Zhang H. H. Zhou C. Gao, Z. Ma. Achieving optimal misclassification proportion in stochastic block model. arXiv:1505.03772, 2015.  9
Bottou, L. Online algorithms and stochastic approximations. In Online Learning and Neural Networks. Cambridge University Press, 1998.
Dean, J, Corrado, G, Monga, R, Chen, K, Devin, M, Le, Q, Mao, M, Ranzato, M, Senior, A, Tucker, P, Yang, K, and Ng, A. Large scale distributed deep networks. In NIPS. 2012.
Krizhevsky, A, Sutskever, I, and Hinton, G. E. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pages 11061114, 2012.
Sermanet, P, Eigen, D, Zhang, X, Mathieu, M, Fergus, R, and LeCun, Y. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks. ArXiv, 2013.
Nocedal, J and Wright, S. Numerical Optimization, Second Edition. Springer New York, 2006.
Polyak, B. T and Juditsky, A. B. Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 30(4):838855, 1992.
Bertsekas, D. P and Tsitsiklis, J. N. Parallel and Distributed Computation. Prentice Hall, 1989.
Hestenes, M. R. Optimization theory: the finite dimensional case. Wiley, 1975.
Boyd, S, Parikh, N, Chu, E, Peleato, B, and Eckstein, J. Distributed optimization and statistical learning via the alternating direction method of multipliers. Found. Trends Mach. Learn., 3(1):1122, 2011.
Shamir, O. Fundamental limits of online and distributed algorithms for statistical learning and estimation. In NIPS. 2014.
Yadan, O, Adams, K, Taigman, Y, and Ranzato, M. Multi-gpu training of convnets. In Arxiv. 2013.
Paine, T, Jin, H, Yang, J, Lin, Z, and Huang, T. Gpu asynchronous stochastic gradient descent to speed up neural network training. In Arxiv. 2013.
Seide, F, Fu, H, Droppo, J, Li, G, and Yu, D. 1-bit stochastic gradient descent and application to dataparallel distributed training of speech dnns. In Interspeech 2014, September 2014.
Bekkerman, R, Bilenko, M, and Langford, J. Scaling up machine learning: Parallel and distributed approaches. Camridge Universityy Press, 2011.
Choromanska, A, Henaff, M. B, Mathieu, M, Arous, G. B, and LeCun, Y. The loss surfaces of multilayer networks. In AISTATS, 2015.
Ho, Q, Cipar, J, Cui, H, Lee, S, Kim, J. K, Gibbons, P. B, Gibson, G. A, Ganger, G, and Xing, E. P. More effective distributed ml via a stale synchronous parallel parameter server. In NIPS. 2013.
Azadi, S and Sra, S. Towards an optimal stochastic alternating direction method of multipliers. In ICML, 2014.
Borkar, V. Asynchronous stochastic approximations. SIAM Journal on Control and Optimization, 36(3):840851, 1998.
Nedic, A, Bertsekas, D, and Borkar, V. Distributed asynchronous incremental subgradient methods. In Inherently Parallel Algorithms in Feasibility and Optimization and their Applications, volume 8 of Studies in Computational Mathematics, pages 381  407. 2001.
Langford, J, Smola, A, and Zinkevich, M. Slow learners are fast. In NIPS, 2009.
Agarwal, A and Duchi, J. Distributed delayed stochastic optimization. In NIPS. 2011.
Recht, B, Re, C, Wright, S. J, and Niu, F. Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent. In NIPS, 2011.
Zinkevich, M, Weimer, M, Smola, A, and Li, L. Parallelized stochastic gradient descent. In NIPS, 2010.
Nesterov, Y. Smooth minimization of non-smooth functions. Math. Program., 103(1):127152, 2005.
Lan, G. An optimal method for stochastic composite optimization. Mathematical Programming, 133(12):365397, 2012.
Sutskever, I, Martens, J, Dahl, G, and Hinton, G. On the importance of initialization and momentum in deep learning. In ICML, 2013.
Zhang, R and Kwok, J. Asynchronous distributed admm for consensus optimization. In ICML, 2014.
Ouyang, H, He, N, Tran, L, and Gray, A. Stochastic alternating direction method of multipliers. In Proceedings of the 30th International Conference on Machine Learning, pages 8088, 2013.
Wan, L, Zeiler, M. D, Zhang, S, LeCun, Y, and Fergus, R. Regularization of neural networks using dropconnect. In ICML, 2013.
Cesa-Bianchi, N, Conconi, A, and Gentile, C. On the generalization ability of on-line learning algorithms. IEEE Transactions on Information Theory, 50(9):20502057, 2004.
Nesterov, Y. Introductory lectures on convex optimization, volume 87. Springer Science & Business Media, 2004.  9
N. Barbieri, F. Bonchi, and G. Manco. Topic-aware social influence propagation models. In ICDM, pages 8190, 2012.
N. Buchbinder, M. Feldman, J. S. Naor, and R. Schwartz. A tight linear time (1/2)approximation for unconstrained submodular maximization. In FOCS, pages 649658, 2012.
G. Calinescu, C. Chekuri, M. Pal, and J. Vondrak. Maximizing a monotone submodular function subject to a matroid constraint. SIAM Journal on Computing, 40(6):17401766, 2011.
P. Domingos and M. Richardson. Mining the network value of customers. In KDD, pages 5766, 2001.
Y. Filmus and J. Ward. Monotone submodular maximization over a matroid via non-oblivious local search. SIAM Journal on Computing, 43(2):514542, 2014.
J. Goldenberg, B. Libai, and E. Muller. Talk of the network: A complex systems look at the underlying process of word-of-mouth. Marketing Letters, 12(3):211223, 2001.
J. Goldenberg, B. Libai, and E. Muller. Using complex systems analysis to advance marketing theory development: Modeling heterogeneity effects on new product growth through stochastic cellular automata. Academy of Marketing Science Review, 9(3):118, 2001.
I. Gridchyn and V. Kolmogorov. Potts model, parametric maxflow and k-submodular functions. In ICCV, pages 23202327, 2013.
A. Huber and V. Kolmogorov. Towards minimizing k-submodular functions. In Combinatorial Optimization, pages 451462. Springer Berlin Heidelberg, 2012.
S. Iwata, S. Tanigawa, and Y. Yoshida. Improved approximation algorithms for k-submodular function maximization. In SODA, 2016. to appear.
D. Kempe, J. Kleinberg, and E. Tardos. Maximizing the spread of influence through a social network. In KDD, pages 137146, 2003.
C.-W. Ko, J. Lee, and M. Queyranne. An exact algorithm for maximum entropy sampling. Operations Research, 43(4):684691, 1995.
A. Krause, H. B. McMahon, C. Guestrin, and A. Gupta. Robust submodular observation selection. The Journal of Machine Learning Research, 9:27612801, 2008.
A. Krause, A. Singh, and C. Guestrin. Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies. The Journal of Machine Learning Research, 9:235284, 2008.
H. Lin and J. Bilmes. Multi-document summarization via budgeted maximization of submodular functions. In NAACL/HLT, pages 912920, 2010.
M. Minoux. Accelerated greedy algorithms for maximizing submodular set functions. Optimization Techniques, 7:234243, 1978.
B. Mirzasoleiman, A. Badanidiyuru, A. Karbasi, J. Vondrak, and A. Krause. Lazier than lazy greedy. In AAAI, pages 18121818, 2015.
G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of approximations for maximizing submodular set functionsI. Mathematical Programming, 14(1):265294, 1978.
M. Richardson and P. Domingos. Mining knowledge-sharing sites for viral marketing. In KDD, pages 6170, 2002.
A. P. Singh, A. Guillory, and J. A. Bilmes. On bisubmodular maximization. In AISTATS, pages 10551063, 2012.
M. Sviridenko. A note on maximizing a submodular set function subject to a knapsack constraint. Operations Research Letters, 32(1):4143, 2004.
J. Ward and S. Zivny. Maximizing k-submodular functions and beyond. arXiv:1409.1399v1, 2014, A preliminary version appeared in SODA, pages 14681481, 2014.  9
M.-F. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. J. Comput. Syst. Sci., 75(1):7889, 2009.
M.-F. Balcan and P. M. Long. Active and passive learning of linear separators under logconcave distributions. In COLT, 2013.
A. Beygelzimer, D. Hsu, J. Langford, and T. Zhang. Active learning with an ERM oracle, 2009. 8
A. Beygelzimer, D. Hsu, J. Langford, and T. Zhang. Agnostic active learning without constraints. In NIPS, 2010.
Nader H. Bshouty and Lynn Burroughs. Maximizing agreements with one-sided error with applications to heuristic learning. Machine Learning, 59(1-2):99123, 2005.
D. A. Cohn, L. E. Atlas, and R. E. Ladner. Improving generalization with active learning. Machine Learning, 15(2), 1994.
K. Crammer, M. J. Kearns, and J. Wortman. Learning from data of variable quality. In NIPS, 2005.
S. Dasgupta. Coarse sample complexity bounds for active learning. In NIPS, 2005.
S. Dasgupta, D. Hsu, and C. Monteleoni. A general agnostic active learning algorithm. In NIPS, 2007.
O. Dekel, C. Gentile, and K. Sridharan. Selective sampling and active learning from single and multiple teachers. JMLR, 13:26552697, 2012.
P. Donmez and J. Carbonell. Proactive learning: Cost-sensitive active learning with multiple imperfect oracles. In CIKM, 2008.
Meng Fang, Xingquan Zhu, Bin Li, Wei Ding, and Xindong Wu. Self-taught active learning from crowds. In ICDM, pages 858863. IEEE, 2012.
S. Hanneke. A bound on the label complexity of agnostic active learning. In ICML, 2007.
D. Hsu. Algorithms for Active Learning. PhD thesis, UC San Diego, 2010.
Panagiotis G Ipeirotis, Foster Provost, Victor S Sheng, and Jing Wang. Repeated labeling using multiple noisy labelers. Data Mining and Knowledge Discovery, 28(2):402441, 2014.
Adam Tauman Kalai, Varun Kanade, and Yishay Mansour. Reliable agnostic learning. J. Comput. Syst. Sci., 78(5):14811495, 2012.
Varun Kanade and Justin Thaler. Distribution-independent reliable learning. In COLT, 2014.
C. H. Lin, Mausam, and D. S. Weld. To re(label), or not to re(label). In HCOMP, 2014.
C.H. Lin, Mausam, and D.S. Weld. Reactive learning: Actively trading off larger noisier training sets against smaller cleaner ones. In ICML Workshop on Crowdsourcing and Machine Learning and ICML Active Learning Workshop, 2015.
L. Malago, N. Cesa-Bianchi, and J. Renders. Online active learning with strong and weak annotators. In NIPS Workshop on Learning from the Wisdom of Crowds, 2014.
R. D. Nowak. The geometry of generalized binary search. IEEE Transactions on Information Theory, 57(12):78937906, 2011.
Hans Ulrich Simon. Pac-learning in the presence of one-sided classication noise. Ann. Math. Artif. Intell., 71(4):283300, 2014.
S. Song, K. Chaudhuri, and A. D. Sarwate. Learning from data with heterogeneous noise using sgd. In AISTATS, 2015.
R. Urner, S. Ben-David, and O. Shamir. Learning from weak teachers. In AISTATS, pages 12521260, 2012.
S. Vijayanarasimhan and K. Grauman. Whats it going to cost you?: Predicting effort vs. informativeness for multi-label image annotations. In CVPR, pages 22622269, 2009.
S. Vijayanarasimhan and K. Grauman. Cost-sensitive active visual category learning. IJCV, 91(1):2444, 2011.
P. Welinder, S. Branson, S. Belongie, and P. Perona. The multidimensional wisdom of crowds. In NIPS, pages 24242432, 2010.
Y. Yan, R. Rosales, G. Fung, and J. G. Dy. Active learning from crowds. In ICML, pages 11611168, 2011.
Y. Yan, R. Rosales, G. Fung, F. Farooq, B. Rao, and J. G. Dy. Active learning from multiple knowledge sources. In AISTATS, pages 13501357, 2012.
C. Zhang and K. Chaudhuri. Beyond disagreement-based agnostic active learning. In NIPS, 2014.  9
Robert E. Schapire and Yoram Singer. BoosTexter: A Boosting-based System for Text Categorization. Machine Learning, 39(2-3):135168, 2000.  8
Zafer Barutcuoglu and Robert E. Schapire and Olga G. Troyanskaya. Hierarchical multi-label prediction of gene function. Bioinformatics, 22(7):227, 2006.
Matthew R. Boutell and Jiebo Luo and Xipeng Shen and Christopher M. Brown. Learning Multi-Label Scene Classification. Pattern Recognition, 37(9):17571771, 2004.
Grigorios Tsoumakas and Ioannis Katakis and Ioannis P. Vlahavas. Mining Multi-label Data. In Data Mining and Knowledge Discovery Handbook, pages 667685, 2010. Springer US.
Krzysztof Dembczynski and Weiwei Cheng and Eyke Hullermeier. Bayes Optimal Multilabel Classification via Probabilistic Classifier Chains. Proceedings of the 27th International Conference on Machine Learning, pages 279286, Haifa, Israel, 2010. Omnipress.
Jesse Read and Bernhard Pfahringer and Geoffrey Holmes and Eibe Frank. Classifier Chains for Multilabel Classification. In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases: Part II, pages 254269, Berlin, Heidelberg, 2009. Springer-Verlag.
Yi Zhang and Jeff G. Schneider. Maximum Margin Output Coding. Proceedings of the 29th International Conference on Machine Learning, pages 15751582, New York, NY, 2012. Omnipress.
Yuhong Guo and Suicheng Gu. Multi-Label Classification Using Conditional Dependency Networks. Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence, pages 1300 1305, Barcelona, Catalonia, Spain, 2011. AAAI Press.
Sheng-Jun Huang and Zhi-Hua Zhou. Multi-Label Learning by Exploiting Label Correlations Locally. Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, Toronto, Ontario, Canada, 2012. AAAI Press.
Feng Kang and Rong Jin and Rahul Sukthankar. Correlated Label Propagation with Application to Multilabel Learning. 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 17191726, New York, NY, 2006. IEEE Computer Society.
Weiwei Liu and Ivor W. Tsang. Large Margin Metric Learning for Multi-Label Prediction. Proceedings of the Twenty-Ninth Conference on Artificial Intelligence, pages 28002806, Texas, USA, 2015. AAAI Press.
Mingkui Tan and Qinfeng Shi and Anton van den Hengel and Chunhua Shen and Junbin Gao and Fuyuan Hu and Zhen Zhang. Learning Graph Structure for Multi-Label Image Classification via Clique Generation. The IEEE Conference on Computer Vision and Pattern Recognition, 2015.
Daniel Hsu and Sham Kakade and John Langford and Tong Zhang. Multi-Label Prediction via Compressed Sensing. Advances in Neural Information Processing Systems, pages 772780, 2009. Curran Associates, Inc.
Yi Zhang and Jeff G. Schneider. Multi-Label Output Codes using Canonical Correlation Analysis. Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pages 873 882, Fort Lauderdale, USA, 2011. JMLR.org.
Farbound Tai and Hsuan-Tien Lin. Multilabel Classification with Principal Label Space Transformation. Neural Computation, 24(9):25082542, 2012.
Min-Ling Zhang and Kun Zhang. Multi-label learning by exploiting label dependency. Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 9991008, QWashington, DC, USA, 2010. ACM.
John Shawe-Taylor and Peter L. Bartlett and Robert C. Williamson and Martin Anthony. Structural Risk Minimization Over Data-Dependent Hierarchies. IEEE Transactions on Information Theory, 44(5):1926 1940, 1998.
Kristin P. Bennett and Nello Cristianini and John Shawe-Taylor and Donghui Wu. Enlarging the Margins in Perceptron Decision Trees. Machine Learning, 41(3):295313, 2000.
Michael J. Kearns and Robert E. Schapire. Efficient Distribution-free Learning of Probabilistic Concepts. Proceedings of the 31st Symposium on the Foundations of Computer Science, pages 382391, Los Alamitos, CA, 1990. IEEE Computer Society Press.
Peter L. Bartlett and John Shawe-Taylor. Generalization Performance of Support Vector Machines and Other Pattern Classifiers. Advances in Kernel Methods - Support Vector Learning, pages 4354, Cambridge, MA, USA, 1998. MIT Press.
Rong-En Fan and Kai-Wei Chang and Cho-Jui Hsieh and Xiang-Rui Wang and Chih-Jen Lin. LIBLINEAR: A Library for Large Linear Classification. Journal of Machine Learning Research, 9:18711874, 2008.
Qi Mao and Ivor Wai-Hung Tsang and Shenghua Gao. Objective-Guided Image Annotation. IEEE Transactions on Image Processing, 22(4):15851597, 2013.  9
Christoph Studer, Patrick Kuppinger, Graeme Pope, and Helmut Bolcskei. Recovery of Sparsely Corrupted Signals. IEEE Transaction on Information Theory, 58(5):31153130, 2012.
Peter J. Rousseeuw and Annick M. Leroy. Robust Regression and Outlier Detection. John Wiley and Sons, 1987.
John Wright, Alan Y. Yang, Arvind Ganesh, S. Shankar Sastry, and Yi Ma. Robust Face Recognition via Sparse Representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(2):210227, 2009.
John Wright and Yi Ma. Dense Error Correction via `1 Minimization. IEEE Transaction on Information Theory, 56(7):35403560, 2010.
Nam H. Nguyen and Trac D. Tran. Exact recoverability from dense corrupted observations via L1 minimization. IEEE Transaction on Information Theory, 59(4):20362058, 2013.
Yudong Chen, Constantine Caramanis, and Shie Mannor. Robust Sparse Regression under Adversarial Corruption. In 30th International Conference on Machine Learning (ICML), 2013.
Brian McWilliams, Gabriel Krummenacher, Mario Lucic, and Joachim M. Buhmann. Fast and Robust Least Squares Estimation in Corrupted Linear Models. In 28th Annual Conference on Neural Information Processing Systems (NIPS), 2014.
Adrien-Marie Legendre (1805). On the Method of Least Squares. In (Translated from the French) D.E. Smith, editor, A Source Book in Mathematics, pages 576579. New York: Dover Publications, 1959.
Peter J. Rousseeuw. Least Median of Squares Regression. Journal of the American Statistical Association, 79(388):871880, 1984.
Peter J. Rousseeuw and Katrien Driessen. Computing LTS Regression for Large Data Sets. Journal of Data Mining and Knowledge Discovery, 12(1):2945, 2006.
Thomas Blumensath and Mike E. Davies. Iterative Hard Thresholding for Compressed Sensing. Applied and Computational Harmonic Analysis, 27(3):265274, 2009.
Prateek Jain, Ambuj Tewari, and Purushottam Kar. On Iterative Hard Thresholding Methods for High-dimensional M-Estimation. In 28th Annual Conference on Neural Information Processing Systems (NIPS), 2014.
Yiyuan She and Art B. Owen. Outlier Detection Using Nonconvex Penalized Regression. arXiv:1006.2592 (stat.ME).
Rahul Garg and Rohit Khandekar. Gradient descent with sparsification: an iterative algorithm for sparse recovery with restricted isometry property. In 26th International Conference on Machine Learning (ICML), 2009.
Allen Y. Yang, Arvind Ganesh, Zihan Zhou, Shankar Sastry, and Yi Ma. A Review of Fast `1 -Minimization Algorithms for Robust Face Recognition. CoRR abs/1007.3753, 2012.
Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection. The Annals of Statistics, 28(5):13021338, 2000.
Thomas Blumensath. Sampling and reconstructing signals from a union of linear subspaces. IEEE Transactions on Information Theory, 57(7):46604671, 2011.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Y. Eldar and G. Kutyniok, editors, Compressed Sensing, Theory and Applications, chapter 5, pages 210268. Cambridge University Press, 2012.  9
R. Agrawal, A. Gupta, Y. Prabhu, and M. Varma. Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages. In WWW, pages 1324, 2013.
J. Weston, A. Makadia, and H. Yee. Label partitioning for sublinear ranking. In ICML, 2013.
D. Hsu, S. Kakade, J. Langford, and T. Zhang. Multi-label prediction via compressed sensing. In NIPS, 2009.
M. Cisse, N. Usunier, T. Artieres, and P. Gallinari. Robust bloom filters for large multilabel classification tasks. In NIPS, pages 18511859, 2013.
F. Tai and H.-T. Lin. Multi-label classification with principal label space transformation. In Workshop proceedings of learning from multi-label data, 2010.
K. Balasubramanian and G. Lebanon. The landmark selection method for multiple output prediction. In ICML, 2012.
W. Bi and J.T.-Y. Kwok. Efficient multi-label classification with many labels. In ICML, 2013.
Y. Zhang and J. G. Schneider. Multi-label output codes using canonical correlation analysis. In AISTATS, pages 873882, 2011.
H.-F. Yu, P. Jain, P. Kar, and I. S. Dhillon. Large-scale multi-label learning with missing labels. ICML, 2014.
Y.-N. Chen and H.-T. Lin. Feature-aware label space dimension reduction for multi-label classification. In NIPS, pages 15381546, 2012.
C.-S. Feng and H.-T. Lin. Multi-label classification with error-correcting codes. JMLR, 20, 2011.
S. Ji, L. Tang, S. Yu, and J. Ye. Extracting shared subspace for multi-label classification. In KDD, 2008.
J. Weston, S. Bengio, and N. Usunier. Wsabie: Scaling up to large vocabulary image annotation. In IJCAI, 2011.
Z. Lin, G. Ding, M. Hu, and J. Wang. Multi-label classification via feature-aware implicit label space encoding. In ICML, pages 325333, 2014.
Y. Prabhu and M. Varma. FastXML: a fast, accurate and stable tree-classifier for extreme multi-label learning. In KDD, pages 263272, 2014.
Wikipedia dataset for the 4th large scale hierarchical text classification challenge, 2014.
A. Ng and M. Jordan. On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes. In NIPS, 2002.
K. Q. Weinberger and L. K. Saul. An introduction to nonlinear dimensionality reduction by maximum variance unfolding. In AAAI, pages 16831686, 2006.
B. Shaw and T. Jebara. Minimum volume embedding. In AISTATS, pages 460467, 2007.
P. Jain, R. Meka, and I. S. Dhillon. Guaranteed rank minimization via singular value projection. In NIPS, pages 937945, 2010.
P. Sprechmann, R. Litman, T. B. Yakar, A. Bronstein, and G. Sapiro. Efficient Supervised Sparse Analysis and Synthesis Operators. In NIPS, 2013.
P. Kar, K. B. Sriperumbudur, P. Jain, and H. Karnick. On the Generalization Ability of Online Learning Algorithms for Pairwise Loss Functions. In ICML, 2013.
J. Leskovec and A. Krevl. SNAP Datasets: Stanford large network dataset collection, 2014.
R. Wetzker, C. Zimmermann, and C. Bauckhage. Analyzing social bookmarking systems: A del.icio.us cookbook. In Mining Social Data (MSoDa) Workshop Proceedings, ECAI, pages 2630, July 2008.
A. Zubiaga. Enhancing navigation on wikipedia with social tags, 2009.
I. Katakis, G. Tsoumakas, and I. Vlahavas. Multilabel text classification for automated tag suggestion. In Proceedings of the ECML/PKDD 2008 Discovery Challenge, 2008.
C. Snoek, M. Worring, J. van Gemert, J.-M. Geusebroek, and A. Smeulders. The challenge problem for automated detection of 101 semantic concepts in multimedia. In ACM Multimedia, 2006.
G. Tsoumakas, I. Katakis, and I. Vlahavas. Effective and effcient multilabel classification in domains with large number of labels. In ECML/PKDD, 2008.
J. Menca E. L.and Furnkranz. Efficient pairwise multilabel classification for large-scale problems in the legal domain. In ECML/PKDD, 2008.
Y.-N. Chen and H.-T. Lin. Feature-aware label space dimension reduction for multi-label classification. In NIPS, pages 15381546, 2012.
B. Hariharan, S. V. N. Vishwanathan, and M. Varma. Efficient max-margin multi-label classification with applications to zero-shot learning. ML, 2012.  9
A. Ben-Tal and A. Nemirovski. Lectures on modern convex optimization, volume 2. 2001.
J. R. Fienup. Phase retrieval algorithms: a comparison. Applied optics, 21:27582769, 1982.
Y. Chen, X. Yi, and C. Caramanis. A convex formulation for mixed regression with two components: Minimax optimal rates. In Conference on Learning Theory (COLT), 2014.
E. J. Cands, T. Strohmer, and V. Voroninski. Phaselift: Exact and stable signal recovery from magnitude measurements via convex programming. Communications on Pure and Applied Mathematics, 66(8):10171026, 2013.
I. Waldspurger, A. dAspremont, and S. Mallat. Phase recovery, maxcut and complex semidefinite programming. Mathematical Programming, 149(1-2):4781, 2015.
Y. Shechtman, Y. C. Eldar, A. Szameit, and M. Segev. Sparsity based sub-wavelength imaging with partially incoherent light via quadratic compressed sensing. Optics express, 19(16), 2011.
E. J. Cands and X. Li. Solving quadratic equations via PhaseLift when there are about as many equations as unknowns. Foundations of Computational Math., 14(5):10171026, 2014.
H. Ohlsson, A. Yang, R. Dong, and S. Sastry. Cprlan extension of compressive sensing to the phase retrieval problem. In Advances in Neural Information Processing Systems (NIPS), 2012.
Y. Chen, Y. Chi, and A. J. Goldsmith. Exact and stable covariance estimation from quadratic sampling via convex programming. IEEE Trans. on Inf. Theory, 61(7):40344059, 2015.
T. Cai and A. Zhang. ROP: Matrix recovery via rank-one projections. Annals of Stats.
K. Jaganathan, S. Oymak, and B. Hassibi. Recovery of sparse 1-D signals from the magnitudes of their Fourier transform. In IEEE ISIT, pages 14731477, 2012.
D. Gross, F. Krahmer, and R. Kueng. A partial derandomization of phaselift using spherical designs. Journal of Fourier Analysis and Applications, 21(2):229266, 2015.
E. J. Cands, X. Li, and M. Soltanolkotabi. Phase retrieval via Wirtinger flow: Theory and algorithms. IEEE Transactions on Information Theory, 61(4):19852007, April 2015.
P. Netrapalli, P. Jain, and S. Sanghavi. Phase retrieval using alternating minimization. NIPS, 2013.
P. Schniter and S. Rangan. Compressive phase retrieval via generalized approximate message passing. IEEE Transactions on Signal Processing, 63(4):10431055, Feb 2015.
A. Repetti, E. Chouzenoux, and J.-C. Pesquet. A nonconvex regularized approach for phase retrieval. International Conference on Image Processing, pages 17531757, 2014.
K. Wei. Phase retrieval via Kaczmarz methods. arXiv:1502.01822, 2015.
C. White, R. Ward, and S. Sanghavi. The local convexity of solving quadratic equations. arXiv:1506.07868, 2015.
Y. Shechtman, A. Beck, and Y. C. Eldar. GESPAR: Efficient phase retrieval of sparse signals. IEEE Transactions on Signal Processing, 62(4):928938, 2014.
M. Soltanolkotabi. Algorithms and Theory for Clustering and Nonconvex Quadratic Programming. PhD thesis, Stanford University, 2014.
L. N. Trefethen and D. Bau III. Numerical linear algebra, volume 50. SIAM, 1997.
E. J. Cands, X. Li, and M. Soltanolkotabi. Phase retrieval from coded diffraction patterns. to appear in Applied and Computational Harmonic Analysis, 2014.
R. Keshavan, A. Montanari, and S. Oh. Matrix completion from noisy entries. Journal of Machine Learning Research, 11:20572078, 2010.
P. Jain, P. Netrapalli, and S. Sanghavi. Low-rank matrix completion using alternating minimization. In ACM symposium on Theory of computing, pages 665674, 2013.
R. Sun and Z. Luo. Guaranteed matrix completion via nonconvex factorization. FOCS, 2015.
S. Arora, R. Ge, T. Ma, and A. Moitra. Simple, efficient, and neural algorithms for sparse coding. Conference on Learning Theory (COLT), 2015.
J. Sun, Q. Qu, and J. Wright. Complete dictionary recovery over the sphere. ICML, 2015.
S. Balakrishnan, M. J. Wainwright, and B. Yu. Statistical guarantees for the EM algorithm: From population to sample-based analysis. arXiv preprint arXiv:1408.2156, 2014.
X. Yi, C. Caramanis, and S. Sanghavi. Alternating minimization for mixed linear regression. International Conference on Machine Learning, June 2014.  9
J. Craig. Complex diseases: Research and applications. Nature Education, 1(1):184, 2008.
J. Varga, C.P. Denton, and F.M. Wigley. Scleroderma: From Pathogenesis to Comprehensive Management. Springer Science & Business Media, 2012.
J. Lotvall et al. Asthma endotypes: a new approach to classification of disease entities within the asthma syndrome. Journal of Allergy and Clinical Immunology, 127(2):355360, 2011.
L.D. Wiggins, D.L. Robins, L.B. Adamson, R. Bakeman, and C.C. Henrich. Support for a dimensional view of autism spectrum disorders in toddlers. Journal of autism and developmental disorders, 42(2):191 200, 2012.
P.J. Castaldi et al. Cluster analysis in the copdgene study identifies subtypes of smokers with distinct patterns of airway disease and emphysema. Thorax, 2014.
S. Saria and A. Goldenberg. Subtyping: What Is It and Its Role in Precision Medicine. IEEE Intelligent Systems, 30, 2015.
D.S. Lee, P.C. Austin, J.L. Rouleau, P.P Liu, D. Naimark, and J.V. Tu. Predicting mortality among patients hospitalized for heart failure: derivation and validation of a clinical model. Jama, 290(19):25812587, 2003.
D. Rizopoulos. Dynamic predictions and prospective accuracy in joint models for longitudinal and timeto-event data. Biometrics, 67(3):819829, 2011.
C. Proust-Lima et al. Joint latent class models for longitudinal and time-to-event data: A review. Statistical Methods in Medical Research, 23(1):7490, 2014.
K.P. Murphy. Machine learning: a probabilistic perspective. MIT press, 2012.
S. Roberts, M. Osborne, M. Ebden, S. Reece, N. Gibson, and S. Aigrain. Gaussian processes for timeseries modelling. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 371(1984):20110550, 2013.
J.Q. Shi, R. Murray-Smith, and D.M. Titterington. Hierarchical gaussian process mixtures for regression. Statistics and computing, 15(1):3141, 2005.
A. Gelman and J. Hill. Data analysis using regression and multilevel/hierarchical models. Cambridge University Press, 2006.
H. Wang et al. High-order multi-task feature learning to identify longitudinal phenotypic markers for alzheimers disease progression prediction. In Advances in Neural Information Processing Systems, pages 12771285, 2012.
J. Ross and J. Dy. Nonparametric mixture of gaussian processes with constraints. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 13461354, 2013.
P.F. Schulam, F.M. Wigley, and S. Saria. Clustering longitudinal clinical marker trajectories from electronic health data: Applications to phenotyping and endotype discovery. In Proceedings of the TwintyNinth AAAI Conference on Artificial Intelligence, 2015.
B.M. Marlin. Modeling user rating profiles for collaborative filtering. In Advances in neural information processing systems, 2003.
G. Adomavicius and A. Tuzhilin. Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions. Knowledge and Data Engineering, IEEE Transactions on, 17(6):734749, 2005.
D. Sontag, K. Collins-Thompson, P.N. Bennett, R.W. White, S. Dumais, and B. Billerbeck. Probabilistic models for personalizing web search. In Proceedings of the fifth ACM international conference on Web search and data mining, pages 433442. ACM, 2012.
C.E. Rasmussen and C.K. Williams. Gaussian Processes for Machine Learning. The MIT Press, 2006.
Y. Allanore et al. Systemic sclerosis. Nature Reviews Disease Primers, 2015.
D. Khanna et al. Clinical course of lung physiology in patients with scleroderma and interstitial lung disease: analysis of the scleroderma lung study placebo group. Arthritis & Rheumatism, 63(10):3078 3085, 2011.
J.Q. Shi, B. Wang, E.J. Will, and R.M. West. Mixed-effects gaussian process functional regression models with application to doseresponse curve prediction. Stat. Med., 31(26):31653177, 2012.  9
Pankaj K Agarwal and Nabil H Mustafa. k-means projective clustering. In Proceedings of the 23rd ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pages 8  155165, 2004.
Keith Ball. An elementary introduction to modern convex geometry. Flavors of geometry, 31:158, 1997.
Emmanuel J Candes, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis? Journal of the ACM, 58(3):11, 2011.
Yudong Chen, Constantine Caramanis, and Shie Mannor. Robust sparse regression under adversarial corruption. In Proceedings of the 30th International Conference on Machine Learning, pages 774782, 2013.
Yudong Chen, Ali Jalali, Sujay Sanghavi, and Huan Xu. Clustering partially observed graphs via convex optimization. The Journal of Machine Learning Research, 15(1):22132238, 2014.
Ehsan Elhamifar and Rene Vidal. Sparse subspace clustering. In CVPR 2009, pages 2790 2797.
Guangcan Liu, Zhouchen Lin, and Yong Yu. Robust subspace segmentation by low-rank representation. In Proceedings of the 27th International Conference on Machine Learning, pages 663670, 2010.
Po-Ling Loh and Martin J Wainwright. High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity. In Advances in Neural Information Processing Systems, pages 27262734, 2011.
Le Lu and Rene Vidal. Combined central and subspace clustering for computer vision applications. In Proceedings of the 23rd international conference on Machine learning, pages 593600, 2006.
Yi Ma, Harm Derksen, Wei Hong, and John Wright. Segmentation of multivariate mixed data via lossy data coding and compression. IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(9):15461562, 2007.
Shankar R Rao, Roberto Tron, Rene Vidal, and Yi Ma. Motion segmentation via robust subspace separation in the presence of outlying, incomplete, or corrupted trajectories. In CVPR 2008.
Mahdi Soltanolkotabi, Emmanuel J Candes, et al. A geometric analysis of subspace clustering with outliers. The Annals of Statistics, 40(4):21952238, 2012.
Mahdi Soltanolkotabi, Ehsan Elhamifar, Emmanuel J Candes, et al. Robust subspace clustering. The Annals of Statistics, 42(2):669699, 2014.
R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society. Series B, pages 267288, 1996.
Rene Vidal. A tutorial on subspace clustering. IEEE Signal Processing Magazine, 28(2):52 68, 2010.
Rene Vidal, Yi Ma, and Shankar Sastry. Generalized principal component analysis (gpca). IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(12):19451959, 2005.
Rene Vidal, Roberto Tron, and Richard Hartley. Multiframe motion segmentation with missing data using powerfactorization and gpca. International Journal of Computer Vision, 79(1):85 105, 2008.
Yu-Xiang Wang and Huan Xu. Noisy sparse subspace clustering. In Proceedings of The 30th International Conference on Machine Learning, pages 8997, 2013.
H. Xu, C. Caramanis, and S. Sanghavi. Robust PCA via outlier pursuit. IEEE Transactions on Information Theory, 58(5):30473064, 2012.
Jingyu Yan and Marc Pollefeys. A general framework for motion segmentation: Independent, articulated, rigid, non-rigid, degenerate and non-degenerate. In ECCV 2006, pages 94106. 2006.
Hao Zhu, Geert Leus, and Georgios B Giannakis. Sparsity-cognizant total least-squares for perturbed compressive sampling. IEEE Transactions on Signal Processing, 59(5):20022016, 2011.  9
A. Majumdar, Image compression by sparse pca coding in curvelet domain, Signal, image and video processing, vol. 3, no. 1, pp. 2734, 2009.
Z. Wang, F. Han, and H. Liu, Sparse principal component analysis for high dimensional multivariate time series, in Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics, pp. 4856, 2013.
A. dAspremont, L. El Ghaoui, M. Jordan, and G. Lanckriet, A direct formulation for sparse pca using semidefinite programming, SIAM review, vol. 49, no. 3, pp. 434448, 2007.  Topic 1 1: percent 2: million 3: money 4: high 5: program 6: number 7: need 8: part 9: problem 10: com  Topic 2  Topic 3  Topic 4  Topic 5  Topic 6  Topic 7  Topic 8  zzz united states zzz u s zzz american attack military palestinian war administration zzz white house games  zzz bush official government president group leader country political american law  company companies market stock business billion analyst firm sales cost  team game season player play point run right home won  cup minutes add tablespoon oil teaspoon water pepper large food  school student children women show book family look hour small  zzz al gore zzz george bush campaign election plan tax public zzz washington member nation  Table 3: B OW:N Y T IMES dataset
. The table lists the words corresponding to the s = 10 nonzero entries of each of the k = 8 extracted components (topics). Words corresponding to higher magnitude entries appear higher in the topic. 8
R. Jiang, H. Fei, and J. Huan, Anomaly localization for network data streams with graph joint sparse pca, in Proceedings of the 17th ACM SIGKDD, pp. 886894, ACM, 2011.
H. Zou, T. Hastie, and R. Tibshirani, Sparse principal component analysis, Journal of computational and graphical statistics, vol. 15, no. 2, pp. 265286, 2006.
H. Kaiser, The varimax criterion for analytic rotation in factor analysis, Psychometrika, vol. 23, no. 3, pp. 187200, 1958.
I. Jolliffe, Rotation of principal components: choice of normalization constraints, Journal of Applied Statistics, vol. 22, no. 1, pp. 2935, 1995.
I. Jolliffe, N. Trendafilov, and M. Uddin, A modified principal component technique based on the lasso, Journal of Computational and Graphical Statistics, vol. 12, no. 3, pp. 531547, 2003.
C. Boutsidis, P. Drineas, and M. Magdon-Ismail, Sparse features for pca-like linear regression, in Advances in Neural Information Processing Systems, pp. 22852293, 2011.
M. Journee, Y. Nesterov, P. Richtarik, and R. Sepulchre, Generalized power method for sparse principal component analysis, The Journal of Machine Learning Research, vol. 11, pp. 517553, 2010.
B. Moghaddam, Y. Weiss, and S. Avidan, Spectral bounds for sparse pca: Exact and greedy algorithms, NIPS, vol. 18, p. 915, 2006.
A. dAspremont, F. Bach, and L. E. Ghaoui, Optimal solutions for sparse principal component analysis, The Journal of Machine Learning Research, vol. 9, pp. 12691294, 2008.
Y. Zhang, A. dAspremont, and L. Ghaoui, Sparse pca: Convex relaxations, algorithms and applications, Handbook on Semidefinite, Conic and Polynomial Optimization, pp. 915940, 2012.
X.-T. Yuan and T. Zhang, Truncated power method for sparse eigenvalue problems, The Journal of Machine Learning Research, vol. 14, no. 1, pp. 899925, 2013.
C. D. Sigg and J. M. Buhmann, Expectation-maximization for sparse and non-negative pca, in Proceedings of the 25th International Conference on Machine Learning, ICML 08, (New York, NY, USA), pp. 960967, ACM, 2008.
D. Papailiopoulos, A. Dimakis, and S. Korokythakis, Sparse pca through low-rank approximations, in Proceedings of The 30th International Conference on Machine Learning, pp. 747755, 2013.
M. Asteris, D. S. Papailiopoulos, and G. N. Karystinos, The sparse principal component of a constant-rank matrix, Information Theory, IEEE Transactions on, vol. 60, pp. 22812290, April 2014.
L. Mackey, Deflation methods for sparse pca, NIPS, vol. 21, pp. 10171024, 2009.
V. Vu and J. Lei, Minimax rates of estimation for sparse pca in high dimensions, in International Conference on Artificial Intelligence and Statistics, pp. 12781286, 2012.
M. Magdon-Ismail and C. Boutsidis, Optimal sparse linear auto-encoders and sparse pca, arXiv preprint arXiv:1502.06626, 2015.
M. Magdon-Ismail, Np-hardness and inapproximability of sparse PCA, CoRR, vol. abs/1502.05675, 2015.
L. Ramshaw and R. E. Tarjan, On minimum-cost assignments in unbalanced bipartite graphs, HP Labs, Palo Alto, CA, USA, Tech. Rep. HPL-2012-40R1, 2012.
N. Halko, P.-G. Martinsson, and J. A. Tropp, Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions, SIAM review, vol. 53, no. 2, pp. 217288, 2011.
M. Asteris, D. Papailiopoulos, A. Kyrillidis, and A. G. Dimakis, Sparse pca via bipartite matchings, arXiv preprint arXiv:1508.00625, 2015.
I. M. Johnstone and A. Y. Lu, On consistency and sparsity for principal components analysis in high dimensions, Journal of the American Statistical Association, vol. 104, no. 486, 2009.
Z. Ma, Sparse principal component analysis and iterative thresholding, The Annals of Statistics, vol. 41, no. 2, pp. 772801, 2013.
V. Q. Vu, J. Cho, J. Lei, and K. Rohe, Fantope projection and selection: A near-optimal convex relaxation of sparse pca, in NIPS, pp. 26702678, 2013.
Z. Wang, H. Lu, and H. Liu, Nonconvex statistical optimization: minimax-optimal sparse pca in polynomial time, arXiv preprint arXiv:1408.5352, 2014.
R. Jenatton, G. Obozinski, and F. Bach, Structured sparse principal component analysis, in Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS, pp. 366373, 2010.
E. Richard, G. R. Obozinski, and J.-P. Vert, Tight convex relaxations for sparse matrix factorization, in Advances in Neural Information Processing Systems, pp. 32843292, 2014.
M. Lichman, UCI machine learning repository, 2013.  9
Ahmed El Alaoui and Michael W Mahoney. Fast randomized kernel methods with statistical guarantees. arXiv preprint arXiv:1411.0306, 2014.
Alex Gittens and Michael W Mahoney. Revisiting the Nystrom method for improved largescale machine learning. In Proceedings of The 30th International Conference on Machine Learning, pages 567575, 2013.
Francis Bach. Sharp analysis of low-rank kernel matrix approximations. In Proceedings of The 26th Conference on Learning Theory, pages 185209, 2013.
Francis Bach. Personal communication, October 2015.
Petros Drineas, Michael W Mahoney, and S Muthukrishnan. Relative-error CUR matrix decompositions. SIAM Journal on Matrix Analysis and Applications, 30(2):844881, 2008.
Michael W Mahoney and Petros Drineas. CUR matrix decompositions for improved data analysis. Proceedings of the National Academy of Sciences, 106(3):697702, 2009.
Petros Drineas, Malik Magdon-Ismail, Michael W Mahoney, and David P Woodruff. Fast approximation of matrix coherence and statistical leverage. The Journal of Machine Learning Research, 13(1):34753506, 2012.
Michael W Mahoney. Randomized algorithms for matrices and data. Foundations and Trends in Machine Learning, 3(2):123224, 2011.
Yuchen Zhang, John Duchi, and Martin Wainwright. Divide and conquer kernel ridge regression. In Proceedings of The 26th Conference on Learning Theory, pages 592617, 2013.
Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning, volume 1. Springer series in statistics Springer, Berlin, 2001.
George Kimeldorf and Grace Wahba. Some results on Tchebycheffian spline functions. Journal of Mathematical Analysis and Applications, 33(1):8295, 1971.
Bernhard Scholkopf, Ralf Herbrich, and Alex J Smola. A generalized representer theorem. In Computational Learning Theory, pages 416426. Springer, 2001.
Shai Fine and Katya Scheinberg. Efficient SVM training using low-rank kernel representations. The Journal of Machine Learning Research, 2:243264, 2002.
Christopher Williams and Matthias Seeger. Using the Nystrom method to speed up kernel machines. In Proceedings of the 14th Annual Conference on Neural Information Processing Systems, pages 682688, 2001.
Sanjiv Kumar, Mehryar Mohri, and Ameet Talwalkar. Sampling techniques for the Nystrom method. In International Conference on Artificial Intelligence and Statistics, pages 304311, 2009.
Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics, 12(4):389434, 2012.
Petros Drineas, Ravi Kannan, and Michael W Mahoney. Fast monte carlo algorithms for matrices I: Approximating matrix multiplication. SIAM Journal on Computing, 36(1):132 157, 2006.
Samprit Chatterjee and Ali S Hadi. Influential observations, high leverage points, and outliers in linear regression. Statistical Science, pages 379393, 1986.
Petros Drineas, Ravi Kannan, and Michael W Mahoney. Fast monte carlo algorithms for matrices II: Computing a low-rank approximation to a matrix. SIAM Journal on Computing, 36(1):158183, 2006.
Petros Drineas, Michael W Mahoney, S Muthukrishnan, and Tamas Sarlos. Faster least squares approximation. Numerische Mathematik, 117(2):219249, 2011.
Alan Frieze, Ravi Kannan, and Santosh Vempala. Fast monte-carlo algorithms for finding low-rank approximations. Journal of the ACM (JACM), 51(6):10251041, 2004.
Francis Bach. Self-concordant analysis for logistic regression. Electronic Journal of Statistics, 4:384414, 2010.  9
N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press, 2006.
Elad Hazan. The convex optimization approach to regret minimization. Optimization for machine learning, page 287, 2011.
Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in Machine Learning, 4(2):107194, 2012.
M.L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley Series in Probability and Statistics. Wiley, 1994.
Raman Arora, Ofer Dekel, and Ambuj Tewari. Online bandit learning against an adaptive adversary: from regret to policy regret. 2012.
Nicolo Cesa-Bianchi, Ofer Dekel, and Ohad Shamir. Online learning with switching costs and other adaptive adversaries. CoRR, abs/1302.4387, 2013.
Neri Merhav, Erik Ordentlich, Gadiel Seroussi, and Marcelo J. Weinberger. On sequential strategies for loss functions with memory. IEEE Transactions on Information Theory, 48(7):19471958, 2002.
Andras Gyorgy and Gergely Neu. Near-optimal rates for limited-delay universal lossy source coding. In ISIT, pages 22182222, 2011.
Sascha Geulen, Berthold Vocking, and Melanie Winkler. Regret minimization for online buffering problems using the weighted majority algorithm. In COLT, pages 132143, 2010.
Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. In FOCS, pages 256261, 1989.
Eyal Gofer. Higher-order regret bounds with switching costs. In Proceedings of The 27th Conference on Learning Theory, pages 210243, 2014.
Ofer Dekel, Jian Ding, Tomer Koren, and Yuval Peres. Bandits with switching costs: T{2/3} regret. arXiv preprint arXiv:1310.2997, 2013.
Anatoly B. Schmidt. Financial Markets and Trading: An Introduction to Market Microstructure and Trading Strategies (Wiley Finance). Wiley, 1 edition, August 2011.
Alexandre DAspremont. Identifying small mean-reverting portfolios. Quant. Finance, 11(3):351364, 2011.
Marco Cuturi and Alexandre Daspremont. Mean reversion with a variance threshold. 28(3):271279, May 2013.
Oren Anava, Elad Hazan, Shie Mannor, and Ohad Shamir. Online learning for time series prediction. arXiv preprint arXiv:1302.6927, 2013.
Oren Anava, Elad Hazan, and Assaf Zeevi. Online time series prediction with missing data. In ICML, 2015.
Michael P Clements and David F Hendry. Multi-step estimation for forecasting. Oxford Bulletin of Economics and Statistics, 58(4):657684, 1996.
Massimiliano Marcellino, James H Stock, and Mark W Watson. A comparison of direct and iterated multistep ar methods for forecasting macroeconomic time series. Journal of Econometrics, 135(1):499 526, 2006.
G.S. Maddala and I.M. Kim. Unit Roots, Cointegration, and Structural Change. Themes in Modern Econometrics. Cambridge University Press, 1998.
Soren Johansen. Estimation and Hypothesis Testing of Cointegration Vectors in Gaussian Vector Autoregressive Models. Econometrica, 59(6):155180, November 1991.
Jakub W Jurek and Halla Yang. Dynamic portfolio selection in arbitrage. In EFA 2006 Meetings Paper, 2007.
Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex optimization. Machine Learning, 69(2-3):169192, 2007.
Laszlo Lovasz and Santosh Vempala. Logconcave functions: Geometry and efficient sampling algorithms. In FOCS, pages 640649. IEEE Computer Society, 2003.
Hariharan Narayanan and Alexander Rakhlin. Random walk approach to regret minimization. In John D. Lafferty, Christopher K. I. Williams, John Shawe-Taylor, Richard S. Zemel, and Aron Culotta, editors, NIPS, pages 17771785. Curran Associates, Inc., 2010.  9
R. R. de Ruyter van Steveninck and W. Bialek. Real-time performance of a movement-senstivive neuron in the blowfly visual system: coding and information transmission in short spike sequences. Proc. R. Soc. Lond. B, 234:379414, 1988.  8
J. Touryan, B. Lau, and Y. Dan. Isolation of relevant visual features from random stimuli for cortical complex cells. Journal of Neuroscience, 22:1081110818, 2002.
B. Aguera y Arcas and A. L. Fairhall. What causes a neuron to spike? Neural Computation, 15(8):1789 1807, 2003.
Tatyana Sharpee, Nicole C. Rust, and William Bialek. Analyzing neural responses to natural signals: maximally informative dimensions. Neural Comput, 16(2):223250, Feb 2004.
O. Schwartz, J. W. Pillow, N. C. Rust, and E. P. Simoncelli. Spike-triggered neural characterization. Journal of Vision, 6(4):484507, 7 2006.
J. W. Pillow and E. P. Simoncelli. Dimensionality reduction in neural models: An information-theoretic generalization of spike-triggered average and covariance analysis. Journal of Vision, 6(4):414428, 4 2006.
Il Memming Park and Jonathan W. Pillow. Bayesian spike-triggered covariance analysis. In J. ShaweTaylor, R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 16921700, 2011.
Il M. Park, Evan W. Archer, Nicholas Priebe, and Jonathan W. Pillow. Spectral methods for neural characterization using generalized quadratic models. In Advances in Neural Information Processing Systems 26, pages 24542462, 2013.
Ross S. Williamson, Maneesh Sahani, and Jonathan W. Pillow. The equivalence of information-theoretic and likelihood-based methods for neural dimensionality reduction. PLoS Comput Biol, 11(4):e1004141, 04 2015.
Kanaka Rajan, Olivier Marre, and Gasper Tkacik. Learning quadratic receptive fields from neural responses to natural stimuli. Neural Computation, 25(7):16611692, 2013/06/19 2013.
Nicole C. Rust, Odelia Schwartz, J. Anthony Movshon, and Eero P. Simoncelli. Spatiotemporal elements of macaque v1 receptive fields. Neuron, 46(6):945956, Jun 2005.
B Vintch, A Zaharia, J A Movshon, and E P Simoncelli. Efficient and direct estimation of a neural subunit model for sensory coding. In Adv. Neural Information Processing Systems (NIPS*12), volume 25, Cambridge, MA, 2012. MIT Press. To be presented at Neural Information Processing Systems 25, Dec 2012.
Brett Vintch, Andrew Zaharia, J Movshon, and Eero P Simoncelli. A convolutional subunit model for neuronal responses in macaque v1. J. Neursoci, page in press, 2015.
HB Barlow and W Ro Levick. The mechanism of directionally selective units in rabbits retina. The Journal of physiology, 178(3):477, 1965.
S. Hochstein and R. Shapley. Linear and nonlinear spatial subunits in y cat retinal ganglion cells. J. Physiol., 262:265284, 1976.
Jonathan B Demb, Kareem Zaghloul, Loren Haarsma, and Peter Sterling. Bipolar cells contribute to nonlinear spatial summation in the brisk-transient (y) ganglion cell in mammalian retina. The Journal of neuroscience, 21(19):74477454, 2001.
Joanna D Crook, Beth B Peterson, Orin S Packer, Farrel R Robinson, John B Troy, and Dennis M Dacey. Y-cell receptive field and collicular projection of parasol ganglion cells in macaque monkey retina. The Journal of neuroscience, 28(44):1127711291, 2008.
PX Joris, CE Schreiner, and A Rees. Neural processing of amplitude-modulated sounds. Physiological reviews, 84(2):541577, 2004.
Kunihiko Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological cybernetics, 36(4):193202, 1980.
T. Serre, L. Wolf, S. Bileschi, M. Riesenhuber, and T. Poggio. Robust object recognition with cortex-like mechanisms. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 29(3):411426, 2007.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, 1998.
AlexandroD. Ramirez and Liam Paninski. Fast inference in generalized linear models via expected loglikelihoods. Journal of Computational Neuroscience, pages 120, 2013.
Philip J Davis. Circulant matrices. American Mathematical Soc., 1979.
M. Sahani and J. Linden. Evidence optimization techniques for estimating stimulus-response functions. NIPS, 15, 2003.  9
F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. Goodfellow, A. Bergeron, N. Bouchard, D. WardeFarley, and Y. Bengio. Theano: New features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.
Y. Bengio, I. Goodfellow, and A. Courville. Deep Learning. Book in preparation for MIT Press, 2015.
J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley, and Y. Bengio. Theano: a CPU and GPU math expression compiler. In Scipy, volume 4, page 3. Austin, TX, 2010.
R. Bridson. Fluid Simulation for Computer Graphics. Ak Peters Series. Taylor & Francis, 2008.
T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High accuracy optical flow estimation based on a theory for warping. In ECCV, pages 2536. 2004.
P. Cheung and H.Y. Yeung. Application of optical-flow technique to significant convection nowcast for terminal areas in Hong Kong. In the 3rd WMO International Symposium on Nowcasting and Very ShortRange Forecasting (WSN12), pages 610, 2012.
K. Cho, B. van Merrienboer, C. Gulcehre, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In EMNLP, pages 1724 1734, 2014.
J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell. Long-term recurrent convolutional networks for visual recognition and description. In CVPR, 2015.
R. H. Douglas. The stormy weather group (Canada). In Radar in Meteorology, pages 6168. 1990.
Urs Germann and Isztar Zawadzki. Scale-dependence of the predictability of precipitation from continental radar images. Part I: Description of the methodology. Monthly Weather Review, 130(12):28592873, 2002.
A. Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):17351780, 1997.
A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In CVPR, 2015.
B. Klein, L. Wolf, and Y. Afek. A dynamic convolutional layer for short range weather prediction. In CVPR, 2015.
P.W. Li, W.K. Wong, K.Y. Chan, and E. S.T. Lai. SWIRLS-An Evolving Nowcasting System. Hong Kong Special Administrative Region Government, 2000.
J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.
R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. In ICML, pages 13101318, 2013.
M. Ranzato, A. Szlam, J. Bruna, M. Mathieu, R. Collobert, and S. Chopra. Video (language) modeling: a baseline for generative models of natural videos. arXiv preprint arXiv:1412.6604, 2014.
M. Reyniers. Quantitative Precipitation Forecasts Based on Radar Observations: Principles, Algorithms and Operational Systems. Institut Royal Meteorologique de Belgique, 2008.
H. Sakaino. Spatio-temporal image pattern prediction method based on a physical model with timevarying optical flow. IEEE Transactions on Geoscience and Remote Sensing, 51(5-2):30233036, 2013.
N. Srivastava, E. Mansimov, and R. Salakhutdinov. Unsupervised learning of video representations using lstms. In ICML, 2015.
J. Sun, M. Xue, J. W. Wilson, I. Zawadzki, S. P. Ballard, J. Onvlee-Hooimeyer, P. Joe, D. M. Barker, P. W. Li, B. Golding, M. Xu, and J. Pinto. Use of NWP for nowcasting convective precipitation: Recent progress and challenges. Bulletin of the American Meteorological Society, 95(3):409426, 2014.
I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In NIPS, pages 31043112, 2014.
T. Tieleman and G. Hinton. Lecture 6.5 - RMSProp: Divide the gradient by a running average of its recent magnitude. Coursera Course: Neural Networks for Machine Learning, 4, 2012.
W.C. Woo and W.K. Wong. Application of optical flow techniques to rainfall nowcasting. In the 27th Conference on Severe Local Storms, 2014.
K. Xu, J. Ba, R. Kiros, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio. Show, attend and tell: Neural image caption generation with visual attention. In ICML, 2015.  9
A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. In NIPS, pages 4148, 2006.
A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learning, 73(3):243272, 2008.
H. H. Bauschke and P. L. Combettes. Convex analysis and monotone operator theory in Hilbert spaces. Springer, New York, 2011.
M. Blondel, K. Seki, and K. Uehara. Block coordinate descent algorithms for large-scale sparse multiclass classification. Machine Learning, 93(1):3152, 2013.
A. Bonnefoy, V. Emiya, L. Ralaivola, and R. Gribonval. A dynamic screening principle for the lasso. In EUSIPCO, 2014.
A. Bonnefoy, V. Emiya, L. Ralaivola, and R. Gribonval. Dynamic Screening: Accelerating First-Order Algorithms for the Lasso and Group-Lasso. IEEE Trans. Signal Process., 63(19):20, 2015.
P. Buhlmann and S. van de Geer. Statistics for high-dimensional data. Springer Series in Statistics. Springer, Heidelberg, 2011. Methods, theory and applications.
B. Efron, T. Hastie, I. M. Johnstone, and R. Tibshirani. Least angle regression. Ann. Statist., 32(2):407499, 2004. With discussion, and a rejoinder by the authors.
L. El Ghaoui, V. Viallon, and T. Rabbani. Safe feature elimination in sparse supervised learning. J. Pacific Optim., 8(4):667698, 2012.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. Liblinear: A library for large linear classification. J. Mach. Learn. Res., 9:18711874, 2008.
O. Fercoq, A. Gramfort, and J. Salmon. Mind the duality gap: safer rules for the lasso. In ICML, pages 333342, 2015.
J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized linear models via coordinate descent. Journal of statistical software, 33(1):1, 2010.
A. Gramfort, M. Kowalski, and M. Hamalainen. Mixed-norm estimates for the M/EEG inverse problem using accelerated gradient methods. Phys. Med. Biol., 57(7):19371961, 2012.
J.-B. Hiriart-Urruty and C. Lemarechal. Convex analysis and minimization algorithms. II, volume 306. Springer-Verlag, Berlin, 1993.
K. Koh, S.-J. Kim, and S. Boyd. An interior-point method for large-scale l1-regularized logistic regression. J. Mach. Learn. Res., 8(8):15191555, 2007.
C. D. Manning and H. Schutze. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, MA, USA, 1999.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. J. Mach. Learn. Res., 12:28252830, 2011.
R. Tibshirani. Regression shrinkage and selection via the lasso. JRSSB, 58(1):267288, 1996.
R. Tibshirani, J. Bien, J. Friedman, T. Hastie, N. Simon, J. Taylor, and R. J. Tibshirani. Strong rules for discarding predictors in lasso-type problems. JRSSB, 74(2):245266, 2012.
R. J. Tibshirani. The lasso problem and uniqueness. Electron. J. Stat., 7:14561490, 2013.
J. Wang, P. Wonka, and J. Ye. Lasso screening rules via dual polytope projection. arXiv preprint arXiv:1211.3966, 2012.
J. Wang, J. Zhou, J. Liu, P. Wonka, and J. Ye. A safe screening rule for sparse logistic regression. In NIPS, pages 10531061, 2014.
Z. J. Xiang, Y. Wang, and P. J. Ramadge. Screening tests for lasso problems. arXiv preprint arXiv:1405.4897, 2014.
M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. JRSSB, 68(1):4967, 2006.  9
Hinton, G. E. & Sejnowski, T. J. (1986) Learning and relearning in boltzmann machines. MIT Press, Cambridge, Mass, 1:282317.
Ackley, D. H., Hinton, G. E. & Sejnowski, T. J. (1985) A learning algorithm for boltzmann machines. Cognitive Science, 9(1):147169.
Amari, S., Kurata, K. & Nagaoka, H. (1992) Information geometry of Boltzmann machines. In IEEE Transactions on Neural Networks, 3: 260271.
Hinton, G. E. & Salakhutdinov, R. R. (2012) A better way to pretrain deep boltzmann machines. In Advances in Neural Information Processing Systems, pp. 24472455 Cambridge, MA: MIT Press.
Opper, M. & Saad, D. (2001) Advanced Mean Field Methods: Theory and Practice. MIT Press, Cambridge, MA.
Hinton, G.E. (2002) Training Products of Experts by Minimizing Contrastive Divergence. Neural Computation, 14(8):17711800.
Hyvarinen, A. (2005) Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6:695708.
Hyvarinen, A. (2007) Some extensions of score matching. Computational statistics & data analysis, 51(5):24992512.
Dawid, A. P., Lauritzen, S. & Parry, M. (2012) Proper local scoring rules on discrete sample spaces. The Annals of Statistics, 40(1):593608.
Gutmann, M. & Hirayama, H. (2012) Bregman divergence as general framework to estimate unnormalized statistical models. arXiv preprint arXiv:1202.3727.
Amari, S & Nagaoka, H. (2000) Methods of Information Geometry, volume 191 of Translations of Mathematical Monographs. Oxford University Press.
Sejnowski, T. J. (1986) Higher-order boltzmann machines. In American Institute of Physics Conference Series, 151:398403.
Good, I. J. (1971) Comment on measuring information and uncertainty, by R. J. Buehler. In Godambe, V. P. & Sprott, D. A. editors, Foundations of Statistical Inference, pp. 337339, Toronto: Holt, Rinehart and Winston.
Fujisawa, H. & Eguchi, S. (2008) Robust parameter estimation with a small bias against heavy contamination. Journal of Multivariate Analysis, 99(9):20532081.
Van der Vaart, A. W. (1998) Asymptotic Statistics. Cambridge University Press.
R Core Team. (2013) R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria.  9
George E P Box. Sampling and Bayes inference in scientific modelling and robustness. J. R. Stat. Soc. Ser. A, 143(4):383430, 1980.
A OHagan. HSSS model criticism. Highly Structured Stochastic Systems, pages 423444, 2003.
Dennis Cook and Sanford Weisberg. Residuals and inuence in regression. Mon. on Stat. and App. Prob., 1982.
A Gelman, J B Carlin, H S Stern, D B Dunson, A Vehtari, and D B Rubin. Bayesian Data Analysis, Third Edition. Chapman & Hall/CRC Texts in Statistical Science. Taylor & Francis, 2013.
Roger B Grosse, Ruslan Salakhutdinov, William T Freeman, and Joshua B Tenenbaum. Exploiting compositionality to explore a large space of model structures. In Conf. on Unc. in Art. Int. (UAI), 2012.
Chris Thornton, Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms. In Proc. Int. Conf. on Knowledge Discovery and Data Mining, KDD 13, pages 847855, New York, NY, USA, 2013. ACM.
James Robert Lloyd, David Duvenaud, Roger Grosse, Joshua B Tenenbaum, and Zoubin Ghahramani. Automatic construction and Natural-Language description of nonparametric regression models. In Association for the Advancement of Artificial Intelligence (AAAI), July 2014.
D Koller, D McAllester, and A Pfeffer. Effective bayesian inference for stochastic programs. Association for the Advancement of Artificial Intelligence (AAAI), 1997.  8
B Milch, B Marthi, S Russel, D Sontag, D L Ong, and A Kolobov. BLOG: Probabilistic models with unknown objects. In Proc. Int. Joint Conf. on Artificial Intelligence, 2005.
Noah D Goodman, Vikash K Mansinghka, Daniel M Roy, Keith Bonawitz, and Joshua B Tenenbaum. Church : a language for generative models. In Conf. on Unc. in Art. Int. (UAI), 2008.
Stan Development Team. Stan: A C++ library for probability and sampling, version 2.2, 2014.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Berhard Scholkopf, and Alexander Smola. A kernel method for the two-sample problem. Journal of Machine Learning Research, 1:110, 2008.
Irwin Guttman. The use of the concept of a future observation in goodness-of-fit problems. J. R. Stat. Soc. Series B Stat. Methodol., 29(1):83100, 1967.
Donald B Rubin. Bayesianly justifiable and relevant frequency calculations for the applied statistician. Ann. Stat., 12(4):11511172, 1984.
Harold Hotelling. A generalized t-test and measure of multivariate dispersion. In Proc. 2nd Berkeley Symp. Math. Stat. and Prob. The Regents of the University of California, 1951.
P J Bickel. A distribution free version of the smirnov two sample test in the p-variate case. Ann. Math. Stat., 40(1):123, 1 February 1969.
Murray Rosenblatt. Remarks on some nonparametric estimates of a density function. Ann. Math. Stat., 27(3):832837, September 1956.
E Parzen. On estimation of a probability density function and mode. Ann. Math. Stat., 1962.
Stephen M Stigler. Do robust estimators work with real data? Ann. Stat., 5(6):10551098, November 1977.
C E Rasmussen and C K Williams. Gaussian Processes for Machine Learning. The MIT Press, Cambridge, MA, USA, 2006.
D Peel and G J McLachlan. Robust mixture modelling using the t distribution. Stat. Comput., 10(4):339 348, 1 October 2000.
Tomoharu Iwata, David Duvenaud, and Zoubin Ghahramani. Warped mixtures for nonparametric cluster shapes. In Conf. on Unc. in Art. Int. (UAI). arxiv.org, 2013.
Geoffrey E Hinton. To recognize shapes, first learn to generate images. Prog. Brain Res., 165:535547, 2007.
Geoffrey E Hinton, Simon Osindero, and Yee Whye Teh. A fast learning algorithm for deep belief nets. Neural Comput., 18(7):15271554, 2006.
Deep learning tutorial - http://www.deeplearning.net/tutorial/, 2014.
Andrew Gordon Wilson and Ryan Prescott Adams. Gaussian process covariance kernels for pattern discovery and extrapolation. In Proc. Int. Conf. Machine Learn., 2013.
Yoav Benjamini and Yosef Hochberg. Controlling the false discovery rate: A practical and powerful approach to multiple testing. J. R. Stat. Soc. Series B Stat. Methodol., 1995.
James M Robins, Aad van der Vaart, and Valerie Venture. Asymptotic distribution of p-values in composite null models. J. Am. Stat. Assoc., 95(452):11431156, 2000.
M J Bayarri and J O Berger. Quantifying surprise in the data and model verification. Bayes. Stat., 1999.
Andrew Gelman. A Bayesian formulation of exploratory data analysis and goodness-of-fit testing. Int. Stat. Rev., 2003.
M J Bayarri and M E Castellanos. Bayesian checking of the second levels of hierarchical models. Stat. Sci., 22(3):322343, August 2007.
Andrew Gelman. Understanding posterior p-values. Elec. J. Stat., 2013.
A E Gelfand, D K Dey, and H Chang. Model determination using predictive distributions with implementation via sampling-based methods. Technical Report 462, Stanford Uni CA Dept Stat, 1992.
E C Marshall and D J Spiegelhalter. Identifying outliers in bayesian hierarchical models: a simulationbased approach. Bayesian Anal., 2(2):409444, June 2007.
Aki Vehtari and Janne Ojanen. A survey of Bayesian predictive methods for model assessment, selection and comparison. Stat. Surv., 6:142228, 2012.
Andrew Gelman, Xiao-Li Meng, and Hal Stern. Posterior predictive assessment of model fitness via realized discrepancies. Stat. Sin., 6:733807, 1996.
K Popper. The logic of scientific discovery. Routledge, 2005.
John Geweke. Getting it right. J. Am. Stat. Assoc., 99(467):799804, September 2004.
Mary Kathryn Cowles and Bradley P Carlin. Markov chain monte carlo convergence diagnostics: A comparative review. J. Am. Stat. Assoc., 91(434):883904, 1 June 1996.  9
K. Boyd, V. S. Costa, J. Davis, and C. D. Page. Unachievable region in precision-recall space and its effect on empirical evaluation. In International Conference on Machine Learning, page 349, 2012.
J. Davis and M. Goadrich. The relationship between precision-recall and ROC curves. In Proceedings of the 23rd International Conference on Machine Learning, pages 233240, 2006.
T. Fawcett. An introduction to ROC analysis. Pattern Recognition Letters, 27(8):861874, 2006.
T. Fawcett and A. Niculescu-Mizil. PAV and the ROC convex hull. Machine Learning, 68(1): 97106, July 2007.
P. A. Flach. The geometry of ROC space: understanding machine learning metrics through ROC isometrics. In Machine Learning, Proceedings of the Twentieth International Conference (ICML 2003), pages 194201, 2003.
P. A. Flach. ROC analysis. In C. Sammut and G. Webb, editors, Encyclopedia of Machine Learning, pages 869875. Springer US, 2010.
D. J. Hand and R. J. Till. A simple generalisation of the area under the ROC curve for multiple class classification problems. Machine Learning, 45(2):171186, 2001.
J. Hernandez-Orallo, P. Flach, and C. Ferri. A unified view of performance metrics: Translating threshold choice into expected classification loss. Journal of Machine Learning Research, 13: 28132869, 2012.
O. O. Koyejo, N. Natarajan, P. K. Ravikumar, and I. S. Dhillon. Consistent binary classification with generalized performance metrics. In Advances in Neural Information Processing Systems, pages 27442752, 2014.
Z. C. Lipton, C. Elkan, and B. Naryanaswamy. Optimal thresholding of classifiers to maximize F1 measure. In Machine Learning and Knowledge Discovery in Databases, volume 8725 of Lecture Notes in Computer Science, pages 225239. Springer Berlin Heidelberg, 2014.
H. Narasimhan, R. Vaish, and S. Agarwal. On the statistical consistency of plug-in classifiers for non-decomposable performance measures. In Advances in Neural Information Processing Systems 27, pages 14931501. 2014.
S. P. Parambath, N. Usunier, and Y. Grandvalet. Optimizing F-measures by cost-sensitive classification. In Advances in Neural Information Processing Systems, pages 21232131, 2014.
J. C. Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In Advances in Large Margin Classifiers, pages 6174. MIT Press, Boston, 1999.
F. Provost and T. Fawcett. Robust classification for imprecise environments. Machine Learning, 42(3):203231, 2001.
B. Sluban and N. Lavrac. Vipercharts: Visual performance evaluation platform. In H. Blockeel, K. Kersting, S. Nijssen, and F. Zelezny, editors, Machine Learning and Knowledge Discovery in Databases, volume 8190 of Lecture Notes in Computer Science, pages 650653. Springer Berlin Heidelberg, 2013.
C. J. Van Rijsbergen. Information Retrieval. Butterworth-Heinemann, Newton, MA, USA, 2nd edition, 1979.
J. Vanschoren, J. N. van Rijn, B. Bischl, and L. Torgo. OpenML: networked science in machine learning. SIGKDD Explorations, 15(2):4960, 2013.
N. Ye, K. M. A. Chai, W. S. Lee, and H. L. Chieu. Optimizing F-measures: A tale of two approaches. In Proceedings of the 29th International Conference on Machine Learning, pages 289296, 2012.
B. Zadrozny and C. Elkan. Obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers. In Proceedings of the Eighteenth International Conference on Machine Learning (ICML 2001), pages 609616, 2001.
M.-J. Zhao, N. Edakunni, A. Pocock, and G. Brown. Beyond Fanos inequality: bounds on the optimal F-score, BER, and cost-sensitive risk and their implications. The Journal of Machine Learning Research, 14(1):10331090, 2013. 9
http://www.water-simulation.com/wsp/about/bwsn/.
N. Alon, I. Gamzu, and M. Tennenholtz. Optimizing budget allocation among channels and influencers. In Proc. of WWW, pages 381388, 2012.
A. Badanidiyuru and J. Vondrak. Fast algorithms for maximizing submodular functions. In Proc. of SODA, pages 14971514, 2014.
Y. Chen, H. Shioi, C. A. F. Montesinos, L. P. Koh, S. Wich, and A. Krause. Active detection via adaptive submodularity. In Proc. of ICML, pages 5563, 2014.
R. Iyer and J. Bilmes. Submodular optimization with submodular cover and submodular knapsack constraints. In Proc. of NIPS, pages 24362444, 2013.
M. Kapralov, I. Post, and J. Vondrak. Online submodular welfare maximization: Greedy is optimal. In Proc. of SODA, pages 12161225, 2012.
D. Kempe, J. Kleinberg, and E. Tardos. Maximizing the spread of influence through a social network. In Proc. of KDD, pages 137146, 2003.
A. Krause and D. Golovin. Submodular function maximization. In Tractability: Practical Approaches to Hard Problems, pages 71104. Cambridge University Press, 2014.
A. Krause and J. Leskovec. Efficient sensor placement optimization for securing large water distribution networks. Journal of Water Resources Planning and Management, 134(6):516 526, 2008.
A. Krause, A. Singh, and C. Guestrin. Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies. The Journal of Machine Learning Research, 9:235284, 2008.
J. Leskovec, A. Krause, C. Guestrin, C. Faloutsos, J. VanBriesen, and N. Glance. Cost-effective outbreak detection in networks. In Proc. of KDD, pages 420429, 2007.
H. Lin and J. Bilmes. Multi-document summarization via budgeted maximization of submodular functions. In Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 912920, 2010.
H. Lin and J. Bilmes. A class of submodular functions for document summarization. In Proc. of NAACL, pages 510520, 2011.
M. Minoux. Accelerated greedy algorithms for maximizing submodular set functions. Optimization Techniques, Lecture Notes in Control and Information Sciences, 7:234243, 1978.
A. Ostfeld, J. G. Uber, E. Salomons, J. W. Berry, W. E. Hart, C. A. Phillips, J.-P. Watson, G. Dorini, P. Jonkergouw, Z. Kapelan, F. di Pierro, S.-T. Khu, D. Savic, D. Eliades, M. Polycarpou, S. R. Ghimire, B. D. Barkdoll, R. Gueli, J. J. Huang, E. A. McBean, W. James, A. Krause, J. Leskovec, S. Isovitsch, J. Xu, C. Guestrin, J. VanBriesen, M. Small, P. Fischbeck, A. Preis, M. Propato, O. Piller, G. B. Trachtman, Z. Y. Wu, and T. Walski. The battle of the water sensor networks (BWSN): A design challenge for engineers and algorithms. Journal of Water Resources Planning and Management, 134(6):556568, 2008.
R. Raz and S. Safra. A sub-constant error-probability low-degree test, and a sub-constant error-probability PCP characterization of NP. In Proc. of STOC, pages 475484, 1997.
T. Soma, N. Kakimura, K. Inaba, and K. Kawarabayashi. Optimal budget allocation: Theoretical guarantee and efficient algorithm. In Proc. of ICML, 2014.
H. O. Song, R. Girshick, S. Jegelka, J. Mairal, Z. Harchaoui, and T. Darrell. On learning to localize objects with minimal supervision. In Proc. of ICML, 2014.
M. Sviridenko, J. Vondrak, and J. Ward. Optimal approximation for submodular and supermodular optimization with bounded curvature. In Proc. of SODA, pages 11341148, 2015.
P.-J. Wan, D.-Z. Du, P. Pardalos, and W. Wu. Greedy approximations for minimum submodular cover with submodular cost. Computational Optimization and Applications, 45(2):463474, 2009.
L. A. Wolsey. An analysis of the greedy algorithm for the submodular set covering problem. Combinatorica, 2(4):385393, 1982. 9
Bahdanau, D., Cho, K., and Bengio, Y. (2015). Neural machine translation by jointly learning to align and translate. In Proceedings of the International Conference on Learning Representations (ICLR 2015).
Baldi, P., Brunak, S., Frasconi, P., Soda, G., and Pollastri, G. (1999). Exploiting the past and the future in protein secondary structure prediction. Bioinformatics, 15(11), 937946.
Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A., Bouchard, N., and Bengio, Y. (2012). Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.
Bayer, J. and Osendorfer, C. (2014). arXiv:1411.7610.  Learning stochastic recurrent networks.  8  arXiv preprint
Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2), 157166.
Bengio, Y., Yao, L., Alain, G., and Vincent, P. (2013). Generalized denoising auto-encoders as generative models. In Advances in Neural Information Processing Systems, pages 899907.
Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-Farley, D., and Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy 2010). Oral Presentation.
Boulanger-Lewandowski, N., Bengio, Y., and Vincent, P. (2012). Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription. In Proceedings of the 29th International Conference on Machine Learning (ICML 2012), pages 11591166.
Brakel, P., Stroobandt, D., and Schrauwen, B. (2013). Training energy-based models for time-series imputation. The Journal of Machine Learning Research, 14(1), 27712797.
Glorot, X. and Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. In International conference on artificial intelligence and statistics, pages 249256.
Goodfellow, I., Mirza, M., Courville, A., and Bengio, Y. (2013). Multi-prediction deep boltzmann machines. In Advances in Neural Information Processing Systems, pages 548556.
Graves, A., Liwicki, M., Fernandez, S., Bertolami, R., Bunke, H., and Schmidhuber, J. (2009). A novel connectionist system for unconstrained handwriting recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(5), 855868.
Graves, A., Mohamed, A.-r., and Hinton, G. (2013). Speech recognition with deep recurrent neural networks. arXiv preprint arXiv:1303.5778.
Haykin, S. (2009). Neural networks and learning machines, volume 3. Pearson Education.
Hermans, M. and Schrauwen, B. (2013). Training and analysing deep recurrent neural networks. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 190198. Curran Associates, Inc.
Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735 1780.
Koutnk, J., Greff, K., Gomez, F., and Schmidhuber, J. (2014). A clockwork RNN. In Proceedings of the 31 st International Conference on Machine Learning.
Maas, A. L., Hannun, A. Y., Jurafsky, D., and Ng, A. Y. (2014). First-pass large vocabulary continuous speech recognition using bi-directional recurrent dnns. arXiv preprint arXiv:1408.2873.
Mikolov, T., Joulin, A., Chopra, S., Mathieu, M., and Ranzato, M. (2014). Learning longer memory in recurrent neural networks. arXiv preprint arXiv:1412.7753.
Pascanu, R., Mikolov, T., and Bengio, Y. (2013). On the difficulty of training recurrent neural networks. In Proceedings of the 30th International Conference on Machine Learning (ICML 2013), pages 13101318.
Raiko, T. and Valpola, H. (2001). Missing values in nonlinear factor analysis. In Proc. of the 8th Int. Conf. on Neural Information Processing (ICONIP01),(Shanghai), pages 822827.
Raiko, T., Berglund, M., Alain, G., and Dinh, L. (2015). Techniques for learning binary stochastic feedforward neural networks. In International Conference on Learning Representations (ICLR 2015), San Diego.
Remes, U., Palomaki, K., Raiko, T., Honkela, A., and Kurimo, M. (2011). Missing-feature reconstruction with a bounded nonlinear state-space model. IEEE Signal Processing Letters, 18(10), 563566.
Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). Learning representations by back-propagating errors. Nature, 323, 533536.
Schuster, M. and Paliwal, K. K. (1997). Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 45(11), 26732681.
Sutskever, I., Martens, J., and Hinton, G. E. (2011). Generating text with recurrent neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML 2011), pages 10171024.
Uria, B., Murray, I., and Larochelle, H. (2014). A deep and tractable density estimator. In Proceedings of The 31st International Conference on Machine Learning, pages 467475.  9
A. Defazio, F. Bach, and S. Lacoste-julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 27, pages 16461654. 2014.
O. Fercoq and P. Richtarik. Accelerated, parallel and proximal coordinate descent. SIAM Journal on Optimization (after minor revision), arXiv:1312.5799, 2013.
O. Fercoq and P. Richtarik. Smooth minimization of nonsmooth functions by parallel coordinate descent. arXiv:1309.5885, 2013.
C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S.S. Keerthi, and S. Sundararajan. A dual coordinate descent method for large-scale linear SVM. In Proc. of the 25th International Conference on Machine Learning, ICML 08, pages 408415, 2008.
M. Jaggi, V. Smith, M. Takac, J. Terhorst, S. Krishnan, T. Hofmann, and M.I. Jordan. Communicationefficient distributed dual coordinate ascent. In Advances in Neural Information Processing Systems 27, pages 30683076. Curran Associates, Inc., 2014.
R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In C.j.c. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 315323. 2013.
J. Konecny, J. Lu, P. Richtarik, and M. Takac. mS2GD: Mini-batch semi-stochastic gradient descent in the proximal setting. arXiv:1410.4744, 2014.
J. Konecny and P. Richtarik. S2GD: Semi-stochastic gradient descent methods. arXiv:1312.1666, 2013.
Q. Lin, Z. Lu, and L. Xiao. An accelerated proximal coordinate gradient method and its application to regularized empirical risk minimization. Technical Report MSR-TR-2014-94, July 2014.
J. Mairal. Incremental majorization-minimization optimization with application to large-scale machine learning. SIAM J. Optim., 25(2):829855, 2015.
A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM J. Optim., 19(4):15741609, 2008.
Y. Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM J. Optim., 22(2):341362, 2012.
Y. Nesterov. Gradient methods for minimizing composite functions. Math. Program., 140(1, Ser. B):125 161, 2013.
Z. Qu and P. Richtarik. Coordinate descent methods with arbitrary sampling II: Expected separable overapproximation. arXiv:1412.8063, 2014.
Z. Qu and P. Richtarik. Coordinate descent methods with arbitrary sampling I: Algorithms and complexity. arXiv:1412.8060, 2014.
P. Richtarik and M. Takac. On optimal probabilities in stochastic coordinate descent methods. Optimization Letters, published online 2015.
P. Richtarik and M. Takac. Parallel coordinate descent methods for big data optimization. Math. Program., published online 2015.
H. Robbins and S. Monro. A stochastic approximation method. Ann. Math. Statistics, 22:400407, 1951.
M. Schmidt, N. Le Roux, and F. Bach. Minimizing finite sums with the stochastic average gradient. arXiv:1309.2388, 2013.
S. Shalev-Shwartz and T. Zhang. Proximal stochastic dual coordinate ascent. arXiv:1211.2717, 2012.
S. Shalev-shwartz and T. Zhang. Accelerated mini-batch stochastic dual coordinate ascent. In Advances in Neural Information Processing Systems 26, pages 378385. 2013.
S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss. J. Mach. Learn. Res., 14(1):567599, February 2013.
M. Takac, A.S. Bijral, P. Richtarik, and N. Srebro. Mini-batch primal and dual methods for svms. In Proc. of the 30th International Conference on Machine Learning (ICML-13), pages 10221030, 2013.
T. Yang. Trading computation for communication: Distributed stochastic dual coordinate ascent. In Advances in Neural Information Processing Systems 26, pages 629637. 2013.
T. Zhang. Solving large scale l.ear prediction problems using stochastic gradient descent algorithms. In Proc. of the 21st International Conference on Machine Learning (ICML-04), pages 919926, 2004.
Y. Zhang and L. Xiao. Stochastic primal-dual coordinate method for regularized empirical risk minimization. In Proc. of the 32nd International Conference on Machine Learning (ICML-15), pages 353361, 2015.
P. Zhao and T. Zhang. Stochastic optimization with importance sampling. ICML, 2015.  9
Abbeel, P., Koller, D., and Ng, A. Learning factor graphs in polynomial time and sample complexity. Journal of Machine Learning Research, 7:17431788, 2006.
Asuncion, A., Liu, Q., Ihler, A., and Smyth, P. Learning with blocks composite likelihood and contrastive divergence. In AISTATS, 2010.
Besag, J. Statistical analysis of non-lattice data. Journal of the Royal Statistical Society. Series D (The Statistician), 24(3):179195, 1975.
Boucheron, S., Lugosi, G., and Massart, P. Concentration Inequalities: A Nonasymptotic Theory of Independence. Oxford University Press, 2013.
Carreira-Peripin, M. A. and Hinton, G. On contrastive divergence learning. In AISTATS, 2005.
Chow, C. I. and Liu, C. N. Approximating discrete probability distributions with dependence trees. IEEE Transactions on Information Theory, 14:462467, 1968.
Descombes, X., Robin Morris, J. Z., and Berthod, M. Estimation of markov Random field prior parameters using Markov chain Monte Carlo maximum likelihood. IEEE Transactions on Image Processing, 8 (7):954963, 1996.
Domke, J. and Liu, X. Projecting Ising model parameters for fast mixing. In NIPS, 2013.
Dyer, M. E., Goldberg, L. A., and Jerrum, M. Matrix norms and rapid mixing for spin systems. Ann. Appl. Probab., 19:71107, 2009.
Geyer, C. Markov chain Monte Carlo maximum likelihood. In Symposium on the Interface, 1991.
Gu, M. G. and Zhu, H.-T. Maximum likelihood estimation for spatial models by Markov chain Monte Carlo stochastic approximation. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(2):339355, 2001.
Hayes, T. A simple condition implying rapid mixing of single-site dynamics on spin systems. In FOCS, 2006.
Heinemann, U. and Globerson, A. Inferning with high girth graphical models. In ICML, 2014.
Hinton, G. A practical guide to training restricted boltzmann machines. Technical report, University of Toronto, 2010.
Huber, M. Simulation reductions for the ising model. Journal of Statistical Theory and Practice, 5(3): 413424, 2012.
Hyvrinen, A. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6:695709, 2005.
Jerrum, M. and Sinclair, A. Polynomial-time approximation algorithms for the ising model. SIAM Journal on Computing, 22:10871116, 1993.
Koller, D. and Friedman, N. Probabilistic Graphical Models: Principles and Techniques. MIT Press, 2009.
Levin, D. A., Peres, Y., and Wilmer, E. L. Markov chains and mixing times. American Mathematical Society, 2006.
Lindsay, B. Composite likelihood methods. Contemporary Mathematics, 80(1):221239, 1988.
Liu, X. and Domke, J. Projecting Markov random field parameters for fast mixing. In NIPS, 2014.
Marlin, B. and de Freitas, N. Asymptotic efficiency of deterministic estimators for discrete energy-based models: Ratio matching and pseudolikelihood. In UAI, 2011.
Mizrahi, Y., Denil, M., and de Freitas, N. Linear and parallel learning of markov random fields. In ICML, 2014.
Papandreou, G. and Yuille, A. L. Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models. In ICCV, 2011.
Salakhutdinov, R. Learning in Markov random fields using tempered transitions. In NIPS, 2009.
Schmidt, M., Roux, N. L., and Bach, F. Convergence rates of inexact proximal-gradient methods for convex optimization. In NIPS, 2011.
Schmidt, U., Gao, Q., and Roth, S. A generative perspective on MRFs in low-level vision. In CVPR, 2010.
Steinhardt, J. and Liang, P. Learning fast-mixing models for structured prediction. In ICML, 2015.
Tieleman, T. Training restricted Boltzmann machines using approximations to the likelihood gradient. In ICML, 2008.
Varin, C., Reid, N., and Firth, D. An overview of composite likelihood methods. Statistica Sinica, 21: 524, 2011.
Wainwright, M. Estimating the "wrong" graphical model: Benefits in the computation-limited setting. Journal of Machine Learning Research, 7:18291859, 2006.
Wainwright, M. and Jordan, M. Graphical models, exponential families, and variational inference. Found. Trends Mach. Learn., 1(1-2):1305, 2008.
Zhu, S. C., Wu, Y., and Mumford, D. Filters, random fields and maximum entropy (FRAME): Towards a unified theory for texture modeling. International Journal of Computer Vision, 27(2):107126, 1998.  9
Alex Graves. Supervised sequence labelling with recurrent neural networks, volume 385. Springer, 2012.
Alex Graves, Marcus Liwicki, Horst Bunke, Jurgen Schmidhuber, and Santiago Fernandez. Unconstrained on-line handwriting recognition with recurrent neural networks. In Advances in Neural Information Processing Systems, pages 577584, 2008.
Alex Graves and Jurgen Schmidhuber. Offline handwriting recognition with multidimensional recurrent neural networks. In Advances in Neural Information Processing Systems, pages 545552, 2009.
Alex Graves, Abdel-ranhman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. In Proceedings of ICASSP, pages 66456649. IEEE, 2013.
Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. CoRR, abs/1412.6550, 2014. URL http://arxiv.org/ abs/1412.6550.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504507, 2006.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 19, 2015.
James Martens. Deep learning via Hessian-free optimization. In Proceedings of the 27th International Conference on Machine Learning, pages 735742, 2010.
James Martens and Ilya Sutskever. Learning recurrent neural networks with Hessian-free optimization. In Proceedings of the 28th International Conference on Machine Learning, pages 10331040, 2011.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735 1780, 1997.
Nicol N Schraudolph. Fast curvature matrix-vector products for second-order gradient descent. Neural Computation, 14(7):17231738, 2002.
Barak A Pearlmutter. Fast exact multiplication by the hessian. Neural Computation, 6(1):147160, 1994.
James Martens and Ilya Sutskever. Training deep and recurrent networks with Hessian-free optimization. In Neural Networks: Tricks of the Trade, pages 479535. Springer, 2012.
Alex Graves, Santiago Fernandez, Faustino Gomez, and Jurgen Schmidhuber. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the 23rd International Conference on Machine Learning, pages 369376, 2006.
Stephen Boyd and Lieven Vandenberghe, editors. Convex Optimization. Cambridge University Press, 2004.
Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural computation, 10(2):251276, 1998.
Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. In International Conference on Learning Representations, 2014.
Hyeyoung Park, S-I Amari, and Kenji Fukumizu. Adaptive natural gradient learning algorithms for various stochastic models. Neural Networks, 13(7):755764, 2000.
Christopher M. Bishop, editor. Pattern Recognition and Machine Learning. Springer, 2007.
Mario Pechwitz, S Snoussi Maddouri, Volker Margner, Noureddine Ellouze, and Hamid Amiri. IFN/ENIT-database of handwritten arabic words. In Proceedings of CIFED, pages 129136, 2002.
DARPA-ISTO. The DARPA TIMIT acoustic-phonetic continuous speech corpus (TIMIT). In speech disc cd1-1.1 edition, 1990.
Alex Graves. Sequence transduction with recurrent neural networks. In ICML Representation Learning Workshop, 2012.
Kai-Fu Lee and Hsiao-Wuen Hon. Speaker-independent phone recognition using hidden markov models. IEEE Transactions on Acoustics, Speech and Signal Processing, 37(11):16411648, 1989.
Alex Graves. Practical variational inference for neural networks. In Advances in Neural Information Processing Systems, pages 23482356, 2011.
Alex Graves. Rnnlib: A recurrent neural network library for sequence learning problems, 2008.  9
Vladimir Vovk and Ivan Petej. VennAbers predictors. In Nevin L. Zhang and Jin Tian, editors, Proceedings of the Thirtieth Conference on Uncertainty in Artificial Intelligence, pages 829 838, Corvallis, OR, 2014. AUAI Press.
Miriam Ayer, H. Daniel Brunk, George M. Ewing, W. T. Reid, and Edward Silverman. An empirical distribution function for sampling with incomplete information. Annals of Mathematical Statistics, 26:641647, 1955.
Xiaoqian Jiang, Melanie Osl, Jihoon Kim, and Lucila Ohno-Machado. Smooth isotonic regression: a new method to calibrate predictive models. AMIA Summits on Translational Science Proceedings, 2011:1620, 2011.
Antonis Lambrou, Harris Papadopoulos, Ilia Nouretdinov, and Alex Gammerman. Reliable probability estimates based on support vector machines for large multiclass datasets. In Lazaros Iliadis, Ilias Maglogiannis, Harris Papadopoulos, Kostas Karatzas, and Spyros Sioutas, editors, Proceedings of the AIAI 2012 Workshop on Conformal Prediction and its Applications, volume 382 of IFIP Advances in Information and Communication Technology, pages 182191, Berlin, 2012. Springer.
Rich Caruana and Alexandru Niculescu-Mizil. An empirical comparison of supervised learning algorithms. In Proceedings of the Twenty Third International Conference on Machine Learning, pages 161168, New York, 2006. ACM.
John C. Platt. Probabilities for SV machines. In Alexander J. Smola, Peter L. Bartlett, Bernhard Scholkopf, and Dale Schuurmans, editors, Advances in Large Margin Classifiers, pages 6174. MIT Press, 2000.
Vladimir Vovk, Alex Gammerman, and Glenn Shafer. Algorithmic Learning in a Random World. Springer, New York, 2005.
Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers. In Carla E. Brodley and Andrea P. Danyluk, editors, Proceedings of the Eighteenth International Conference on Machine Learning, pages 609 616, San Francisco, CA, 2001. Morgan Kaufmann.
Vladimir Vovk, Ivan Petej, and Valentina Fedorova. Large-scale probabilistic predictors with and without guarantees of validity. Technical report, arXiv.org e-Print archive, November 2015. A full version of this paper.
Richard E. Barlow, D. J. Bartholomew, J. M. Bremner, and H. Daniel Brunk. Statistical Inference under Order Restrictions: The Theory and Application of Isotonic Regression. Wiley, London, 1972.
Chu-In Charles Lee. The Min-Max algorithm and isotonic regression. Annals of Statistics, 11:467477, 1983.
Gordon D. Murray. Nonconvergence of the minimax order algorithm. Biometrika, 70:490491, 1983.
Vladimir N. Vapnik. Intelligent learning: Similarity control and knowledge transfer. Talk at the 2015 Yandex School of Data Analysis Conference Machine Learning: Prospects and Applications, 6 October 2015, Berlin.
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms. MIT Press, Cambridge, MA, third edition, 2009.
Vladimir Vovk. The fundamental nature of the log loss function. In Lev D. Beklemishev, Andreas Blass, Nachum Dershowitz, Berndt Finkbeiner, and Wolfram Schulte, editors, Fields of Logic and Computation II: Essays Dedicated to Yuri Gurevich on the Occasion of His 75th Birthday, volume 9300 of Lecture Notes in Computer Science, pages 307318, Cham, 2015. Springer.
Allan H. Murphy. A new vector partition of the probability score. Journal of Applied Meteorology, 12:595600, 1973.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. The WEKA data mining software: an update. SIGKDD Explorations, 11:1018, 2011.
A. Frank and A. Asuncion. UCI machine learning repository, 2015.  9
Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: NIPS. (2012) 11061114
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: CVPR. (2015)
Sun, Y., Liang, D., Wang, X., Tang, X.: Deepid3: Face recognition with very deep neural networks. In: arXiv:1502.00873. (2015)
Dong, C., Loy, C.C., He, K., , Tang, X.: Learning a deep convolutional network for image super-resolution. In: ECCV. (2014)
Xie, J., Xu, L., Chen, E.: Image denoising and inpainting with deep neural networks. In: NIPS. (2012)
Burger, H.C., Schuler, C.J., Harmeling, S.: compete with bm3d? In: CVPR. (2012)  Image denoising: Can plain neural networks
Xu, L., Ren, J.S., Liu, C., Jia, J.: Deep convolutional neural network for image deconvolution. In: NIPS. (2014)
Eigen, D., Krishnan, D., Fergus, R.: Restoring an image taken through a window covered with dirt or rain. In: ICCV. (2013)
Xu, L., Ren, J.S., Yan, Q., Liao, R., Jia, J.: Deep edge-aware filters. In: ICML. (2015)
Shepard, D.: A two-dimensional interpolation function for irregularly-spaced data. In: 23rd ACM national conference. (1968)
Timofte, R., Smet, V.D., Gool, L.V.: A+: Adjusted anchored neighborhood regression for fast super-resolution. In: ACCV. (2014)
LeCun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning applied to document recognition. In: Proceedings of IEEE. (1998)
Zeyde, R., Elad, M., Protter, M.: On single image scale-up using sparse-representations. Curves and Surfaces 6920 (2012) 711730 8
Bevilacqua, M., Roumy, A., Guillemot, C., Morel, M.L.A.: Low-complexity single-image super-resolution based on nonnegative neighbor embedding. In: BMVC. (2012)
Chang, H., Yeung, D.Y., Xiong, Y.: Super-resolution through neighbor embedding. In: CVPR. (2004)
Timofte, R., Smet, V.D., Gool, L.V.: Anchored neighborhood regression for fast examplebased super-resolution. In: ICCV. (2013)
Duchi, J., Hazan, E., Singer, Y.: Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research 12 (2011) 21212159  9
P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization algorithms on matrix manifolds. Princeton University Press, 2009.
D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding. In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms (SODA), pages 10271035, 2007.
R. Bhatia. Positive Definite Matrices. Princeton University Press, 2007.
C. M. Bishop. Pattern recognition and machine learning. Springer, 2007.
S. Bonnabel. Stochastic gradient descent on riemannian manifolds. Automatic Control, IEEE Transactions on, 58(9):22172229, 2013.
N. Boumal, B. Mishra, P.-A. Absil, and R. Sepulchre. Manopt, a matlab toolbox for optimization on manifolds. The Journal of Machine Learning Research, 15(1):14551459, 2014.
S. Burer, R. D. Monteiro, and Y. Zhang. Solving semidefinite programs via nonlinear programming. part i: Transformations and derivatives. Technical Report TR99-17, Rice University, Houston TX, 1999.
S. Dasgupta. Learning mixtures of gaussians. In Foundations of Computer Science, 1999. 40th Annual Symposium on, pages 634644. IEEE, 1999.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39:138, 1977.
R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classification. John Wiley & Sons, 2nd edition, 2000.
R. Ge, Q. Huang, and S. M. Kakade. Learning Mixtures of Gaussians in High Dimensions. arXiv:1503.00424, 2015.
R. Hosseini and M. Mashal. Mixest: An estimation toolbox for mixture models. arXiv preprint arXiv:1507.06065, 2015.
R. Hosseini and S. Sra. Differential geometric optimization for Gaussian mixture models. arXiv:1506.07677, 2015.
M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural computation, 6(2):181214, 1994.
M. Journee, F. Bach, P.-A. Absil, and R. Sepulchre. Low-rank optimization on the cone of positive semidefinite matrices. SIAM Journal on Optimization, 20(5):23272351, 2010.
R. W. Keener. Theoretical Statistics. Springer Texts in Statistics. Springer, 2010.
J. M. Lee. Introduction to Smooth Manifolds. Number 218 in GTM. Springer, 2012.
J. Ma, L. Xu, and M. I. Jordan. Asymptotic convergence rate of the em algorithm for gaussian mixtures. Neural Computation, 12(12):28812907, 2000.
G. J. McLachlan and D. Peel. Finite mixture models. John Wiley and Sons, New Jersey, 2000.
A. Moitra and G. Valiant. Settling the polynomial learnability of mixtures of gaussians. In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE Symposium on, pages 93102. IEEE, 2010.
K. P. Murphy. Machine Learning: A Probabilistic Perspective. MIT Press, 2012.
I. Naim and D. Gildea. Convergence of the EM algorithm for gaussian mixtures with unbalanced mixing coefficients. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 16551662, 2012.
J. Nocedal and S. J. Wright. Numerical Optimization. Springer, 2006.
R. A. Redner and H. F. Walker. Mixture densities, maximum likelihood, and the EM algorithm. Siam Review, 26:195239, 1984.
W. Ring and B. Wirth. Optimization methods on riemannian manifolds and their application to shape space. SIAM Journal on Optimization, 22(2):596627, 2012.
R. Salakhutdinov, S. T. Roweis, and Z. Ghahramani. Optimization with EM and Expectation-ConjugateGradient. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pages 672679, 2003.
S. Sra and R. Hosseini. Geometric optimisation on positive definite matrices for elliptically contoured distributions. In Advances in Neural Information Processing Systems, pages 25622570, 2013.
S. Sra and R. Hosseini. Conic Geometric Optimization on the Manifold of Positive Definite Matrices. SIAM Journal on Optimization, 25(1):713739, 2015.
C. Udriste. Convex functions and optimization methods on Riemannian manifolds. Kluwer, 1994.
R. J. Vanderbei and H. Y. Benson. On formulating semidefinite programming problems as smooth convex nonlinear optimization problems. Technical report, 2000.
B. Vandereycken. Low-rank matrix completion by riemannian optimization. SIAM Journal on Optimization, 23(2):12141236, 2013.
J. J. Verbeek, N. Vlassis, and B. Krose. Efficient greedy learning of gaussian mixture models. Neural computation, 15(2):469485, 2003.
A. Wiesel. Geodesic convexity and covariance estimation. IEEE Transactions on Signal Processing, 60 (12):618289, 2012.
L. Xu and M. I. Jordan. On convergence properties of the EM algorithm for Gaussian mixtures. Neural Computation, 8:129151, 1996.
D. Zoran and Y. Weiss. Natural images, gaussian mixtures and dead leaves. In Advances in Neural Information Processing Systems, pages 17361744, 2012.  9
Rie K. Ando and Tong Zhang. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6:18171853, 2005.
Rie K. Ando and Tong Zhang. Two-view feature generation model for semi-supervised learning. In Proceedings of ICML, 2007.
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model. Journal of Marchine Learning Research, 3:11371155, 2003.
Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of ICML, 2008.
Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493 2537, 2011.
Paramveer S. Dhillon, Dean Foster, and Lyle Ungar. Multi-view learning of word embeddings via CCA. In Proceedings of NIPS, 2011.
Jianfeng Gao, Patric Pantel, Michael Gamon, Xiaodong He, and Li dent. Modeling interestingness with deep neural networks. In Proceedings of EMNLP, 2014.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment classification: A deep learning approach. In Proceedings of ICML, 2011.
Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv:1207.0580, 2012.
Thorsten Joachims. Transductive inference for text classification using support vector machines. In Proceedings of ICML, 1999.
Rie Johnson and Tong Zhang. Effective use of word order for text categorization with convolutional neural networks. In Proceedings of NAACL HLT, 2015.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network for modeling sentences. In Proceedings of ACL, pages 655665, 2014.
Yoon Kim. Convolutional neural networks for sentence classification. In Proceedings of EMNLP, pages 17461751, 2014.
Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In Proceedings of ICML, 2014.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. In Proceedings of the IEEE.
David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. RCV1: A new benchmark collection for text categorization research. Journal of Marchine Learning Research, 5:361397, 2004.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of ACL, 2011.
Gregoire Mesnil, Tomas Mikolov, MarcAurelio Ranzato, and Yoshua Bengio. Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews. arXiv:1412.5335v5 (4 Feb 2015 version), 2014.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. In Proceedings of NIPS, 2013.
Andriy Mnih and Geoffrey E. Hinton. A scalable hierarchical distributed language model. In NIPS, 2008.
Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Gregoire Mensnil. A latent semantic model with convolutional-pooling structure for information retrieval. In Proceedings of CIKM, 2014.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu, and Bing Qin. Learning sentiment-specific word embedding for twitter sentiment classification. In Proceedings of ACL, pages 15551565, 2014.
Joseph Turian, Lev Rainov, and Yoshua Bengio. Word representations: A simple and general method for semi-supervised learning. In Proceedings of ACL, pages 384394, 2010.
Jason Weston, Sumit Chopra, and Keith Adams. #tagspace: Semantic embeddings from hashtags. In Proceedings of EMNLP, pages 18221827, 2014.
Liheng Xu, Kang Liu, Siwei Lai, and Jun Zhao. Product feature mining: Semantic clues versus syntactic constituents. In Proceedings of ACL, pages 336346, 2014.
Puyang Xu and Ruhi Sarikaya. Convolutional neural network based triangular CRF for joint intent detection and slot filling. In ASRU, 2013.  9
R. Marinescu and R. Dechter. AND/OR branch-and-bound search for combinatorial optimization in graphical models. Artificial Intelligence, 173(16-17):14571491, 2009.
A. Kishimoto and R. Marinescu. Recursive best-first AND/OR search for optimization in graphical models. In International Conference on Uncertainty in Artificial Intelligence (UAI), pages 400409, 2014.
R. Korf. Linear-space best-first search. Artificial Intelligence, 62(1):4178, 1993.
A. Kishimoto, A. Fukunaga, and A. Botea. Evaluation of a simple, scalable, parallel best-first search strategy. Artificial Intelligence, 195:222248, 2013.
Wahid Chrabakh and Rich Wolski. Gradsat: A parallel SAT solver for the Grid. Technical report, University of California at Santa Barbara, 2003.
M. Campbell, A. Joseph Hoane Jr., and F.-h. Hsu. Deep Blue. Artificial Intelligence, 134(1 2):5783, 2002.
M. Enzenberger, M. Muller, B. Arneson, and R. Segal. FUEGO - an open-source framework for board games and Go engine based on Monte-Carlo tree search. IEEE Transactions on Computational Intelligence and AI in Games, 2(4):259270, 2010.
L. Otten and R. Dechter. A case study in complexity estimation: Towards parallel branch-andbound over graphical models. In Uncertainty in Artificial Intelligence (UAI), pages 665674, 2012.
J. Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann, 1988.
S. L. Lauritzen. Graphical Models. Clarendon Press, 1996.
R. Dechter and R. Mateescu. AND/OR search spaces for graphical models. Artificial Intelligence, 171(2-3):73106, 2007.
R. Marinescu and R. Dechter. Memory intensive AND/OR search for combinatorial optimization in graphical models. Artificial Intelligence, 173(16-17):14921524, 2009.
A. Nagai. Df-pn Algorithm for Searching AND/OR Trees and Its Applications. PhD thesis, The University of Tokyo, 2002.
M. Fishelson and D. Geiger. Exact genetic linkage computations for general pedigrees. Bioinformatics, 18(1):189198, 2002.
C. Yanover, O. Schueler-Furman, and Y. Weiss. Minimizing and learning energy functions for side-chain prediction. Journal of Computational Biology, 15(7):899911, 2008.
A. Grama and V. Kumar. State of the art in parallel search techniques for discrete optimization problems. IEEE Transactions on Knowledge and Data Engineering, 11(1):2835, 1999.
D. Pennock. Logarithmic time parallel Bayesian inference. In Uncertainty in Artificial Intelligence (UAI), pages 431438, 1998.
Y. Xia and V. K. Prasanna. Junction tree decomposition for parallel exact inference. In IEEE International Symposium on Parallel and Distributed Processing (IPDPS), 2008.
L. V. Allis, M. van der Meulen, and H. J. van den Herik. Proof-number search. Artificial Intelligence, 66(1):91124, 1994.
A. Kishimoto, M. Winands, M. Muller, and J.-T. Saito. Game-tree search using proof numbers: The first twenty years. ICGA Journal, Vol. 35, No. 3, 35(3):131156, 2012.
T. Kaneko. Parallel depth first proof number search. In AAAI Conference on Artificial Intelligence, pages 95100, 2010.
J-T. Saito, M. H. M. Winands, and H. J. van den Herik. Randomized parallel proof-number search. In Advances in Computers Games Conference (ACG09), volume 6048 of Lecture Notes in Computer Science, pages 7587. Springer, 2010.
K. Hoki, T. Kaneko, A. Kishimoto, and T. Ito. Parallel dovetailing and its application to depthfirst proof-number search. ICGA Journal, 36(1):2236, 2013.  9
L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Semantic image segmentation with deep convolutional nets and fully connected crfs. In ICLR, 2015.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248255, 2009.
D. Eigen and R. Fergus. Nonparametric image parsing using adaptive neighbor sets. In CVPR, pages 27992806, 2012.
D. Eigen, J. Rolfe, R. Fergus, and Y. LeCun. Understanding deep architectures using a recursive convolutional network. In ICLR, 2014.  8
C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learning hierarchical features for scene labeling. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 35(8):19151929, 2013.
S. Gould, R. Fulton, and D. Koller. Decomposing a scene into geometric and semantically consistent regions. In ICCV, pages 18, 2009.
D. Grangier, L. Bottou, and R. Collobert. Deep convolutional networks for scene parsing. In ICML Deep Learning Workshop, volume 3, 2009.
A. Graves, M. Liwicki, S. Fernandez, R. Bertolami, H. Bunke, and J. Schmidhuber. A novel connectionist system for unconstrained handwriting recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 31(5):855868, 2009.
A. Graves, A.-r. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In ICASSP, pages 66456649, 2013.
Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the ACM International Conference on Multimedia, pages 675678, 2014.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, pages 10971105, 2012.
Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541551, 1989.
V. Lempitsky, A. Vedaldi, and A. Zisserman. A pylon model for semantic segmentation. In NIPS, pages 14851493, 2011.
M. Liang and X. Hu. Recurrent convolutional neural network for object recognition. In CVPR, pages 33673375, 2015.
C. Liu, J. Yuen, and A. Torralba. Nonparametric scene parsing via label transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 33(12):23682382, 2011.
J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.
M. Mostajabi, P. Yadollahpour, and G. Shakhnarovich. Feedforward semantic segmentation with zoomout features. In CVPR, 2015.
R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fidler, R. Urtasun, and A. Yuille. The role of context for object detection and semantic segmentation in the wild. In CVPR, pages 891898, 2014.
P. H. Pinheiro and R. Collobert. Recurrent convolutional neural networks for scene parsing. In ICML, 2014.
P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014.
A. Sharma, O. Tuzel, and M.-Y. Liu. Recursive context propagation network for semantic scene labeling. In NIPS, pages 24472455. 2014.
K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014.
G. Singh and J. Kosecka. Nonparametric scene parsing with adaptive feature relevance and semantic context. In CVPR, pages 31513157, 2013.
R. Socher, C. C. Lin, C. Manning, and A. Y. Ng. Parsing natural scenes and natural language with recursive neural networks. In ICML, pages 129136, 2011.
I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In NIPS, pages 31043112, 2014.
J. Tighe and S. Lazebnik. Finding things: Image parsing with regions and per-exemplar detectors. In CVPR, pages 30013008, 2013.
J. Tighe and S. Lazebnik. Superparsing: Scalable nonparametric image parsing with superpixels. International Journal of Computer Vision (IJCV), 101(2):329349, 2013.
P. J. Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78(10):15501560, 1990.
S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du, C. Huang, and P. Torr. Conditional random fields as recurrent neural networks. In ICCV, 2015.  9
B. Bidyuk and R. Dechter. Cutset Sampling for Bayesian Networks. Journal of Artificial Intelligence Research, 28:148, 2007.
R Braz, Eyal Amir, and Dan Roth. Lifted first-order probabilistic inference. In Proceedings of the 19th international joint conference on Artificial intelligence, pages 13191325. Citeseer, 2005.
George Casella and Christian P Robert. Rao-blackwellisation of sampling schemes. Biometrika, 83(1):8194, 1996.
M. Chavira and A. Darwiche. On probabilistic inference by weighted model counting. Artificial Intelligence, 172(6-7):772799, 2008.
Luc De Raedt and Kristian Kersting. Probabilistic inductive logic programming. Springer, 2008.
Rina Dechter and Robert Mateescu. And/or search spaces for graphical models. Artificial intelligence, 171(2):73106, 2007.
Michael R. Genesereth and Eric Kao. Introduction to Logic, Second Edition. Morgan & Claypool Publishers, 2013.
Vibhav Gogate and Pedro Domingos. Exploiting logical structure in lifted probabilistic inference. In Statistical Relational Artificial Intelligence, 2010.
Vibhav Gogate and Pedro Domingos. Probabilistic theorem proving. In Proceedings of the Twenty-Seventh Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI11), pages 256265, Corvallis, Oregon, 2011. AUAI Press.
Vibhav Gogate, Abhay Kumar Jha, and Deepak Venugopal. Advances in lifted importance sampling. In AAAI, 2012.
Abhay Jha, Vibhav Gogate, Alexandra Meliou, and Dan Suciu. Lifted inference seen from the other side: The tractable features. In Advances in Neural Information Processing Systems, pages 973981, 2010.
Brian Milch, Bhaskara Marthi, Stuart Russell, David Sontag, Daniel L Ong, and Andrey Kolobov. Blog: Probabilistic models with unknown objects. Statistical relational learning, page 373, 2007.
M. Niepert. Lifted probabilistic inference: An MCMC perspective. In UAI 2012 Workshop on Statistical Relational Artificial Intelligence, 2012.
M. Niepert. Symmetry-aware marginal density estimation. In Twenty-Seventh AAAI Conference on Artificial Intelligence, pages 725731, 2013.
David Poole. First-order probabilistic inference. In IJCAI, volume 3, pages 985991. Citeseer, 2003.
David Poole, Fahiem Bacchus, and Jacek Kisynski. Towards completely lifted search-based probabilistic inference. arXiv preprint arXiv:1107.4035, 2011.
Matthew Richardson and Pedro Domingos. Markov logic networks. Machine learning, 62(12):107136, 2006.
T. Sang, P. Beame, and H. Kautz. Solving Bayesian networks by weighted model counting. In Proceedings of the Twentieth National Conference on Artificial Intelligence, pages 475482, 2005.
Dan Suciu, Abhay Jha, Vibhav Gogate, and Alexandra Meliou. Lifted inference seen from the other side: The tractable features. In NIPS, 2010.
Nima Taghipour, Jesse Davis, and Hendrik Blockeel. First-order decomposition trees. In Advances in Neural Information Processing Systems, pages 10521060, 2013.
Guy Van den Broeck, Nima Taghipour, Wannes Meert, Jesse Davis, and Luc De Raedt. Lifted probabilistic inference by first-order knowledge compilation. In Proceedings of the TwentySecond international joint conference on Artificial Intelligence-Volume Volume Three, pages 21782185. AAAI Press, 2011.
Deepak Venugopal and Vibhav Gogate. On lifting the gibbs sampling algorithm. In Advances in Neural Information Processing Systems, pages 16551663, 2012. 9
R.M. Neal. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, 2, 2011.
M.A. Beaumont. Estimation of population growth or decline in genetically monitored populations. Genetics, 164(3):11391160, 2003.
C. Andrieu and G.O. Roberts. The pseudo-marginal approach for efficient Monte Carlo computations. The Annals of Statistics, 37(2):697725, April 2009.
M. Filippone and M. Girolami. Pseudo-marginal Bayesian inference for Gaussian Processes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2014.
P. Marjoram, J. Molitor, V. Plagnol, and S. Tavare. Markov chain Monte Carlo without likelihoods. Proceedings of the National Academy of Sciences, 100(26):1532415328, 2003.
S.A. Sisson and Y. Fan. Likelihood-free Markov chain Monte Carlo. Handbook of Markov chain Monte Carlo, 2010.
T. Chen, E. Fox, and C. Guestrin. Stochastic Gradient Hamiltonian Monte Carlo. In ICML, pages 16831691, 2014.
E. Meeds, R. Leenders, and M. Welling. Hamiltonian ABC. In UAI, 2015.
M. Betancourt. The Fundamental Incompatibility of Hamiltonian Monte Carlo and Data Subsampling. arXiv preprint arXiv:1502.01510, 2015.
H. Haario, E. Saksman, and J. Tamminen. Adaptive proposal distribution for random walk Metropolis algorithm. Computational Statistics, 14(3):375395, 1999.
C. Andrieu and J. Thoms. A tutorial on adaptive MCMC. Statistics and Computing, 18(4):343 373, December 2008.
D. Sejdinovic, H. Strathmann, M. Lomeli, C. Andrieu, and A. Gretton. Kernel Adaptive Metropolis-Hastings. In ICML, 2014.
B. Sriperumbudur, K. Fukumizu, R. Kumar, A. Gretton, and A. Hyvarinen. Density Estimation in Infinite Dimensional Exponential Families. arXiv preprint arXiv:1312.3516, 2014.
A. Hyvarinen. Estimation of non-normalized statistical models by score matching. JMLR, 6:695709, 2005.
Larry Wasserman. All of nonparametric statistics. Springer, 2006.
C.E. Rasmussen. Gaussian Processes to Speed up Hybrid Monte Carlo for Expensive Bayesian Integrals. Bayesian Statistics 7, pages 651659, 2003.
A. Rahimi and B. Recht. Random features for large-scale kernel machines. In NIPS, pages 11771184, 2007.
M. Betancourt, S. Byrne, and M. Girolami. Optimizing The Integrator Step Size for Hamiltonian Monte Carlo. arXiv preprint arXiv:1503.01916, 2015.
A. Berlinet and C. Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and Statistics. Kluwer, 2004.
A. Hyvarinen. Some extensions of score matching. Computational Statistics & Data Analysis, 51:24992512, 2007.
B.K. Sriperumbudur and Z. Szabo. Optimal rates for random Fourier features. In NIPS, 2015.
G.O. Roberts and R.L. Tweedie. Geometric convergence and central limit theorems for multidimensional Hastings and Metropolis algorithms. Biometrika, 83(1):95110, 1996.
G.O. Roberts and J.S. Rosenthal. Coupling and ergodicity of adaptive Markov chain Monte Carlo algorithms. Journal of Applied Probability, 44(2):458475, 03 2007.
K. Bache and M. Lichman. UCI Machine Learning Repository, 2013.
A. Gretton, K. Borgwardt, B. Scholkopf, A. J. Smola, and M. Rasch. A kernel two-sample test. JMLR, 13:723773, 2012.
S. N. Wood. Statistical inference for noisy nonlinear ecological dynamic systems. Nature, 466(7310):11021104, 08 2010.
C. Zhang, B. Shahbaba, and H. Zhao. Hamiltonian Monte Carlo Acceleration Using Neural Network Surrogate functions. arXiv preprint arXiv:1506.05555, 2015.
J. Shawe-Taylor and N. Cristianini. Kernel methods for pattern analysis. Cambridge university press, 2004.
Q. Le, T. Sarlos, and A. Smola. Fastfoodapproximating kernel expansions in loglinear time. In ICML, 2013.
K.L. Mengersen and R.L. Tweedie. Rates of convergence of the Hastings and Metropolis algorithms. The Annals of Statistics, 24(1):101121, 1996.  9
Yasin Abbasi-Yadkori, Csaba Szepesvari, and David Tax. Improved algorithms for linear stochastic
bandits. In Advances in Neural Information Processing Systems, pages 23122320, 2011.
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Online-to-confidence-set conversions and
application to sparse stochastic bandits. In AISTATS, volume 22, pages 19, 2012.
Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs.
arXiv preprint arXiv:1209.3352, 2012.
Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. The Journal of Machine Learning Research, 3:397422, 2003.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
problem. Machine Learning, 47:235256, 2002.
Kristin P Bennett and Olvi L Mangasarian. Bilinear separation of two sets inn-space. Computational
Optimization and Applications, 2(3):207227, 1993.
Sebastien Bubeck and Nicolo Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multiarmed Bandit Problems. Foundations and Trends in Machine Learning. Now Publishers Incorporated, 2012. ISBN 9781601986269.
Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit
feedback. In COLT, pages 355366, 2008.
Akshay Krishnamurthy, Alekh Agarwal, and Miroslav Dudik. Efficient contextual semi-bandit
learning. arXiv preprint arXiv:1502.05890, 2015.
Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesvari. Tight regret bounds for stochastic combinatorial semi-bandits. arXiv preprint arXiv:1410.0949, 2014.
Tor Lattimore, Koby Crammer, and Csaba Szepesvari. Optimal resource allocation with semi-bandit
feedback. In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence (UAI),
Marek Petrik and Shlomo Zilberstein. Robust approximate bilinear programming for value function
approximation. The Journal of Machine Learning Research, 12:30273063, 2011.
Paat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. Mathematics of
Operations Research, 35(2):395411, 2010.
Thomas Sowell. Is Reality Optional?: And Other Essays. Hoover Institution Press, 1993.
Adam Albright and Bruce Hayes. Rules vs. analogy in english past tenses: A computational/experimental study. Cognition, 90:119161, 2003.
Brenden M Lake, Ruslan R Salakhutdinov, and Josh Tenenbaum. One-shot learning by inverting a compositional causal process. In Advances in neural information processing systems, pages 25262534, 2013.
Noah D. Goodman, Vikash K. Mansinghka, Daniel M. Roy, Keith Bonawitz, and Joshua B. Tenenbaum. Church: a language for generative models. In UAI, pages 220229, 2008.
Sumit Gulwani, Jose Hernandez-Orallo, Emanuel Kitzelmann, Stephen Muggleton, Ute Schmid, and Ben Zorn. Inductive programming meets the real world. Commun. ACM, 2015.
Francois Fleuret, Ting Li, Charles Dubout, Emma K Wampler, Steven Yantis, and Donald Geman. Comparing machines and humans on a visual categorization test. PNAS, 108(43):1762117625, 2011.
Ray J Solomonoff. A formal theory of inductive inference. Information and control, 7(1):122, 1964.
Armando Solar Lezama. Program Synthesis By Sketching. PhD thesis, EECS Department, University of California, Berkeley, Dec 2008.
Leonardo De Moura and Nikolaj Bjrner. Z3: An efficient smt solver. In Tools and Algorithms for the Construction and Analysis of Systems, pages 337340. Springer, 2008.
Emina Torlak and Rastislav Bodik. Growing solver-aided languages with rosette. In Proceedings of the 2013 ACM international symposium on New ideas, new paradigms, and reflections on programming & software, pages 135152. ACM, 2013.
Stanislas Dehaene, Veronique Izard, Pierre Pica, and Elizabeth Spelke. Core knowledge of geometry in an amazonian indigene group. Science, 311(5759):381384, 2006.
David D. Thornburg. Friends of the turtle. Compute!, March 1983.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, November 1998.
Mark S Seidenberg and David C Plaut. Quasiregularity and its discontents: the legacy of the past tense debate. Cognitive science, 38(6):11901228, 2014.
Erwin Chan and Constantine Lignos. Investigating the relationship between linguistic representation and computation through an unsupervised model of human morphology learning. Research on Language and Computation, 8(2-3):209238, 2010.
R Piepenbrock Baayen, R and L Gulikers. CELEX2 LDC96L14. Philadelphia: Linguistic Data Consortium, 1995. Web download.
Martin A. Fischler and Robert C. Bolles. Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography. Commun. ACM, 24(6):381395, June 1981.
Sami Virpioja, Peter Smit, Stig-Arne Grnroos, and Mikko Kurimo. Morfessor 2.0: Python implementation and extensions for morfessor baseline. Technical report, Aalto University, Helsinki, 2013.
John R. Koza. Genetic programming - on the programming of computers by means of natural selection. Complex adaptive systems. MIT Press, 1993.
Eric Schkufza, Rahul Sharma, and Alex Aiken. Stochastic superoptimization. In ACM SIGARCH Computer Architecture News, volume 41, pages 305316. ACM, 2013.
Sumit Gulwani. Automating string processing in spreadsheets using input-output examples. In POPL, pages 317330, New York, NY, USA, 2011. ACM.
Yarden Katz, Noah D. Goodman, Kristian Kersting, Charles Kemp, and Joshua B. Tenenbaum. Modeling semantic cognition as logical dimensionality reduction. In CogSci, pages 7176, 2008.
Percy Liang, Michael I. Jordan, and Dan Klein. Learning programs: A hierarchical bayesian approach. In Johannes Furnkranz and Thorsten Joachims, editors, ICML, pages 639646. Omnipress, 2010.
Sumit Gulwani, Susmit Jha, Ashish Tiwari, and Ramarathnam Venkatesan. Synthesis of loop-free programs. In PLDI, pages 6273, New York, NY, USA, 2011. ACM.
D. E. Rumelhart and J. L. McClelland. On learning the past tenses of english verbs. In Parallel distributed processing: Explorations in the microstructure of cognition, pages Volume 2, 216271. Bradford Books/MIT Press, 1986.
John Goldsmith. Unsupervised learning of the morphology of a natural language. Comput. Linguist., 27(2):153198, June 2001.
Timothy J. ODonnell. Productivity and Reuse in Language: A Theory of Linguistic Computation and Storage. The MIT Press, 2015.
Karol Gregor, Ivo Danihelka, Alex Graves, and Daan Wierstra. DRAW: A recurrent neural network for image generation. CoRR, abs/1502.04623, 2015.  9
Kawato, M. (1999). Internal models for motor control and trajectory planning. Current opinion in neurobiology, 9(6), 718-727.
Lackner, J. R., & Dizio, P. (1998). Gravitoinertial force background level affects adaptation to coriolis force perturbations of reaching movements. Journal of neurophysiology, 80(2), 546553.
Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1988). Learning representations by backpropagating errors. Cognitive modeling, 5.
Williams, R. J., & Zipser, D. (1989). A learning algorithm for continually running fully recurrent neural networks. Neural computation, 1(2), 270-280.
Jaeger, H. (2001). The echo state approach to analysing and training recurrent neural networkswith an erratum note. Bonn, Germany: German National Research Center for Information Technology GMD Technical Report, 148, 34.
Maass, W., Natschlger, T., & Markram, H. (2002). Real-time computing without stable states: A new framework for neural computation based on perturbations. Neural computation, 14(11), 2531-2560.
Sussillo, D., & Abbott, L. F. (2009). Generating coherent patterns of activity from chaotic neural networks. Neuron, 63(4), 544-557.
Legenstein, R., Naeger, C., & Maass, W. (2005). What can a neuron learn with spike-timingdependent plasticity?. Neural computation, 17(11), 2337-2382.
Pfister, J., Toyoizumi, T., Barber, D., & Gerstner, W. (2006). Optimal spike-timing-dependent plasticity for precise action potential firing in supervised learning. Neural computation, 18(6), 1318-1348.
Ponulak, F., & Kasinski, A. (2010). Supervised learning in spiking neural networks with ReSuMe: sequence learning, classification, and spike shifting. Neural Computation, 22(2), 467510.
Memmesheimer, R. M., Rubin, R., lveczky, B. P., & Sompolinsky, H. (2014). Learning precisely timed spikes. Neuron, 82(4), 925-938.
Gutig, R., & Sompolinsky, H. (2006). The tempotron: a neuron that learns spike timingbased decisions. Nature neuroscience, 9(3), 420-428.
van Vreeswijk, C., & Sompolinsky, H. (1996). Chaos in neuronal networks with balanced excitatory and inhibitory activity. Science, 274(5293), 1724-1726.
Brunel, N. (2000). Dynamics of networks of randomly connected excitatory and inhibitory spiking neurons. Journal of Physiology-Paris, 94(5), 445-463.
Boerlin, M., Machens, C. K., & Deneve, S. (2013). Predictive coding of dynamical variables in balanced spiking networks. PLoS computational biology, 9(11), e1003258.
Bourdoukan, R., Barrett, D., Machens, C. K & Deneve, S. (2012). Learning optimal spikebased representations. In Advances in neural information processing systems (pp. 2285-2293).
Vertechi, P., Brendel, W., & Machens, C. K. (2014). Unsupervised learning of an efficient short-term memory network. In Advances in Neural Information Processing Systems (pp. 36533661).
Watanabe, M., & Kano, M. (2011). Climbing fiber synapse elimination in cerebellar Purkinje cells. European Journal of Neuroscience, 34(10), 1697-1710.
Chen, C., Kano, M., Abeliovich, A., Chen, L., Bao, S., Kim, J. J., ... & Tonegawa, S. (1995). Impaired motor coordination correlates with persistent multiple climbing fiber innervation in PKC mutant mice. Cell, 83(7), 1233-1242.
Eccles, J. C., Llinas, R., & Sasaki, K. (1966). The excitatory synaptic action of climbing fibres on the Purkinje cells of the cerebellum. The Journal of Physiology, 182(2), 268-296.
Knudsen, E. I. (1994). Supervised Learning in the Brain. The Journal of Neuroscience 14(7), 39853997.
Thalmeier, D., Uhlmann, M., Kappen, H.J., & Memmesheimer, R. Learning universal computations with spikes. under review.  9
A. Anandkumar, R. Ge, D. Hsu, S. Kakade, and M. Telgarsky. Tensor decompositions for learning latent variable models. Journal of Machine Learning Research, 15:27732832, 2014.
S. Bhojanapalli and S. Sanghavi. A new sampling technique for tensors. arXiv:1502.05023, 2015.
D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. Journal of machine Learning research, 3:9931022, 2003.
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. Hruschka Jr, and T. M. Mitchell. Toward an architecture for never-ending language learning. In AAAI, 2010.
J. D. Carroll and J.-J. Chang. Analysis of individual differences in multidimensional scaling via an n-way generalization of eckart-young decomposition. Psychometrika, 35(3):283319, 1970.
A. Chaganty and P. Liang. Estimating latent-variable graphical models using moments and likelihoods. In ICML, 2014.
M. Charikar, K. Chen, and M. Farach-Colton. Finding frequent items in data streams. Theoretical Computer Science, 312(1):315, 2004.
J. H. Choi and S. Vishwanathan. DFacTo: Distributed factorization of tensors. In NIPS, 2014.
C. Dwork, V. Feldman, M. Hardt, T. Pitassi, O. Reingold, and A. Roth. Preserving statistical validity in adaptive data analysis. In STOC, 2015.
A. S. Field and D. Graupe. Topographic component (parallel factor) analysis of multichannel evoked potentials: practical issues in trilinear spatiotemporal decomposition. Brain Topography, 3(4):407423, 1991.
T. L. Griffiths and M. Steyvers. Finding scientific topics. Proceedings of the National Academy of Sciences, 101(suppl 1):52285235, 2004.
R. A. Harshman. Foundations of the PARAFAC procedure: Models and conditions for an explanatory multi-modal factor analysis. UCLA Working Papers in Phonetics, 16:184, 1970.
F. Huang, S. Matusevych, A. Anandkumar, N. Karampatziakis, and P. Mineiro. Distributed latent dirichlet allocation via tensor factorization. In NIPS Optimization Workshop, 2014.
F. Huang, U. N. Niranjan, M. U. Hakeem, and A. Anandkumar. Fast detection of overlapping communities via online tensor methods. arXiv:1309.0787, 2013.
A. Jain. Fundamentals of digital image processing, 1989.
U. Kang, E. Papalexakis, A. Harpale, and C. Faloutsos. Gigatensor: Scaling tensor analysis up by 100 times - algorithms and discoveries. In KDD, 2012.
B. Klimt and Y. Yang. Introducing the enron corpus. In CEAS, 2004.
T. Kolda and B. Bader. The tophits model for higher-order web link analysis. In Workshop on link analysis, counterterrorism and security, 2006.
T. Kolda and B. Bader. Tensor decompositions and applications. SIAM Review, 51(3):455500, 2009.
T. G. Kolda and J. Sun. Scalable tensor decompositions for multi-aspect data mining. In ICDM, 2008.
M. Mrup, L. K. Hansen, C. S. Herrmann, J. Parnas, and S. M. Arnfred. Parallel factor analysis as an exploratory tool for wavelet transformed event-related eeg. NeuroImage, 29(3):938947, 2006.
R. Pagh. Compressed matrix multiplication. In ITCS, 2012.
N. Pham and R. Pagh. Fast and scalable polynomial kernels via explicit feature maps. In KDD, 2013.
A.-H. Phan, P. Tichavsky, and A. Cichocki. Fast alternating LS algorithms for high order CANDECOMP/PARAFAC tensor factorizations. IEEE Transactions on Signal Processing, 61(19):48344846, 2013.
X.-H. Phan and C.-T. Nguyen. GibbsLDA++: A C/C++ implementation of latent dirichlet allocation (lda), 2007.
M. Ptrascu and M. Thorup. The power of simple tabulation hashing. Journal of the ACM, 59(3):14, 2012.
C. Tsourakakis. MACH: Fast randomized tensor decompositions. In SDM, 2010.
H.-Y. Tung and A. Smola. Spectral methods for indian buffet process inference. In NIPS, 2014.
C. Wang, X. Liu, Y. Song, and J. Han. Scalable moment-based inference for latent dirichlet allocation. In ECML/PKDD, 2014.
Y. Wang and J. Zhu. Spectral methods for supervised topic models. In NIPS, 2014.  9
R. Basri and D. Jacobs. Lambertian reflectance and linear subspaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(2):218233, 2003.
A. Blum, C. Dwork, F. McSherry, and K. Nissim. Practical privacy: the SULQ framework. In PODS, 2015.
P. S. Bradley and O. L. Mangasarian. k-plane clustering. Journal of Global Optimization, 16(1), 2000.
K. Chaudhuri, A. Sarwate, and K. Sinha. Near-optimal algorithms for differentially private principal components. In NIPS, 2012.
Y. Chen, A. Jalali, S. Sanghavi, and H. Xu. Clustering partially observed graphs via convex optimization. The Journal of Machine Learning Research, 15(1):22132238, 2014.
C. Dimitrakakis, B. Nelson, A. Mitrokotsa, and B. I. Rubinstein. Robust and private bayesian inference. In Algorithmic Learning Theory, pages 291305. Springer, 2014.
C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov, and M. Naor. Our data, ourselves: Privacy via distributed noise generation. In EUROCRYPT, 2006.
C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. In TCC, 2006.
C. Dwork and A. Roth. The algorithmic foundations of differential privacy. Foundations and Trends in Theoretical Computer Science, 9(34):211407, 2014.
C. Dwork, K. Talwar, A. Thakurta, and L. Zhang. Analyze Gauss: Optimal bounds for privacy-preserving principal component analysis. In STOC, 2014.
E. Elhamifar and R. Vidal. Sparse subspace clustering: Algorithm, theory and applications. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(11):27652781, 2013.
D. Feldman, M. Schmidt, and C. Sohler. Turning big data into tiny data: Constant-size coresets for k-means, pca and projective clustering. In SODA, 2013.
A. Georghiades, P. Belhumeur, and D. Kriegman. From few to many: Illumination cone models for face recognition under variable lighting and pose. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(6):643660, 2001.
R. Heckel and H. Bolcskei. Robust subspace clustering via thresholding. arXiv:1307.4891, 2013.
J. Ho, M.-H. Yang, J. Lim, K.-C. Lee, and D. Kriegman. Clustering appearances of objects under varying illumination conditions. In CVPR, 2003.
P. Hoff. Simulation of the matrix bingham-conmises-fisher distribution, with applications to multivariate and relational data. Journal of Computational and Graphical Statistics, 18(2):438456, 2009.
G. Liu, Z. Lin, S. Yan, J. Sun, Y. Ma, and Y. Yu. Robust recovery of subspace structures by low-rank representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(1):171184, 2012.
F. McSherry and K. Talwar. Mechanism design via differential privacy. In FOCS, 2007.
B. McWilliams and G. Montana. Subspace clustering of high-dimensional data: a predictive approach. Data Mining and Knowledge Discovery, 28(3):736772, 2014.
D. J. Mir. Differential privacy: an exploration of the privacy-utility landscape. PhD thesis, Rutgers University, 2013.
B. Nasihatkon and R. Hartley. Graph connectivity in sparse subspace clustering. In CVPR, 2011.
K. Nissim, S. Raskhodnikova, and A. Smith. Smooth sensitivity and sampling in private data analysis. In STOC, 2007.
R. Ostrovksy, Y. Rabani, L. Schulman, and C. Swamy. The effectiveness of Lloyd-type methods for the k-means problem. In FOCS, 2006.
M. Soltanolkotabi, E. J. Candes, et al. A geometric analysis of subspace clustering with outliers. The Annals of Statistics, 40(4):21952238, 2012.
M. Soltanolkotabi, E. Elhamifa, and E. Candes. Robust subspace clustering. The Annals of Statistics, 42(2):669699, 2014.
D. Su, J. Cao, N. Li, E. Bertino, and H. Jin. Differentially private k-means clustering. arXiv, 2015.
M. Tipping and C. Bishop. Mixtures of probabilistic principle component anlyzers. Neural computation, 11(2):443482, 1999.
Y. Wang, Y.-X. Wang, and A. Singh. Clustering consistent sparse subspace clustering. arXiv, 2015.
Y. Wang, Y.-X. Wang, and A. Singh. A deterministic analysis of noisy sparse subspace clustering for dimensionality-reduced data. In ICML, 2015.
Y. Wang and J. Zhu. DP-space: Bayesian nonparametric subspace clustering with small-variance asymptotic analysis. In ICML, 2015.
Y.-X. Wang, S. Fienberg, and A. Smola. Privacy for free: Posterior sampling and stochastic gradient monte carlo. In ICML, 2015.
Y.-X. Wang and H. Xu. Noisy sparse subspace clustering. In ICML, pages 8997, 2013.
A. Zhang, N. Fawaz, S. Ioannidis, and A. Montanari. Guess who rated this movie: Identifying users through subspace clustering. arXiv, 2012.
Z. Zhang, K. L. Chan, J. Kwok, and D.-Y. Yeung. Bayesian inference on principal component analysis using reversible jump markov chain monte carlo. In AAAI, 2004.  9
Harish G. Ramaswamy and Shivani Agarwal. Classification calibration dimension for general multiclass losses. In Advances in Neural Information Processing Systems, pages 20782086, 2012.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT press, 2012.
Michael Collins. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 18, 2002.
Nathan D. Ratliff, J Andrew Bagnell, and Martin Zinkevich. (Approximate) subgradient methods for structured prediction. In International Conference on Artificial Intelligence and Statistics, pages 380 387, 2007.
Youssef Mroueh, Tomaso Poggio, Lorenzo Rosasco, and Jean-Jeacques Slotine. Multiclass learning with simplex coding. In Advances in Neural Information Processing Systems, pages 27892797, 2012.
Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in Machine Learning, 4(2):107194, 2011.
Albert B.J. Novikoff. On convergence proofs on perceptrons. In Proceedings of the Symposium on the Mathematical Theory of Automata, volume 12, pages 615622, 1962.
Yaoyong Li, Hugo Zaragoza, Ralf Herbrich, John Shawe-Taylor, and Jaz S. Kandola. The perceptron algorithm with uneven margins. In Proceedings of the Nineteenth International Conference on Machine Learning, pages 379386, 2002.
Gunnar Ratsch and Jyrki Kivinen. Extended classification with modified Perceptron, 2002. Presented at the NIPS 2002 Workshop: Beyond Classification and Regression: Learning Rankings, Preferences, Equality Predicates, and Other Structures; abstract available at http://www.cs.cornell.edu/ people/tj/ranklearn/raetsch_kivinen.pdf.
Koby Crammer and Yoram Singer. Ultraconservative online algorithms for multiclass problems. The Journal of Machine Learning Research, 3:951991, 2003.
Koby Crammer and Yoram Singer. On the algorithmic implementation of multiclass kernel-based vector machines. The Journal of Machine Learning Research, 2:265292, 2002.
Koby Crammer and Yoram Singer. Pranking with ranking. Advances in Neural Information Procession Systems, 14:641647, 2002.
David Cossock and Tong Zhang. Statistical analysis of bayes optimal subset ranking. IEEE Transactions on Information Theory, 54(11):51405154, 2008.
Ambuj Tewari and Sougata Chaudhuri. Generalization error bounds for learning to rank: Does the length of document lists matter? In Proceedings of the 32nd International Conference on Machine Learning, volume 37 of JMLR Workshop and Conference Proceedings, 2015.
Koby Crammer and Yoram Singer. A family of additive online algorithms for category ranking. The Journal of Machine Learning Research, 3:10251058, 2003.
Eneldo Loza Menca and Johannes Furnkranz. Pairwise learning of multilabel classifications with perceptrons. In IEEE International Joint Conference on Neural Networks, pages 28992906, 2008.
Sham M. Kakade, Shai Shalev-Shwartz, and Ambuj Tewari. Regularization techniques for learning with matrices. Journal of Machine Learning Research, 13:18651890, 2012.  9
S. Arora, E. Hazan, and S. Kale. Fast algorithms for approximate semidefinite programming using the multiplicative weights update method. In Foundations of Computer Science, 2005. FOCS 2005. 46th Annual IEEE Symposium on, pages 339348. IEEE, 2005.
S. Arora, E. Hazan, and S. Kale. The multiplicative weights update method: a meta-algorithm and applications. Theory of Computing, 8(1):121164, 2012.
N. Bansal, A. Blum, and S. Chawla. Correlation clustering. Machine Learning, 56(1-3):89113, 2004.
F. Bonchi, A. Gionis, and A. Ukkonen. Overlapping correlation clustering. Knowledge and information systems, 35(1):132, 2013.
M. Brand. Fast low-rank modifications of the thin singular value decomposition. Linear algebra and its applications, 415(1):2030, 2006.
S. Burer and R. D. Monteiro. A projected gradient algorithm for solving the maxcut sdp relaxation. Optimization methods and Software, 15(3-4):175200, 2001.
M. Elsner and W. Schudy. Bounding and comparing methods for correlation clustering beyond ilp. In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing, pages 1927. Association for Computational Linguistics, 2009.
M. X. Goemans. Semidefinite programming in combinatorial optimization. Math. Program., 79:143161, 1997.
M. X. Goemans and D. P. Williamson. Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. Journal of the ACM (JACM), 42(6):11151145, 1995.
M. Grotschel, L. Lovasz, and A. Schrijver. Geometric Algorithms and Combinatorial Optimization, volume 2 of Algorithms and Combinatorics. Springer, 1988.
C. Helmberg and F. Rendl. A spectral bundle method for semidefinite programming. SIAM Journal on Optimization, 10(3):673696, 2000.
D. Hush, P. Kelly, C. Scovel, and I. Steinwart. Qp algorithms with guaranteed accuracy and run time for support vector machines. Journal of Machine Learning Research, 7:733769, 2006.
G. Iyengar, D. J. Phillips, and C. Stein. Approximating semidefinite packing programs. SIAM Journal on Optimization, 21(1):231268, 2011.
V. Jethava, A. Martinsson, C. Bhattacharyya, and D. Dubhashi. Lovasz  function, svms and finding dense subgraphs. The Journal of Machine Learning Research, 14(1):34953536, 2013.
V. Jethava, J. Sznajdman, C. Bhattacharyya, and D. Dubhashi. Lovasz , svms and applications. In Information Theory Workshop (ITW), 2013 IEEE, pages 15. IEEE, 2013.
F. D. Johanson, A. Chattoraj, C. Bhattacharyya, and D. Dubhashi. Supplementary material, 2015.
D. E. Knuth. The sandwich theorem. Electr. J. Comb., 1, 1994.
H. Lin and J. Bilmes. A class of submodular functions for document summarization. In Proc. of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesVolume 1, pages 510520. Association for Computational Linguistics, 2011.
L. Lovasz. On the shannon capacity of a graph. IEEE Transactions on Information Theory, 25(1):17, 1979.
L. Lovasz and K. Vesztergombi. Geometric representations of graphs. Paul Erdos and his Mathematics, 1999.
R. Mart, A. Duarte, and M. Laguna. Advanced scatter search for the max-cut problem. INFORMS Journal on Computing, 21(1):2638, 2009.
D. T. Pham, S. S. Dimov, and C. Nguyen. Selection of k in k-means clustering. Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science, 219(1):103 119, 2005.
B. Scholkopf, J. C. Platt, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson. Estimating the support of a high-dimensional distribution. Neural computation, 13(7):14431471, 2001.
A. Schrijver. A comparison of the delsarte and lovasz bounds. Information Theory, IEEE Transactions on, 25(4):425429, 1979.
J. Wang, T. Jebara, and S.-F. Chang. Semi-supervised learning using greedy max-cut. The Journal of Machine Learning Research, 14(1):771800, 2013.  9
F. R. Bach and E. Moulines. Non-Asymptotic Analysis of Stochastic Approximation Algorithms for Machine Learning. In NIPS, 2011.
A. Bellet, A. Habrard, and M. Sebban. A Survey on Metric Learning for Feature Vectors and Structured Data. Technical report, arXiv:1306.6709, June 2013.
A. Bellet, A. Habrard, and M. Sebban. Metric Learning. Morgan & Claypool Publishers, 2015.
L. Bottou and O. Bousquet. The Tradeoffs of Large Scale Learning. In NIPS, 2007.
S. Clemencon, G. Lugosi, and N. Vayatis. Ranking and empirical risk minimization of U-statistics. Ann. Statist., 36, 2008.
S. Clemencon, S. Robbiano, and J. Tressou. Maximal deviations of incomplete U -processes with applications to Empirical Risk Sampling. In SDM, 2013.
S. Clemencon. On U-processes and clustering performance. In NIPS, pages 3745, 2011.
S. Clemencon, P. Bertail, and E. Chautru. Scaling up M-estimation via sampling designs: The HorvitzThompson stochastic gradient descent. In IEEE Big Data, 2014.
A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives. In NIPS, 2014.
B. Delyon. Stochastic Approximation with Decreasing Gain: Convergence and Asymptotic Theory, 2000.
G. Fort. Central limit theorems for stochastic approximation with controlled Markov Chain. EsaimPS, 2014.
J. Furnkranz, E. Hullermeier, and S. Vanderlooy. Binary Decomposition Methods for Multipartite Ranking. In ECML/PKDD, pages 359374, 2009.
A. Herschtal and B. Raskutti. Optimising area under the ROC curve using gradient descent. In ICML, page 49, 2004.
S. Janson. The asymptotic distributions of Incomplete U -statistics. Z. Wahrsch. verw. Gebiete, 66:495 505, 1984.
R. Johnson and T. Zhang. Accelerating Stochastic Gradient Descent using Predictive Variance Reduction. In NIPS, pages 315323, 2013.
P. Kar, B. Sriperumbudur, P. Jain, and H. Karnick. On the Generalization Ability of Online Learning Algorithms for Pairwise Loss Functions. In ICML, 2013.
H. J. Kushner and G. Yin. Stochastic approximation and recursive algorithms and applications, volume 35. Springer Science & Business Media, 2003.
N. Le Roux, M. W. Schmidt, and F. Bach. A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets. In NIPS, 2012.
A. J. Lee. U-Statistics: Theory and Practice. 1990.
J. Mairal. Incremental Majorization-Minimization Optimization with Application to Large-Scale Machine Learning. Technical report, arXiv:1402.4419, 2014.
D. Needell, R. Ward, and N. Srebro. Stochastic Gradient Descent, Weighted Sampling, and the Randomized Kaczmarz algorithm. In NIPS, pages 10171025, 2014.
A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust Stochastic Approximation Approach to Stochastic Programming. SIAM Journal on Optimization, 19(4):15741609, 2009.
Y. Nesterov. Introductory lectures on convex optimization, volume 87. Springer, 2004.
M. Norouzi, D. J. Fleet, and R. Salakhutdinov. Hamming Distance Metric Learning. In NIPS, pages 10701078, 2012.
M. Pelletier. Weak convergence rates for stochastic approximation with application to multiple targets and simulated annealing. Ann. Appl.Prob, 1998.
Q. Qian, R. Jin, J. Yi, L. Zhang, and S. Zhu. Efficient Distance Metric Learning by Adaptive Sampling and Mini-Batch Stochastic Gradient Descent (SGD). Machine Learning, 99(3):353372, 2015.
P. Zhao, S. Hoi, R. Jin, and T. Yang. AUC Maximization. In ICML, pages 233240, 2011.
P. Zhao and T. Zhang. Stochastic Optimization with Importance Sampling for Regularized Loss Minimization. In ICML, 2015.  9
Y. Amsterdamer, S. B. Davidson, T. Milo, S. Novgorodov, and A. Somech. OASSIS: query driven crowd mining. In SIGMOD, pages 589600, 2014.
J.-Y. Audibert, S. Bubeck, et al. Best arm identification in multi-armed bandits. COLT, 2010.
Z. Bar-Yossef. The complexity of massive data set computations. PhD thesis, University of California, 2002.
S. Bubeck, N. Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and trends in machine learning, 5(1):1122, 2012.
S. Bubeck, R. Munos, and G. Stoltz. Pure exploration in finitely-armed and continuous-armed bandits. Theoretical Computer Science, 412(19):18321852, 2011.
S. Chen, T. Lin, I. King, M. R. Lyu, and W. Chen. Combinatorial pure exploration of multiarmed bandits. In Advances in Neural Information Processing Systems, pages 379387, 2014.
D. P. Dubhashi and A. Panconesi. Concentration of measure for the analysis of randomized algorithms. Cambridge University Press, 2009.
E. Even-Dar, S. Mannor, and Y. Mansour. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. The Journal of Machine Learning Research, 7:10791105, 2006.
V. Gabillon, M. Ghavamzadeh, and A. Lazaric. Best arm identification: A unified approach to fixed budget and fixed confidence. In Advances in Neural Information Processing Systems, pages 32123220, 2012.
S. Kalyanakrishnan and P. Stone. Efficient selection of multiple bandit arms: Theory and practice. In ICML, pages 511518, 2010.
S. Kalyanakrishnan, A. Tewari, P. Auer, and P. Stone. PAC subset selection in stochastic multiarmed bandits. In ICML, pages 655662, 2012.
S. Mannor and J. N. Tsitsiklis. The sample complexity of exploration in the multi-armed bandit problem. The Journal of Machine Learning Research, 5:623648, 2004.
A. G. Parameswaran, S. Boyd, H. Garcia-Molina, A. Gupta, N. Polyzotis, and J. Widom. Optimal crowd-powered rating and filtering algorithms. PVLDB, 7(9):685696, 2014.
C. Sheng, Y. Tao, and J. Li. Exact and approximate algorithms for the most connected vertex problem. TODS, 37(2):12, 2012.
J. Wang, E. Lo, and M. L. Yiu. Identifying the most connected vertices in hidden bipartite graphs using group testing. TKDE, 25(10):22452256, 2013.
Y. Zhou, X. Chen, and J. Li. Optimal PAC multiple arm identification with applications to crowdsourcing. In ICML, pages 217225, 2014.
M. Zhu, D. Papadias, J. Zhang, and D. L. Lee. Top-k spatial joins. TKDE, 17(4):567579, 2005.  9
Joshua I Gold and Michael N Shadlen. The neural basis of decision making. Annu Rev Neurosci, 30:535574, 2007.
I. D. John. A statistical decision theory of simple reaction time. Australian Journal of Psychology, 19(1):2734, 1967. 8
R. Duncan Luce. Response Times: Their Role in Inferring Elementary Mental Organization. Number 8 in Oxford Psychology Series. Oxford University Press, 1986.
Abraham Wald. Sequential Analysis. Wiley, New York, 1947.
Xiao-Jing Wang. Probabilistic decision making by slow reverberation in cortical circuits. Neuron, 36(5):955968, Dec 2002.
Rajesh P N Rao. Bayesian computation in recurrent neural circuits. Neural Comput, 16(1):1 38, Jan 2004.
Jeffrey M Beck, Wei Ji Ma, Roozbeh Kiani, Tim Hanks, Anne K Churchland, Jamie Roitman, Michael N Shadlen, Peter E Latham, and Alexandre Pouget. Probabilistic population codes for Bayesian decision making. Neuron, 60(6):11421152, December 2008.
Anne K Churchland, R. Kiani, R. Chaudhuri, Xiao-Jing Wang, Alexandre Pouget, and M. N. Shadlen. Variance as a signature of neural computations during decision making. Neuron, 69(4):818831, Feb 2011.
Rafal Bogacz, Eric Brown, Jeff Moehlis, Philip Holmes, and Jonathan D. Cohen. The physics of optimal decision making: a formal analysis of models of performance in two-alternative forced-choice tasks. Psychol Rev, 113(4):700765, October 2006.
Michael N Shadlen, Roozbeh Kiani, Timothy D Hanks, and Anne K Churchland. Neurobiology of decision making: An intentional framework. In Christoph Engel and Wolf Singer, editors, Better Than Conscious? Decision Making, the Humand Mind, and Implications For Institutions. MIT Press, 2008.
Anne K Churchland, Roozbeh Kiani, and Michael N Shadlen. Decision-making with multiple alternatives. Nat Neurosci, 11(6):693702, Jun 2008.
Peter Dayan and Nathaniel D Daw. Decision theory, reinforcement learning, and the brain. Cogn Affect Behav Neurosci, 8(4):429453, Dec 2008.
Sebastian Bitzer, Hame Park, Felix Blankenburg, and Stefan J Kiebel. Perceptual decision making: Drift-diffusion model is equivalent to a bayesian model. Frontiers in Human Neuroscience, 8(102), 2014.
W. T. Newsome and E. B. Pare. A selective impairment of motion perception following lesions of the middle temporal visual area MT. J Neurosci, 8(6):22012211, June 1988.
Praveen K. Pilly and Aaron R. Seitz. What a difference a parameter makes: a psychophysical comparison of random dot motion algorithms. Vision Res, 49(13):15991612, Jun 2009.
Angela J. Yu and Peter Dayan. Inference, attention, and decision in a Bayesian neural architecture. In Lawrence K. Saul, Yair Weiss, and Leon Bottou, editors, Advances in Neural Information Processing Systems 17, pages 15771584. MIT Press, Cambridge, MA, 2005.
Alec Solway and Matthew M. Botvinick. Goal-directed decision making as probabilistic inference: a computational framework and potential neural correlates. Psychol Rev, 119(1):120 154, January 2012.
Yanping Huang, Abram Friesen, Timothy Hanks, Mike Shadlen, and Rajesh Rao. How prior probability influences decision making: A unifying probabilistic model. In P. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 12771285. 2012.
Jamie D Roitman and Michael N Shadlen. Response of neurons in the lateral intraparietal area during a combined visual discrimination reaction time task. J Neurosci, 22(21):94759489, Nov 2002.
Timothy D. Hanks, Charles D. Kopec, Bingni W. Brunton, Chunyu A. Duan, Jeffrey C. Erlich, and Carlos D. Brody. Distinct relationships of parietal and prefrontal cortices to evidence accumulation. Nature, Jan 2015.
Sophie Deneve. Making decisions with unknown sensory reliability. Front Neurosci, 6:75, 2012.  9
Arindam Banerjee, Sheng Chen, Farideh Fazayeli, and Vidyashankar Sivakumar. Estimation with norm regularization. In Advances in Neural Information Processing Systems, pages 15561564, 2014.
Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classification, and risk bounds. Journal of the American Statistical Association, 101(473):138156, 2006.
Peter J Bickel and Elizaveta Levina. Some theory for Fishers linear discriminant function, naive Bayes, and some alternatives when there are many more variables than observations. Bernoulli, pages 9891010, 2004.
Peter J Bickel and Elizaveta Levina. Covariance regularization by thresholding. The Annals of Statistics, pages 25772604, 2008.
C.M. Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics. Springer, 2006. ISBN 9780387310732.
Peter Buhlmann and Sara Van De Geer. Statistics for high-dimensional data: methods, theory and applications. Springer Science & Business Media, 2011.
Tony Cai and Weidong Liu. A direct estimation approach to sparse linear discriminant analysis. Journal of the American Statistical Association, 106(496), 2011.
Emmanuel Candes and Terence Tao. The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, pages 23132351, 2007.
Venkat Chandrasekaran, Benjamin Recht, Pablo A Parrilo, and Alan S Willsky. The convex geometry of linear inverse problems. Foundations of Computational Mathematics, 12(6):805849, 2012.  8
Weizhu Chen, Zhenghao Wang, and Jingren Zhou. Large-scale L-BFGS using MapReduce. In Advances in Neural Information Processing Systems, pages 13321340, 2014.
L. Devroye, L. Gyorfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer New York, 1996.
Luc Devroye. A probabilistic theory of pattern recognition, volume 31. Springer Science & Business Media, 1996.
David Donoho and Jiashun Jin. Higher criticism thresholding: Optimal feature selection when useful features are rare and weak. Proceedings of the National Academy of Sciences, 105(39):1479014795, 2008.
Jianqing Fan and Yingying Fan. High dimensional classification using features annealed independence rules. Annals of statistics, 36(6):2605, 2008.
Jianqing Fan, Yang Feng, and Xin Tong. A road to classification in high dimensional space: the regularized optimal affine discriminant. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74(4):745771, 2012.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. LIBLINEAR: A library for large linear classification. The Journal of Machine Learning Research, 9:18711874, 2008.
Yingying Fan, Jiashun Jin, Zhigang Yao, et al. Optimal classification in sparse gaussian graphic model. The Annals of Statistics, 41(5):25372571, 2013.
Manuel Fernandez-Delgado, Eva Cernadas, Senen Barro, and Dinani Amorim. Do we need hundreds of classifiers to solve real world classification problems? The Journal of Machine Learning Research, 15 (1):31333181, 2014.
Jerome Friedman, Trevor Hastie, and Rob Tibshirani. Regularization paths for generalized linear models via coordinate descent. Journal of statistical software, 33(1):1, 2010.
Siddharth Gopal and Yiming Yang. Distributed training of Large-scale Logistic models. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 289297, 2013.
T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer, 2009.
Mladen Kolar and Han Liu. Feature selection in high-dimensional classification. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 329337, 2013.
Vladimir Koltchinskii. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems: Ecole dEte de Probabilites de Saint-Flour XXXVIII-2008, volume 2033. Springer Science & Business Media, 2011.
Qing Mai, Hui Zou, and Ming Yuan. A direct approach to sparse discriminant analysis in ultra-high dimensions. Biometrika, page asr066, 2012.
Enno Mammen, Alexandre B Tsybakov, et al. Smooth discrimination analysis. The Annals of Statistics, 27(6):18081829, 1999.
Sahand Negahban, Bin Yu, Martin J Wainwright, and Pradeep K Ravikumar. A unified framework for high-dimensional analysis of M -estimators with decomposable regularizers. In Advances in Neural Information Processing Systems, pages 13481356, 2009.
Andrew Y. Ng and Michael I. Jordan. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. In Advances in Neural Information Processing Systems 14 (NIPS 2001), 2001.
Jun Shao, Yazhen Wang, Xinwei Deng, Sijian Wang, et al. Sparse linear discriminant analysis by thresholding for high dimensional data. The Annals of Statistics, 39(2):12411265, 2011.
Robert Tibshirani, Trevor Hastie, Balasubramanian Narasimhan, and Gilbert Chu. Diagnosis of multiple cancer types by shrunken centroids of gene expression. Proceedings of the National Academy of Sciences, 99(10):65676572, 2002.
Sijian Wang and Ji Zhu. Improved centroids estimation for the nearest shrunken centroid classifier. Bioinformatics, 23(8):972979, 2007.
Tong Zhang. Statistical behavior and consistency of classification methods based on convex risk minimization. Annals of Statistics, pages 5685, 2004.  9
P. K. Agarwal and J. M. Phillips. An efficient algorithm for 2d euclidean 2-center with outliers. In ESA, pages 6475, 2008.
C. C. Aggarwal, J. L. Wolf, and P. S Yu. Method for targeted advertising on the web based on accumulated self-learning data, clustering users and semantic node graph techniques, March 30 2004. US Patent 6,714,975.
N. Ailon, R. Jaiswal, and C. Monteleoni. Streaming k-means approximation. In NIPS, pages 1018, 2009.  8
A. Andoni, A. Nikolov, K. Onak, and G. Yaroslavtsev. Parallel algorithms for geometric graph problems. In STOC, pages 574583, 2014.
B. Bahmani, R. Kumar, and S. Vassilvitskii. Densest subgraph in streaming and mapreduce. PVLDB, 5 (5):454465, 2012.
Bahman Bahmani, Benjamin Moseley, Andrea Vattani, Ravi Kumar, and Sergei Vassilvitskii. Scalable k-means++. PVLDB, 5(7):622633, 2012.
M. Balcan, S. Ehrlich, and Y. Liang. Distributed k-means and k-median clustering on general communication topologies. In NIPS, pages 19952003, 2013.
Rafael Barbosa, Alina Ene, Huy Nguyen, and Justin Ward. The power of randomization: Distributed submodular maximization on massive datasets. In ICML, pages 12361244, 2015.
A. Z. Broder, L. G. Pueyo, V. Josifovski, S. Vassilvitskii, and S. Venkatesan. Scalable k-means by ranked retrieval. In WSDM, pages 233242, 2014.
M. Charikar, S. Khuller, D. M. Mount, and G. Narasimhan. Algorithms for facility location problems with outliers. In SODA, pages 642651, 2001.
M. Chen, K. Q. Weinberger, O. Chapelle, D. Kedem, and Z. Xu. Classifier cascade for minimizing feature evaluation cost. In AIStats, pages 218226, 2012.
F. Chierichetti, R. Kumar, and A. Tomkins. Max-cover in map-reduce. In WWW, pages 231240, 2010.
J. Dean and S. Ghemawat. MapReduce: Simplified data processing on large clusters. In OSDI, pages 137150, 2004.
A. Ene, S. Im, and B. Moseley. Fast clustering using MapReduce. In KDD, pages 681689, 2011.
J. Feldman, S. Muthukrishnan, A. Sidiropoulos, C. Stein, and Z. Svitkina. On distributing symmetric streaming computations. In SODA, pages 710719, 2008.
T. F. Gonzalez. Clustering to minimize the maximum intercluster distance. Theoretical Computer Science, 38(0):293  306, 1985. ISSN 0304-3975.
S. Guha, A. Meyerson, N. Mishra, R. Motwani, and L. OCallaghan. Clustering data streams: Theory and practice. IEEE Trans. Knowl. Data Eng., 15(3):515528, 2003.
Sudipto Guha, Rajeev Rastogi, and Kyuseok Shim. Techniques for clustering massive data sets. In Clustering and Information Retrieval, volume 11 of Network Theory and Applications, pages 3582. Springer US, 2004. ISBN 978-1-4613-7949-2.
M. Hassani, E. Muller, and T. Seidl. EDISKCO: energy efficient distributed in-sensor-network k-center clustering with outliers. In SensorKDD-Workshop, pages 3948, 2009.
D. S. Hochbaum and D. B. Shmoys. A best possible heuristic for the k-center problem. Mathematics of Operations Research, 10(2):180184, 1985.
H. J. Karloff, S. Suri, and S. Vassilvitskii. A model of computation for MapReduce. In SODA, pages 938948, 2010.
L. Kaufman and P. J. Rousseeuw. Finding Groups in Data: An Introduction to Cluster Analysis. WileyInterscience, 9th edition, March 1990. ISBN 0471878766.
Ravi Kumar, Benjamin Moseley, Sergei Vassilvitskii, and Andrea Vattani. Fast greedy algorithms in mapreduce and streaming. In SPAA, pages 110, 2013.
R. M. McCutchen and S. Khuller. Streaming algorithms for k-center clustering with outliers and with anonymity. In APPROX-RANDOM, pages 165178, 2008.
B. Mirzasoleiman, A. Karbasi, R. Sarkar, and A. Krause. Distributed submodular maximization: Identifying representative elements in massive data. In NIPS, pages 20492057, 2013.
M. Shindler, A. Wong, and A. W. Meyerson. Fast and accurate k-means for large datasets. In NIPS, pages 23752383, 2011.
S. Suri and S. Vassilvitskii. Counting triangles and the curse of the last reducer. In WWW, pages 607614, 2011.
A. Tsanas, M. A Little, P. E McSharry, and L. O Ramig. Enhanced classical dysphonia measures and sparse regression for telemonitoring of parkinsons disease progression. In ICASSP, pages 594597. IEEE, 2010.
S. Tyree, K.Q. Weinberger, K. Agrawal, and J. Paykin. Parallel boosted regression trees for web search ranking. In WWW, pages 387396. ACM, 2011.
O. Zamir, O. Etzioni, O. Madani, and R. M Karp. Fast and intuitive clustering of web documents. In KDD, volume 97, pages 287290, 1997.
Z. Zhao, G. Wang, A.R. Butt, M. Khan, V.S.A. Kumar, and M.V. Marathe. Sahad: Subgraph analysis in massive networks using hadoop. In IPDPS, pages 390401, May 2012.  9
J. T. Abbott, J. L. Austerweil, and T. L. Griffiths, Human memory search as a random walk in a semantic network, in NIPS, 2012, pp. 30503058.
B. D. Abrahao, F. Chierichetti, R. Kleinberg, and A. Panconesi, Trace complexity of network inference. CoRR, vol. abs/1308.2954, 2013.
L. Bottou, Stochastic gradient tricks, in Neural Networks, Tricks of the Trade, Reloaded, ser. Lecture Notes in Computer Science (LNCS 7700), G. Montavon, G. B. Orr, and K.-R. Muller, Eds. Springer, 2012, pp. 430445.
A. Z. Broder, Generating random spanning trees, in FOCS. IEEE Computer Society, 1989, pp. 442447.
A. S. Chan, N. Butters, J. S. Paulsen, D. P. Salmon, M. R. Swenson, and L. T. Maloney, An assessment of the semantic network in patients with alzheimers disease. Journal of Cognitive Neuroscience, vol. 5, no. 2, pp. 254261, 1993.
J. R. Cockrell and M. F. Folstein, Mini-mental state examination. Principles and practice of geriatric psychiatry, pp. 140141, 2002.
P. G. Doyle and J. L. Snell, Random Walks and Electric Networks. matical Association of America, 1984.  Washington, DC: Mathe-
R. Durrett, Essentials of stochastic processes, 2nd ed., ser. Springer texts in statistics. York: Springer, 2012.
P. Flory, Principles of polymer chemistry.  New  Cornell University Press, 1953.
A. M. Glenberg and S. Mehta, Optimal foraging in semantic memory, Italian Journal of Linguistics, 2009.
M. Gomez Rodriguez, J. Leskovec, and A. Krause, Inferring networks of diffusion and influence, Max-Planck-Gesellschaft. New York, NY, USA: ACM Press, July 2010, pp. 1019 1028.
J. Goi, G. Arrondo, J. Sepulcre, I. Martincorena, N. V. de Mendizbal, B. Corominas-Murtra, B. Bejarano, S. Ardanza-Trevijano, H. Peraita, D. P. Wall, and P. Villoslada, The semantic organization of the animal category: evidence from semantic verbal fluency and network theory. Cognitive Processing, vol. 12, no. 2, pp. 183196, 2011.
T. L. Griffiths, M. Steyvers, and A. Firl, Google and the mind: Predicting fluency with pagerank, Psychological Science, vol. 18, no. 12, pp. 10691076, 2007.
N. M. Henley, A psychological study of the semantics of animal terms. Journal of Verbal Learning and Verbal Behavior, vol. 8, no. 2, pp. 176184, Apr. 1969.
T. T. Hills, P. M. Todd, and M. N. Jones, Optimal foraging in semantic memory, Psychological Review, pp. 431440, 2012.
D. Kempe, J. Kleinberg, and E. Tardos, Maximizing the spread of influence through a social network, in Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD 03. New York, NY, USA: ACM, 2003, pp. 137146.
F. Pasquier, F. Lebert, L. Grymonprez, and H. Petit, Verbal fluency in dementia of frontal lobe type and dementia of Alzheimer type. Journal of Neurology, vol. 58, no. 1, pp. 8184, 1995.
B. T. Polyak and A. B. Juditsky, Acceleration of stochastic approximation by averaging, SIAM J. Control Optim., vol. 30, no. 4, pp. 838855, July 1992.
T. T. Rogers, A. Ivanoiu, K. Patterson, and J. R. Hodges, Semantic memory in Alzheimers disease and the frontotemporal dementias: a longitudinal study of 236 patients. Neuropsychology, vol. 20, no. 3, pp. 319335, 2006.
D. Ruppert, Efficient estimations from a slowly convergent robbins-monro process, Cornell University Operations Research and Industrial Engineering, Tech. Rep., 1988.
A. Troyer, M. Moscovitch, G. Winocur, M. Alexander, and D. Stuss, Clustering and switching on verbal fluency: The effects of focal fronal- and temporal-lobe lesions, Neuropsychologia, vol. 36, no. 6, 1998.  9
S. Rendle and L. Schmidt-Thieme. Pairwise interaction tensor factorization for personalized tag recommendation. In International Conference on Web Search and Data Mining, 2010.
G.I. Allen. Sparse higher-order principal components analysis. In International Conference on Artificial Intelligence and Statistics, 2012.
J. Zahn, S. Poosala, A. Owen, D. Ingram, et al. AGEMAP: A gene expression database for aging in mice. PLOS Genetics, 3:23262337, 2007.
T. Cai, W. Liu, and H.H. Zhou. Estimating sparse precision matrix: Optimal rates of convergence and adaptive estimation. Annals of Statistics, 2015.
C. Leng and C.Y. Tang. Sparse matrix graphical models. Journal of the American Statistical Association, 107:11871200, 2012.
J. Yin and H. Li. Model selection and estimation in the matrix normal graphical model. Journal of Multivariate Analysis, 107:119140, 2012.
T. Tsiligkaridis, A. O. Hero, and S. Zhou. On convergence of Kronecker graphical Lasso algorithms. IEEE Transactions on Signal Processing, 61:17431755, 2013.
S. Zhou. Gemini: Graph estimation with matrix variate normal instances. Annals of Statistics, 42:532562, 2014.
S. He, J. Yin, H. Li, and X. Wang. Graphical model selection and estimation for high dimensional tensor data. Journal of Multivariate Analysis, 128:165185, 2014.
P. Jain, P. Netrapalli, and S. Sanghavi. Low-rank matrix completion using alternating minimization. In Symposium on Theory of Computing, pages 665674, 2013.
P. Netrapalli, P. Jain, and S. Sanghavi. Phase retrieval using alternating minimization. In Advances in Neural Information Processing Systems, pages 27962804, 2013.
J. Sun, Q. Qu, and J. Wright. Complete dictionary recovery over the sphere. arXiv:1504.06785, 2015.
S. Arora, R. Ge, T. Ma, and A. Moitra. Simple, efficient, and neural algorithms for sparse coding. arXiv:1503.00778, 2015.
A. Anandkumar, R. Ge, D. Hsu, S. Kakade, and M. Telgarsky. Tensor decompositions for learning latent variable models. Journal of Machine Learning Research, 15:27732832, 2014.
W. Sun, J. Lu, H. Liu, and G. Cheng. Provable sparse tensor decomposition. arXiv:1502.01425, 2015.
S. Zhe, Z. Xu, X. Chu, Y. Qi, and Y. Park. Scalable nonparametric multiway data analysis. In International Conference on Artificial Intelligence and Statistics, 2015.
S. Zhe, Z. Xu, Y. Qi, and P. Yu. Sparse bayesian multiview learning for simultaneous association discovery and diagnosis of alzheimers disease. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.
T. Kolda and B. Bader. Tensor decompositions and applications. SIAM Review, 51:455500, 2009.
R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society, Series B, 58:267288, 1996.
M. Yuan and Y. Lin. Model selection and estimation in the gaussian graphical model. Biometrika, 94:1935, 2007.
J. Friedman, H. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graphical Lasso. Biostatistics, 9:432441, 2008.
A. J. Rothman, P. J. Bickel, E. Levina, and J. Zhu. Sparse permutation invariant covariance estimation. Electronic Journal of Statistics, 2:494515, 2008.
W. Sun, J. Wang, and Y. Fang. Consistent selection of tuning parameters via variable selection stability. Journal of Machine Learning Research, 14:34193440, 2013.
M. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes. Springer, 2011.
J. Fan, Y. Feng, and Y. Wu. Network exploration via the adaptive Lasso and scad penalties. Annals of Statistics, 3:521541, 2009.
P. Zhao and B. Yu. On model selection consistency of Lasso. Journal of Machine Learning Research, 7:25412567, 2006.
P. Ravikumar, M.J. Wainwright, G. Raskutti, and B. Yu. High-dimensional covariance estimation by minimizing `1 -penalized log-determinant divergence. Electronic Journal of Statistics, 5:935980, 2011.
Z. Wang, H. Liu, and T. Zhang. Optimal computational and statistical rates of convergence for sparse nonconvex learning problems. Annals of Statistics, 42:21642201, 2014.
T. Zhao, H. Liu, K. Roeder, J. Lafferty, and L. Wasserman. The huge package for high-dimensional undirected graph estimation in R. Journal of Machine Learning Research, 13:10591062, 2012.
A. Gupta and D. Nagar. Matrix variate distributions. Chapman and Hall/CRC Press, 2000.
P. Hoff. Separable covariance arrays via the Tucker product, with applications to multivariate relational data. Bayesian Analysis, 6:179196, 2011.
A.P. Dawid. Some matrix-variate distribution theory: Notational considerations and a bayesian application. Biometrika, 68:265274, 1981.
S. Negahban and M.J. Wainwright. Estimation of (near) low-rank matrices with noise and high-dimensional scaling. Annals of Statistics, 39:10691097, 2011.  9
A. Agarwal. Selective sampling algorithms for cost-sensitive multiclass prediction. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pages 12201228, 2013.
M.-F. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. J. Comput. Syst. Sci., 75(1):7889, 2009.
M.-F. Balcan and P. M. Long. Active and passive learning of linear separators under logconcave distributions. In COLT, 2013.
A. Beygelzimer, D. Hsu, J. Langford, and T. Zhang. Agnostic active learning without constraints. In NIPS, 2010.
L. Cam and G. Yang. Asymptotics in Statistics: Some Basic Concepts. Springer Series in Statistics. Springer New York, 2000.
J. Cornell. Experiments with Mixtures: Designs, Models, and the Analysis of Mixture Data (third ed.). Wiley, 2002.
S. Dasgupta. Coarse sample complexity bounds for active learning. In NIPS, 2005.
S. Dasgupta. Two faces of active learning. Theor. Comput. Sci., 412(19), 2011.
S. Dasgupta and D. Hsu. Hierarchical sampling for active learning. In ICML, 2008.
S. Dasgupta, D. Hsu, and C. Monteleoni. A general agnostic active learning algorithm. In NIPS, 2007.
R. Frostig, R. Ge, S. M. Kakade, and A. Sidford. Competing with the empirical risk minimizer in a single pass. arXiv preprint arXiv:1412.6606, 2014.
Q. Gu, T. Zhang, C. Ding, and J. Han. Selective labeling via error bound minimization. In In Proc. of Advances in Neural Information Processing Systems (NIPS) 25, Lake Tahoe, Nevada, United States, 2012.
Q. Gu, T. Zhang, and J. Han. Batch-mode active learning via error bound minimization. In 30th Conference on Uncertainty in Artificial Intelligence (UAI), 2014.
S. Hanneke. A bound on the label complexity of agnostic active learning. In ICML, 2007.
M. Kaariainen. Active learning in the non-realizable case. In ALT, 2006.
L. Le Cam. Asymptotic Methods in Statistical Decision Theory. Springer, 1986.
E. L. Lehmann and G. Casella. Theory of point estimation, volume 31. Springer Science & Business Media, 1998.
R. D. Nowak. The geometry of generalized binary search. IEEE Transactions on Information Theory, 57(12):78937906, 2011.
S. Sabato and R. Munos. Active regression through stratification. In NIPS, 2014.
R. Urner, S. Wulff, and S. Ben-David. Plal: Cluster-based active learning. In COLT, 2013.
A. W. van der Vaart. Asymptotic Statistics. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2000.
C. Zhang and K. Chaudhuri. Beyond disagreement-based agnostic active learning. In Proc. of Neural Information Processing Systems, 2014.  9
M. Aubry and B. C. Russell. Understanding deep features with computer-generated imagery. In ICCV, 2015.
M. Aubry, D. Maturana, A. A. Efros, B. Russell, and J. Sivic. Seeing 3D chairs: exemplar part-based 2D-3D alignment using a large dataset of CAD models. In CVPR, 2014.
J. Ba and D. Kingma. Adam: A method for stochastic optimization. In ICLR, 2015.
Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In ICML, 2009.
V. Blanz and T. Vetter. A morphable model for the synthesis of 3D faces. In SIGGRAPH, 1999.
B. Cheung, J. Livezey, A. Bansal, and B. Olshausen. Discovering hidden factors of variation in deep networks. In ICLR, 2015.
W. Ding and G. Taylor. Mental rotation by optimizing transforming distance. In NIPS Deep Learning and Representation Learning Workshop, 2014.
A. Dosovitskiy, J. Springenberg, and T. Brox. Learning to generate chairs with convolutional neural networks. In CVPR, 2015.
J. Flynn, I. Neulander, J. Philbin, and N. Snavely. Deepstereo: Learning to predict new views from the worlds imagery. arXiv preprint arXiv:1506.06825, 2015.
R. Girshick. Fast R-CNN. In ICCV, 2015.
R. Gross, I. Matthews, J. Cohn, T. Kanade, and S. Baker. Multi-PIE. Image and Vision Computing, 28 (5):807813, May 2010.
G. E. Hinton, A. Krizhevsky, and S. D. Wang. Transforming auto-encoders. In ICANN, 2011.
Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.
N. Kholgade, T. Simon, A. Efros, and Y. Sheikh. 3D object manipulation in a single photograph using stock 3D models. In SIGGRAPH, 2014.
D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In ICLR, 2014.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.
T. D. Kulkarni, W. Whitney, P. Kohli, and J. B. Tenenbaum. Deep convolutional inverse graphics network. In NIPS, 2015.
J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.
V. Michalski, R. Memisevic, and K. Konda. Modeling deep temporal dependencies with recurrent grammar cells. In NIPS, 2014.
V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. Playing atari with deep reinforcement learning. In NIPS Deep Learning Workshop, 2013.
J. Oh, X. Guo, H. Lee, R. Lewis, and S. Singh. Action-conditional video prediction using deep networks in atari games. In NIPS, 2015.
S. Reed, K. Sohn, Y. Zhang, and H. Lee. Learning to disentangle factors of variation with manifold interaction. In ICML, 2014.
R. N. Shepard and J. Metzler. Mental rotation of three dimensional objects. Science, 171(3972):701703, 1971.
K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.
J. B. Tenenbaum and W. T. Freeman. Separating style and content with bilinear models. Neural Computation, 12(6):12471283, 2000.
T. Tieleman. Optimizing neural networks that generate images. PhD thesis, University of Toronto, 2014.
O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. In CVPR, 2015.
W. Zaremba and I. Sutskever. Learning to execute. arXiv preprint arXiv:1410.4615, 2014.
X. Zhu, Z. Lei, J. Yan, D. Yi, and S. Z. Li. High-fidelity pose and expression normalization for face recognition in the wild. In CVPR, 2015.
Z. Zhu, P. Luo, X. Wang, and X. Tang. Multi-view perceptron: a deep model for learning face identity and view representations. In NIPS, 2014.  9
:  propagating h a The corresponding gradient contributions on parameters (weights and biases), collected along the way, are straightforward once we have the associated a(k) . Specifically they are b(k) = a(k) and W (k) = h(k1) (a(k) )T . Similarly for the input layer W (1) = x(a(1) )T , and for the output layer W = (o  y)hT . Parameters are then updated through a gradient descent step W (k)  W (k)  W (k) and b(k)  b(k)  b(k) , where  is a positive learning-rate. Similarly for the output layer which will be our main focus here: W  W  W . 2.2  The easy part: input layer forward propagation and weight update  It is easy and straightforward to efficiently compute the forward propagation, and the backpropagation and weight update part for the input layer when we have a very large Din -dimensional but Ksparse input vector x with appropriate sparse representation. Specifically we suppose that x is represented as a pair of vectors u, v of length (at most) K, where u contains integer indexes and v the associated real values of the elements of x such that xi = 0 if i  / u, and xuk = vk .   Forward propagation through the input layer: The sparse representation of x as the positions of K elements together with their value makes it cheap to compute W (1)T x. Even though W (1) may be a huge full Din  d matrix, only K of its rows (those corresponding to the non-zero entries of x) need to be visited and summed to compute W (1)T x. Precisely, with our (u, v) sparse PK (1) (1) representation of x this operation can be written as W (1)T x = k=1 vk W:uk where each W:uk is a d-dimensional vector, making this an O(Kd) operation rather than O(Dd).  Gradient and update through input layer: Let us for now suppose that we were able to get gradients (through backpropagation) up to the first hidden layer activations a(1)  Rd in the form of gradient vector a(1) = aL (1) . The corresponding gradient-based update to input layer weights W (1) is simply W (1)  W (1)  x(a(1) )T . This is a rank-one update to W (1) . Here again, we see that only the K rows of W (1) associated to the (at most) K non-zero entries of x need to be (1) (1) modified. Precisely this operation can be written as: W:uk  W:uk vk a(1) k  {1, . . . , K} making this again a O(Kd) operation rather than O(Dd). 3
M  2.3  The hard part: output layer propagation and weight update  Given some network input x, we suppose we can compute without difficulty through forward propagation the associated last hidden layer representation h  Rd . From then on:   Computing the final output o = W h incurs a prohibitive computational cost of O(Dd) since W is a full D  d matrix. Note that there is a-priori no reason for representation h to be sparse (e.g. with a sigmoid non-linearity) but even if it was, this would not fundamentally change the problem since it is D that is extremely large, and we supposed d reasonably sized already. Computing the residual (o  y) and associated squared error loss ko  yk2 incurs an additional O(D) cost. T  The gradient on h that we need to backpropagate to lower layers is h = L h = 2W (o  y) which is another O(Dd) matrix-vector product.  Finally, when performing the corresponding output weight update W  W  (o  y)hT we see that it is a rank-one update that updates all D  d elements of W , which again incurs a prohibitive O(Dd) computational cost.  For very large D, all these three O(Dd) operations are prohibitive, and the fact that y is sparse, seen from this perspective, doesnt help, since neither o nor o  y will be sparse.  3  A computationally efficient algorithm for performing the exact online gradient update  Previously proposed workarounds are approximate or use stochastic sampling. We propose a different approach that results in the exact same, yet efficient gradient update, remarkably without ever having to compute large output o. 3.1  Computing the squared error loss L and the gradient with respect to h efficiently  Suppose that, we have, for a network input example x, computed the last hidden representation h  Rd through forward propagation. The networks D dimensional output o = W h is then in principle compared to the high dimensional target y  RD . The corresponding squared error loss 2 is L = kW h  yk . As we saw in Section 2.3, computing it in the direct naive way would have a prohibitive computational complexity of O(Dd + D) = O(Dd) because computing output W h with a full D  d matrix W and a typically non-sparse h is O(Dd). Similarly, to backpropagate the gradient through the network, we need to compute the gradient of loss L with respect to last kW hyk2 hidden layer representation h. This is h = L = 2W T (W h  y). So again, if h = h we were to compute it directly in this manner, the computational complexity would be a prohibitive O(Dd). Provided we have maintained an up-to-date matrix Q = W T W , which is of reasonable size d  d and can be cheaply maintained as we will see in Section 3.3, we can rewrite these two operations so as to perform them in O(d2 ): Loss computation:  Gradient on h:  O(Dd)  L = =  z}|{ k W h yk2  h =  T  (W h  y) (W h  y)  = hT W T W h  y T W h  hT W T y + y T y = hT Qh  2hT (W T y) + y T y = hT ( Qh 2 W T y ) + y T y |{z} | {z } |{z} O(d2 )  O(Kd)  (1)  L h  = = = =  kW h  yk2 h 2W T (W h  y)  2 WTWh  WTy 2( Qh  W T y ) |{z} | {z } O(d2 )  O(K)    (2)  O(Kd)  The terms in O(Kd) and O(K) are due to leveraging the K-sparse representation of target vector y. With K  D and d  D, we get altogether a computational cost of O(d2 ) which can be several orders of magnitude cheaper than the prohibitive O(Dd) of the direct approach. 4  3.2  Efficient gradient update of W  The gradient of the squared error loss with respect to output layer weight matrix W is kW hyk2 W  L W  =  = 2(W h  y)h . And the corresponding gradient descent update to W would be Wnew  W  2(W h  y)hT , where  is a positive learning rate. Again, computed in this manner, this induces a prohibitive O(Dd) computational complexity, both to compute output and residual W h  y, and then to update all the Dd elements of W (since generally neither W h  y nor h will be sparse). All D  d elements of W must be accessed during this update. On the surface this seems hopeless. But we will now see how we can achieve the exact same update on W in O(d2 ). The trick is to represent W implicitly as the factorization |{z} W = |{z} V |{z} U and update U and V instead T  Dd  a) Unew  b) Vnew  Dd dd  = U  2(U h)hT = V +  (3)  T 2y(Unew h)T  (4)  This results in implicitly updating W as we did explicitly in the naive approach as we now prove:  Vnew Unew  T (V + 2y(Unew h)T ) Unew  =  T = V Unew + 2y(Unew h)T Unew 1 = V Unew + 2yhT Unew Unew 1 = V (U  2(U h)hT ) + 2yhT (Unew Unew )  = V U  2V U hhT + 2yhT = V U  2(V U h  y)hT = W  2(W h  y)T hT = Wnew  We see that the update of U in Eq. 3 is a simple O(d2 ) operation. Following this simple rank-one update to U , we can use the Sherman-Morrison formula to derive the corresponding rank-one update to U T which will also be O(d2 ): T Unew  =  U T +  2  2 (U  1  2 khk  T  h)hT  (5)  T h, an O(d2 ) operation needed in Eq. 4. The ensuing rank-one It is then easy to compute the Unew update of V in Eq 4, thanks to the K-sparsity of y is only O(Kd): only the K rows V associated to non-zero elements in y are accessed and updated, instead of all D rows of W we had to modify in the naive update! Note that with the factored representation of W as V U , we only have W implicitly, so the W T y terms that entered in the computation of L and h in the previous paragraph need to be adapted slightly as y = W T y = U T (V T y), which becomes O(d2 + Kd) rather than O(Kd) in computational complexity. But this doesnt change the overall O(d2 ) complexity of these computations.  3.3  Bookkeeping: keeping an up-to-date Q and U T  We have already seen, in Eq. 5, how we can cheaply maintain an up-to-date U T following our update of U . Similarly, following our updates to U and V , we need to keep an up-to-date Q = W T W which is needed to efficiently compute the loss L (Eq. 1) and gradient h (Eq. 2). We have shown that updates to U and V in equations 3 and 4 are equivalent to implicitly updating W as Wnew  W  2(W h  y)hT , and this translates into the following update to Q = W T W : z  =  Qnew  =  Qh  U T (V T y)   Q  2 hz T + zhT + (4 2 L)hhT  (6)  The proof is straightforward but due to space constraints we put it in supplementary material. One can see that this last bookkeeping operation also has a O(d2 ) computational complexity. 5  3.4  Putting it all together: detailed algorithm and expected benefits  We have seen that we can efficiently compute cost L, gradient with respect to h (to be later backpropagated further) as well as updating U and V and performing the bookkeeping for U T and Q. Algorithm 1 describes the detailed algorithmic steps that we put together from the equations derived above. Having K  d  D we see that the proposed algorithm requires O(d2 ) operations, whereas the standard approach required O(Dd) operations. If we take K  d , we may state more precisely that the proposed algorithm, for computing the loss and the gradient updates will require roughly 12d2 operations whereas the standard approach required roughly 3Dd operations. So overD all the proposed algorithm change corresponds to a computational speedup by a factor of 4d . For D = 200 000 and d = 500 the expected speedup is thus 100. Note that the advantage is not only in computational complexity, but also in memory access. For each example, the standard approach needs to access and change all D  d elements of matrix W , whereas the proposed approach only accesses the much smaller number K  d elements of V as well as the three d  d matrices U , U T , and Q. So overall we have a substantially faster algorithm, which, while doing so implicitly, will nevertheless perform the exact same gradient update as the standard approach. We want to emphasize here that our approach is completely different from simply chaining 2 linear layers U and V and performing ordinary gradient descent updates on them: this would result in the same prohibitive computational complexity as the standard approach, and such ordinary separate gradient updates to U and V would not be equivalent to the ordinary gradient update to W = V U . Algorithm 1 Efficient computation of cost L, gradient on h, and update to parameters U and V Step Operation Computational Number of # complexity multiply-adds 1: 2: 3: 4: 5: 6: 7: 8: 9:  3.5  h = Qh y = U T (V T y) z = h  y h = 2z L = hT h  2hT y + y T y Unew = U  2(U h)hT T = Unew 2 T U T + 12khk h)hT 2 (U T Vnew = V + 2y(Unew h)T Qnew =  Q  2 hz T + zhT + (4 2 L)hhT Altogether:  O(d2 ) O(Kd + d2 ) O(d) O(d) O(2d + K) O(d2 ) O(d2 )  d2 Kd + d2 d d 2d + K + 1 2d2 + d 2 2d + 2d + 3  O(d2 + Kd) O(d2 )  d2 + K + Kd 4 + 2d + 3d2  O(d2 ) provided K<dD   12d2 elementary operations  Controlling numerical stability and extension to the minibatch case  The update of U in Equation 3 may over time lead U to become ill-conditioned. To prevent this, we regularly (every 100 updates) monitor its conditioning number. If either the smallest or largest singular value moves outside an acceptable range2 , we bring it back to 1 by doing an appropriate rank-1 update to V (which costs Dd operations, but is only done rarely). Our algorithm can also be straightforwardly extended to the minibatch case (the derivations are given in the supplementary material section) and yields the same theoretical speedup factor with respect to the standard naive approach. But one needs to be careful in order to keep the computation of U T h reasonably efficient: depending on the size of the minibatch m, it may be more efficient to solve the corresponding linear equation for each minibatch from scratch rather than updating U T with the Woodbury equation (which generalizes the Sherman-Morrison formula for m > 1). 2  More details on our numerical stabilization procedure can be found in the supplementary material  6  3.6  Generalization to a broader class of loss functions  The approach that we just detailed for linear output and squared error can be extended to a broader, though restricted, family of loss functions. We call it the spherical family of loss functions because it includes the spherical alternative to the softmax, thus named in
. Basically it contains any loss 2 function P 2 that can be expressed as a function of only the oc associated to non-zero yc and of kok = j oj the squared norm of the whole output vector, which we can compute cheaply, irrespective of D, as we did above3 . This family does not include the standard softmax loss log o2 + P c 2 j (oj +)  Pexp(oc ) , j exp(oj )  but it  does include the spherical softmax4 : log . Due to space constraints we will not detail this extension here, only give a sketch of how it can be obtained. Deriving it may not appear obvious at first, but it is relatively straightforward once we realize that: a) the gain in computing the squared error loss comes from being able to very cheaply compute the sum of squared activations kok2 (a scalar quantity), and will thus apply equally well to other losses that can be expressed based on that quantity (like the spherical softmax). b) generalizing our gradient update trick to such losses follows naturally from gradient backpropagation: the gradient is first backpropagated from the final loss to the scalar sum of squared activations, and from there on follows the same path and update procedure as for the squared error loss.  4  Experimental validation  We implemented both a CPU version using blas and a parallel GPU (Cuda) version using cublas of the proposed algorithm5 . We evaluated the GPU and CPU implementations by training word embeddings with simple neural language models, in which a probability map of the next word given its preceding n-gram is learned by a neural network. We used a Nvidia Titan Black GPU and a i7-4820K @ 3.70GHz CPU and ran experiments on the one billion word dataset
, which is composed of 0.8 billions words belonging to a vocabulary of 0.8 millions words. We evaluated the resulting word embeddings with the recently introduced Simlex-999 score
, which measures the similarity between words. We also compared our approach to unfactorised versions and to a two-layer hierarchical softmax. Figure 2 and 3 (left) illustrate the practical speedup of our approach for the output layer only. Figure 3 (right) shows that out LST (Large Sparse Target) models are much faster to train than the softmax models and converge to only slightly lower Simlex-999 scores. Table 1 summarizes the speedups for the different output layers we tried, both on CPU and GPU. We also empirically verified that our proposed factored algorithm learns the model weights (V U ) as the corresponding naive unfactored algorithms W , as it theoretically should, and followed the same learning curves (as a function of number of iterations, not time!).  5  Conclusion and future work  We introduced a new algorithmic approach to efficiently compute the exact gradient updates for training deep networks with very large sparse targets. Remarkably the complexity of the algorithm is independent of the target size, which allows tackling very large problems. Our CPU and GPU implementations yield similar speedups to the theoretical one and can thus be used in practical applications, which could be explored in further work. In particular, neural language models seem good candidates. But it remains unclear how using a loss function other than the usual softmax might affect the quality of the resulting word embeddings so further research needs to be carried out in this direction. This includes empirically investigating natural extensions of the approach we described to other possible losses in the spherical family such as the spherical-softmax. Acknowledgements: We wish to thank Yves Grandvalet for stimulating discussions, aglar Glehre for pointing us to
, the developers of Theano
for making these libraries available to build on, and NSERC and Ubisoft for their financial support. 3  P In addition loss functions in this family are also allowed to depend on sum(o) = j oj which we can also P P compute cheaply without computing o, by tracking w = j W:j whereby sum(o) = j W:jT h = wT h. 4 where c is the correct class label, and  is a small positive constant that we added to the spherical interpretation in
for numerical stability: to guarantee we never divide by 0 nor take the log of 0. 5 Open source code is available at: https://github.com/pascal20100/factored_output_layer  7  Table 1: Speedups with respect to the baseline naive model on CPU, for a minibatch of 128 and the whole vocabulary of D = 793471 words. This is a model with two hidden layers of d = 300 neurons. Model output layer only speedup whole model speedup cpu unfactorised (naive) 1 1 gpu unfactorised (naive) 6.8 4.7 gpu hierarchical softmax 125.2 178.1 cpu factorised 763.3 501 gpu factorised 3257.3 1852.3  1  10  un-factorised CPU un-factorised GPU factorised GPU factorised CPU h_softmax GPU  0.008  Timing (sec) of a minibatch of size 128  Timing (sec) of a minibatch of size 128  0.010  0.006  0.004  0.002  un-factorised CPU un-factorised GPU factorised GPU factorised CPU h_softmax GPU  0  10  -1  10  -2  10  -3  0.000 0  2000  4000 6000 Size of the vocabulary D  8000  10  10000  1  2  10  10  4  3  10 10 Size of the vocabulary D  5  10  6  10  Figure 2: Timing of different algorithms. Time taken by forward and backward propagations in the output layer, including weight update, on a minibatch of size 128 for different sizes of vocabulary D on both CPU and GPU. The input size d is fixed to 300. The Timing of a 2 layer hierarchical softmax efficient GPU implementation (h_softmax) is also provided for comparison. Right plot is in log-log scale. As expected, the timings of factorized versions are independent of vocabulary size.  0.25  cpu_unfact / cpu_fact, experimental gpu_unfact / gpu_fact, experimental unfact / fact, theoretical cpu_unfact / gpu_fact, experimental cpu_unfact / gpu_unfact, experimental  1400  1200  0.15 SimLex-999  Speedup  1000  0.20  800  600  0.05  LST CPU LST GPU Softmax CPU Softmax GPU H-Softmax GPU  0.00  400  0.05  200  0 0  0.10  100  200 300 400 500 600 700 Size of the vocabulary D (in thousands)  0.10  800  -1  10  0  10  1  10 Training time (hours)  2  10  3  10  Figure 3: Left: Practical and theoretical speedups for different sizes of vocabulary D and fixed input size d=300. The practical unfact / fact speedup is similar to the theoretical one. Right: Evolution of the Simlex-999 score obtained with different models as a function of training time (CPU softmax times were extrapolated from fewer iterations). Softmax models are zero hidden-layer models, while our large sparse target (LST) models have two hidden layers. These were the best architectures retained in both cases (surprisingly the softmax models with hidden layers performed no better on this task). The extra non-linear layers in LST may help compensate for the lack of a softmax. LST models converge to slightly lower scores at similar speed as the hierarchical softmax model but significantly faster than softmax models. 8
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, ImageNet Large Scale Visual Recognition Challenge, International Journal of Computer Vision, 2015.
W. Ouyang and X. Wang, Joint deep learning for pedestrian detection, in International Conference on Computer Vision, pp. 20562063, 2013.
A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates, et al., Deepspeech: Scaling up end-to-end speech recognition, arXiv preprint arXiv:1412.5567, 2014.
B. Benjamin, P. Gao, E. McQuinn, S. Choudhary, A. Chandrasekaran, J.-M. Bussat, R. Alvarez-Icaza, J. Arthur, P. Merolla, and K. Boahen, Neurogrid: A mixed-analog-digital multichip system for largescale neural simulations, Proceedings of the IEEE, vol. 102, no. 5, pp. 699716, 2014.
E. Painkras, L. Plana, J. Garside, S. Temple, F. Galluppi, C. Patterson, D. Lester, A. Brown, and S. Furber, SpiNNaker: A 1-W 18-core system-on-chip for massively-parallel neural network simulation, IEEE Journal of Solid-State Circuits, vol. 48, no. 8, pp. 19431953, 2013.
T. Pfeil, A. Grubl, S. Jeltsch, E. Muller, P. Muller, M. A. Petrovici, M. Schmuker, D. Bruderle, J. Schemmel, and K. Meier, Six networks on a universal neuromorphic computing substrate, Frontiers in neuroscience, vol. 7, 2013.
P. A. Merolla, J. V. Arthur, R. Alvarez-Icaza, A. S. Cassidy, J. Sawada, F. Akopyan, B. L. Jackson, N. Imam, C. Guo, Y. Nakamura, et al., A million spiking-neuron integrated circuit with a scalable communication network and interface, Science, vol. 345, no. 6197, pp. 668673, 2014.
D. Rumelhart, G. Hinton, and R. Williams, Learning representations by back-propagating errors, Nature, vol. 323, no. 6088, pp. 533536, 1986.
P. Moerland and E. Fiesler, Neural network adaptations to hardware implementations, in Handbook of neural computation (E. Fiesler and R. Beale, eds.), New York: Institute of Physics Publishing and Oxford University Publishing, 1997.
E. Fiesler, A. Choudry, and H. J. Caulfield, Weight discretization paradigm for optical neural networks, in The Hague90, 12-16 April, pp. 164173, International Society for Optics and Photonics, 1990.
Y. Cao, Y. Chen, and D. Khosla, Spiking deep convolutional neural networks for energy-efficient object recognition, International Journal of Computer Vision, vol. 113, no. 1, pp. 5466, 2015.
P. U. Diehl, D. Neil, J. Binas, M. Cook, S.-C. Liu, and M. Pfeiffer, Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing, in International Joint Conference on Neural Networks, 2015, in press.
L. K. Muller and G. Indiveri, Rounding methods for neural networks with low resolution synaptic weights, arXiv preprint arXiv:1504.05767, 2015.
J. Zhao, J. Shawe-Taylor, and M. van Daalen, Learning in stochastic bit stream neural networks, Neural Networks, vol. 9, no. 6, pp. 991  998, 1996.
Z. Cheng, D. Soudry, Z. Mao, and Z. Lan, Training binary multilayer neural networks for image classification using expectation backpropgation, arXiv preprint arXiv:1503.03562, 2015.
E. Stromatias, D. Neil, F. Galluppi, M. Pfeiffer, S. Liu, and S. Furber, Scalable energy-efficient, lowlatency implementations of spiking deep belief networks on spinnaker, in International Joint Conference on Neural Networks, IEEE, 2015, in press.
A. S. Cassidy, P. Merolla, J. V. Arthur, S. Esser, B. Jackson, R. Alvarez-Icaza, P. Datta, J. Sawada, T. M. Wong, V. Feldman, A. Amir, D. Rubin, F. Akopyan, E. McQuinn, W. Risk, and D. S. Modha, Cognitive computing building block: A versatile and efficient digital neuron model for neurosynaptic cores, in International Joint Conference on Neural Networks, 2013.
G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, Improving neural networks by preventing co-adaptation of feature detectors, arXiv preprint arXiv:1207.0580, 2012.
A. J. Bell and T. J. Sejnowski, An information-maximization approach to blind separation and blind deconvolution, Neural computation, vol. 7, no. 6, pp. 11291159, 1995.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, Gradient-based learning applied to document recognition, Proceedings of the IEEE, vol. 86, no. 11, pp. 22782324, 1998.
G. E. Hinton and R. R. Salakhutdinov, Reducing the dimensionality of data with neural networks, Science, vol. 313, no. 5786, pp. 504507, 2006.  9
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 10971105, 2012.  8
Alex Graves and Jurgen Schmidhuber. Framewise phoneme classification with bidirectional lstm and other neural network architectures. Neural Networks, 18(5):602610, 2005.
Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural language processing (almost) from scratch. JMLR, 12:24932537, 2011.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, 1998.
Yaniv Taigman, Ming Yang, MarcAurelio Ranzato, and Lior Wolf. Deepface: Closing the gap to human-level performance in face verification. In CVPR, pages 17011708. IEEE, 2014.
Adam Coates, Brody Huval, Tao Wang, David Wu, Bryan Catanzaro, and Ng Andrew. Deep learning with cots hpc systems. In 30th ICML, pages 13371345, 2013.
Mark Horowitz. Energy table for 45nm process, Stanford VLSI wiki.
JP Rauschecker. Neuronal mechanisms of developmental plasticity in the cats visual system. Human neurobiology, 3(2):109114, 1983.
Christopher A Walsh. Peter huttenlocher (1931-2013). Nature, 502(7470):172172, 2013.
Misha Denil, Babak Shakibi, Laurent Dinh, Nando de Freitas, et al. Predicting parameters in deep learning. In Advances in Neural Information Processing Systems, pages 21482156, 2013.
Vincent Vanhoucke, Andrew Senior, and Mark Z Mao. Improving the speed of neural networks on cpus. In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop, 2011.
Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear structure within convolutional networks for efficient evaluation. In NIPS, pages 12691277, 2014.
Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep convolutional networks using vector quantization. arXiv preprint arXiv:1412.6115, 2014.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. arXiv preprint arXiv:1409.4842, 2014.
Stephen Jose Hanson and Lorien Y Pratt. Comparing biases for minimal network construction with back-propagation. In Advances in neural information processing systems, pages 177185, 1989.
Yann Le Cun, John S. Denker, and Sara A. Solla. Optimal brain damage. In Advances in Neural Information Processing Systems, pages 598605. Morgan Kaufmann, 1990.
Babak Hassibi, David G Stork, et al. Second order derivatives for network pruning: Optimal brain surgeon. Advances in neural information processing systems, pages 164164, 1993.
Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, and Yixin Chen. Compressing neural networks with the hashing trick. arXiv preprint arXiv:1504.04788, 2015.
Qinfeng Shi, James Petterson, Gideon Dror, John Langford, Alex Smola, and SVN Vishwanathan. Hash kernels for structured data. The Journal of Machine Learning Research, 10:26152637, 2009.
Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh Attenberg. Feature hashing for large scale multitask learning. In ICML, pages 11131120. ACM, 2009.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. JMLR, 15:19291958, 2014.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep neural networks? In Advances in Neural Information Processing Systems, pages 33203328, 2014.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult. Neural Networks, IEEE Transactions on, 5(2):157166, 1994.
Yangqing Jia, et al. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014.
Suraj Srinivas and R Venkatesh Babu. Data-free parameter pruning for deep neural networks. arXiv preprint arXiv:1507.06149, 2015.
Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex Smola, Le Song, and Ziyu Wang. Deep fried convnets. arXiv preprint arXiv:1412.7149, 2014.
Maxwell D Collins and Pushmeet Kohli. Memory bounded deep convolutional networks. arXiv preprint arXiv:1412.1442, 2014.  9
A. E. Alaoui and M. Mahoney. Fast randomized kernel ridge regression with statistical guarantees. In NIPS, 2015.
K. Chaudhuri, C. Monteleoni, and A. D. Sarwate. Differentially private empirical risk minimization. Journal of Machine Learning Research, 12:10691109, 2011.
A. Cotter, J. Keshet, and N. Srebro. Explicit approximations of the Gaussian kernel. Technical report, 2011. http://arxiv.org/pdf/1109.4603.pdf.
S. Csorgo. Multivariate empirical characteristic functions. Zeitschrift fur Wahrscheinlichkeitstheorie und Verwandte Gebiete, 55:203229, 1981.
S. Csorgo and V. Totik. On how long interval is the empirical characteristic function uniformly consistent? Acta Scientiarum Mathematicarum, 45:141149, 1983. 4  Fd is monotonically decreasing in d, F1 = 2.  8
P. Drineas and M. W. Mahoney. On the Nystrom method for approximating a Gram matrix for improved kernel-based learning. Journal of Machine Learning Research, 6:21532175, 2005.
A. Feuerverger and R. A. Mureika. The empirical characteristic function and its applications. Annals of Statistics, 5(1):8898, 1977.
G. B. Folland. Real Analysis: Modern Techniques and Their Applications. Wiley-Interscience, 1999.
B. Kulis and K. Grauman. Kernelized locality-sensitive hashing. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34:10921104, 2012.
D. Lopez-Paz, K. Muandet, B. Scholkopf, and I. Tolstikhin. Towards a learning theory of cause-effect inference. JMLR W&CP  ICML, pages 14521461, 2015.
S. Maji, A. C. Berg, and J. Malik. Efficient classification for additive kernel SVMs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35:6677, 2013.
J. Oliva, W. Neiswanger, B. Poczos, E. Xing, and J. Schneider. Fast function to function regression. JMLR W&CP  AISTATS, pages 717725, 2015.
A. Rahimi and B. Recht. Random features for large-scale kernel machines. In NIPS, pages 11771184, 2007.
A. Rahimi and B. Recht. Uniform approximation of functions with random bases. In Allerton, pages 555561, 2008.
L. Rosasco, M. Santoro, S. Mosci, A. Verri, and S. Villa. A regularization approach to nonlinear variable selection. JMLR W&CP  AISTATS, 9:653660, 2010.
L. Rosasco, S. Villa, S. Mosci, M. Santoro, and A. Verri. Nonparametric sparsity and regularization. Journal of Machine Learning Research, 14:16651714, 2013.
B. Scholkopf and A. J. Smola. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press, 2002.
L. Shi, X. Guo, and D.-X. Zhou. Hermite learning with gradient data. Journal of Computational and Applied Mathematics, 233:30463059, 2010.
Q. Shi, J. Petterson, G. Dror, J. Langford, A. Smola, A. Strehl, and V. Vishwanathan. Hash kernels. AISTATS, 5:496503, 2009.
B. K. Sriperumbudur, K. Fukumizu, A. Gretton, A. Hyvarinen, and R. Kumar. sity estimation in infinite dimensional exponential families. Technical report, http://arxiv.org/pdf/1312.3516.pdf.  Den2014.
H. Strathmann, D. Sejdinovic, S. Livingstone, Z. Szabo, and A. Gretton. Gradient-free Hamiltonian Monte Carlo with efficient kernel exponential families. In NIPS, 2015.
D. J. Sutherland and J. Schneider. On the error of random Fourier features. In UAI, pages 862871, 2015.
A. Vedaldi and A. Zisserman. Efficient additive kernels via explicit feature maps. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34:480492, 2012.
H. Wendland. Scattered Data Approximation. Cambridge University Press, 2005.
C. K. I. Williams and M. Seeger. Using the Nystrom method to speed up kernel machines. In NIPS, pages 682688, 2001.
Y. Ying, Q. Wu, and C. Campbell. Learning the coordinate gradients. Advances in Computational Mathematics, 37:355378, 2012.
J. E. Yukich. Some limit theorems for the empirical process indexed by functions. Probability Theory and Related Fields, 74:7190, 1987.
D.-X. Zhou. Derivative reproducing properties for kernel methods in learning theory. Journal of Computational and Applied Mathematics, 220:456463, 2008.  9
A. Ahmed, Q. Ho, C. H. Teo, J. Eisenstein, E. P. Xing, and A. J. Smola. Online inference for the infinite topic-cluster model: Storylines from streaming text. In International Conference on Artificial Intelligence and Statistics, pages 101109, 2011.
S. I. Amari. Natural gradient works efficiently in learning. Neural Computation, 10(2):251276, 1998.
J. M. Bernardo and A. F. Smith. Bayesian Theory, volume 405. John Wiley & Sons, 2009.
D. M. Blei, M. I. Jordan, et al. Variational inference for Dirichlet process mixtures. Bayesian Analysis, 1(1):121143, 2006.
D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. The Journal of Machine Learning Research, 3:9931022, 2003.
V. D. Blondel, M. Esch, C. Chan, F. Clrot, P. Deville, E. Huens, F. Morlot, Z. Smoreda, and C. Ziemlicki. Data for development: the D4D challenge on mobile phone data. arXiv preprint arXiv:1210.0137, 2012.
L. Bottou. Online learning and stochastic approximations. Online learning in Neural Networks, 17:9, 1998.
T. Broderick, N. Boyd, A. Wibisono, A. C. Wilson, and M. Jordan. Streaming variational Bayes. In Advances in Neural Information Processing Systems, pages 17271735, 2013.
A. Doucet, S. Godsill, and C. Andrieu. On sequential Monte Carlo sampling methods for Bayesian filtering. Statistics and Computing, 10(3):197208, 2000.
B. Efron and R. J. Tibshirani. An introduction to the bootstrap. CRC press, 1994.
M. D. Escobar and M. West. Bayesian density estimation and inference using mixtures. Journal of the American Statistical Association, 90(430):577588, 1995.
Z. Ghahramani and H. Attias. Online variational Bayesian learning. In Slides from talk presented at NIPS 2000 Workshop on Online learning, pages 101109, 2000.
M. D. Hoffman and D. M. Blei. Structured stochastic variational inference. In International Conference on Artificial Intelligence and Statistics, pages 101109, 2015.
M. D. Hoffman, D. M. Blei, C. Wang, and J. Paisley. Stochastic variational inference. The Journal of Machine Learning Research, 14(1):13031347, 2013.
A. Honkela and H. Valpola. On-line variational Bayesian learning. In 4th International Symposium on Independent Component Analysis and Blind Signal Separation, pages 803808, 2003.
M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for graphical models. Machine learning, 37(2):183233, 1999.
D. P. Kingma and M. Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114, 2013.
R. Ranganath, S. Gerrish, and D. M. Blei. Black box variational inference. In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics, pages 805813, 2014.
H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics, pages 400407, 1951.
L. K. Saul and M. I. Jordan. Exploiting tractable substructures in intractable networks. Advances in Neural Information Processing Systems, pages 486492, 1996.
J. Sethuraman. A constructive definition of Dirichlet priors. Statistica Sinica, 4:639650, 1994.
A. Tank, N. Foti, and E. Fox. Streaming variational inference for Bayesian nonparametric mixture models. In International Conference on Artificial Intelligence and Statistics, 2015.
L. Theis and M. D. Hoffman. A trust-region method for stochastic variational inference with applications to streaming data. In International Conference on Machine Learning, 2015.
M. Titsias and M. Lzaro-Gredilla. Doubly stochastic variational Bayes for non-conjugate inference. In Proceedings of the 31st International Conference on Machine Learning, pages 19711979, 2014.
M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1(1-2):1305, Jan. 2008.
H. Wallach, I. Murray, R. Salakhutdinov, and D. Mimno. Evaluation methods for topic models. In International Conference on Machine Learning, 2009.
L. Yao, D. Mimno, and A. McCallum. Efficient methods for topic model inference on streaming document collections. In Conference on Knowledge Discovery and Data Mining, pages 937946. ACM, 2009.  9
F. Bach. On the Equivalence between Quadrature Rules and Random Features. arXiv:1502.06800, 2015.
F. Bach, S. Lacoste-Julien, and G. Obozinski. On the Equivalence between Herding and Conditional Gradient Algorithms. In Proceedings of the 29th International Conference on Machine Learning, pages 13591366, 2012.
Y. Chen, L. Bornn, N. de Freitas, M. Eskelin, J. Fang, and M. Welling. Herded Gibbs Sampling. Journal of Machine Learning Research, 2015. To appear.
Y. Chen, M. Welling, and A. Smola. Super-Samples from Kernel Herding. In Proceedings of the Conference on Uncertainty in Artificial Intelligence, pages 109116, 2010.
P. Conrad, M. Girolami, S. Sarkka, A. Stuart, and K. Zygalakis. Probability Measures for Numerical Solutions of Differential Equations. arXiv:1506.04592, 2015.
P. Diaconis. Bayesian Numerical Analysis. Statistical Decision Theory and Related Topics IV, pages 163175, 1988.
J. Dick and F. Pillichshammer. Digital Nets and Sequences - Discrepancy Theory and Quasi-Monte Carlo Integration. Cambridge University Press, 2010.
J. C. Dunn. Convergence Rates for Conditional Gradient Sequences Generated by Implicit Step Length Rules. SIAM Journal on Control and Optimization, 18(5):473487, 1980.
M. Frank and P. Wolfe. An Algorithm for Quadratic Programming. Naval Research Logistics Quarterly, 3:95110, 1956.
D. Garber and E. Hazan. Faster Rates for the Frank-Wolfe Method over Strongly-Convex Sets. In Proceedings of the 32nd International Conference on Machine Learning, pages 541549, 2015.
Z. Ghahramani and C. Rasmussen. Bayesian Monte Carlo. In Advances in Neural Information Processing Systems, pages 489496, 2003.
T. Gunter, R. Garnett, M. Osborne, P. Hennig, and S. Roberts. Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature. In Advances in Neural Information Processing Systems, 2014.
J.B. Hamrick and T.L. Griffiths. Mental Rotation as Bayesian Quadrature. In NIPS 2013 Workshop on Bayesian Optimization in Theory and Practice, 2013.
P. Hennig. Probabilistic Interpretation of Linear Solvers. SIAM Journal on Optimization, 25:234260, 2015.
P. Hennig, M. Osborne, and M. Girolami. Probabilistic Numerics and Uncertainty in Computations. Proceedings of the Royal Society A, 471(2179), 2015.
F. Huszar and D. Duvenaud. Optimally-Weighted Herding is Bayesian Quadrature. In Uncertainty in Artificial Intelligence, pages 377385, 2012.
M. Jaggi. Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization. In Proceedings of the 30th International Conference on Machine Learning, volume 28, pages 427435, 2013.
S. Lacoste-Julien, F. Lindsten, and F. Bach. Sequential Kernel Herding : Frank-Wolfe Optimization for Particle Filtering. In Proceedings of the 18th International Conference on Artificial Intelligence and Statistics, pages 544552, 2015.
C.J. Oates, F. Dondelinger, N. Bayani, J. Korkola, J.W. Gray, and S. Mukherjee. Causal Network Inference using Biochemical Kinetics. Bioinformatics, 30(17):i468i474, 2014.
C.J. Oates, M. Girolami, and N. Chopin. Control Functionals for Monte Carlo Integration. arXiv: 1410.2392, 2015.
A. OHagan. Monte Carlo is Fundamentally Unsound. Journal of the Royal Statistical Society, Series D, 36(2):247249, 1984.
A. OHagan. Bayes-Hermite Quadrature. Journal of Statistical Planning and Inference, 29:245260, 1991.
M. Osborne, R. Garnett, S. Roberts, C. Hart, S. Aigrain, and N. Gibson. Bayesian Quadrature for Ratios. In Proceedings of the 15th International Conference on Artificial Intelligence and Statistics, pages 832 840, 2012.
A. B. Owen. A Constraint on Extensible Quadrature Rules. Numerische Mathematik, pages 18, 2015.
S. Sarkka, J. Hartikainen, L. Svensson, and F. Sandblom. On the Relation between Gaussian Process Quadratures and Sigma-Point Methods. arXiv:1504.05994, 2015.
M. Schober, D. Duvenaud, and P. Hennig. Probabilistic ODE solvers with Runge-Kutta means. In Advances in Neural Information Processing Systems 27, pages 739747, 2014.
B. Sriperumbudur, A. Gretton, K. Fukumizu, B. Scholkopf, and G. Lanckriet. Hilbert Space Embeddings and Metrics on Probability Measures. Journal of Machine Learning Research, 11:15171561, 2010.  9
Y. Bengio, P. Simard, and P. Frasconi. Learning long term dependencies is hard. IEEE Transactions on Neural Networks, 5(2):157166, 1994.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8), 1997.
I. Sutskever, O. Vinyals, and Q. Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, NIPS, 2014.
O. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever, and G. Hinton. Grammar as a foreign language. In arXiv:1412.7449, 2014.  8
O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2015.
J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell. Long-term recurrent convolutional networks for visual recognition and description. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2015.
Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In Proceedings of the International Conference on Machine Learning, ICML, 2009.
J. D. Lafferty, A. McCallum, and F. C. N. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML, pages 282289, San Francisco, CA, USA, 2001. Morgan Kaufmann Publishers Inc.
H. Daume III, J. Langford, and D. Marcu. Search-based structured prediction as classification. Machine Learning Journal, 2009.
S. Ross, G. J. Gordon, and J. A. Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the Workshop on Artificial Intelligence and Statistics, AISTATS, 2011.
A. Venkatraman, M. Herbert, and J. A. Bagnell. Improving multi-step prediction of learned time series models. In Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI, 2015.
M. Collins and B. Roark. Incremental parsing with the perceptron algorithm. In Proceedings of the Association for Computational Linguistics, ACL, 2004.
Y. Goldberg and J. Nivre. A dynamic oracle for arc-eager dependency parsing. In Proceedings of COLING, 2012.
J. Mao, W. Xu, Y. Yang, J. Wang, Z. H. Huang, and A. Yuille. Deep captioning with multimodal recurrent neural networks (m-rnn). In International Conference on Learning Representations, ICLR, 2015.
R. Kiros, R. Salakhutdinov, and R. Zemel. Unifying visual-semantic embeddings with multimodal neural language models. In TACL, 2015.
A. Karpathy and F.-F. Li. Deep visual-semantic alignments for generating image descriptions. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2015.
H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Dollar, J. Gao, X. He, M. Mitchell, J. C. Platt, C. L. Zitnick, and G. Zweig. From captions to visual concepts and back. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2015.
R. Vedantam, C. L. Zitnick, and D. Parikh. CIDEr: Consensus-based image description evaluation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2015.
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick. Microsoft coco: Common objects in context. arXiv:1405.0312, 2014.
S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the International Conference on Machine Learning, ICML, 2015.
Y. Cui, M. R. Ronchi, T.-Y. Lin, P. Dollr, and L. Zitnick. http://mscoco.org/dataset/#captions-challenge2015, 2015.  Microsoft coco captioning challenge.
D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations, ICLR, 2015.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel. Ontonotes: The 90% solution. In Proceedings of the Human Language Technology Conference of the NAACL, Short Papers, pages 5760, New York City, USA, June 2006. Association for Computational Linguistics.
D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz, J. Silovsky, G. Stemmer, and K. Vesely. The kaldi speech recognition toolkit. In IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Society, December 2011. IEEE Catalog No.: CFP11SRW-USB.
N. Jaitly. Exploring Deep Learning Methods for discovering features in speech signals. PhD thesis, University of Toronto, 2014.
Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. End-to-end continuous speech recognition using attention-based recurrent nn: First results. arXiv preprint arXiv:1412.1602, 2014.  9
D. Amelunxen, M. Lotz, M. B. McCoy, and J. A. Tropp. Living on the edge: A geometric theory of phase transitions in convex optimization. Inform. Inference, 2014.
A. Argyriou, R. Foygel, and N. Srebro. Sparse prediction with the k-support norm. In NIPS, 2012.
A. Banerjee, S. Chen, F. Fazayeli, and V. Sivakumar. Estimation with norm regularization. In NIPS, 2014.
A. Banerjee, S. Merugu, I. S. Dhillon, and J. Ghosh. Clustering with bregman divergences. JMLR, 2005.
T. Cai, T. Liang, and A. Rakhlin. Geometrizing local rates of convergence for linear inverse problems. arXiv preprint, 2014.
E. J. Candes, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? ACM, 2011.
E. J. Candes and Y. Plan. Matrix completion with noise. Proceedings of the IEEE, 2010.
E. J. Candes and B. Recht. Exact matrix completion via convex optimization. FoCM, 2009.
Emmanuel J Candes and Terence Tao. Decoding by linear programming. Information Theory, IEEE Transactions on, 2005.
V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky. The convex geometry of linear inverse problems. Foundations of Computational Mathematics, 2012.
M. A. Davenport, Y. Plan, E. Berg, and M. Wootters. 1-bit matrix completion. Inform. Inference, 2014.
R. M. Dudley. The sizes of compact subsets of hilbert space and continuity of gaussian processes. Journal of Functional Analysis, 1967.
A. Edelman. Eigenvalues and condition numbers of random matrices. Journal on Matrix Analysis and Applications, 1988.
M. Fazel, H Hindi, and S. P. Boyd. A rank minimization heuristic with application to minimum order system approximation. In American Control Conference, 2001.
J. Forster and M. Warmuth. Relative expected instantaneous loss bounds. Journal of Computer and System Sciences, 2002.
S. Gunasekar, P. Ravikumar, and J. Ghosh. Exponential family matrix completion under structural constraints. In ICML, 2014.
L. Jacob, J. P. Vert, and F. R. Bach. Clustered multi-task learning: A convex formulation. In NIPS, 2009.
R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from a few entries. IEEE Trans. IT, 2010.
R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from noisy entries. JMLR, 2010.
O. Klopp. Noisy low-rank matrix completion with general sampling distribution. Bernoulli, 2014.
O. Klopp. Matrix completion by singular value thresholding: sharp bounds. arXiv preprint arXiv, 2015.
Vladimir Koltchinskii, Karim Lounici, Alexandre B Tsybakov, et al. Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion. The Annals of Statistics, 2011.
M. Ledoux and M. Talagrand. Probability in Banach Spaces: isoperimetry and processes. Springer, 1991.
A. E. Litvak, A. Pajor, M. Rudelson, and N. Tomczak-Jaegermann. Smallest singular value of random matrices and geometry of random polytopes. Advances in Mathematics, 2005.
A. M. McDonald, M. Pontil, and D. Stamos. New perspectives on k-support and cluster norms. arXiv preprint, 2014.
S. Negahban and M. J. Wainwright. Restricted strong convexity and weighted matrix completion: Optimal bounds with noise. JMLR, 2012.
S. Negahban, B. Yu, M. J. Wainwright, and P. Ravikumar. A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers. In NIPS, 2009.
B. Recht. A simpler approach to matrix completion. JMLR, 2011.
E. Richard, G. Obozinski, and J.-P. Vert. Tight convex relaxations for sparse matrix factorization. In ArXiv e-prints, 2014.
N. Srebro and A. Shraibman. Rank, trace-norm and max-norm. In Learning Theory. Springer, 2005.
M. Talagrand. Majorizing measures: the generic chaining. The Annals of Probability, 1996.
M. Talagrand. Majorizing measures without measures. Annals of probability, 2001.
M. Talagrand. Upper and Lower Bounds for Stochastic Processes. Springer, 2014.
J. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics, 2012.
J. A. Tropp. Convex recovery of a structured signal from independent random linear measurements. arXiv preprint, 2014.
R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. Compressed sensing, pages 210268, 2012.
R. Vershynin. Estimation in high dimensions: a geometric perspective. ArXiv e-prints, 2014.
A. G. Watson. Characterization of the subdifferential of some matrix norms. Linear Algebra and its Applications, 1992.
E. Yang and P. Ravikumar. Dirty statistical models. In NIPS, 2013.  9
T. Evgeniou, C. A. Micchelli, and M. Pontil. Learning multiple tasks with kernel methods. JMLR, 6:615637, 2005.
A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. ML, 73:243272, 2008.
K. Lounici, M. Pontil, A. B. Tsybakov, and S. van de Geer. Taking advantage of sparsity in multi-task learning. In COLT, 2009.
A. Jalali, P. Ravikumar, S. Sanghavi, and C. Ruan. A dirty model for multi-task learning. In NIPS, 2010.
P. Jawanpuria and J. S. Nath. Multi-task multiple kernel learning. In SDM, 2011.
A. Maurer, M. Pontil, and B. Romera-paredes. Sparse coding for multitask and transfer learning. In ICML, 2013.
P. Jawanpuria, J. S. Nath, and G. Ramakrishnan. Generalized hierarchical kernel learning. JMLR, 16:617 652, 2015.
R. Caruana. Multitask learning. ML, 28:4175, 1997.
Y. Zhang and D. Y. Yeung. A convex formulation for learning task relationships in multi-task learning. In UAI, 2010.
Z. Kang, K. Grauman, and F. Sha. Learning with whom to share in multi-task feature learning. In ICML, 2011.
P. Jawanpuria and J. S. Nath. A convex feature learning formulation for latent task structure discovery. In ICML, 2012.
L. Jacob, F. Bach, and J. P. Vert. Clustered multi-task learning: A convex formulation. In NIPS, 2008.
C. A. Micchelli and M. Pontil. Kernels for multitask learning. In NIPS, 2005.
A. Caponnetto, C. A. Micchelli, M. Pontil, and Y. Ying. Universal multi-task kernels. JMLR, 9:1615 1646, 2008.
M. A. Alvarez, L. Rosasco, and N. D. Lawrence. Kernels for vector-valued functions: a review. Foundations and Trends in Machine Learning, 4:195266, 2012.
T. Evgeniou and M. Pontil. Regularized multitask learning. In KDD, 2004.
F. Dinuzzo, C. S. Ong, P. Gehler, and G. Pillonetto. Learning output kernels with block coordinate descent. In ICML, 2011.
C. Ciliberto, Y. Mroueh, T. Poggio, and L. Rosasco. Convex learning of multiple tasks and their structure. In ICML, 2015.
S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss. JMLR, 14(1):567599, 2013.
B. Scholkopf and A. Smola. Learning with Kernels. MIT Press, 2002.
M. Hein and O. Bousquet. Kernels, associated structures and generalizations. Technical Report TR-127, Max Planck Institute for Biological Cybernetics, 2004.
A. Ben-Israel and B. Mond. What is invexity ? J. Austral. Math. Soc. Ser. B, 28:19, 1986.
F. Hiai. Monotonicity for entrywise functions of matrices. 431(8):1125  1146, 2009.  Linear Algebra and its Applications,
R. A. Horn. The theory of infinitely divisible matrices and kernels. Trans. Amer. Math. Soc., 136:269286, 1969.
M. Lapin, B. Schiele, and M. Hein. Scalable multitask representation learning for scene classification. In CVPR, 2014.
B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva. Learning deep features for scene recognition using places database. In NIPS, 2014.
M. Koskela and J. Laaksonen. Convolutional network features for scene recognition. In Proceedings of the ACM International Conference on Multimedia, 2014.
J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. SUN database: Large-scale scene recognition from abbey to zoo. In CVPR, 2010.  9
L. R. Rabiner. A tutorial on hidden Markov models and selected applications in speech recognition. Proc. of the IEEE, 77(2):257286, 1989.
Zoubin Ghahramani. An introduction to hidden Markov models and Bayesian networks. International Journal of Pattern Recognition and Machine Intelligence, 15(01):942, 2001.
Jason Ernst and Manolis Kellis. Discovery and characterization of chromatin states for systematic annotation of the human genome. Nature Biotechnology, 28(8):817825, 2010.
Matthew J. Beal, Zoubin Ghahramani, and Carl E. Rasmussen. The infinite hidden Markov model. In Neural Information Processing Systems, 2001.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):15661581, 2006.
Emily B. Fox, Erik B. Sudderth, Michael I. Jordan, and Alan S. Willsky. A sticky HDP-HMM with application to speaker diarization. Annals of Applied Statistics, 5(2A):10201056, 2011.
Matthew J. Johnson and Alan S. Willsky. Stochastic variational inference for Bayesian time series models. In International Conference on Machine Learning, 2014.
Nicholas Foti, Jason Xu, Dillon Laird, and Emily Fox. Stochastic variational inference for hidden Markov models. In Neural Information Processing Systems, 2014.
Andreas Stolcke and Stephen Omohundro. Hidden Markov model induction by Bayesian model merging. In Neural Information Processing Systems, 1993.
Chong Wang and David M Blei. A split-merge MCMC algorithm for the hierarchical Dirichlet process. arXiv preprint arXiv:1201.1657, 2012.
Jason Chang and John W Fisher III. Parallel sampling of HDPs using sub-cluster splits. In Neural Information Processing Systems, 2014.
Emily B. Fox, Michael C. Hughes, Erik B. Sudderth, and Michael I. Jordan. Joint modeling of multiple time series via the beta process with application to motion capture segmentation. Annals of Applied Statistics, 8(3):12811313, 2014.
Michael C. Hughes and Erik B. Sudderth. Memoized online variational inference for Dirichlet process mixture models. In Neural Information Processing Systems, 2013.
Michael C. Hughes, Dae Il Kim, and Erik B. Sudderth. Reliable and scalable variational inference for the hierarchical Dirichlet process. In Artificial Intelligence and Statistics, 2015.
Yee Whye Teh, Kenichi Kurihara, and Max Welling. Collapsed variational inference for HDP. In Neural Information Processing Systems, 2008.
Michael Bryant and Erik B. Sudderth. Truly nonparametric online variational inference for hierarchical Dirichlet processes. In Neural Information Processing Systems, 2012.
Percy Liang, Slav Petrov, Michael I Jordan, and Dan Klein. The infinite PCFG using hierarchical Dirichlet processes. In Empirical Methods in Natural Language Processing, 2007.
Matthew James Beal. Variational algorithms for approximate Bayesian inference. PhD thesis, University of London, 2003.
Matt Hoffman, David Blei, Chong Wang, and John Paisley. Stochastic variational inference. Journal of Machine Learning Research, 14(1), 2013.
Tamara Broderick, Nicholas Boyd, Andre Wibisono, Ashia C. Wilson, and Michael I. Jordan. Streaming variational Bayes. In Neural Information Processing Systems, 2013.
Chong Wang and David Blei. Truncation-free online variational inference for Bayesian nonparametric models. In Neural Information Processing Systems, 2012.
J. Van Gael, Y. Saatci, Y. W. Teh, and Z. Ghahramani. Beam sampling for the infinite hidden Markov model. In International Conference on Machine Learning, 2008.
NIST. Rich transcriptions database. http://www.nist.gov/speech/tests/rt/, 2007.
Michael M. Hoffman, Orion J. Buske, Jie Wang, Zhiping Weng, Jeff A. Bilmes, and William S. Noble. Unsupervised pattern discovery in human chromatin structure through genomic segmentation. Nature methods, 9(5):473476, 2012.  9
A. U. Asuncion, P. Smyth, and M. Welling. Asynchronous distributed learning of topic models. In Advances in Neural Information Processing Systems 21, pages 8188, 2008.
R. Bardenet, A. Doucet, and C. Holmes. Towards scaling up Markov chain Monte Carlo: An adaptive subsampling approach. In Proceedings of the 31st International Conference on Machine Learning, 2014.
D. P. Bertsekas. Nonlinear Programming. Athena Scientific, Belmont, MA, 2nd edition, 1990.
T. Broderick, N. Boyd, A. Wibisono, A. C. Wilson, and M. I. Jordan. Streaming variational Bayes. In Advances in Neural Information Processing Systems 26, pages 17271735, 2013.
T. Campbell and J. P. How. Approximate decentralized Bayesian inference. In 30th Conference on Uncertainty in Artificial Intelligence, 2014.
T. M. Cover and J. A. Thomas. Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing). Wiley-Interscience, 2006.
J. Dean and S. Ghemawat. MapReduce: Simplified data processing on large clusters. Communications of the ACM, 51(1):107113, Jan. 2008.
F. Doshi-Velez, D. A. Knowles, S. Mohamed, and Z. Ghahramani. Large scale nonparametric Bayesian inference: Data parallelisation in the Indian buffet process. In Advances in Neural Information Processing Systems 22, pages 12941302, 2009.
J. C. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:21212159, 2011.
A. Gelman, J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. B. Rubin. Bayesian Data Analysis, Third Edition. Chapman and Hall/CRC, 2013.
M. D. Hoffman, D. M. Blei, C. Wang, and J. Paisley. Stochastic variational inference. Journal of Machine Learning Research, 14(1):13031347, May 2013.
M. Johnson, J. Saunderson, and A. Willsky. Analyzing Hogwild parallel Gaussian Gibbs sampling. In Advances in Neural Information Processing Systems 26, pages 27152723, 2013.
R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems 26, pages 315323, 2013.
A. Korattikara, Y. Chen, and M. Welling. Austerity in MCMC land: Cutting the Metropolis-Hastings budget. In Proceedings of the 31st International Conference on Machine Learning, 2014.
H. W. Kuhn. The Hungarian method for the assignment problem. Naval Research Logistics Quarterly, 2 (1-2):8397, 1955.
D. Maclaurin and R. P. Adams. Firefly Monte Carlo: Exact MCMC with subsets of data. In Proceedings of 30th Conference on Uncertainty in Artificial Intelligence, 2014.
S. Mandt and D. M. Blei. Smoothed gradients for stochastic variational inference. In Advances in Neural Information Processing Systems 27, pages 24382446, 2014.
W. Neiswanger, C. Wang, and E. Xing. Asymptotically exact, embarrassingly parallel MCMC. In 30th Conference on Uncertainty in Artificial Intelligence, 2014.
R. Nishihara, I. Murray, and R. P. Adams. Parallel MCMC with generalized elliptical slice sampling. Journal of Machine Learning Research, 15:20872112, 2014.
F. Niu, B. Recht, C. Re, and S. Wright. Hogwild!: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems 24, pages 693701, 2011.
R. Ranganath, C. Wang, D. M. Blei, and E. P. Xing. An adaptive learning rate for stochastic variational inference. In Proceedings of the 30th International Conference on Machine Learning, pages 298306, 2013.
S. L. Scott, A. W. Blocker, and F. V. Bonassi. Bayes and big data: The consensus Monte Carlo algorithm. In Bayes 250, 2013.
H. Strathmann, D. Sejdinovic, and M. Girolami. Unbiased Bayes for big data: Paths of partial posteriors. arXiv:1501.03326, 2015.
X. Wang and D. B. Dunson. Parallel MCMC via Weierstrass sampler. arXiv:1312.4605, 2013.
M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient Langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning, 2011.  9
Shun-Ichi Amari, Natural gradient works efficiently in learning, Neural computation 10 (1998).
Richard H Byrd, Gillian M Chin, Will Neveitt, and Jorge Nocedal, On the use of stochastic hessian information in optimization methods for machine learning, SIAM Journal on Optimization (2011).
Jock A Blackard and Denis J Dean, Comparative accuracies of artificial neural networks and discriminant analysis in predicting forest cover types from cartographic variables, Computers and electronics in agriculture (1999), 131151.
Richard H Byrd, SL Hansen, Jorge Nocedal, and Yoram Singer, A stochastic quasi-newton method for large-scale optimization, arXiv preprint arXiv:1401.7020 (2014).
Christopher M. Bishop, Neural networks for pattern recognition, Oxford University Press, 1995.
Lon Bottou, Large-scale machine learning with stochastic gradient descent, COMPSTAT, 2010.
Jinho Baik and Jack W Silverstein, Eigenvalues of large sample covariance matrices of spiked population models, Journal of Multivariate Analysis 97 (2006), no. 6, 13821408.
Stephen Boyd and Lieven Vandenberghe, Convex optimization, Cambridge University Press, 2004.
Jian-Feng Cai, Emmanuel J Cands, and Zuowei Shen, A singular value thresholding algorithm for matrix completion, SIAM Journal on Optimization 20 (2010), no. 4, 19561982.
Louis HY Chen, Larry Goldstein, and Qi-Man Shao, Normal approximation by SteinAZs method, Springer Science, 2010.
Lee H Dicker and Murat A Erdogdu, Flexible results for quadratic forms with applications to variance components estimation, arXiv preprint arXiv:1509.04388 (2015).
David L Donoho and Matan Gavish, The optimal hard threshold for singular values is 4/sqrt3, arXiv:1305.5870 (2013).
David L Donoho, Matan Gavish, and Iain M Johnstone, Optimal shrinkage of eigenvalues in the spiked covariance model, arXiv preprint arXiv:1311.0851 (2013).
John Duchi, Elad Hazan, and Yoram Singer, Adaptive subgradient methods for online learning and stochastic optimization, J. Mach. Learn. Res. 12 (2011), 21212159.
Murat A Erdogdu and Andrea Montanari, Convergence rates of sub-sampled Newton methods, arXiv preprint arXiv:1508.02810 (2015).
Jerome Friedman, Trevor Hastie, and Rob Tibshirani, Regularization paths for generalized linear models via coordinate descent, Journal of statistical software 33 (2010), no. 1, 1.
Michael P Friedlander and Mark Schmidt, Hybrid deterministic-stochastic methods for data fitting, SIAM Journal on Scientific Computing 34 (2012), no. 3, A1380A1405.
Franz Graf, Hans-Peter Kriegel, Matthias Schubert, Sebastian Plsterl, and Alexander Cavallaro, 2d image registration in ct images using radial image descriptors, MICCAI 2011, Springer, 2011.
Alison L Gibbs and Francis E Su, On choosing and bounding probability metrics, ISR 70 (2002).
Daphne Koller and Nir Friedman, Probabilistic graphical models, MIT press, 2009.
M. Lichman, UCI machine learning repository, 2013.
Nicolas Le Roux and Andrew W Fitzgibbon, A fast natural newton method, ICML, 2010.
Nicolas Le Roux, Pierre-A Manzagol, and Yoshua Bengio, Topmoumoute online natural gradient algorithm, NIPS, 2008.
Chih-J Lin, Ruby C Weng, and Sathiya Keerthi, Trust region newton method for logistic regression, JMLR (2008).
James Martens, Deep learning via hessian-free optimization, ICML, 2010, pp. 735742.
Peter McCullagh and John A Nelder, Generalized linear models, vol. 2, Chapman and Hall, 1989.
Yurii Nesterov, A method for unconstrained convex minimization problem with the rate of convergence o (1/k2), Doklady AN SSSR, vol. 269, 1983, pp. 543547.
, Introductory lectures on convex optimization: A basic course, vol. 87, Springer, 2004.
Mark Schmidt, Nicolas Le Roux, and Francis Bach, Minimizing finite sums with the stochastic average gradient, arXiv preprint arXiv:1309.2388 (2013).
Charles M Stein, Estimation of the mean of a multivariate normal distribution, Annals of Statistics (1981), 11351151.
Roman Vershynin, Introduction to the non-asymptotic analysis of random matrices, arXiv:1011.3027 (2010).
Oriol Vinyals and Daniel Povey, Krylov Subspace Descent for Deep Learning, AISTATS, 2012.  9
Alexandr Andoni, Piotr Indyk, Huy L. Nguyen, and Ilya Razenshteyn. Beyond locality-sensitive hashing. In SODA, 2014. Full version at http://arxiv.org/abs/1306.1547.
Alexandr Andoni and Ilya Razenshteyn. Optimal data-dependent hashing for approximate near neighbors. In STOC, 2015. Full version at http://arxiv.org/abs/1501.01062.
Moses S. Charikar. Similarity estimation techniques from rounding algorithms. In STOC, 2002.
Gregory Shakhnarovich, Trevor Darrell, and Piotr Indyk. Nearest-Neighbor Methods in Learning and Vision: Theory and Practice. MIT Press, Cambridge, MA, 2005.
Hanan Samet. Foundations of multidimensional and metric data structures. Morgan Kaufmann, 2006.
Sariel Har-Peled, Piotr Indyk, and Rajeev Motwani. Approximate nearest neighbor: Towards removing the curse of dimensionality. Theory of Computing, 8(14):321350, 2012.
Herv Jgou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(1):117128, 2011.
Ludwig Schmidt, Matthew Sharifi, and Ignacio Lopez Moreno. Large-scale speaker identification. In ICASSP, 2014.
Narayanan Sundaram, Aizana Turmukhametova, Nadathur Satish, Todd Mostak, Piotr Indyk, Samuel Madden, and Pradeep Dubey. Streaming similarity search over one billion tweets using parallel localitysensitive hashing. In VLDB, 2013.
Moshe Dubiner. Bucketing coding and information theory for the statistical high-dimensional nearestneighbor problem. IEEE Transactions on Information Theory, 56(8):41664179, 2010.
Alexandr Andoni and Ilya Razenshteyn. Tight lower bounds for data-dependent locality-sensitive hashing, 2015. Available at http://arxiv.org/abs/1507.04299.
Anshumali Shrivastava and Ping Li. Fast near neighbor search in high-dimensional binary data. In Machine Learning and Knowledge Discovery in Databases, pages 474489. Springer, 2012.
Anshumali Shrivastava and Ping Li. Densifying one permutation hashing via rotation for fast near neighbor search. In ICML, 2014.
Qin Lv, William Josephson, Zhe Wang, Moses Charikar, and Kai Li. Multi-probe lsh: efficient indexing for high-dimensional similarity search. In VLDB, 2007.
Kengo Terasawa and Yuzuru Tanaka. Spherical lsh for approximate nearest neighbor search on unit hypersphere. In Algorithms and Data Structures, pages 2738. Springer, 2007.
Kave Eshghi and Shyamsundar Rajaram. Locality sensitive hash functions based on concomitant rank order statistics. In KDD, 2008.
Nir Ailon and Bernard Chazelle. The fast JohnsonLindenstrauss transform and approximate nearest neighbors. SIAM Journal on Computing, 39(1):302322, 2009.
Kilian Q. Weinberger, Anirban Dasgupta, John Langford, Alexander J. Smola, and Josh Attenberg. Feature hashing for large scale multitask learning. In ICML, 2009.
Rajeev Motwani, Assaf Naor, and Rina Panigrahy. Lower bounds on locality sensitive hashing. SIAM Journal on Discrete Mathematics, 21(4):930935, 2007.
Ryan ODonnell, Yi Wu, and Yuan Zhou. Optimal lower bounds for locality-sensitive hashing (except when q is tiny). ACM Transactions on Computation Theory, 6(1):5, 2014.
Anirban Dasgupta, Ravi Kumar, and Tams Sarls. Fast locality-sensitive hashing. In KDD, 2011.
Nir Ailon and Holger Rauhut. Fast and RIP-optimal transforms. Discrete & Computational Geometry, 52(4):780798, 2014.
Uriel Feige and Gideon Schechtman. On the optimality of the random hyperplane rounding technique for MAX CUT. Random Structures and Algorithms, 20(3):403440, 2002.
Malcolm Slaney, Yury Lifshits, and Junfeng He. Optimal parameters for locality-sensitive hashing. Proceedings of the IEEE, 100(9):26042623, 2012.
Moshe Lichman. UCI machine learning repository, 2013.
Persi Diaconis and David Freedman. A dozen de Finetti-style results in search of a theory. Annales de linstitut Henri Poincar (B) Probabilits et Statistiques, 23(S2):397423, 1987.  9
Yoshua Bengio, Aaron C. Courville, and Pascal Vincent. Representation learning: A review and new perspectives. Technical report, University of Montreal, 2012.
Charles F. Cadieu and Bruno A. Olshausen. Learning intermediate-level representations of form and motion from natural movies. Neural Computation, 2012.
Taco S Cohen and Max Welling. Transformation properties of learned visual representations. arXiv preprint arXiv:1412.7659, 2014.
Ross Goroshin, Joan Bruna, Jonathan Tompson, David Eigen, and Yann LeCun. Unsupervised learning of spatiotemporally coherent metrics. arXiv preprint arXiv:1412.6056, 2014.
Geoffrey E Hinton, Alex Krizhevsky, and Sida D Wang. Transforming auto-encoders. In Artificial Neural Networks and Machine LearningICANN 2011, pages 4451. Springer, 2011.
Christoph Kayser, Wolfgang Einhauser, Olaf Dummer, Peter Konig, and Konrad Kding. Extracting slow subspaces from natural videos leads to complex cells. In ICANN2001, 2001.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, volume 1, page 4, 2012.
Hossein Mobahi, Ronana Collobert, and Jason Weston. Deep learning from temporal coherence in video. In ICML, 2009.
Bruno A Olshausen and David J Field. Sparse coding of sensory inputs. Current opinion in neurobiology, 14(4):481487, 2004.
Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. Learning and transferring midlevel image representations using convolutional neural networks. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 17171724. IEEE, 2014.
M Ranzato, Fu Jie Huang, Y-L Boureau, and Yann LeCun. Unsupervised learning of invariant feature hierarchies with applications to object recognition. In Computer Vision and Pattern Recognition, 2007. CVPR07. IEEE Conference on, pages 18. IEEE, 2007.
MarcAurelio Ranzato, Arthur Szlam, Joan Bruna, Michael Mathieu, Ronan Collobert, and Sumit Chopra. Video (language) modeling: a baseline for generative models of natural videos. arXiv preprint arXiv:1412.6604, 2014.
Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Anticipating the future by watching unlabeled video. arXiv preprint arXiv:1504.08023, 2015.
Xiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using videos. arXiv preprint arXiv:1505.00687, 2015.
Laurenz Wiskott and Terrence J. Sejnowski. Slow feature analysis: Unsupervised learning of invariances. Neural Computation, 2002.
Matthew D Zeiler, Dilip Krishnan, Graham W Taylor, and Robert Fergus. Deconvolutional networks. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 25282535. IEEE, 2010.  9
S. Ahn, A. Korattikara, and M. Welling. Bayesian posterior sampling via stochastic gradient fisher
scoring. In ICML 2012, 2012.
F. Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with convergence
rate o(1/n). In Advances in Neural Information Processing Systems 26 (NIPS), pages 773781.
B. Cousins and S. Vempala. Bypassing kls: Gaussian cooling and an o (n3 ) volume algorithm.
Arxiv preprint arXiv:1409.6011, 2014.
A. Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave densities. Arxiv preprint arXiv:1412.7392, 2014.
M. Dyer, A. Frieze, and R. Kannan. A random polynomial-time algorithm for approximating the
volume of convex bodies. Journal of the ACM (JACM), 38(1):117, 1991.
R. Kannan and H. Narayanan. Random walks on polytopes and an affine interior point method for
linear programming. Mathematics of Operations Research, 37:120, 2012.
L. Lovasz and S. Vempala. Hit-and-run from a corner. SIAM J. Comput., 35(4):9851005, 2006.
L. Lovasz and S. Vempala. The geometry of logconcave functions and sampling algorithms. Random
Structures & Algorithms, 30(3):307358, 2007.
A. Nemirovski and D. Yudin. Problem Complexity and Method Efficiency in Optimization. Wiley
Interscience, 1983.
G. Pflug. Stochastic minimization with constant step-size: asymptotic laws. SIAM J. Control and
Optimization, 24(4):655666, 1986.
H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical Statistics,
22:400407, 1951.
A. Skorokhod. Stochastic equations for diffusion processes in a bounded region. Theory of Probability & Its Applications, 6(3):264274, 1961.
H. Tanaka. Stochastic differential equations with reflecting boundary condition in convex regions.
Hiroshima Mathematical Journal, 9(1):163177, 1979.
L. Tweedie and G. Roberts. Exponential convergence of langevin distributions and their discrete
approximations. Bernoulli, 2(4):341363, 1996.
M. Welling and Y.W. Teh. Bayesian learning via stochastic gradient langevin dynamics. In ICML
2011, 2011.
Liberated pixel cup. http://lpc.opengameart.org/. Accessed: 2015-05-21.
P. Bartha. Analogy and analogical reasoning. In The Stanford Encyclopedia of Philosophy. Fall 2013 edition, 2013.
P. Benard, F. Cole, M. Kass, I. Mordatch, J. Hegarty, M. S. Senn, K. Fleischer, D. Pesare, and K. Breeden. Stylizing animation by example. ACM Transactions on Graphics, 32(4):119, 2013.
B. Cheung, J. A. Livezey, A. K. Bansal, and B. A. Olshausen. Discovering hidden factors of variation in deep networks. In ICLR Workshop, 2015.
T. Cohen and M. Welling. Learning the irreducible representations of commutative Lie groups. In ICML, 2014.
T. Cohen and M. Welling. Transformation properties of learned visual representations. In ICLR, 2015.
G. Desjardins, A. Courville, and Y. Bengio. Disentangling factors of variation via generative entangling. arXiv preprint arXiv:1210.5474, 2012.
W. Ding and G. W. Taylor. Mental rotation by optimizing transforming distance. arXiv preprint arXiv:1406.3010, 2014.
P. Dollar, V. Rabaud, and S. Belongie. Learning to traverse image manifolds. In NIPS, 2007.
A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning to generate chairs with convolutional neural networks. In CVPR, 2015.
S. Fidler, S. Dickinson, and R. Urtasun. 3d object detection and viewpoint estimation with a deformable 3d cuboid model. In NIPS, 2012.
A. Hertzmann, C. Jacobs, N. Oliver, B. Curless, and D. Salesin. Image analogies. In SIGGRAPH, 2001.
S. J. Hwang, K. Grauman, and F. Sha. Analogy-preserving semantic embedding for visual object categorization. In NIPS, 2013.
Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.
D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In ICLR, 2014.
D. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling. Semi-supervised learning with deep generative models. In NIPS, 2014.
T. D. Kulkarni, W. Whitney, P. Kohli, and J. B. Tenenbaum. Deep convolutional inverse graphics network. In NIPS, 2015.
O. Levy, Y. Goldberg, and I. Ramat-Gan. Linguistic regularities in sparse and explicit word representations. In CoNLL-2014, 2014.
R. Memisevic and G. E. Hinton. Learning to represent spatial transformations with factored higher-order boltzmann machines. Neural Computation, 22(6):14731492, 2010.
V. Michalski, R. Memisevic, and K. Konda. Modeling deep temporal dependencies with recurrent grammar cells. In NIPS, 2014.
T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS, 2013.
J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In EMNLP, 2014.
S. Reed, K. Sohn, Y. Zhang, and H. Lee. Learning to disentangle factors of variation with manifold interaction. In ICML, 2014.
S. Rifai, Y. Bengio, A. Courville, P. Vincent, and M. Mirza. Disentangling factors of variation for facial expression recognition. In ECCV. 2012.
J. Susskind, R. Memisevic, G. Hinton, and M. Pollefeys. Modeling the joint density of two images under a variety of transformations. In CVPR, 2011.
Y. Tang, R. Salakhutdinov, and G. Hinton. Tensor analyzers. In ICML, 2013.
J. B. Tenenbaum and W. T. Freeman. Separating style and content with bilinear models. Neural computation, 12(6):12471283, 2000.
P. D. Turney. Similarity of semantic relations. Computational Linguistics, 32(3):379416, 2006.
J. Yang, S. Reed, M.-H. Yang, and H. Lee. Weakly-supervised disentangling with recurrent transformations for 3d view synthesis. In NIPS, 2015.
Z. Zhu, P. Luo, X. Wang, and X. Tang. Multi-view perceptron: a deep model for learning face identity and view representations. In NIPS, 2014.  9
E. J. Cands and B. Recht, Exact matrix completion via convex optimization, Foundations of Computational mathematics, vol. 9, no. 6, pp. 717772, 2009.
E. J. Cands and T. Tao, The power of convex relaxation: Near-optimal matrix completion, Information Theory, IEEE Transactions on, vol. 56, no. 5, pp. 20532080, 2010.
R. H. Keshavan, A. Montanari, and S. Oh, Matrix completion from a few entries, Information Theory, IEEE Transactions on, vol. 56, no. 6, pp. 29802998, 2010.
D. Gross, Y.-K. Liu, S. T. Flammia, S. Becker, and J. Eisert, Quantum state tomography via compressed sensing, Physical review letters, vol. 105, no. 15, p. 150401, 2010.
A. Saade, F. Krzakala, and L. Zdeborov, Spectral clustering of graphs with the bethe hessian, in Advances in Neural Information Processing Systems, 2014, pp. 406414.
A. Saade, F. Krzakala, M. Lelarge, and L. Zdeborov, Spectral detection in the censored block model, IEEE International Symposium on Information Theory (ISIT2015), to appear, 2015.
J.-F. Cai, E. J. Cands, and Z. Shen, A singular value thresholding algorithm for matrix completion, SIAM Journal on Optimization, vol. 20, no. 4, pp. 19561982, 2010.
F. Krzakala, C. Moore, E. Mossel, J. Neeman, A. Sly, L. Zdeborov, and P. Zhang, Spectral redemption in clustering sparse networks, Proc. Natl. Acad. Sci., vol. 110, no. 52, pp. 20 93520 940, 2013.
C. Bordenave, M. Lelarge, and L. Massouli, Non-backtracking spectrum of random graphs: community detection and non-regular ramanujan graphs, 2015, arXiv:1501.06087.
J. J. Hopfield, Neural networks and physical systems with emergent collective computational abilities, Proc. Nat. Acad. Sci., vol. 79, no. 8, pp. 25542558, 1982.
J. S. Yedidia, W. T. Freeman, and Y. Weiss, Bethe free energy, kikuchi approximations, and belief propagation algorithms, Advances in neural information processing systems, vol. 13, 2001.
M. Mezard and A. Montanari, Information, Physics, and Computation. Oxford University Press, 2009.
D. J. Amit, H. Gutfreund, and H. Sompolinsky, Spin-glass models of neural networks, Physical Review A, vol. 32, no. 2, p. 1007, 1985.
B. Wemmenhove and A. Coolen, Finite connectivity attractor neural networks, Journal of Physics A: Mathematical and General, vol. 36, no. 37, p. 9617, 2003.
I. P. Castillo and N. Skantzos, The littlehopfield model on a sparse random graph, Journal of Physics A: Mathematical and General, vol. 37, no. 39, p. 9087, 2004.
P. Zhang, Nonbacktracking operator for the ising model and its applications in systems with multiple states, Physical Review E, vol. 91, no. 4, p. 042120, 2015.
J. M. Mooij and H. J. Kappen, Validity estimates for loopy belief propagation on binary real-world networks. in Advances in Neural Information Processing Systems, 2004, pp. 945952.
F. Ricci-Tersenghi, The bethe approximation for solving the inverse ising problem: a comparison with other inference methods, J. Stat. Mech.: Th. and Exp., p. P08015, 2012.
L. Zdeborov, Statistical physics of hard optimization problems, acta physica slovaca, vol. 59, no. 3, pp. 169303, 2009.
Y. Kabashima, F. Krzakala, M. Mzard, A. Sakata, and L. Zdeborov, Phase transitions and sample complexity in bayes-optimal matrix factorization, 2014, arXiv:1402.1298.
T. Rogers, I. P. Castillo, R. Khn, and K. Takeda, Cavity approach to the spectral density of sparse symmetric random matrices, Phys. Rev. E, vol. 78, no. 3, p. 031116, 2008.
C. Bordenave and M. Lelarge, Resolvent of large random graphs, Random Structures and Algorithms, vol. 37, no. 3, pp. 332352, 2010.
P. Jain, P. Netrapalli, and S. Sanghavi, Low-rank matrix completion using alternating minimization, in Proceedings of the forty-fifth annual ACM symposium on Theory of computing. ACM, 2013, pp. 665674.
M. Hardt, Understanding alternating minimization for matrix completion, in Foundations of Computer Science (FOCS), 2014 IEEE 55th Annual Symposium on. IEEE, 2014, pp. 651660.
D. C. Liu and J. Nocedal, On the limited memory bfgs method for large scale optimization, Mathematical programming, vol. 45, no. 1-3, pp. 503528, 1989.
S. G. Johnson, The nlopt nonlinear-optimization package, 2014.
R. H. Keshavan, A. Montanari, and S. Oh, Low-rank matrix completion with noisy observations: a quantitative comparison, in 47th Annual Allerton Conference on Communication, Control, and Computing, 2009, pp. 12161222.  9
M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proc. 20th Int. Conf. Mach. Learning (ICML), pages 928936, 2003.
A. Kalai and S. Vempala. Efficient algorithms for online decision problems. J. Comput. Sys. Sci., 71:291 307, 2005. Extended abstract in Proc. 16th Ann. Conf. Comp. Learning Theory (COLT), 2003.
A. Blum. On-line algorithms in machine learning. In A. Fiat and G. Woeginger, editors, Online algorithms, volume 1442 of LNCS, chapter 14, pages 306325. Springer Berlin Heidelberg, 1998.
S. Shalev-Shwartz. Online learning and online convex optimization. Found. Trends Mach. Learn., 4(2):107194, 2011.
E. Hazan. Introduction to online convex optimization. Internet draft available at http://ocobook. cs.princeton.edu, 2015.
E. Takimoto and M. Warmuth. Path kernels and multiplicative updates. J. Mach. Learn. Research, 4:773 818, 2003.  8
D. Helmbold and R. Schapire. Predicting nearly as well as the best pruning of a decision tree. Mach. Learn. J., 27(1):6168, 1997.
A. Blum, S. Chawla, and A. Kalai. Static optimality and dynamic search optimality in lists and trees. Algorithmica, 36(3):249260, 2003.
T. M. Cover. Universal portfolios. Math. Finance, 1(1):129, 1991.
K. Crammer and Y. Singer. A family of additive online algorithms for category ranking. J. Mach. Learn. Research, 3:10251058, 2003.
X. He, J. Pan, O. Jin, T. Xu, B. Liu, T. Xu, Y. Shi, A. Atallah, R. Herbrich, S. Bowers, and J. Quionero Candela. Practical lessons from predicting clicks on ads at facebook. In Proc. 20th ACM Conf. Knowl. Disc. and Data Mining (KDD), pages 19. ACM, 2014.
S. Amuru and R. M. Buehrer. Optimal jamming using delayed learning. In 2014 IEEE Military Comm. Conf. (MILCOM), pages 15281533. IEEE, 2014.
I. Menache, O. Shamir, and N. Jain. On-demand, spot, or both: Dynamic resource allocation for executing batch jobs in the cloud. In 11th Int. Conf. on Autonomic Comput. (ICAC), 2014.
M.J. Weinberger and E. Ordentlich. On delayed prediction of individual sequences. IEEE Trans. Inf. Theory, 48(7):19591976, 2002.
P. Joulani, A. Gyrgy, and C. Szepesvri. Online learning under delayed feedback. In Proc. 30th Int. Conf. Mach. Learning (ICML), volume 28, 2013.
D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed Computation: Numerical Methods. PrenticeHall, 1989.
B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: a lock-free approach to parallelizing stochastic gradient descent. In Adv. Neural Info. Proc. Sys. 24 (NIPS), pages 693701, 2011.
J. Duchi, M.I. Jordan, and B. McMahan. Estimation, optimization, and parallelism when data is sparse. In Adv. Neural Info. Proc. Sys. 26 (NIPS), pages 28322840, 2013.
H.B. McMahan and M. Streeter. Delay-tolerant algorithms for asynchronous distributed online learning. In Adv. Neural Info. Proc. Sys. 27 (NIPS), pages 29152923, 2014.
J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. J. Mach. Learn. Research, 12:21212159, July 2011.
J. Langford, A. J. Smola, and M. Zinkevich. Slow learners are fast. In Adv. Neural Info. Proc. Sys. 22 (NIPS), pages 23312339, 2009.
J. Liu, S. J. Wright, C. R, V. Bittorf, and S. Sridhar. An asynchronous parallel stochastic coordiante descent algorithm. J. Mach. Learn. Research, 16:285322, 2015.
J. C. Duchi, T. Chaturapruek, and C. R. Asynchronous stochastic convex optimization. abs/1508.00882, 2015. To appear in Adv. Neural Info. Proc. Sys. 28 (NIPS), 2015.  CoRR,
S. J. Wright. Coordinate descent algorithms. Math. Prog., 151(334), 2015.
D. Riabko. On the flexibility of theoretical models for pattern recognition. PhD thesis, University of London, April 2005.
N. Cesa-Bianchi, Y. Freund, D. Haussler, D.P. Helmbold, R.E. Schapire, and M.K. Warmuth. How to use expert advice. J. Assoc. Comput. Mach., 44(3):426485, 1997.  9
F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. Optimization with sparsity-inducing penalties. Foundations and Trends in Machine Learning, 4(1):1106, Jan. 2012.
H. H. Bauschke and P. L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces. Springer, 2011.
M. Bazaraa, H. Sherali, and C. Shetty. Nonlinear Programming: Theory and Algorithms. WileyInterscience, 2006.
J. Borwein and A. Lewis. Convex Analysis and Nonlinear Optimization, Second Edition. Canadian Mathematical Society, 2006.
S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
X. Chen, Q. Lin, S. Kim, J. Carbonell, and E. Xing. Smoothing proximal gradient method for general structured sparse regression. Annals of Applied Statistics, pages 719752, 2012.
W. Deng, W. Yin, and Y. Zhang. Group sparse optimization by alternating direction method. Technical report, Rice CAAM Report TR11-06, 2011.
L. El Ghaoui, V. Viallon, and T. Rabbani. Safe feature elimination in sparse supervised learning. Pacific Journal of Optimization, 8:667698, 2012.
J.-B. Hiriart-Urruty. From convex optimization to nonconvex optimization. necessary and sufficient conditions for global optimality. In Nonsmooth optimization and related topics. Springer, 1988.
R. Jenatton, A. Gramfort, V. Michel, G. Obozinski, E. Eger, F. Bach, and B. Thirion. Multiscale mining of fmri data with hierarchical structured sparsity. SIAM Journal on Imaging Science, pages 835856, 2012.
R. Jenatton, J. Mairal, G. Obozinski, and F. Bach. Proximal methods for hierarchical sparse coding. Journal of Machine Learning Research, 12:22972334, 2011.
K. Jia, T. Chan, and Y. Ma. Robust and practical face recognition via structured sparsity. In European Conference on Computer Vision, 2012.
S. Kim and E. Xing. Tree-guided group lasso for multi-task regression with structured sparsity. In International Conference on Machine Learning, 2010.
S. Kim and E. Xing. Tree-guided group lasso for multi-response regression with structured sparsity, with an application to eqtl mapping. The Annals of Applied Statistics, 2012.
J. Liu, S. Ji, and J. Ye. SLEP: Sparse Learning with Efficient Projections. Arizona State University, 2009.
J. Liu and J. Ye. Moreau-Yosida regularization for grouped tree structure learning. In Advances in neural information processing systems, 2010.
J. Liu, Z. Zhao, J. Wang, and J. Ye. Safe screening with variational inequalities and its application to lasso. In International Conference on Machine Learning, 2014.
M. Liu, D. Zhang, P. Yap, and D. Shen. Tree-guided sparse coding for brain disease classification. In Medical Image Computing and Computer-Assisted Intervention, 2012.
K. Ogawa, Y. Suzuki, and I. Takeuchi. Safe screening of non-support vectors in pathwise SVM computation. In ICML, 2013.
A. Ruszczynski. Nonlinear Optimization. Princeton University Press, 2006.
R. Tibshirani, J. Bien, J. Friedman, T. Hastie, N. Simon, J. Taylor, and R. Tibshirani. Strong rules for discarding predictors in lasso-type problems. Journal of the Royal Statistical Society Series B, 74:245 266, 2012.
J. Wang, W. Fan, and J. Ye. Fused lasso screening rules via the monotonicity of subdifferentials. IEEE Transactions on Pattern Analysis and Machine Intelligence, PP(99):11, 2015.
J. Wang, P. Wonka, and J. Ye. Scaling svm and least absolute deviations via exact data reduction. In International Conference on Machine Learning, 2014.
J. Wang, P. Wonka, and J. Ye. Lasso screening rules via dual polytope projection. Journal of Machine Learning Research, 16:10631101, 2015.
J. Wang and J. Ye. Two-Layer feature reduction for sparse-group lasso via decomposition of convex sets. Advances in neural information processing systems, 2014.
Z. J. Xiang, H. Xu, and P. J. Ramadge. Learning sparse representation of high dimensional data on large scale dictionaries. In NIPS, 2011.
D. Yogatama, M. Faruqui, C. Dyer, and N. Smith. Learning word representations with hierarchical sparse coding. In International Conference on Machine Learning, 2015.
D. Yogatama and N. Smith. Linguistic structured sparsity in text categorization. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2014.
M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society Series B, 68:4967, 2006.
P. Zhao, G. Rocha, and B. Yu. The composite absolute penalties family for grouped and hierarchical variable selection. Annals of Statistics, 2009.
H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society Series B, 67:301320, 2005.  9
J. Yedidia, W. Freeman, and Y. Weiss, Constructing free-energy approximations and generalized belief propagation algorithms, IEEE Transactions on Information Theory, vol. 51, no. 7, pp. 2282  2312, 2005.
T. J. Richardson and R. L. Urbanke, Modern Coding Theory. Cambridge University Press, 2008.
M. Mezard and A. Montanari, Information, physics, and computation, ser. Oxford Graduate Texts. Oxford: Oxford Univ. Press, 2009. 8
M. J. Wainwright and M. I. Jordan, Graphical models, exponential families, and variational inference, Foundations and Trends in Machine Learning, vol. 1, no. 1, pp. 1305, 2008.
J. Gonzalez, Y. Low, and C. Guestrin. Residual splash for optimally parallelizing belief propagation, in International Conference on Artificial Intelligence and Statistics, 2009.
Y. Low, J. Gonzalez, A. Kyrola, D. Bickson, C. Guestrin, and J. M. Hellerstein, GraphLab: A New Parallel Framework for Machine Learning, in Conference on Uncertainty in Artificial Intelligence (UAI), 2010.
A. Kyrola, G. E. Blelloch, and C. Guestrin. GraphChi: Large-Scale Graph Computation on Just a PC, in Operating Systems Design and Implementation (OSDI), 2012.
R. Chandra, R. Menon, L. Dagum, D. Kohr, D. Maydan, and J. McDonald, Parallel Programming in OpenMP, Morgan Kaufmann, ISBN 1-55860-671-8, 2000.
M. Bayati, D. Shah, and M. Sharma, Max-product for maximum weight matching: Convergence, correctness, and lp duality, IEEE Transactions on Information Theory, vol. 54, no. 3, pp. 1241 1251, 2008.
S. Sanghavi, D. Malioutov, and A. Willsky, Linear Programming Analysis of Loopy Belief Propagation for Weighted Matching, in Neural Information Processing Systems (NIPS), 2007
B. Huang, and T. Jebara, Loopy belief propagation for bipartite maximum weight bmatching, in Artificial Intelligence and Statistics (AISTATS), 2007.
M. Bayati, C. Borgs, J. Chayes, R. Zecchina, Belief-Propagation for Weighted b-Matchings on Arbitrary Graphs and its Relation to Linear Programs with Integer Solutions, SIAM Journal in Discrete Math, vol. 25, pp. 9891011, 2011.
N. Ruozzi, Nicholas, and S. Tatikonda, st Paths using the min-sum algorithm, in 46th Annual Allerton Conference on Communication, Control, and Computing, 2008.
S. Sanghavi, D. Shah, and A. Willsky, Message-passing for max-weight independent set, in Neural Information Processing Systems (NIPS), 2007.
D. Gamarnik, D. Shah, and Y. Wei, Belief propagation for min-cost network flow: convergence & correctness, in SODA, pp. 279292, 2010.
S. Park, and J. Shin, Max-Product Belief Propagation for Linear Programming: Convergence and Correctness, arXiv preprint arXiv:1412.4972, to appear in Conference on Uncertainty in Artificial Intelligence (UAI), 2015.
M. Trick. Networks with additional structured constraints, PhD thesis, Georgia Institute of Technology, 1978.
M. Padberg, and M. Rao. Odd minimum cut-sets and b-matchings, in Mathematics of Operations Research, vol. 7, no. 1, pp. 6780, 1982.
M. Grotschel, and O. Holland. Solving matching problems with linear programming, in Mathematical Programming, vol. 33, no. 3, pp. 243259, 1985.
M Fischetti, and A. Lodi. Optimizing over the first Chvatal closure, in Mathematical Programming, vol. 110, no. 1, pp. 320, 2007.
K. Chandrasekaran, L. A. Vegh, and S. Vempala. The cutting plane method is polynomial for perfect matchings, in Foundations of Computer Science (FOCS), 2012
V. Kolmogorov, Blossom V: a new implementation of a minimum cost perfect matching algorithm, Mathematical Programming Computation, vol. 1, no. 1, pp. 4367, 2009.
J. Edmonds, Paths, trees, and flowers, Canadian Journal of Mathematics, vol. 3, pp. 449 467, 1965.
D. Malioutov, J. Johnson, and A. Willsky, Walk-sums and belief propagation in gaussian graphical models, J. Mach. Learn. Res., vol. 7, pp. 2031-2064, 2006.
Y. Weiss, C. Yanover, and T Meltzer, MAP Estimation, Linear Programming and Belief Propagation with Convex Free Energies, in Conference on Uncertainty in Artificial Intelligence (UAI), 2007.
C. Moallemi and B. Roy, Convergence of min-sum message passing for convex optimization, in 45th Allerton Conference on Communication, Control and Computing, 2008.  9
Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8):3037, 2009.
Xiaoxue Zhao, Weinan Zhang, and Jun Wang. Interactive collaborative filtering. In Proceedings of the 22nd ACM international conference on Conference on information & knowledge management, pages 14111420. ACM, 2013.
Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In NIPS, pages 22492257, 2011.
Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In ICML (3), pages 127135, 2013.
Ruslan Salakhutdinov and Andriy Mnih. Probabilistic matrix factorization. In NIPS, volume 1, pages 21, 2007.
Ruslan Salakhutdinov and Andriy Mnih. Bayesian probabilistic matrix factorization using markov chain monte carlo. In ICML, pages 880887, 2008.
Nicolas Chopin. A sequential particle filter method for static models. Biometrika, 89(3):539 552, 2002.
Pierre Del Moral, Arnaud Doucet, and Ajay Jasra. Sequential monte carlo samplers. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(3):411436, 2006.
Arnaud Doucet, Nando De Freitas, Kevin Murphy, and Stuart Russell. Rao-blackwellised particle filtering for dynamic bayesian networks. In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence, pages 176183. Morgan Kaufmann Publishers Inc., 2000.
A. Gelman and X. L Meng. A note on bivariate distributions that are conditionally normal. Amer. Statist., 45:125126, 1991.
B. C. Arnold, E. Castillo, J. M. Sarabia, and L. Gonzalez-Vega. Multiple modes in densities with normal conditionals. Statist. Probab. Lett., 49:355363, 2000.
B. C. Arnold, E. Castillo, and J. M. Sarabia. Conditionally specified distributions: An introduction. Statistical Science, 16(3):249274, 2001.
Aditya Gopalan, Shie Mannor, and Yishay Mansour. Thompson sampling for complex online problems. In Proceedings of The 31st International Conference on Machine Learning, pages 100108, 2014.
Michal Valko, Remi Munos, Branislav Kveton, and Tomas Kocak. Spectral bandits for smooth graph functions. In 31th International Conference on Machine Learning, 2014.
Tomas Kocak, Michal Valko, Remi Munos, and Shipra Agrawal. Spectral thompson sampling. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, 2014.
Claudio Gentile, Shuai Li, and Giovanni Zappella. Online clustering of bandits. arXiv preprint arXiv:1401.8257, 2014.
Odalric-Ambrym Maillard and Shie Mannor. Latent bandits. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pages 136144, 2014.  9
J. R. Angelos, C. C. Cowen, and S. K. Narayan. Triangular truncation and finding the norm of a hadamard multiplier. Linear Algebra and its Applications, 170:117  135, 1992.
A. Beck. On the convergence of alternating minimization with applications to iteratively reweighted least squares and decomposition schemes. SIAM Journal on Optimization, 25(1):185209, 2015.
A. Beck, E. Pauwels, and S. Sabach. The cyclic block coordinate gradient method for convex optimization problems. 2015. Preprint, available on arXiv:1502.03716v1.
A. Beck and L. Tetruashvili. On the convergence of block coordinate descent type methods. SIAM Journal on Optimization, 23(4):20372060, 2013.
D. P. Bertsekas. Nonlinear Programming, 2nd ed. Athena Scientific, Belmont, MA, 1999.
L. Grippo and M. Sciandrone. On the convergence of the block nonlinear Gauss-Seidel method under convex constraints. Operations Research Letters, 26:127136, 2000.
M. Hong, X. Wang, M. Razaviyayn, and Z.-Q. Luo. Iteration complexity analysis of block coordinate descent methods. 2013. Preprint, available online arXiv:1310.6957.
Z. Lu and L. Xiao. On the complexity analysis of randomized block-coordinate descent methods. 2013. accepted by Mathematical Programming.
Z. Lu and L. Xiao. Randomized block coordinate non-monotone gradient method for a class of nonlinear programming. 2013. Preprint.
Z.-Q. Luo and P. Tseng. On the convergence of the coordinate descent method for convex differentiable minimization. Journal of Optimization Theory and Application, 72(1):735, 1992.
Y. Nesterov. Introductory lectures on convex optimization: A basic course. Springer, 2004.
Y. Nesterov. Efficiency of coordiate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341362, 2012.
J. Nutini, M. Schmidt, I. H. Laradji, M. Friedlander, and H. Koepke. Coordinate descent converges faster with the Gauss-Southwell rule than random selection. In the Proceeding of the 30th International Conference on Machine Learning (ICML), 2015.
M. J. D. Powell. On search directions for minimization algorithms. Mathematical Programming, 4:193201, 1973.
M. Razaviyayn, M. Hong, and Z.-Q. Luo. A unified convergence analysis of block successive minimization methods for nonsmooth optimization. SIAM Journal on Optimization, 23(2):11261153, 2013.
M. Razaviyayn, M. Hong, Z.-Q. Luo, and J. S. Pang. Parallel successive convex approximation for nonsmooth nonconvex optimization. In the Proceedings of the Neural Information Processing (NIPS), 2014.
P. Richtarik and M. Takac. Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function. Mathematical Programming, 144:138, 2014.
A. Saha and A. Tewari. On the nonasymptotic convergence of cyclic coordinate descent method. SIAM Journal on Optimization, 23(1):576601, 2013.
P. Tseng. Convergence of a block coordinate descent method for nondifferentiable minimization. Journal of Optimization Theory and Applications, 103(9):475494, 2001.
Y. Xu and W. Yin. A block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor factorization and completion. SIAM Journal on Imaging Sciences, 6(3):17581789, 2013.  9
Fadi A. Aloul, Igor L. Markov, and Karem A. Sakallah. Shatter: efficient symmetry-breaking for boolean satisfiability. In Proc. of DAC, pages 836839, 2003.
Udi Apsel, Kristian Kersting, and Martin Mladenov. Lifting relational MAP-LPs using cluster signatures. In Proc. of AAAI, pages 24032409, 2014.
Gilles Audemard, Belaid Benhamou, and Laurent Henocque. Predicting and detecting symmetries in FOL finite model search. J. Automated Reasoning, 36(3):177212, 2006.
Hung B. Bui, Tuyen N. Huynh, and Rodrigo de Salvo Braz. Exact lifted inference with distinct soft evidence on every object. In Proc. of AAAI, 2012.  8
Hung Hai Bui, Tuyen N. Huynh, and Sebastian Riedel. Automorphism groups of graphical models and lifted variational inference. In Proc. of UAI, pages 132141, 2013.
Vasek Chvatal and Endre Szemeredi. Many hard examples for resolution. J. ACM, 35(4):759768, 1988.
James Crawford, Matthew Ginsberg, Eugene Luks, and Amitabha Roy. Symmetry-breaking predicates for search problems. In Proc. of KR, pages 148159, 1996.
Rodrigo de Salvo Braz, Eyal Amir, and Dan Roth. Lifted first-order probabilistic inference. In Proc. of IJCAI, pages 13191325, 2005.
Guy Van den Broeck and Adnan Darwiche. On the complexity and approximation of binary evidence in lifted inference. In Proc. of NIPS, pages 28682876, 2013.
Guy Van den Broeck and Jesse Davis. Conditioning in first-order knowledge compilation and lifted probabilistic inference. In Proc. of AAAI, 2012.
Guy Van den Broeck, Nima Taghipour, Wannes Meert, Jesse Davis, and Luc De Raedt. Lifted probabilistic inference by first-order knowledge compilation. In Proc. of IJCAI, pages 21782185, 2011.
Pedro Domingos and Daniel Lowd. Markov Logic: An Interface Layer for Artificial Intelligence. Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers, 2009.
Pierre Flener, Justin Pearson, Meinolf Sellmann, Pascal Van Hentenryck, and Magnus Agren. Dynamic structural symmetry breaking for constraint satisfaction problems. Constraints, 14(4):506538, 2009.
Vibhav Gogate and Pedro M. Domingos. Probabilistic theorem proving. In Proc. of UAI, pages 256265, 2011.
Federico Heras, Javier Larrosa, and Albert Oliveras. MiniMaxSAT: An Efficient Weighted Max-SAT solver. J. Artificial Intelligence Research, 31:132, 2008.
Hadi Katebi, Karem A. Sakallah, and Igor L. Markov. Symmetry and satisfiability: An update. In Theory and Applications of Satisfiability Testing, volume 6175, pages 113127, 2010.
Daniel Le Berre and Anne Parrain. The Sat4j library, release 2.2. JSAT, 7:5964, 2010.
E. M. Luks. Isomorphism of graphs of bounded valence can be tested in polynomial time. J. of Computer System Science, 25:4265, 1982.
Ruben Martins, Vasco Manquinho, and Ines Lynce. Open-WBO: A modular maxsat solver,. In Theory and Applications of Satisfiability Testing, volume 8561 of Lecture Notes in Computer Science. 2014.
Pedro Meseguer and Carme Torras. Exploiting symmetries within constraint satisfaction search. Artificial Intelligence, 129(1-2):133163, 2001.
Happy Mittal, Prasoon Goyal, Vibhav G. Gogate, and Parag Singla. New rules for domain independent lifted MAP inference. In Proc. of NIPS, pages 649657, 2014.
Martin Mladenov, Babak Ahmadi, and Kristian Kersting. Lifted linear programming. In Proc, of AISTATS, pages 788797, 2012.
Martin Mladenov, Amir Globerson, and Kristian Kersting. Lifted message passing as reparametrization of graphical models. In Proc. of UAI, pages 603612, 2014.
Mathias Niepert. Markov chains on orbits of permutation groups. In Proc, of UAI, pages 624633, 2012.
Mathias Niepert. Symmetry-aware marginal density estimation. In Proc. of AAAI, 2013.
Jan Noessner, Mathias Niepert, and Heiner Stuckenschmidt. Rockit: Exploiting parallelism and symmetry for MAP inference in statistical relational models. In Proc. of AAAI, 2013.
David Poole. First-order probabilistic inference. In Proc. of IJCAI, pages 985991, 2003.
Meinolf Sellmann and Pascal Van Hentenryck. Structural symmetry breaking. In Proc. of IJCAI, pages 298303, 2005.
Parag Singla and Pedro Domingos. Lifted first-order belief propagation. In Proc. of AAAI, pages 1094 1099, 2008.
Parag Singla, Aniruddh Nath, and Pedro M. Domingos. Approximate lifting techniques for belief propagation. In Proc. of AAAI, pages 24972504, 2014.
Deepak Venugopal and Vibhav Gogate. Evidence-based clustering for scalable inference in Markov logic. In Proc. of ECML/PKDD, pages 258273, 2014.
Toby Walsh. Symmetry breaking constraints: Recent results. In Proc. of AAAI, 2012.  9
Louigi Addario-Berry, Nicolas Broutin, Luc Devroye, Gabor Lugosi, et al. On combinatorial testing problems. The Annals of Statistics, 38(5):30633092, 2010.
Brendan PW Ames. Guaranteed clustering and biclustering via semidefinite programming. Mathematical Programming, pages 137, 2012.
Brendan PW Ames and Stephen A Vavasis. Convex optimization for the planted k-disjointclique problem. Mathematical Programming, 143(1-2):299337, 2014.
Ery Arias-Castro, Emmanuel J Candes, Arnaud Durand, et al. Detection of an anomalous cluster in a network. The Annals of Statistics, 39(1):278304, 2011.
Sivaraman Balakrishnan, Mladen Kolar, Alessandro Rinaldo, Aarti Singh, and Larry Wasserman. Statistical and computational tradeoffs in biclustering. In NIPS 2011 Workshop on Computational Trade-offs in Statistical Learning, 2011.
Shankar Bhamidi, Partha S Dey, and Andrew B Nobel. Energy landscape for large average submatrix detection problems in gaussian random matrices. arXiv preprint arXiv:1211.2284, 2012.
Yudong Chen and Jiaming Xu. Statistical-computational tradeoffs in planted problems and submatrix localization with a growing number of clusters and submatrices. arXiv preprint arXiv:1402.1267, 2014.
Yizong Cheng and George M Church. Biclustering of expression data. In ISMB, volume 8, pages 93103, 2000.
Laura Lazzeroni and Art Owen. Plaid models for gene expression data. Statistica Sinica, 12(1):6186, 2002.
Jason D Lee, Dennis L Sun, Yuekai Sun, and Jonathan E Taylor. Exact post-selection inference with the lasso. arXiv preprint arXiv:1311.6238, 2013.
Zongming Ma and Yihong Wu. Computational barriers in minimax submatrix detection. arXiv preprint arXiv:1309.5914, 2013.
Andrey A Shabalin, Victor J Weigman, Charles M Perou, and Andrew B Nobel. Finding large average submatrices in high dimensional data. The Annals of Applied Statistics, pages 9851012, 2009.  9
A. Bellet and A. Habrard. Robustness and generalization for metric learning. Neurocomputing, 151(14):259267, 2015.
D. Chen, X. Cao, L. Wang, F. Wen, and J. Sun. Bayesian face revisited: A joint formulation. In European Conference on Computer Vision (ECCV), 2012.
D. Chen, X. Cao, F. Wen, and J. Sun. Blessing of dimensionality: High-dimensional feature and its efficient compression for face verification. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013.
K. Fukunaga. Introduction to Statistical Pattern Recognition. San Diego: Academic Press, 1990.
A. Globerson and S. Roweis. Metric learning by collapsing classes. In Advances in Neural Information Processing Systems (NIPS), 2005.
J. Goldberger, S. Roweis, G. Hinton, and R. Salakhutdinov. Neighbourhood components analysis. In Advances in Neural Information Processing Systems (NIPS), 2004.
J. Hu, J. Lu, and Y. Tan. Discriminative deep metric learning for face verification in the wild. In Computer Vision and Pattern Recognition (CVPR), pages 18751882, 2014.
G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical Report 0749, University of Massachusetts, Amherst, October 2007.
J. Huang, Q. Qiu, R. Calderbank, and G. Sapiro. Geometry-aware deep transform. In International Conference on Computer Vision, 2015.
G. Sapiro Q. Qiu. Learning transformations for clustering and classification. Journal of Machine Learning Research (JMLR), pages 187225, 2015.
C. Sumit, R. Hadsell, and Y. LeCun. Learning a similarity metric discriminatively, with application to face verification. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 1, pages 539546, 2005.
Y. Sun, Y. Chen, X. Wang, and X. Tang. Deep learning face representation by joint identification-verification. In Advances in Neural Information Processing Systems (NIPS), pages 19881996, 2014.
Y. Sun, X. Wang, and X. Tang. Deep learning face representation from predicting 10,000 classes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 18911898, 2014.
Y. Taigman, M. Yang, M. A. Ranzato, and L. Wolf. Deepface: Closing the gap to humanlevel performance in face verification. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 17011708, 2014.
V. N. Vapnik. An overview of statistical learning theory. IEEE Transactions on Neural Networks, 10(5):988999, 1999.
K. Q. Weinberger and L. K. Saul. Distance metric learning for large margin nearest neighbor classification. Journal of Machine Learning Research, 10:207244, 2009.
E. P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell. Distance metric learning, with application to clustering with side-information. In Advances in Neural Information Processing Systems (NIPS), 2002.
H. Xu and S. Mannor. Robustness and generalization. Machine Learning, 86(3):391423, 2012.
Z. Zha, T. Mei, M. Wang, Z. Wang, and X. Hua. Robust distance metric learning with auxiliary knowledge. In International Joint Conference on Artificial Intelligence (IJCAI), 2009.  9
S. Agrawal and N. Goyal. Analysis of thompson sampling for the multi-armed bandit problem. CoRR, abs/1111.1797, 2011.
Cesa-Bianchi N. Freund Y. Auer, P. and R. E. Schapire. Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Foundations of Computer Science, 1995. Proceedings., 36th Annual Symposium on, pages 322331, Oct 1995.
Sebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends in Machine Learning, 5:1122, 2012.
R. Busa-Fekete and B. Kegl. Fast boosting using adversarial bandits. In T. Joachims J. Furnkranz, editor, 27th International Conference on Machine Learning (ICML 2010), pages 143150, Haifa, Israel, June 2010.
E. Bareinboim, A. Forney, and J. Pearl. Bandits with unobserved confounders: A causal approach. Technical Report R-460, <http://ftp.cs.ucla.edu/pub/stat ser/r460L.pdf>, Cognitive Systems Laboratory, UCLA, 2015.
S. Bubeck and A. Slivkins. The best of both worlds: stochastic and adversarial bandits. CoRR, abs/1202.4473, 2012.
O. Chapelle and L. Li. An empirical evaluation of thompson sampling. In J. ShaweTaylor, R.S. Zemel, P.L. Bartlett, F. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 22492257. Curran Associates, Inc., 2011.
Miroslav Dudk, Daniel Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev Reyzin, and Tong Zhang. Efficient optimal learning for contextual bandits. CoRR, abs/1106.2369, 2011.
Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. J. Mach. Learn. Res., 7:10791105, December 2006.
R.A. Fisher. The Design of Experiments. Oliver and Boyd, Edinburgh, 6th edition, 1951.
T.L Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics, 6(1):4  22, 1985.
John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side information. In J.C. Platt, D. Koller, Y. Singer, and S.T. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 817824. Curran Associates, Inc., 2008.
P. A. Ortega and D. A. Braun. A minimum relative entropy principle for learning and acting. J. Artif. Int. Res., 38(1):475511, May 2010.
J. Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, New York, 2000. Second ed., 2009.
Herbert Robbins. Some aspects of the sequential design of experiments. Bull. Amer. Math. Soc., 58(5):527535, 09 1952.
Yevgeny Seldin, Peter L. Bartlett, Koby Crammer, and Yasin Abbasi-Yadkori. Prediction with limited advice and multiarmed bandits with paid observations. In International Conference on Machine Learning, Beijing, China, 2014.
S. L. Scott. A modern bayesian look at the multi-armed bandit. Applied Stochastic Models in Business and Industry, 26(6):639658, 2010.
A. Slivkins. Contextual bandits with similarity information. J. Mach. Learn. Res., 15(1):25332568, January 2014.
I. Shpitser and J Pearl. What counterfactuals can be tested. In Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence (UAI 2007), pages 352359. AUAI Press, Vancouver, BC, Canada, 2007.  9
Robert E. Schapire and Yoav Freund. Boosting: Foundations and Algorithms. The MIT Press, 2012.
Leo Breiman. Bagging predictors. Machine learning, 24(2):123140, 1996.
Leo Breiman. Random forests. Machine learning, 45(1):532, 2001.
Yehuda Koren. The bellkor solution to the netflix grand prize. 2009.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 2015.
Robert E Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. Annals of statistics, pages 16511686, 1998.
Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-supervised learning.
Xiaojin Zhu and Andrew B Goldberg. Introduction to semi-supervised learning. Synthesis lectures on artificial intelligence and machine learning, 3(1):1130, 2009.
Akshay Balsubramani and Yoav Freund. Optimally combining classifiers using unlabeled data. In Conference on Learning Theory, 2015.
Rich Caruana, Nikos Karampatziakis, and Ainur Yessenalina. An empirical evaluation of supervised learning in high dimensions. In Proceedings of the 25th international conference on Machine learning, pages 96103. ACM, 2008.
Yi Lin and Yongho Jeon. Random forests and adaptive nearest neighbors. Journal of the American Statistical Association, 101(474):578590, 2006.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:21212159, 2011.
Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh Attenberg. Feature hashing for large scale multitask learning. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 11131120. ACM, 2009.
Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics, pages 11891232, 2001.
Yoav Freund and Robert E Schapire. Game theory, on-line prediction and boosting. In Proceedings of the ninth annual conference on Computational learning theory, pages 325332. ACM, 1996.
P Kumar Mallapragada, Rong Jin, Anil K Jain, and Yi Liu. Semiboost: Boosting for semi-supervised learning. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 31(11):20002014, 2009.
Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. J. Comput. Syst. Sci., 55(1):119139, 1997.
Thorsten Joachims. Transductive inference for text classification using support vector machines. In Proceedings of the Sixteenth International Conference on Machine Learning, pages 200209. Morgan Kaufmann Publishers Inc., 1999.
Alexander Philip Dawid and Allan M Skene. Maximum likelihood estimation of observer error-rates using the em algorithm. Applied statistics, pages 2028, 1979.
Fabio Parisi, Francesco Strino, Boaz Nadler, and Yuval Kluger. Ranking and combining multiple predictors without labeled data. Proceedings of the National Academy of Sciences, 111(4):12531258, 2014.
Frank Moosmann, Bill Triggs, and Frederic Jurie. Fast discriminative visual codebooks using randomized clustering forests. In Twentieth Annual Conference on Neural Information Processing Systems (NIPS06), pages 985992. MIT Press, 2007.
Yoav Freund, Robert E Schapire, Yoram Singer, and Manfred K Warmuth. Using and combining predictors that specialize. In Proceedings of the twenty-ninth annual ACM symposium on Theory of computing, pages 334343. ACM, 1997.
Predicting a Biological Response. 2012. https://www.kaggle.com/c/bioresponse.
Give Me Some Credit. 2011. https://www.kaggle.com/c/GiveMeSomeCredit.  9
Sebatien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1122, 2012.
Shie Mannor and Ohad Shamir. From bandits to experts: on the value of side-observations. In Advances in Neural Information Processing Systems 24 (NIPS), pages 684692, 2011.
Noga Alon, Nicolo Cesa-Bianchi, Claudio Gentile, and Yishay Mansour. From bandits to experts: A tale of domination and independence. In Advances in Neural Information Processing Systems 26 (NIPS), pages 16101618, 2013.
Tomas Kocak, Gergely Neu, Michal Valko, and Remi Munos. Efficient learning by implicit exploration in bandit problems with side observations. In Advances in Neural Information Processing Systems 27 (NIPS), pages 613621, 2014.
Noga Alon, Nicolo Cesa-Bianchi, Ofer Dekel, and Tomer Koren. Online learning with feedback graphs: beyond bandits. In Proceedings of The 28th Conference on Learning Theory (COLT), pages 2335, 2015.
Stephane Caron, Branislav Kveton, Marc Lelarge, and Smriti Bhagat. Leveraging side observations in stochastic bandits. In Proceedings of the 28th Conference on Uncertainty in Artificial Intelligence (UAI), pages 142151, 2012.
Swapna Buccapatnam, Atilla Eryilmaz, and Ness B. Shroff. Stochastic bandits with side observations on networks. SIGMETRICS Perform. Eval. Rev., 42(1):289300, June 2014.
Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge University Press, Cambridge, 2006.
Gabor Bartok, Dean P. Foster, David Pal, Alexander Rakhlin, and Csaba Szepesvari. Partial monitoring  classification, regret bounds, and algorithms. Mathematics of Operations Research, 39:967997, 2014.
Tian Lin, Bruno Abrahao, Robert Kleinberg, John Lui, and Wei Chen. Combinatorial partial monitoring game with linear feedback and its applications. In Proceedings of the 31st International Conference on Machine Learning (ICML), pages 901909, 2014.
Tor Lattimore, Andras Gyorgy, and Csaba Szepesvari. On learning the optimal waiting time. In Peter Auer, Alexander Clark, Thomas Zeugmann, and Sandra Zilles, editors, Algorithmic Learning Theory, volume 8776 of Lecture Notes in Computer Science, pages 200214. Springer International Publishing, 2014.
Todd L. Graves and Tze Leung Lai. Asymptotically efficient adaptive choice of control laws incontrolled markov chains. SIAM Journal on Control and Optimization, 35(3):715743, 1997.
Yifan Wu, Andras Gyorgy, and Csaba Szepesvari. Online learning with Gaussian payoffs and side observations. arXiv preprint arXiv:1510.08108, 2015.
Lihong Li, Remi Munos, and Csaba Szepesvari. Toward minimax off-policy value estimation. In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics (AISTATS), pages 608616, 2015.
Stefan Magureanu, Richard Combes, and Alexandre Proutiere. Lipschitz bandits: Regret lower bounds and optimal algorithms. In Proceedings of The 27th Conference on Learning Theory (COLT), pages 975999, 2014.
Emilie Kaufmann, Olivier Cappe, and Aurelien Garivier. On the complexity of best arm identification in multi-armed bandit models. The Journal of Machine Learning Research, 2015. (to appear).
Richard Combes and Alexandre Proutiere. Unimodal bandits: Regret lower bounds and optimal algorithms. In Proceedings of the 31st International Conference on Machine Learning (ICML), pages 521529, 2014.  9
E. Abbe and C. Sandon. Recovering communities in the general stochastic block model without knowing the parameters. arXiv:1503.00609, 2015.
E. Abbe and C. Sandon. Recovering communities in the general stochastic block model without knowing the parameters. Manuscript, 2015.
E. Abbe, A. S. Bandeira, and G. Hall. Exact recovery in the stochastic block model. arXiv:1405.3267, 2014.
P. J. Bickel and A. Chen. A nonparametric view of network models and newman-girvan and other modularities. Proceedings of the National Academy of Sciences of the United States of America, 106:21068 21073, 2009.
P. J. Bickel, A. Chen, and E. Levina. The method of moments and degree distributions for network models. Annals of Statistics, 39(5):22802301, 2011.
J. Blocki, A. Blum, A. Datta, and O. Sheffet. Differentially private data analysis of social networks via restricted sensitivity. In Innovations in Theoretical Computer Science (ITCS), pages 8796, 2013.
B. Bollobas, S. Janson, and O. Riordan. The phase transition in inhomogeneous random graphs. Random Struct. Algorithms, 31:3122, 2007.
C. Borgs, J. T. Chayes, L. Lovasz, V. Sos, and K. Vesztergombi. Counting graph homomorphisms. In Topics in Discrete Mathematics (eds. M. Klazar, J. Kratochvil, M. Loebl, J. Matousek, R. Thomas, P.Valtr), pages 315371. Springer, 2006.
C. Borgs, J. T. Chayes, L. Lovasz, V. Sos, and K. Vesztergombi. Convergent graph sequences I: Subgraph frequencies, metric properties, and testing. Advances in Math., 219:18011851, 2008.
C. Borgs, J. T. Chayes, L. Lovasz, V. Sos, and K. Vesztergombi. Convergent graph sequences II: Multiway cuts and statistical physics. Ann. of Math., 176:151219, 2012.
C. Borgs, J. T. Chayes, H. Cohn, and Y. Zhao. An Lp theory of sparse graph convergence I: limits, sparse random graph models, and power law distributions. arXiv:1401.2906, 2014.
C. Borgs, J. T. Chayes, H. Cohn, and Y. Zhao. An Lp theory of sparse graph convergence II: LD convergence, quotients, and right convergence. arXiv:1408.0744, 2014.
S. Chatterjee. Matrix estimation by universal singular value thresholding. Annals of Statistics, 43(1): 177214, 2015.
S. Chen and S. Zhou. Recursive mechanism: towards node differential privacy and unrestricted joins. In ACM SIGMOD International Conference on Management of Data, pages 653664, 2013.
D. S. Choi, P. J. Wolfe, and E. M. Airoldi. Stochastic blockmodels with a growing number of classes. Biometrika, 99:273284, 2012.
P. Diaconis and S. Janson. Graph limits and exchangeable random graphs. Rendiconti di Matematica, 28: 3361, 2008.
C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. In S. Halevi and T. Rabin, editors, TCC, volume 3876, pages 265284, 2006.
C. Gao, Y. Lu, and H. H. Zhou. Rate-optimal graphon estimation. arXiv:1410.5837, 2014.
P. D. Hoff, A. E. Raftery, and M. S. Handcock. Latent space approaches to social network analysis. Journal of the American Statistical Association, 97(460):10901098, 2002.
P. Holland, K. Laskey, and S. Leinhardt. Stochastic blockmodels: First steps. Soc Netw, 5:109137, 1983.
S. P. Kasiviswanathan, K. Nissim, S. Raskhodnikova, and A. Smith. Analyzing graphs with nodedifferential privacy. In Theory of Cryptography Conference (TCC), pages 457476, 2013.
O. Klopp, A. Tsybakov, and N. Verzelen. Oracle inequalities for network models and sparse graphon estimation. arXiv:1507.04118, 2015.
L. Lovasz and B. Szegedy. Limits of dense graph sequences. Journal of Combinatorial Theory, Series B, 96:933957, 2006.
W. Lu and G. Miklau. Exponential random graph estimation under differential privacy. In 20th ACM SIGKDD International Conference on Knowledge discovery and data mining, pages 921930, 2014.
F. McSherry and K. Talwar. Mechanism design via differential privacy. In FOCS, pages 94103. IEEE, 2007.
S. Raskhodnikova and A. Smith. High-dimensional Lipschitz extensions and node-private analysis of network data. arXiv:1504.07912, 2015.
K. Rohe, S. Chatterjee, and B. Yu. Spectral clustering and the high-dimensional stochastic blockmodel. Ann. Statist., 39(4):18781915, 08 2011.
P. Wolfe and S. C. Olhede. Nonparametric graphon estimation. arXiv:1309.5936, 2013.  9
B. Alexe, T. Deselaers, and V. Ferrari. Measuring the objectness of image windows. PAMI, 34(11):2189 2202, Nov 2012. 7
P. Arbelaez, J. P. Tuset, J. T.Barron, F. Marques, and J. Malik. Multiscale combinatorial grouping. In CVPR, 2014. 1, 2, 7
M. Blaschko. Branch and bound strategies for non-maximal suppression in object detection. In EMMCVPR, pages 385398, 2011. 1, 3, 5
M. B. Blaschko and C. H. Lampert. Learning to localize objects with structured output regression. In ECCV, 2008. 2
N. Buchbinder, M. Feldman, J. Naor, and R. Schwartz. A tight (1/2) linear-time approximation to unconstrained submodular maximization. In FOCS, 2012. 5  8
J. Carbonell and J. Goldstein. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR 98, pages 335336, 1998. 3
J. Carreira and C. Sminchisescu. Constrained parametric min-cuts for automatic object segmentation. In CVPR, 2010. 1, 2, 7
M.-M. Cheng, Z. Zhang, W.-Y. Lin, and P. Torr. Bing:binarized normed gradients for objectness estimation at 300fps. In CVPR, 2014. 1
N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005. 1, 3
T. Deselaers, B. Alexe, and V. Ferrari. Localizing objects while learning their appearance. In ECCV, 2010. 1
D. Dey, T. Liu, M. Hebert, and J. A. Bagnell. Contextual sequence prediction with application to control library optimization. In Robotics Science and Systems (RSS), 2012. 3, 4
E.L.Lawler and D.E.Wood. Branch-and-bound methods: A survey. Operations Research, 14(4):699719, 1966. 2
M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results. http://www.pascalnetwork.org/challenges/VOC/voc2007/workshop/index.html. 2, 6
M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. http://www.pascalnetwork.org/challenges/VOC/voc2012/workshop/index.html. 2, 6
U. Feige, V. Mirrokni, and J. Vondrk. Maximizing non-monotone submodular functions. In FOCS, 2007. 5
P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part based models. PAMI, 32(9):16271645, 2010. 1, 3
R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014. 1, 3
A. Gonzalez-Garcia, A. Vezhnevets, and V. Ferrari. An active search strategy for efficient object detection. In CVPR, 2015. 3
K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV, 2014. 1, 3
J. Hosang, R. Benenson, and B. Schiele. How good are detection proposals, really? In BMVC, 2014. 3, 7
T. Joachims, T. Finley, and C.-N. Yu. Cutting-plane training of structural svms. Machine Learning, 77(1):2759, 2009. 2
D. Kempe, J. Kleinberg, and E. Tardos. Maximizing the spread of influence through a social network. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), 2003. 3
P. Krahenbuhl and V. Koltun. Learning to propose objects. In CVPR, 2015. 7
A. Krause and D. Golovin. Submodular function maximization. In Tractability: Practical Approaches to Hard Problems (to appear). Cambridge University Press, 2014. 2
A. Krause, A. Singh, and C. Guestrin. Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies. J. Mach. Learn. Res., 9:235284, 2008. 3
C. H. Lampert, M. B. Blaschko, and T. Hofmann. Efficient subwindow search: A branch and bound framework for object localization. TPMAI, 31(12):21292142, 2009. 1, 2, 3, 4, 5
H. Lin and J. Bilmes. A class of submodular functions for document summarization. In ACL, 2011. 3
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollr, and C. L. Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014. 2, 6
M. Minoux. Accelerated greedy algorithms for maximizing submodular set functions. Optimization Techniques, pages 234243, 1978. 2, 6
G. Nemhauser, L. Wolsey, and M. Fisher. An analysis of approximations for maximizing submodular set functions. Mathematical Programming, 14(1):265294, 1978. 2, 3
A. Prasad, S. Jegelka, and D. Batra. Submodular meets structured: Finding diverse subsets in exponentially-large structured item sets. In NIPS, 2014. 3
S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In NIPS, 2015. 1
S. Ross, J. Zhou, Y. Yue, D. Dey, and J. A. Bagnell. Learning policies for contextual submodular prediction. In ICML, 2013. 4
P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014. 1
M. Streeter and D. Golovin. An online algorithm for maximizing submodular functions. In NIPS, 2008. 4
C. Szegedy, S. Reed, and D. Erhan. Scalable, high-quality object detection. In CVPR, 2014. 1, 3
C. Szegedy, A. Toshev, and D. Erhan. Deep neural networks for object detection. In NIPS, 2013. 1
B. Taskar, C. Guestrin, and D. Koller. Max-margin markov networks. In NIPS, 2003. 2
J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders. Selective search for object recognition. IJCV, 2013. 1, 2, 3, 6, 7
P. Viola and M. J. Jones. Robust real-time face detection. Int. J. Comput. Vision, 57(2):137154, May 2004. 1, 3
C. Zitnick and P. Dollar. Edge boxes: Locating object proposals from edges. In ECCV, 2014. 1, 2, 3, 4, 5, 7  9
Matthew James Beal. Variational algorithms for approximate Bayesian inference. PhD thesis, 2003.
David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of Machine Learning Research, 3, 2003.
Joseph-Frederic Bonnans, Jean Charles Gilbert, Claude Lemarechal, and Claudia A Sagastizabal. Numerical optimization: theoretical and practical aspects. Springer Science & Business Media, 2006.
Georges Bonnet. Transformations des signaux aleatoires a travers les systemes non lineaires sans memoire. Annals of Telecommunications, 19(9):203220, 1964.
George E Dahl, Tara N Sainath, and Geoffrey E Hinton. Improving deep neural networks for lvcsr using rectified linear units and dropout. In ICASSP, 2013.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:21212159, 2011.
Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and Samy Bengio. Why does unsupervised pre-training help deep learning? Journal of Machine Learning Research, 11:625660, 2010.
Thomas S Ferguson. Location and scale parameters in exponential families of distributions. Annals of Mathematical Statistics, pages 9861001, 1962.
Zhe Gan, Chunyuan Li, Ricardo Henao, David Carlson, and Lawrence Carin. Deep temporal sigmoid belief networks for sequence modeling. In NIPS, 2015.
Karol Gregor, Ivo Danihelka, Alex Graves, and Daan Wierstra. Draw: A recurrent neural network for image generation. In ICML, 2015.
James Hensman, Magnus Rattray, and Neil D Lawrence. Fast variational inference in the conjugate exponential family. In NIPS, 2012.
Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The wake-sleep algorithm for unsupervised neural networks. Science, 268(5214):11581161, 1995.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504507, 2006.
Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. Journal of Machine Learning Research, 14(1):13031347, 2013.
Mohammad E Khan. Decoupled variational gaussian inference. In NIPS, 2014.
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep generative models. In NIPS, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014.
James Martens. Deep learning via hessian-free optimization. In ICML, 2010.
Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. In ICML, 2014.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529533, 2015.
Jiquan Ngiam, Adam Coates, Ahbik Lahiri, Bobby Prochnow, Quoc V Le, and Andrew Y Ng. On optimization methods for deep learning. In ICML, 2011.
Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. arXiv preprint arXiv:1301.3584, 2013.
Barak A Pearlmutter. Fast exact multiplication by the hessian. Neural computation, 6(1):147160, 1994.
Robert Price. A useful theorem for nonlinear devices having gaussian inputs. Information Theory, IRE Transactions on, 4(2):6972, 1958.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In ICML, 2014.
Tim Salimans. Markov chain monte carlo and variational inference: Bridging the gap. In ICML, 2015.
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In NIPS, 2014.
Michalis Titsias and Miguel Lazaro-Gredilla. Doubly stochastic variational bayes for non-conjugate inference. In ICML, 2014.  9
Vladimir Rokhlin, Arthur Szlam, and Mark Tygert. A randomized algorithm for principal component analysis. SIAM Journal on Matrix Analysis and Applications, 31(3):11001124, 2009.
Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Review, 53(2):217288, 2011.
Tamas Sarlos. Improved approximation algorithms for large matrices via random projections. In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS), 2006.
Per-Gunnar Martinsson, Vladimir Rokhlin, and Mark Tygert. A randomized algorithm for the approximation of matrices. Technical Report 1361, Yale University, 2006.
Kenneth Clarkson and David Woodruff. Low rank approximation and regression in input sparsity time. In Proceedings of the 45th Annual ACM Symposium on Theory of Computing (STOC), pages 8190, 2013.
Antoine Liutkus. Randomized SVD, 2014. MATLAB Central File Exchange.
Daisuke Okanohara. redsvd: RandomizED SVD. https://code.google.com/p/redsvd/, 2010.  8
David Hall et al. ScalaNLP: Breeze. http://www.scalanlp.org/, 2009.
IBM Reseach Division, Skylark Team. libskylark: Sketching-based Distributed Matrix Computations for Machine Learning. IBM Corporation, Armonk, NY, 2014.
F. Pedregosa et al. Scikit-learn: Machine learning in Python. JMLR, 12:28252830, 2011.
Arthur Szlam, Yuval Kluger, and Mark Tygert. An implementation of a randomized algorithm for principal component analysis. arXiv:1412.3510, 2014.
Zohar Karnin and Edo Liberty. Online PCA with spectral bounds. In Proceedings of the 28th Annual Conference on Computational Learning Theory (COLT), pages 505509, 2015.
Rafi Witten and Emmanuel J. Candes. Randomized algorithms for low-rank matrix factorizations: Sharp performance bounds. Algorithmica, 31(3):118, 2014.
Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near-optimal column-based matrix reconstruction. SIAM Journal on Computing, 43(2):687717, 2014.
David P. Woodruff. Sketching as a tool for numerical linear algebra. Found. Trends in Theoretical Computer Science, 10(1-2):1157, 2014.
Andrew Tulloch. Fast randomized singular value decomposition. http://research.facebook. com/blog/294071574113354/fast-randomized-svd/, 2014.
Jane Cullum and W.E. Donath. A block Lanczos algorithm for computing the q algebraically largest eigenvalues and a corresponding eigenspace of large, sparse, real symmetric matrices. In IEEE Conference on Decision and Control including the 13th Symposium on Adaptive Processes, pages 505509, 1974.
Gene Golub and Richard Underwood. The block Lanczos method for computing eigenvalues. Mathematical Software, (3):361377, 1977.
Nathan Halko, Per-Gunnar Martinsson, Yoel Shkolnisky, and Mark Tygert. An algorithm for the principal component analysis of large data sets. SIAM Journal on Scientific Computing, 33(5):25802594, 2011.
Nathan Halko. Randomized methods for computing low-rank approximations of matrices. PhD thesis, U. of Colorado, 2012.
Ming Gu. Subspace iteration randomization and singular value problems. arXiv:1408.2208, 2014.
Timothy A. Davis and Yifan Hu. The university of florida sparse matrix collection. ACM Transactions on Mathematical Software, 38(1):1:11:25, December 2011.
Jure Leskovec, Lada A. Adamic, and Bernardo A. Huberman. The dynamics of viral marketing. ACM Transactions on the Web, 1(1), May 2007.
Y. Saad. On the rates of convergence of the Lanczos and the Block-Lanczos methods. SIAM Journal on Numerical Analysis, 17(5):687706, 1980.
Cameron Musco and Christopher Musco. Randomized block Krylov methods for stronger and faster approximate singular value decomposition. arXiv:1504.05477, 2015.
Yousef Saad. Numerical Methods for Large Eigenvalue Problems: Revised Edition, volume 66. 2011.
Gene Golub, Franklin Luk, and Michael Overton. A block Lanczos method for computing the singular values and corresponding singular vectors of a matrix. ACM Trans. Math. Softw., 7(2):149169, 1981.
G.H. Golub and C.F. Van Loan. Matrix Computations. Johns Hopkins University Press, 3rd edition, 1996.
Ren-Cang Li and Lei-Hong Zhang. Convergence of the block Lanczos method for eigenvalue clusters. Numerische Mathematik, 131(1):83113, 2015.
Michael B. Cohen, Sam Elder, Cameron Musco, Christopher Musco, and Madalina Persu. Dimensionality reduction for k-means clustering and low rank approximation. In Proceedings of the 47th Annual ACM Symposium on Theory of Computing (STOC), 2015.
Friedrich L. Bauer. Das verfahren der treppeniteration und verwandte verfahren zur losung algebraischer eigenwertprobleme. Zeitschrift fur angewandte Mathematik und Physik ZAMP, 8(3):214235, 1957.
J. Kuczynski and H. Wozniakowski. Estimating the largest eigenvalue by the power and Lanczos algorithms with a random start. SIAM Journal on Matrix Analysis and Applications, 13(4):10941122, 1992.
Kin Cheong Sou and Anders Rantzer. On the minimum rank of a generalized matrix approximation problem in the maximum singular value norm. In Proceedings of the 19th International Symposium on Mathematical Theory of Networks and Systems (MTNS), 2010.
Per-Gunnar Martinsson, Arthur Szlam, and Mark Tygert. Normalized power iterations for the computation of SVD, 2010. NIPS Workshop on Low-rank Methods for Large-scale Machine Learning.
Jure Leskovec, Jon Kleinberg, and Christos Faloutsos. Graphs over time: Densification laws, shrinking diameters and possible explanations. In Proceedings of the 11th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pages 177187, 2005.
Jason Rennie. 20 newsgroups. http://qwone.com/jason/20Newsgroups/, May 2015.  9
T Zhang, K Liu, and J Zhao. Cross Lingual Entity Linking with Bilingual Topic Model. In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, 2013.
Yunchao Gong, Qifa Ke, Michael Isard, and Svetlana Lazebnik. A Multi-View Embedding Space for Modeling Internet Images, Tags, and Their Semantics. International Journal of Computer Vision, 106(2):210233, oct 2013.
Tomoharu Iwata, T. Yamada, and N. Ueda. Modeling Social Annotation Data with Content Relevance using a Topic Model. In Advances in Neural Information Processing Systems. Citeseer, 2009.
Bin Li, Qiang Yang, and Xiangyang Xue. Transfer Learning for Collaborative Filtering via a RatingMatrix Generative Model. In Proceedings of the 26th Annual International Conference on Machine Learning, 2009.
H. Hotelling. Relations Between Two Sets of Variants. Biometrika, 28:321377, 1936.
S Akaho. A Kernel Method for Canonical Correlation Analysis. In Proceedings of International Meeting on Psychometric Society, number 4, 2001.
Alexei Vinokourov, John Shawe-Taylor, and Nello Cristianini. Inferring a Semantic Representation of Text via Cross-Language Correlation Analysis. In Advances in Neural Information Processing Systems, 2003.
Yaoyong Li and John Shawe-Taylor. Using KCCA for Japanese-English Cross-Language Information Retrieval and Document Classification. Journal of Intelligent Information Systems, 27(2):117133, sep 2006.
Nikhil Rasiwasia, Jose Costa Pereira, Emanuele Coviello, Gabriel Doyle, Gert R.G. Lanckriet, Roger Levy, and Nuno Vasconcelos. A New Approach to Cross-Modal Multimedia Retrieval. In Proceedings of the International Conference on Multimedia, 2010.
Patrik Kamencay, Robert Hudec, Miroslav Benco, and Martina Zachariasov. 2D-3D Face Recognition Method Based on a Modified CCA-PCA Algorithm. International Journal of Advanced Robotic Systems, 2014.
Tomoharu Iwata, Shinji Watanabe, and Hiroshi Sawada. Fashion Coordinates Recommender System Using Photographs from Fashion Magazines. In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence. AAAI Press, jul 2011.
Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y Ng. Multimodal Deep Learning. In Proceedings of The 28th International Conference on Machine Learning, pages 689 696, 2011.
Galen Andrew, Raman Arora, Jeff Bilmes, and Karen Livescu. Deep Canonical Correlation Analysis. In Proceedings of The 30th International Conference on Machine Learning, pages 12471255, 2013.
Alex Smola, Arthur Gretton, Le Song, and Bernhard Scholkopf. A Hilbert Space Embedding for Distributions. In Algorithmic Learning Theory. 2007.
A. Gretton, K. Fukumizu, C.H. Teo, L. Song, B. Scholkopf, and A.J. Smola. A Kernel Statistical Test of Independence. In Advances in Neural Information Processing Systems, 2008.
Krikamol Muandet, Kenji Fukumizu, Francesco Dinuzzo, and Bernhard Scholkopf. Learning from Distributions via Support Measure Machines. In Advances in Neural Information Processing Systems, 2012.
Krikamol Muandet and Bernhard Scholkopf. One-Class Support Measure Machines for Group Anomaly Detection. In Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, 2013.
M Dudik, S J Phillips, and R E Schapire. Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling. Journal of Machine Learning Research, 8:12171260, 2007.
Dino Sejdinovic, Arthur Gretton, and Wicher Bergsma. A Kernel Test for Three-Variable Interactions. In Advances in Neural Information Processing Systems, 2013.
Yuya Yoshikawa, Tomoharu Iwata, and Hiroshi Sawada. Latent Support Measure Machines for Bag-ofWords Data Classification. In Advances in Neural Information Processing Systems, 2014.
Yuya Yoshikawa, Tomoharu Iwata, and Hiroshi Sawada. Non-linear Regression for Bag-of-Words Data via Gaussian Process Latent Variable Set Model. In Proceedings of the 29th AAAI Conference on Artificial Intelligence, 2015.
Bharath K. Sriperumbudur, Arthur Gretton, Kenji Fukumizu, Bernhard Scholkopf, and Gert R. G. Lanckriet. Hilbert Space Embeddings and Metrics on Probability Measures. The Journal of Machine Learning Research, 11:15171561, 2010.
Dong C. Liu and Jorge Nocedal. On the Limited Memory BFGS Method for Large Scale Optimization. Mathematical Programming, 45(1-3):503528, aug 1989.  9
Pedro Domingos, Stanley Kok, Hoifung Poon, Matthew Richardson, and Parag Singla. Unifying logical and statistical AI. In AAAI, 2006.
Noah D. Goodman, Vikash K. Mansinghka, Daniel M. Roy, Keith Bonawitz, and Joshua B. Tenenbaum. Church: A language for generative models. In UAI, 2008.
Matthew D. Hoffman and Andrew Gelman. The no-u-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. JMLR, 15(1):15931623, 2014.
Rajesh Ranganath, Sean Gerrish, and David M. Blei. Black box variational inference. In AISTATS, 2014.
Iain Murray, Ryan Prescott Adams, and David J.C. MacKay. Elliptical slice sampling. In AISTATS, 2010.
Trung V. Nguyen and Edwin V. Bonilla. Automated variational inference for Gaussian process models. In NIPS. 2014.
Hannes Nickisch and Carl Edward Rasmussen. Approximations for binary Gaussian process classification. JMLR, 9(10), 2008.
James Hensman, Nicolo Fusi, and Neil D Lawrence. Gaussian processes for big data. In UAI, 2013.
Yarin Gal, Mark van der Wilk, and Carl Rasmussen. Distributed variational inference in sparse Gaussian process regression and latent variable models. In NIPS. 2014.
Trung V. Nguyen and Edwin V. Bonilla. Collaborative multi-output Gaussian processes. In UAI, 2014.
Trung V. Nguyen and Edwin V. Bonilla. Fast allocation of Gaussian process experts. In ICML, 2014.
Joaquin Quinonero-Candela and Carl Edward Rasmussen. A unifying view of sparse approximate Gaussian process regression. JMLR, 6:19391959, 2005.
Michalis Titsias. Variational learning of inducing variables in sparse Gaussian processes. In AISTATS, 2009.
Andrew G. Wilson, David A. Knowles, and Zoubin Ghahramani. Gaussian process regression networks. In ICML, 2012.
Edward Snelson, Carl Edward Rasmussen, and Zoubin Ghahramani. Warped Gaussian processes. In NIPS, 2003.
Sethu Vijayakumar and Stefan Schaal. Locally weighted projection regression: An O(n) algorithm for incremental real time learning in high dimensional space. In ICML, 2000.
Neil D Lawrence, Matthias Seeger, and Ralf Herbrich. Fast sparse Gaussian process methods: The informative vector machine. In NIPS, 2002.
Ed Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs. In NIPS, 2006.
Mauricio A Alvarez and Neil D Lawrence. Computationally efficient convolved multiple output Gaussian processes. JMLR, 12(5):14591500, 2011.
Mauricio A. Alvarez, David Luengo, Michalis K. Titsias, and Neil D. Lawrence. Efficient multioutput Gaussian processes through variational inducing kernels. In AISTATS, 2010.
Manfred Opper and Cedric Archambeau. The variational Gaussian approximation revisited. Neural Computation, 21(3):786792, 2009.
Andreas Damianou and Neil Lawrence. Deep Gaussian processes. In AISTATS, 2013.
James Hensman, Alexander Matthews, and Zoubin Ghahramani. Scalable variational Gaussian process classification. In AISTATS, 2015.
Zichao Yang, Andrew Gordon Wilson, Alexander J. Smola, and Le Song. A la carte  learning fast kernels. In AISTATS, 2015.
Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian processes for machine learning. The MIT Press, 2006.
Christopher K.I. Williams and David Barber. Bayesian classification with Gaussian processes. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 20(12):13421351, 1998.
Jesper Mller, Anne Randi Syversveen, and Rasmus Plenge Waagepetersen. Log Gaussian Cox processes. Scandinavian journal of statistics, 25(3):451482, 1998.
K. Bache and M. Lichman. UCI machine learning repository, 2013.
R.G. Jarrett. A note on the intervals between coal-mining disasters. Biometrika, 66(1):191193, 1979.  9
Oded Goldreich and Dana Ron. On testing expansion in bounded-degree graphs. In Studies in Complexity and Cryptography. Miscellanea on the Interplay between Randomness and Computation. Springer, 2011.
Peter Lofgren, Siddhartha Banerjee, Ashish Goel, and C Seshadhri. FAST-PPR: Scaling personalized PageRank estimation for large graphs. In ACM SIGKDD14, 2014.
Reid Andersen, Fan Chung, and Kevin Lang. Local graph partitioning using PageRank vectors. In IEEE FOCS06, 2006.
Reid Andersen, Christian Borgs, Jennifer Chayes, John Hopcraft, Vahab S Mirrokni, and Shang-Hua Teng. Local computation of PageRank contributions. In Algorithms and Models for the Web-Graph. Springer, 2007.
Kyle Kloster and David F Gleich. Heat kernel based community detection. In ACM SIGKDD14, 2014.
Satyen Kale, Yuval Peres, and C Seshadhri. Noise tolerance of expanders and sublinear expander reconstruction. In IEEE FOCS08, 2008.
Rajeev Motwani, Rina Panigrahy, and Ying Xu. Estimating sum by weighted sampling. In Automata, Languages and Programming, pages 5364. Springer, 2007.
Siddhartha Banerjee and Peter Lofgren. Fast bidirectional probability estimation in markov models. Technical report, 2015. http://arxiv.org/abs/1507.05998.
Devdatt P Dubhashi and Alessandro Panconesi. Concentration of measure for the analysis of randomized algorithms. Cambridge University Press, 2009.
Purnamrita Sarkar, Andrew W Moore, and Amit Prakash. Fast incremental proximity search in large graphs. In Proceedings of the 25th international conference on Machine learning, pages 896903. ACM, 2008.
Wolfgang Doeblin. Elements dune theorie generale des chaines simples constantes de markoff. In Annales Scientifiques de lEcole Normale Superieure, volume 57, pages 61111. Societe mathematique de France, 1940.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation ranking: bringing order to the web. 1999.
Ronny Lempel and Shlomo Moran. The stochastic approach for link-structure analysis (SALSA) and the TKC effect. Computer Networks, 33(1):387401, 2000.
Sahand Negahban, Sewoong Oh, and Devavrat Shah. Iterative ranking from pair-wise comparisons. In Advances in Neural Information Processing Systems, pages 24742482, 2012.
Jacob Steinhardt and Percy Liang. Learning fast-mixing models for structured prediction. In ICML15, 2015.
Krishna B Athreya and Orjan Stenflo. Perfect sampling for Doeblin chains. Sankhya: The Indian Journal of Statistics, pages 763777, 2003.
Fan Chung. The heat kernel as the pagerank of a graph. Proceedings of the National Academy of Sciences, 104(50):1973519740, 2007.
Christina E Lee, Asuman Ozdaglar, and Devavrat Shah. Computing the stationary distribution locally. In Advances in Neural Information Processing Systems, pages 13761384, 2013.
Lubos Takac and Michal Zabovsky. Data analysis in public social networks. In International. Scientific Conf. & Workshop Present Day Trends of Innovations, 2012.
Alan Mislove, Massimiliano Marcon, Krishna P. Gummadi, Peter Druschel, and Bobby Bhattacharjee. Measurement and Analysis of Online Social Networks. In Proceedings of the 5th ACM/Usenix Internet Measurement Conference (IMC07), San Diego, CA, October 2007.
Stanford Network Analysis Platform (SNAP). http://http://snap.stanford.edu/. Accessed: 2014-02-11.
Paolo Boldi, Marco Rosa, Massimo Santini, and Sebastiano Vigna. Layered label propagation: A multi resolution coordinate-free ordering for compressing social networks. In ACM WWW11, 2011.
Laboratory for Web Algorithmics. http://law.di.unimi.it/datasets.php. Accessed: 201402-11.  9
T. Bengtsson, P. Bickel, and B. Li. Curse-of-dimensionality revisited: Collapse of the particle filter in very large scale systems. In Probability and statistics: Essays in honor of David A. Freedman, pages 316334. Institute of Mathematical Statistics, 2008.
J. Cheng. Sampling algorithms for estimating the mean of bounded random variables. Computational Statistics, 16(1):123, 2001.
J. Cheng and M. Druzdzel. AIS-BN: An adaptive importance sampling algorithm for evidential reasoning in large Bayesian networks. Journal of Artificial Intelligence Research, 2000.
P. Dagum and M. Luby. An optimal approximation algorithm for Bayesian inference. Artificial Intelligence, 93(1):127, 1997.
P. Dagum, R. Karp, M. Luby, and S. Ross. An optimal algorithm for Monte Carlo estimation. SIAM Journal on Computing, 29:14841496, 2000.
N. De Freitas, P. Hjen-Srensen, M. Jordan, and S. Russell. Variational MCMC. In UAI, 2001.
R. Dechter and I. Rish. Mini-buckets: A general scheme for bounded inference. Journal of the ACM, 50 (2):107153, 2003.
R. Fung and K. Chang. Weighing and integrating evidence for stochastic simulation in Bayesian networks. In UAI, 1990.
A. Globerson and T. Jaakkola. Approximate inference using conditional entropy decompositions. In UAI, pages 130138, 2007.
V. Gogate. Sampling Algorithms for Probabilistic Graphical Models with Determinism. PhD thesis, UC Irvine, 2009.
V. Gogate and R. Dechter. Sampling-based lower bounds for counting queries. Intelligenza Artificiale, 5 (2):171188, 2011.
T. Hazan and T. Jaakkola. On the partition function and random maximum a-posteriori perturbations. In ICML, 2012.
D. Koller and N. Friedman. Probabilistic graphical models: principles and techniques. MIT press, 2009.
S. Lauritzen. Graphical models. Oxford University Press, 1996.
J. Liu. Monte Carlo strategies in scientific computing. Springer Science & Business Media, 2008.
Q. Liu. Reasoning and Decisions in Probabilistic Graphical ModelsA Unified Framework. PhD thesis, UC Irvine, 2014.
Q. Liu and A. Ihler. Bounding the partition function using Holders inequality. In ICML, 2011.
R. Mateescu, K. Kask, V. Gogate, and R. Dechter. Join-graph propagation algorithms. JAIR, 37(1): 279328, 2010.
A. Maurer and M. Pontil. Empirical Bernstein bounds and sample-variance penalization. In COLT, pages 115124, 2009.
V. Mnih, C. Szepesvari, and J.-Y. Audibert. Empirical Bernstein stopping. In ICML, 2008.
M.-S. Oh and J. Berger. Adaptive importance sampling in Monte Carlo integration. J. Stat. Comput. Simul., 41(3-4):143168, 1992.
F. Orabona, T. Hazan, A. Sarwate, and T. Jaakkola. On measure concentration of random maximum a-posteriori perturbations. In ICML, 2014.
G. Papandreou and A. Yuille. Perturb-and-map random fields: Using discrete optimization to learn and sample from energy models. In ICCV, 2011.
N. Ruozzi. The bethe partition function of log-supermodular graphical models. In NIPS, 2012.
T. Salimans, D. Kingma, and M. Welling. Markov chain Monte Carlo and variational inference: Bridging the gap. In ICML, 2015.
R. Shachter and M. Peot. Simulation approaches to general probabilistic inference on belief networks. In UAI, 1990.
E. Sudderth, M. Wainwright, and A. Willsky. Loop series and bethe variational bounds in attractive graphical models. In NIPS, pages 14251432, 2007.
M. Wainwright. Estimating the wrong graphical model: Benefits in the computation-limited setting. JMLR, 7:18291859, 2006.
M. Wainwright and M. Jordan. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1(1-2):1305, 2008.
M. Wainwright, T. Jaakkola, and A. Willsky. A new class of upper bounds on the log partition function. IEEE Trans. Information Theory, 51(7):23132335, 2005.
Y. Wexler and D. Geiger. Importance sampling via variational optimization. In UAI, 2007.
C. Yuan and M. Druzdzel. An importance sampling algorithm based on evidence pre-propagation. In UAI, pages 624631, 2002.
C. Yuan and M. Druzdzel. Importance sampling algorithms for Bayesian networks: Principles and performance. Mathematical and Computer Modeling, 43(9):11891207, 2006.
C. Yuan and M. Druzdzel. Generalized evidence pre-propagated importance sampling for hybrid Bayesian networks. In AAAI, volume 7, pages 12961302, 2007.
C. Yuan and M. Druzdzel. Theoretical analysis and practical insights on importance sampling in Bayesian networks. International Journal of Approximate Reasoning, 46(2):320333, 2007.  9
D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:9931022, 2003.
D. M. Blei and M. I. Jordan. Variational inference for Dirichlet process mixtures. Bayesian Analysis, 1(1):121143, 2006.
M. D. Hoffman, D. M. Blei, C. Wang, and J. Paisley. Stochastic variational inference. Journal of Machine Learning Research, 14(1):13031347, 2013.
D. J. C. MacKay. Information Theory, Inference, and Learning Algorithms. Cambridge University Press, 2003. Chapter 33.
C. M. Bishop. Pattern Recognition and Machine Learning. Springer, New York, 2006. Chapter 10.
R. E. Turner and M. Sahani. Two problems with variational expectation maximisation for time-series models. In D. Barber, A. T. Cemgil, and S. Chiappa, editors, Bayesian Time Series Models. 2011.
B. Wang and M. Titterington. Inadequacy of interval estimates corresponding to variational Bayesian approximations. In Workshop on Articial Intelligence and Statistics, pages 373380, 2004.
H. Rue, S. Martino, and N. Chopin. Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations. Journal of the Royal Statistical Society: Series B (statistical methodology), 71(2):319392, 2009.
G. Parisi. Statistical Field Theory, volume 4. Addison-Wesley New York, 1988.
M. Opper and O. Winther. Variational linear response. In Advances in Neural Information Processing Systems, 2003.
M. Opper and D. Saad. Advanced mean eld methods: Theory and practice. MIT press, 2001.
T. Tanaka. Information geometry of mean-eld approximation. Neural Computation, 12(8):19511968, 2000.
H. J. Kappen and F. B. Rodriguez. Efcient learning in Boltzmann machines using linear response theory. Neural Computation, 10(5):11371156, 1998.
M. Welling and Y. W. Teh. Linear response algorithms for approximate inference in graphical models. Neural Computation, 16(1):197221, 2004.
P. A. d. F. R. Hjen-Srensen, O. Winther, and L. K. Hansen. Mean-eld approaches to independent component analysis. Neural Computation, 14(4):889918, 2002.
T. Tanaka. Mean-eld theory of Boltzmann machine learning. Physical Review E, 58(2):2302, 1998.
M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1(1-2):1305, 2008.
J. D. Hadeld. MCMC methods for multi-response generalized linear mixed models: The MCMCglmm R package. Journal of Statistical Software, 33(2):122, 2010.
M. Lubin and I. Dunning. Computing in operations research using Julia. INFORMS Journal on Computing, 27(2):238248, 2015.
D. Bates and D. Eddelbuettel. Fast and elegant numerical linear algebra using the RcppEigen package. Journal of Statistical Software, 52(5):124, 2013.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, 1998.
M. Plummer, N. Best, K. Cowles, and K. Vines. CODA: Convergence diagnosis and output analysis for MCMC. R News, 6(1):711, 2006.
X. L. Meng and D. B. Rubin. Using EM to obtain asymptotic variance-covariance matrices: The SEM algorithm. Journal of the American Statistical Association, 86(416):899909, 1991.
A. Wachter and L. T. Biegler. On the implementation of an interior-point lter line-search algorithm for large-scale nonlinear programming. Mathematical Programming, 106(1):2557, 2006.  9
Rajeev Agrawal, Demosthenis Teneketzis, and Venkatachalam Anantharam. Asymptotically efficient adaptive allocation schemes for controlled i.i.d. processes: Finite parameter space. IEEE Transactions on Automatic Control, 34(3):258267, 1989.
Shipra Agrawal and Navin Goyal. Analysis of Thompson sampling for the multi-armed bandit problem. In Proceeding of the 25th Annual Conference on Learning Theory, pages 39.139.26, 2012.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47:235256, 2002.
Gabor Bartok, Navid Zolghadr, and Csaba Szepesvari. An adaptive algorithm for finite stochastic partial monitoring. In Proceedings of the 29th International Conference on Machine Learning, 2012.
Wei Chen, Yajun Wang, and Yang Yuan. Combinatorial multi-armed bandit: General framework, results and applications. In Proceedings of the 30th International Conference on Machine Learning, pages 151159, 2013.
Baek-Young Choi, Sue Moon, Zhi-Li Zhang, Konstantina Papagiannaki, and Christophe Diot. Analysis of point-to-point packet delay in an operational network. In Proceedings of the 23rd Annual Joint Conference of the IEEE Computer and Communications Societies, 2004.
Richard Combes, Stefan Magureanu, Alexandre Proutiere, and Cyrille Laroche. Learning to rank: Regret lower bounds and efficient algorithms. In Proceedings of the 2015 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems, 2015.
Yi Gai, Bhaskar Krishnamachari, and Rahul Jain. Combinatorial network optimization with unknown variables: Multi-armed bandits with linear rewards and individual observations. IEEE/ACM Transactions on Networking, 20(5):14661478, 2012.
Aurelien Garivier and Olivier Cappe. The KL-UCB algorithm for bounded stochastic bandits and beyond. In Proceeding of the 24th Annual Conference on Learning Theory, pages 359 376, 2011.
Branislav Kveton, Csaba Szepesvari, Zheng Wen, and Azin Ashkan. Cascading bandits: Learning to rank in the cascade model. In Proceedings of the 32nd International Conference on Machine Learning, 2015.
Branislav Kveton, Zheng Wen, Azin Ashkan, Hoda Eydgahi, and Brian Eriksson. Matroid bandits: Fast combinatorial optimization with learning. In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence, pages 420429, 2014.
Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesvari. Tight regret bounds for stochastic combinatorial semi-bandits. In Proceedings of the 18th International Conference on Artificial Intelligence and Statistics, 2015.
Shyong Lam and Jon Herlocker. MovieLens Dataset. http://grouplens.org/datasets/movielens/, 2015.
Thanh Le, Csaba Szepesvari, and Rong Zheng. Sequential learning for multi-channel wireless network monitoring with channel switching costs. IEEE Transactions on Signal Processing, 62(22):59195929, 2014.
Tian Lin, Bruno Abrahao, Robert Kleinberg, John Lui, and Wei Chen. Combinatorial partial monitoring game with linear feedback and its applications. In Proceedings of the 31st International Conference on Machine Learning, pages 901909, 2014.
Christos Papadimitriou and Kenneth Steiglitz. Combinatorial Optimization. Dover Publications, Mineola, NY, 1998.
Neil Spring, Ratul Mahajan, and David Wetherall. Measuring ISP topologies with Rocketfuel. IEEE / ACM Transactions on Networking, 12(1):216, 2004.
William. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3-4):285294, 1933.
Zheng Wen, Branislav Kveton, and Azin Ashkan. Efficient learning in large-scale combinatorial semi-bandits. In Proceedings of the 32nd International Conference on Machine Learning, 2015. 9
J. S. Liu. Monte Carlo Strategies in Scientific Computing. Springer Series in Statistics. Springer-Verlag, 2001.
R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction (Adaptive Computation and Machine Learning). A Bradford Book, 1998.
D. Levin, Y. Peres, and E. Wilmer. Markov Chains and Mixing Times. AMS, 2008.
S. P. Meyn and R. L. Tweedie. Markov Chains and Stochastic Stability. Springer, 1993.
C. Kipnis and S. R. S. Varadhan. Central limit theorem for additive functionals of reversible markov processes and applications to simple exclusions. Comm. Math. Phys., 104(1):119, 1986. 6 For the  interval, we only plug-in lower bounds on  and  only where these quantities appear as 1/ and 1/ in Eq. (4). It is then possible to solve for observable bounds on  . See Appendix D for details.  8
I. Kontoyiannis, L. A. Lastras-Montano, and S. P. Meyn. Exponential bounds and stopping rules for MCMC and general Markov chains. In VALUETOOLS, page 45, 2006.
M.-F. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. In ICML, pages 6572, 2006.
V. Mnih, Cs. Szepesvari, and J.-Y. Audibert. Empirical Bernstein stopping. In ICML, pages 672679, 2008.
A. Maurer and M. Pontil. Empirical Bernstein bounds and sample-variance penalization. In COLT, 2009.
L. Li, M. L. Littman, T. J. Walsh, and A. L. Strehl. Knows what it knows: a framework for self-aware learning. Machine Learning, 82(3):399443, 2011.
J. M. Flegal and G. L. Jones. Implementing MCMC: estimating with confidence. In Handbook of Markov chain Monte Carlo, pages 175197. Chapman & Hall/CRC, 2011.
B. M. Gyori and D. Paulin. Non-asymptotic confidence intervals for MCMC in practice. arXiv:1212.2016, 2014.
A. Swaminathan and T. Joachims. Counterfactual risk minimization: Learning from logged bandit feedback. In ICML, 2015.
D. Gillman. A Chernoff bound for random walks on expander graphs. SIAM Journal on Computing, 27(4):12031220, 1998.
C. A. Leon and F. Perron. Optimal Hoeffding bounds for discrete reversible Markov chains. Annals of Applied Probability, pages 958970, 2004.
D. Paulin. Concentration inequalities for Markov chains by Marton couplings and spectral methods. Electronic Journal of Probability, 20:132, 2015.
S. T. Garren and R. L. Smith. Estimating the second largest eigenvalue of a Markov transition matrix. Bernoulli, 6:215242, 2000.
G. L. Jones and J. P. Hobert. Honest exploration of intractable probability distributions via markov chain monte carlo. Statist. Sci., 16(4):312334, 11 2001.
Y. Atchade. Markov Chain Monte Carlo confidence intervals. Bernoulli, 2015. (to appear).
B. Yu. Rates of convergence for empirical processes of stationary mixing sequences. The Annals of Probability, 22(1):94116, January 1994.
R. L. Karandikar and M. Vidyasagar. Rates of uniform convergence of empirical means with mixing processes. Statistics and Probability Letters, 58(3):297307, 2002.
D. Gamarnik. Extension of the PAC framework to finite and countable Markov chains. IEEE Transactions on Information Theory, 49(1):338345, 2003.
M. Mohri and A. Rostamizadeh. Stability bounds for non-iid processes. In NIPS, 2008.
M. Mohri and A. Rostamizadeh. Rademacher complexity bounds for non-i.i.d. processes. In NIPS, 2009.
I. Steinwart and A. Christmann. Fast learning from non-i.i.d. observations. In NIPS, 2009.
I. Steinwart, D. Hush, and C. Scovel. Learning from dependent observations. Journal of Multivariate Analysis, 100(1):175194, 2009.
D. McDonald, C. Shalizi, and M. Schervish. Estimating beta-mixing coefficients. In AISTATS, pages 516524, 2011.
T. Batu, L. Fortnow, R. Rubinfeld, W. D. Smith, and P. White. Testing that distributions are close. In FOCS, pages 259269. IEEE, 2000.
T. Batu, L. Fortnow, R. Rubinfeld, W. D. Smith, and P. White. Testing closeness of discrete distributions. Journal of the ACM (JACM), 60(1):4:24:25, 2013.
N. Bhatnagar, A. Bogdanov, and E. Mossel. The computational complexity of estimating MCMC convergence time. In RANDOM, pages 424435. Springer, 2011.
D. Hsu, A. Kontorovich, and C. Szepesvari. Mixing time estimation in reversible Markov chains from a single sample path. CoRR, abs/1506.02903, 2015.
J. Tropp. An introduction to matrix concentration inequalities. Foundations and Trends in Machine Learning, 2015.
S. Bernstein. Sur lextension du theoreme limite du calcul des probabilites aux sommes de quantites dependantes. Mathematische Annalen, 97:159, 1927.
G. W. Stewart and J. Sun. Matrix perturbation theory. Academic Press, Boston, 1990.
C. D. Meyer Jr. The role of the group generalized inverse in the theory of finite Markov chains. SIAM Review, 17(3):443464, 1975.
G. Cho and C. Meyer. Comparison of perturbation bounds for the stationary distribution of a Markov chain. Linear Algebra and its Applications, 335:137150, 2001.  9
C. Acerbi. Spectral measures of risk: a coherent representation of subjective risk aversion. Journal of Banking & Finance, 26(7):15051518, 2002.
P. Artzner, F. Delbaen, J. Eber, and D. Heath. Coherent measures of risk. Mathematical finance, 9(3):203 228, 1999.
O. Bardou, N. Frikha, and G. Pages. Computing VaR and CVaR using stochastic approximation and adaptive unconstrained importance sampling. Monte Carlo Methods and Applications, 15(3):173210, 2009.
N. Bauerle and J. Ott. Markov decision processes with average-value-at-risk criteria. Mathematical Methods of Operations Research, 74(3):361379, 2011.
D. Bertsekas. Dynamic programming and optimal control. Athena Scientific, 4th edition, 2012.
V. Borkar. A sensitivity formula for risk-sensitive cost and the actorcritic algorithm. Systems & Control Letters, 44(5):339346, 2001.
S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2009.
Y. Chow and M. Ghavamzadeh. Algorithms for CVaR optimization in MDPs. In NIPS 27, 2014.
Y. Chow and M. Pavone. A unifying framework for time-consistent, risk-averse model predictive control: theory and algorithms. In American Control Conference, 2014.
E. Delage and S. Mannor. Percentile optimization for Markov decision processes with parameter uncertainty. Operations Research, 58(1):203213, 2010.
M. Fu. Gradient estimation. In Simulation, volume 13 of Handbooks in Operations Research and Management Science, pages 575  616. Elsevier, 2006.
J. Hadar and W. R. Russell. Rules for ordering uncertain prospects. The American Economic Review, pages 2534, 1969.
D. Iancu, M. Petrik, and D. Subramanian. arXiv:1106.6102, 2011.  Tight approximations of dynamic risk measures.
V. Konda and J. Tsitsiklis. Actor-critic algorithms. In NIPS, 2000.
P. Marbach and J. Tsitsiklis. Simulation-based optimization of Markov reward processes. IEEE Transactions on Automatic Control, 46(2):191209, 1998.
H. Markowitz. Portfolio selection: Efficient diversification of investment. John Wiley and Sons, 1959.
P. Milgrom and I. Segal. Envelope theorems for arbitrary choice sets. Econometrica, 70(2):583601, 2002.
J. Moody and M. Saffell. Learning to trade via direct reinforcement. Neural Networks, IEEE Transactions on, 12(4):875889, 2001.
T. Osogami. Robustness and risk-sensitivity in Markov decision processes. In NIPS, 2012.
M. Petrik and D. Subramanian. An approximate solution method for large risk-averse Markov decision processes. In UAI, 2012.
L. Prashanth and M. Ghavamzadeh. Actor-critic algorithms for risk-sensitive MDPs. In NIPS 26, 2013.
S. Rachev and S. Mittnik. Stable Paretian models in finance. John Willey & Sons, New York, 2000.
R. Rockafellar and S. Uryasev. Optimization of conditional value-at-risk. Journal of risk, 2:2142, 2000.
A. Ruszczynski. Risk-averse dynamic programming for Markov decision processes. Mathematical Programming, 125(2):235261, 2010.
A. Ruszczynski and A. Shapiro. Optimization of convex risk functions. Math. OR, 31(3):433452, 2006.
A. Shapiro, D. Dentcheva, and A. Ruszczynski. Lectures on stochastic programming, chapter 6, pages 253332. SIAM, 2009.
R. Sutton and A. Barto. Reinforcement learning: An introduction. Cambridge Univ Press, 1998.
R. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In NIPS 13, 2000.
A. Tamar, D. Di Castro, and S. Mannor. Policy gradients with variance related risk criteria. In International Conference on Machine Learning, 2012.
A. Tamar, Y. Glassner, and S. Mannor. Optimizing the CVaR via sampling. In AAAI, 2015.
A. Tamar, S. Mannor, and H. Xu. Scaling up robust MDPs using function approximation. In International Conference on Machine Learning, 2014.  9
J. D. Abernethy, E. Hazan, and A. Rakhlin. Interior-point methods for full-information and bandit online learning. Information Theory, IEEE Transactions on, 58(7):41644175, 2012.
M. Anthony and P. L. Bartlett. Neural network learning: Theoretical foundations. cambridge university press, 2009.
K. S. Azoury and M. K. Warmuth. Relative loss bounds for on-line density estimation with the exponential family of distributions. Machine Learning, 43(3):211246, 2001.
O. Bousquet and A. Elisseeff. Stability and generalization. The Journal of Machine Learning Research, 2:499526, 2002.
N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press, 2006.
N. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning algorithms. IEEE Transactions on Information Theory, 50(9):20502057, 2004.
G. H. Golub and C. F. Van Loan. Matrix computations, volume 3. JHU Press, 2012.
E. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for online convex optimization. Machine Learning, 69(2-3):169192, 2007.
D. Hsu, S. M. Kakade, and T. Zhang. Random design analysis of ridge regression. Foundations of Computational Mathematics, 14(3):569600, 2014.
R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, pages 315323, 2013.
S. M. Kakade, K. Sridharan, and A. Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In Advances in neural information processing systems, pages 793800, 2009.
J. Kivinen and M. K. Warmuth. Averaging expert predictions. In Computational Learning Theory, pages 153167. Springer, 1999.
T. Koren. Open problem: Fast stochastic exp-concave optimization. In Conference on Learning Theory, pages 10731075, 2013.
G. Lecue and S. Mendelson. Performance of empirical risk minimization in linear aggregation. arXiv preprint arXiv:1402.5763, 2014.
M. Mahdavi, L. Zhang, and R. Jin. Lower and upper bounds on the generalization of stochastic exponentially concave optimization. In Proceedings of The 28th Conference on Learning Theory, 2015.
S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss. The Journal of Machine Learning Research, 14(1):567599, 2013.
S. Shalev-Shwartz and T. Zhang. Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization. Mathematical Programming, pages 141, 2014.
S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan. Learnability, stability and uniform convergence. The Journal of Machine Learning Research, 11:26352670, 2010.
O. Shamir. The sample complexity of learning linear predictors with the squared loss. arXiv preprint arXiv:1406.5143, 2014.
N. Srebro, K. Sridharan, and A. Tewari. Smoothness, low noise and fast rates. In Advances in neural information processing systems, pages 21992207, 2010.
K. Sridharan, S. Shalev-Shwartz, and N. Srebro. Fast rates for regularized objectives. In Advances in Neural Information Processing Systems, pages 15451552, 2009.
V. Vovk. Competitive on-line statistics. International Statistical Review, 69(2):213248, 2001.  9
P. J. Burt, Edward, and E. H. Adelson. The laplacian pyramid as a compact image code. IEEE Transactions on Communications, 31:532540, 1983.
A. Coates, H. Lee, and A. Y. Ng. An analysis of single layer networks in unsupervised feature learning. In AISTATS, 2011.
J. S. De Bonet. Multiresolution sampling procedure for analysis and synthesis of texture images. In Proceedings of the 24th annual conference on Computer graphics and interactive techniques, pages 361368. ACM Press/Addison-Wesley Publishing Co., 1997.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248255. IEEE, 2009.
E. Denton, S. Chintala, A. Szlam, and R. Fergus. Deep generative image models using a laplacian pyramid of adversarial networks: Supplementary material. http://soumith.ch/eyescream.
A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning to generate chairs with convolutional neural networks. arXiv preprint arXiv:1411.5928, 2014.
A. A. Efros and T. K. Leung. Texture synthesis by non-parametric sampling. In ICCV, volume 2, pages 10331038. IEEE, 1999.
S. A. Eslami, N. Heess, C. K. Williams, and J. Winn. The shape boltzmann machine: a strong model of object shape. International Journal of Computer Vision, 107(2):155176, 2014.
W. T. Freeman, T. R. Jones, and E. C. Pasztor. Example-based super-resolution. Computer Graphics and Applications, IEEE, 22(2):56 65, 2002.
J. Gauthier. Conditional generative adversarial nets for convolutional face generation. Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester 2014 2014.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, pages 26722680. 2014.
K. Gregor, I. Danihelka, A. Graves, and D. Wierstra. DRAW: A recurrent neural network for image generation. CoRR, abs/1502.04623, 2015.
J. Hays and A. A. Efros. Scene completion using millions of photographs. ACM Transactions on Graphics (TOG), 26(3):4, 2007.
G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504507, 2006.
S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167v3, 2015.
D. P. Kingma and M. Welling. Auto-encoding variational bayes. ICLR, 2014.
A. Krizhevsky. Learning multiple layers of features from tiny images. Masters Thesis, Deptartment of Computer Science, University of Toronto, 2009.
A. Krizhevsky, G. E. Hinton, et al. Factored 3-way restricted boltzmann machines for modeling natural images. In AISTATS, pages 621628, 2010.
M. Mirza and S. Osindero. Conditional generative adversarial nets. CoRR, abs/1411.1784, 2014.
B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision research, 37(23):33113325, 1997.
S. Osindero and G. E. Hinton. Modeling image patches with a directed hierarchy of markov random fields. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, NIPS, pages 11211128. 2008.
J. Portilla and E. P. Simoncelli. A parametric texture model based on joint statistics of complex wavelet coefficients. International Journal of Computer Vision, 40(1):4970, 2000.
M. Ranzato, V. Mnih, J. M. Susskind, and G. E. Hinton. Modeling natural images using gated MRFs. IEEE Transactions on Pattern Analysis & Machine Intelligence, (9):22062222, 2013.
D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and variational inference in deep latent gaussian models. arXiv preprint arXiv:1401.4082, 2014.
S. Roth and M. J. Black. Fields of experts: A framework for learning image priors. In In CVPR, pages 860867, 2005.
R. Salakhutdinov and G. E. Hinton. Deep boltzmann machines. In AISTATS, pages 448455, 2009.
E. P. Simoncelli, W. T. Freeman, E. H. Adelson, and D. J. Heeger. Shiftable multiscale transforms. Information Theory, IEEE Transactions on, 38(2):587607, 1992.
J. Sohl-Dickstein, E. A. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. CoRR, abs/1503.03585, 2015.
L. Theis and M. Bethge. Generative image modeling using spatial LSTMs. Dec 2015.
P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with denoising autoencoders. In ICML, pages 10961103, 2008.
J. Wright, Y. Ma, J. Mairal, G. Sapiro, T. S. Huang, and S. Yan. Sparse representation for computer vision and pattern recognition. Proceedings of the IEEE, 98(6):10311044, 2010.
Y. Zhang, F. Yu, S. Song, P. Xu, A. Seff, and J. Xiao. Large-scale scene understanding challenge. In CVPR Workshop, 2015.
S. C. Zhu, Y. Wu, and D. Mumford. Filters, random fields and maximum entropy (frame): Towards a unified theory for texture modeling. International Journal of Computer Vision, 27(2):107126, 1998.
D. Zoran and Y. Weiss. From learning models of natural image patches to whole image restoration. In ICCV, 2011.  9
Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The Pascal Visual Object Classes (VOC) challenge. IJCV, 88(2):303338, 2010.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Semantic image segmentation with deep convolutional nets and fully connected CRFs. In ICLR, 2015.
Bharath Hariharan, Pablo Arbelaez, Ross Girshick, and Jitendra Malik. Hypercolumns for object segmentation and fine-grained localization. In CVPR, 2015.
Bharath Hariharan, Pablo Arbelaez, Ross Girshick, and Jitendra Malik. Simultaneous detection and segmentation. In ECCV, 2014.
Mohammadreza Mostajabi, Payman Yadollahpour, and Gregory Shakhnarovich. Feedforward semantic segmentation with zoom-out features. CVPR, 2015.
Pedro O. Pinheiro and Ronan Collobert. Weakly supervised semantic segmentation with convolutional networks. In CVPR, 2015.
George Papandreou, Liang-Chieh Chen, Kevin Murphy, and Alan L Yuille. Weakly-and semi-supervised learning of a DCNN for semantic image segmentation. In ICCV, 2015.
Deepak Pathak, Evan Shelhamer, Jonathan Long, and Trevor Darrell. Fully convolutional multi-class multiple instance learning. In ICLR, 2015.
Jifeng Dai, Kaiming He, and Jian Sun. BoxSup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation. In ICCV, 2015.
Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, and Philip Torr. Conditional random fields as recurrent neural networks. In ICCV, 2015.
Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han. Learning deconvolution network for semantic segmentation. In ICCV, 2015.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. In ICLR Workshop, 2014.
C Lawrence Zitnick and Piotr Dollar. Edge boxes: Locating object proposals from edges. In ECCV, 2014.
Bharath Hariharan, Pablo Arbelaez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. Semantic contours from inverse detectors. In ICCV, 2011.
Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: a large-scale hierarchical image database. In CVPR, 2009.  9
Bastien, Frederic, Lamblin, Pascal, Pascanu, Razvan, Bergstra, James, Goodfellow, Ian J., Bergeron,
Arnaud, Bouchard, Nicolas, and Bengio, Yoshua. Theano: new features and speed improvements.
Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.
Bekas, Costas, Kokiopoulou, Effrosyni, and Saad, Yousef. An estimator for the diagonal of a matrix.
Applied numerical mathematics, 57(11):12141229, 2007.
Bradley, Andrew M and Murray, Walter. Matrix-free approximate equilibration. arXiv preprint
arXiv:1110.2805, 2011.
Choromanska, Anna, Henaff, Mikael, Mathieu, Michael, Arous, Grard Ben, and LeCun, Yann. The
loss surface of multilayer networks, 2014.
Datta, Biswa Nath. Numerical Linear Algebra and Applications, Second Edition. SIAM, 2nd edition,
2010. ISBN 0898716853, 9780898716856.
Dauphin, Yann, Pascanu, Razvan, Gulcehre, Caglar, Cho, Kyunghyun, Ganguli, Surya, and Bengio,
Yoshua. Identifying and attacking the saddle point problem in high-dimensional non-convex
optimization. In NIPS2014, 2014.
Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine Learning Research, 2011.
Guggenheimer, Heinrich W., Edelman, Alan S., and Johnson, Charles R. A simple estimate of the
condition number of a linear system. The College Mathematics Journal, 26(1):pp. 25, 1995.
ISSN 07468342. URL http://www.jstor.org/stable/2687283.
LeCun, Yann, Bottou, Leon, Orr, Genevieve B., and Muller, Klaus-Robert. Efficient backprop. In
Neural Networks, Tricks of the Trade, Lecture Notes in Computer Science LNCS 1524. Springer
Verlag, 1998.
Martens, J. Deep learning via Hessian-free optimization. In ICML2010, pp. 735742, 2010.
Martens, James, Sutskever, Ilya, and Swersky, Kevin. Estimating the hessian by back-propagating
curvature. arXiv preprint arXiv:1206.6464, 2012.
Pascanu, Razvan and Bengio, Yoshua. Revisiting natural gradient for deep networks. In International Conference on Learning Representations 2014(Conference Track), April 2014.
Schaul, Tom, Antonoglou, Ioannis, and Silver, David. Unit tests for stochastic optimization. arXiv
preprint arXiv:1312.6055, 2013.
Schraudolph, Nicol N. Fast curvature matrix-vector products for second-order gradient descent.
Neural Computation, 14(7):17231738, 2002.
Sluis, AVD. Condition numbers and equilibration of matrices. Numerische Mathematik, 14(1):
1423, 1969.
Sutskever, Ilya, Martens, James, Dahl, George, and Hinton, Geoffrey. On the importance of initialization and momentum in deep learning. In ICML, 2013.
Tieleman, Tijmen and Hinton, Geoffrey. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4, 2012.
Vinyals, Oriol and Povey, Daniel. Krylov subspace descent for deep learning. arXiv preprint
arXiv:1111.4259, 2011.
Zeiler, Matthew D. ADADELTA: an adaptive learning rate method. Technical report, arXiv
1212.5701, 2012. URL http://arxiv.org/abs/1212.5701.
K.A. Bollen. Structural Equations with Latent Variables. John Wiley & Sons, New York, USA, 1989.  8
P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. MIT Press, Cambridge, USA, 2nd edition, 2000.
D.M. Chickering. Optimal structure identification with greedy search. Journal of Machine Learning Research, 3:507554, 2002.
M.H. Maathuis, M. Kalisch, and P. Buhlmann. Estimating high-dimensional intervention effects from observational data. Annals of Statistics, 37:31333164, 2009.
A. Hauser and P. Buhlmann. Characterization and greedy learning of interventional Markov equivalence classes of directed acyclic graphs. Journal of Machine Learning Research, 13:24092464, 2012.
P.O. Hoyer, D. Janzing, J.M. Mooij, J. Peters, and B. Scholkopf. Nonlinear causal discovery with additive noise models. In Advances in Neural Information Processing Systems 21 (NIPS), pages 689696, 2009.
S. Shimizu, T. Inazumi, Y. Sogawa, A. Hyvarinen, Y. Kawahara, T. Washio, P.O. Hoyer, and K. Bollen. DirectLiNGAM: A direct method for learning a linear non-Gaussian structural equation model. Journal of Machine Learning Research, 12:12251248, 2011.
J.M. Mooij, D. Janzing, T. Heskes, and B. Scholkopf. On causal discovery with cyclic additive noise models. In Advances in Neural Information Processing Systems 24 (NIPS), pages 639647, 2011.
A. Hyttinen, F. Eberhardt, and P. O. Hoyer. Learning linear cyclic causal models with latent variables. Journal of Machine Learning Research, 13:33873439, 2012.
S.L. Lauritzen and T.S. Richardson. Chain graph models and their causal interpretations. Journal of the Royal Statistical Society, Series B, 64:321348, 2002.
G. Lacerda, P. Spirtes, J. Ramsey, and P.O. Hoyer. Discovering cyclic causal models by independent components analysis. In Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence (UAI), pages 366374, 2008.
R. Scheines, F. Eberhardt, and P.O. Hoyer. Combining experiments to discover linear cyclic models with latent variables. In International Conference on Artificial Intelligence and Statistics (AISTATS), pages 185192, 2010.
J. Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, New York, USA, 2nd edition, 2009.
F. Eberhardt, P. O. Hoyer, and R. Scheines. Combining experiments to discover linear cyclic models with latent variables. In International Conference on Artificial Intelligence and Statistics (AISTATS), pages 185192, 2010.
J. Peters, P. Buhlmann, and N. Meinshausen. Causal inference using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society, Series B, to appear., 2015.
A.L. Jackson, S.R. Bartz, J. Schelter, S.V. Kobayashi, J. Burchard, M. Mao, B. Li, G. Cavet, and P.S. Linsley. Expression profiling reveals off-target gene regulation by RNAi. Nature Biotechnology, 21:635 637, 2003.
M.M. Kulkarni, M. Booker, S.J. Silver, A. Friedman, P. Hong, N. Perrimon, and B. Mathey-Prevot. Evidence of off-target effects associated with long dsrnas in drosophila melanogaster cell-based assays. Nature methods, 3:833838, 2006.
D. Eaton and K. Murphy. Exact Bayesian structure learning from uncertain interventions. In International Conference on Artificial Intelligence and Statistics (AISTATS), pages 107114, 2007.
F. Eberhardt and R. Scheines. Interventions and causal inference. Philosophy of Science, 74:981995, 2007.
K. Korb, L. Hope, A. Nicholson, and K. Axnick. Varieties of causal intervention. In Proceedings of the Pacific Rim Conference on AI, pages 322331, 2004.
J. Tian and J. Pearl. Causal discovery from changes. In Proceedings of the 17th Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI), pages 512522, 2001.
K. Sachs, O. Perez, D. Peer, D. Lauffenburger, and G. Nolan. Causal protein-signaling networks derived from multiparameter single-cell data. Science, 308:523529, 2005.
A. Ziehe, P. Laskov, G. Nolte, and K.-R. Muller. A fast algorithm for joint diagonalization with nonorthogonal transformations and its application to blind source separation. Journal of Machine Learning Research, 5:801818, 2004.
R.E. Burkard. Quadratic assignment problems. In P. M. Pardalos, D.-Z. Du, and R. L. Graham, editors, Handbook of Combinatorial Optimization, pages 27412814. Springer New York, 2nd edition, 2013.
N. Meinshausen and P. Buhlmann. Stability selection. Journal of the Royal Statistical Society, Series B, 72:417473, 2010.
J.M. Mooij and T. Heskes. Cyclic causal discovery from continuous equilibrium data. In Proceedings of the 29th Annual Conference on Uncertainty in Artificial Intelligence (UAI), pages 431439, 2013.  9
P. Artzner, F. Delbaen, J. Eber, and D. Heath. Coherent measures of risk. Mathematical finance, 9(3): 203228, 1999.
N. Bauerle and J. Ott. Markov decision processes with average-value-at-risk criteria. Mathematical Methods of Operations Research, 74(3):361379, 2011.
D. Bertsekas. Dynamic programming and optimal control, Vol II. Athena Scientific, 4th edition, 2012.
V. Borkar and R. Jain. Risk-constrained Markov decision processes. IEEE Transaction of Automatic Control, 59(9):2574  2579, 2014.
Y. Chow and M. Ghavamzadeh. Algorithms for CVaR optimization in MDPs. In Advances in Neural Information Processing Systems 27, pages 35093517, 2014.
K. Dowd. Measuring market risk. John Wiley & Sons, 2007.
J. Filar, D. Krass, and K. Ross. Percentile performance criteria for limiting average Markov decision processes. Automatic Control, IEEE Transactions on, 40(1):210, 1995.
W. Haskell and R. Jain. A convex analytic approach to risk-aware Markov decision processes. SIAM Journal of Control and Optimization, 2014.
R. A. Howard and J. E. Matheson. Risk-sensitive Markov decision processes. Management Science, 18 (7):356369, 1972.
G. Iyengar. Robust dynamic programming. Mathematics of Operations Research, 30(2):257280, 2005.
G. Iyengar and A. Ma. Fast gradient descent method for mean-CVaR optimization. Annals of Operations Research, 205(1):203212, 2013.
S. Mannor, D. Simester, P. Sun, and J. Tsitsiklis. Bias and variance approximation in value function estimates. Management Science, 53(2):308322, 2007.
S. Mannor, O. Mebel, and H. Xu. Lightning does not strike twice: Robust MDPs with coupled uncertainty. In International Conference on Machine Learning, pages 385392, 2012.
P. Milgrom and I. Segal. Envelope theorems for arbitrary choice sets. Econometrica, 70(2):583601, 2002.
A. Nilim and L. El Ghaoui. Robust control of Markov decision processes with uncertain transition matrices. Operations Research, 53(5):780798, 2005.
T. Osogami. Robustness and risk-sensitivity in markov decision processes. In Advances in Neural Information Processing Systems, pages 233241, 2012.
G. Pflug and A. Pichler. Time consistent decisions and temporal decomposition of coherent risk functionals. Optimization online, 2015.
M. Phillips. Interpolation and approximation by polynomials, volume 14. Springer Science & Business Media, 2003.
L. Prashanth. Policy gradients for cvar-constrained mdps. In Algorithmic Learning Theory, pages 155 169. Springer, 2014.
R. Rockafellar and S. Uryasev. Optimization of conditional value-at-risk. Journal of risk, 2:2142, 2000.
R. Rockafellar, S. Uryasev, and M. Zabarankin. Master funds in portfolio analysis with general deviation measures. Journal of Banking & Finance, 30(2):743778, 2006.
G. Serraino and S. Uryasev. Conditional value-at-risk (CVaR). In Encyclopedia of Operations Research and Management Science, pages 258266. Springer, 2013.
A. Shapiro, D. Dentcheva, and A. Ruszczynski. Lectures on stochastic programming. SIAM, 2009.
M. Sobel. The variance of discounted Markov decision processes. Journal of Applied Probability, pages 794802, 1982.
A. Tamar, Y. Glassner, and S. Mannor. Optimizing the CVaR via sampling. In AAAI, 2015.
S. Uryasev, S. Sarykalin, G. Serraino, and K. Kalinchenko. VaR vs CVaR in risk management and optimization. In CARISMA conference, 2010.
H. Xu and S. Mannor. The robustness-performance tradeoff in Markov decision processes. In Advances in Neural Information Processing Systems, pages 15371544, 2006.  9
A. Agarwal and J. C. Duchi. Distributed delayed stochastic optimization. In Advances in Neural Information Processing Systems 24, 2011.
P. Baldi, P. Sadowski, and D. Whiteson. Searching for exotic particles in high-energy physics with deep learning. Nature Communications, 5, July 2014.
D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed Computation: Numerical Methods. Prentice-Hall, Inc., 1989.
J. C. Duchi, M. I. Jordan, and H. B. McMahan. Estimation, optimization, and parallelism when data is sparse. In Advances in Neural Information Processing Systems 26, 2013.
J. C. Duchi, S. Chaturapruek, and C. Re. Asynchronous stochastic convex optimization. arXiv:1508.00882
J. C. Duchi, S. Chaturapruek, and C. Re. Asynchronous stochastic convex optimization, 2015. URL https://www.codalab.org/worksheets/. Code for reproducing experiments.
Y. M. Ermoliev. On the stochastic quasi-gradient method and stochastic quasi-Feyer sequences. Kibernetika, 2:7283, 1969.
A. Juditsky, A. Nemirovski, and C. Tauvel. Solving variational inequalities with the stochastic mirror-prox algorithm. Stochastic Systems, 1(1):1758, 2011.
L. Le Cam and G. L. Yang. Asymptotics in Statistics: Some Basic Concepts. Springer, 2000.
E. L. Lehmann and G. Casella. Theory of Point Estimation, Second Edition. Springer, 1998.
D. Lewis, Y. Yang, T. Rose, and F. Li. RCV1: A new benchmark collection for text categorization research. Journal of Machine Learning Research, 5:361397, 2004.
M. Lichman. UCI machine learning repository, 2013. URL http://archive.ics.uci.edu/ml.
J. Liu, S. J. Wright, C. Re, V. Bittorf, and S. Sridhar. An asynchronous parallel stochastic coordinate descent algorithm. In Proceedings of the 31st International Conference on Machine Learning, 2014.
A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):15741609, 2009.
F. Niu, B. Recht, C. Re, and S. Wright. Hogwild: a lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems 24, 2011.
B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 30(4):838855, 1992.
B. Recht and C. Re. Beneath the valley of the noncommutative arithmetic-geometric mean inequality: conjectures, case-studies, and consequences. In Proceedings of the Twenty Fifth Annual Conference on Computational Learning Theory, 2012.
P. Richtarik and M. Takac. Parallel coordinate descent methods for big data optimization. Mathematical Programming, page Online first, 2015. URL http://link.springer.com/article/10.1007/s10107-015-0901-6.
H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical Statistics, 22:400407, 1951.
H. Robbins and D. Siegmund. A convergence theorem for non-negative almost supermartingales and some applications. In Optimizing Methods in Statistics, pages 233257. Academic Press, New York, 1971.
A. W. van der Vaart. Asymptotic Statistics. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 1998. ISBN 0-521-49603-9.  9
Sebastian Thrun and Tom M. Mitchell. Lifelong robot learning. Technical report, Robotics and Autonomous Systems, 1993.
Jonathan Baxter. A model of inductive bias learning. Journal of Artificial Intelligence Research, 12:149198, 2000.
Leslie G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):11341142, 1984.
Vladimir N. Vapnik. Estimation of Dependencies Based on Empirical Data. Springer, 1982.
Andreas Maurer. Algorithmic stability and meta-learning. Journal of Machine Learning Research (JMLR), 6:967994, 2005.
Gilles Blanchard, Gyemin Lee, and Clayton Scott. Generalizing from several related classification tasks to a new unlabeled sample. In Conference on Neural Information Processing Systems (NIPS), 2011.
Anastasia Pentina and Christoph H. Lampert. A PAC-Bayesian bound for lifelong learning. In International Conference on Machine Learing (ICML), 2014.
Andreas Maurer. Transfer bounds for linear feature learning. Machine Learning, 75:327350, 2009.
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. Sparse coding for multitask and transfer learning. In International Conference on Machine Learing (ICML), 2013.
Maria-Florina Balcan, Avrim Blum, and Santosh Vempala. Efficient representations for lifelong learning and autoencoding. In Workshop on Computational Learning Theory (COLT), 2015.
David A. McAllester. Some PAC-Bayesian theorems. Machine Learning, 37(3):355363, 1999.
Matthias Seeger. PAC-Bayesian generalisation error bounds for gaussian process classification. Journal of Machine Learning Research (JMLR), 3:233269, 2003.
Liva Ralaivola, Marie Szafranski, and Guillaume Stempfel. Chromatic PAC-Bayes bounds for non-iid data: Applications to ranking and stationary -mixing processes. Journal of Machine Learning Research (JMLR), 2010.
Daniel Ullman and Edward Scheinerman. Fractional Graph Theory: A Rational Approach to the Theory of Graphs. Wiley Interscience Series in Discrete Mathematics, 1997.
Monroe. D. Donsker and S. R. Srinivasa Varadhan. Asymptotic evaluation of certain Markov process expectations for large time. I. Communications on Pure and Applied Mathematics, 28:147, 1975.
Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58:1330, 1963.
Yevgeny Seldin, Franois Laviolette, Nicol Cesa-Bianchi, John Shawe-Taylor, and Peter Auer. PAC-Bayesian inequalities for martingales. IEEE Transactions on Information Theory, 58:70867093, 2012.
David A. McAllester. Simplified PAC-Bayesian margin bounds. In Workshop on Computational Learning Theory (COLT), 2003.
Francois Laviolette and Mario Marchand. PAC-Bayes risk bounds for stochastic averages and majority votes of sample-compressed classifiers. Journal of Machine Learning Research (JMLR), 8:14611487, 2007.
John Langford and John Shawe-Taylor. PAC-Bayes and margins. In Conference on Neural Information Processing Systems (NIPS), 2002.
Pascal Germain, Alexandre Lacasse, Francois Laviolette, and Mario Marchand. PAC-Bayesian learning of linear classifiers. In International Conference on Machine Learing (ICML), 2009.  9
A L Q U I E R , P. and B I A U , G . (2013). Sparse single-index model. Journal of Machine Learning Research, 14 243280.
B E R T H E T , Q . and R I G O L L E T , P. (2013). Complexity theoretic lower bounds for sparse principal component detection. In Conference on Learning Theory.
B O Y D , S ., P A R I K H , N ., C H U , E ., P E L E AT O , B . and E C K S T E I N , J . (2011). Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and R in Machine Learning, 3 1122. Trends  8
C A I , T. T., M A , Z . and W U , Y. (2013). Sparse PCA: Optimal rates and adaptive estimation. Annals of Statistics, 41 30743110.
C A N D  S , E . J ., E L D A R , Y. C ., S T R O H M E R , T. and V O R O N I N S K I , V. (2013). Phase retrieval via matrix completion. SIAM Journal on Imaging Sciences, 6 199225.
D  A S P R E M O N T , A ., B A C H , F. and E L G H A O U I , L . (2008). Optimal solutions for sparse principal component analysis. Journal of Machine Learning Research, 9 12691294.
D  A S P R E M O N T , A ., E L G H A O U I , L ., J O R D A N , M . I . and L A N C K R I E T , G . R . (2007). A direct formulation for sparse PCA using semidefinite programming. SIAM Review 434448.
D E L E C R O I X , M ., H R I S TA C H E , M . and P AT I L E A , V. (2000). Optimal smoothing in semiparametric index approximation of regression functions. Tech. rep., Interdisciplinary Research Project: Quantification and Simulation of Economic Processes.
D E L E C R O I X , M ., H R I S TA C H E , M . and P AT I L E A , V. (2006). On semiparametric M -estimation in single-index regression. Journal of Statistical Planning and Inference, 136 730769.
E L D A R , Y. C . and M E N D E L S O N , S . (2014). Phase retrieval: Stability and recovery guarantees. Applied and Computational Harmonic Analysis, 36 473494.
G O P I , S ., N E T R A PA L L I , P., J A I N , P. and N O R I , A . (2013). One-bit compressed sensing: Provable support and vector recovery. In International Conference on Machine Learning.
H A R D L E , W., H A L L , P. and I C H I M U R A , H . (1993). Optimal smoothing in single-index models. Annals of Statistics, 21 157178.
H R I S TA C H E , M ., J U D I T S K Y , A . and S P O K O I N Y , V. (2001). Direct estimation of the index coefficient in a single-index model. Annals of Statistics, 29 pp. 595623.
J A C Q U E S , L ., L A S K A , J . N ., B O U F O U N O S , P. T. and B A R A N I U K , R . G . (2011). Robust 1-bit compressive sensing via binary stable embeddings of sparse vectors. arXiv preprint arXiv:1104.3160.
K A K A D E , S . M ., K A N A D E , V., S H A M I R , O . and K A L A I , A . (2011). Efficient learning of generalized linear and single index models with isotonic regression. In Advances in Neural Information Processing Systems.
K A L A I , A . T. and S A S T R Y , R . (2009). The isotron algorithm: High-dimensional isotonic regression. In Conference on Learning Theory.
M A , Z . (2013). Sparse principal component analysis and iterative thresholding. The Annals of Statistics, 41 772801.
M A S S A R T , P. and P I C A R D , J . (2007). Concentration inequalities and model selection, vol. 1896. Springer.
N ATA R A J A N , N ., D H I L L O N , I ., R AV I K U M A R , P. and T E WA R I , A . (2013). Learning with noisy labels. In Advances in Neural Information Processing Systems.
P L A N , Y. and V E R S H Y N I N , R . (2013). One-bit compressed sensing by linear programming. Communications on Pure and Applied Mathematics, 66 12751297.
P L A N , Y., V E R S H Y N I N , R . and Y U D O V I N A , E . (2014). High-dimensional estimation with geometric constraints. arXiv preprint arXiv:1404.3749.
P O W E L L , J . L ., S T O C K , J . H . and S T O K E R , T. M . (1989). Semiparametric estimation of index coefficients. Econometrica, 57 pp. 14031430.
S H E N , H . and H U A N G , J . (2008). Sparse principal component analysis via regularized low rank matrix approximation. Journal of Multivariate Analysis, 99 10151034.
S T O K E R , T. M . (1986). Consistent estimation of scaled coefficients. Econometrica, 54 pp. 14611481.
T I B S H I R A N I , J . and M A N N I N G , C . D . (2013). Robust logistic regression using shift parameters. arXiv preprint arXiv:1305.4987.
V E R S H Y N I N , R . (2010). Introduction to the non-asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027.
V U , V. Q ., C H O , J ., L E I , J . and R O H E , K . (2013). Fantope projection and selection: A near-optimal convex relaxation of sparse PCA. In Advances in Neural Information Processing Systems.
W I T T E N , D ., T I B S H I R A N I , R . and H A S T I E , T. (2009). A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis. Biostatistics, 10 515534.
Y I , X ., W A N G , Z ., C A R A M A N I S , C . and L I U , H . (2015). Optimal linear estimation under unknown nonlinear transform. arXiv preprint arXiv:1505.03257.
Y U , B . (1997). Assouad, Fano, and Le Cam. In Festschrift for Lucien Le Cam. Springer, 423435.
Y U A N , X . - T. and Z H A N G , T. (2013). Truncated power method for sparse eigenvalue problems. Journal of Machine Learning Research, 14 899925.
Z O U , H ., H A S T I E , T. and T I B S H I R A N I , R . (2006). Sparse principal component analysis. Journal of Computational and Graphical Statistics, 15 265286.  9
F. Anselmi, J. Z. Leibo, L. Rosasco, J. Mutch, A. Tacchetti, and T. Poggio, Unsupervised learning of invariant representations in hierarchical architectures., CoRR, vol. abs/1311.4158, 2013.
J. Bruna and S. Mallat, Invariant scattering convolution networks, CoRR, vol. abs/1203.1513, 2012.
G. Hinton, A. Krizhevsky, and S. Wang, Transforming auto encoders, ICANN-11, 2011.
Y. Bengio, A. C. Courville, and P. Vincent, Representation learning: A review and new perspectives, IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 8, pp. 17981828, 2013.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, Gradient-based learning applied to document recognition, in Proceedings of the IEEE, vol. 86, pp. 22782324, 1998.
A. Krizhevsky, I. Sutskever, and G. E. Hinton, Imagenet classification with deep convolutional neural networks., in NIPS, pp. 11061114, 2012.
P. Niyogi, F. Girosi, and T. Poggio, Incorporating prior information in machine learning by creating virtual examples, in Proceedings of the IEEE, pp. 21962209, 1998.
Y.-A. Mostafa, Learning from hints in neural networks, Journal of complexity, vol. 6, pp. 192198, June 1990.
V. N. Vapnik, Statistical learning theory. A Wiley-Interscience Publication 1998.
I. Steinwart and A. Christmann, Support vector machines. Information Science and Statistics, New York: Springer, 2008.
B. Haasdonk, A. Vossen, and H. Burkhardt, Invariance in kernel methods by haar-integration kernels., in SCIA , Springer, 2005.
P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe, Convexity, classification, and risk bounds, Journal of the American Statistical Association, vol. 101, no. 473, pp. 138156, 2006.
G. Wahba, Spline models for observational data, vol. 59 of CBMS-NSF Regional Conference Series in Applied Mathematics. Philadelphia, PA: SIAM, 1990.
W. B. Johnson and J. Lindenstrauss, Extensions of lipschitz mappings into a hilbert space., Conference in modern analysis and probability, 1984.
A. Rahimi and B. Recht, Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning., in NIPS 2008.
A. Rahimi and B. Recht, Uniform approximation of functions with random bases, in Proceedings of the 46th Annual Allerton Conference, 2008.
C. Williams and M. Seeger, Using the nystrm method to speed up kernel machines, in NIPS, 2001.
F. R. Bach, On the equivalence between quadrature rules and random features, CoRR, vol. abs/1502.06800, 2015.
C. Walder and O. Chapelle, Learning with transformation invariant kernels, in NIPS, 2007.
Y. Cho and L. K. Saul, Kernel methods for deep learning, in NIPS, pp. 342350, 2009.
L. Bo, X. Ren, and D. Fox, Kernel descriptors for visual recognition, in NIPS., 2010.
J. Mairal, P. Koniusz, Z. Harchaoui, and C. Schmid, Convolutional kernel networks, in NIPS, 2014.
A. Tacchetti, P. K. Mallapragada, M. Santoro, and L. Rosasco, Gurls: a least squares library for supervised learning, CoRR, vol. abs/1303.0934, 2013.
S. Voinea, C. Zhang, G. Evangelopoulos, L. Rosasco, and T. Poggio, Word-level invariant representations from acoustic waveforms, vol. 14, pp. 32013205, September 2014.
M. Benzeghiba, R. De Mori, O. Deroo, S. Dupont, T. Erbes, D. Jouvet, L. Fissore, P. Laface, A. Mertins, C. Ris, R. Rose, V. Tyagi, and C. Wellekens, Automatic speech recognition and speech variability: A review, Speech Communication, vol. 49, pp. 763786, 01 2007.  9
Sivaraman Balakrishnan, Martin J Wainwright, and Bin Yu. Statistical guarantees for the EM algorithm: From population to sample-based analysis. arXiv preprint arXiv:1408.2156, 2014.
T Tony Cai and Anru Zhang. Rop: Matrix recovery via rank-one projections. The Annals of Statistics, 43(1):102138, 2015.
Emmanuel Candes and Terence Tao. The Dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, pages 23132351, 2007.
Emmanuel J Candes and Yaniv Plan. Tight oracle inequalities for low-rank matrix recovery from a minimal number of noisy random measurements. Information Theory, IEEE Transactions on, 57(4):2342 2359, 2011.
Arun Tejasvi Chaganty and Percy Liang. Spectral experts for estimating mixtures of linear regressions. arXiv preprint arXiv:1306.3729, 2013.
Yudong Chen, Sujay Sanghavi, and Huan Xu. Improved graph clustering. Information Theory, IEEE Transactions on, 60(10):64406455, Oct 2014.
Yudong Chen, Xinyang Yi, and Constantine Caramanis. A convex formulation for mixed regression with two components: Minimax optimal rates. In Conf. on Learning Theory, 2014.
Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society. Series B (methodological), pages 138, 1977.
Po-Ling Loh and Martin J Wainwright. High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity. In Advances in Neural Information Processing Systems, pages 2726 2734, 2011.
Po-Ling Loh and Martin J Wainwright. Corrupted and missing predictors: Minimax bounds for highdimensional linear regression. In Information Theory Proceedings (ISIT), 2012 IEEE International Symposium on, pages 26012605. IEEE, 2012.
Jinwen Ma and Lei Xu. Asymptotic convergence properties of the em algorithm with respect to the overlap in the mixture. Neurocomputing, 68:105129, 2005.
Geoffrey McLachlan and Thriyambakam Krishnan. The EM algorithm and extensions, volume 382. John Wiley & Sons, 2007.
Sahand Negahban, Martin J Wainwright, et al. Estimation of (near) low-rank matrices with noise and high-dimensional scaling. The Annals of Statistics, 39(2):10691097, 2011.
Sahand Negahban, Bin Yu, Martin J Wainwright, and Pradeep K Ravikumar. A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers. In Advances in Neural Information Processing Systems, pages 13481356, 2009.
Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM review, 52(3):471501, 2010.
Nicolas Stadler, Peter Buhlmann, and Sara Van De Geer. L1-penalization for mixture regression models. Test, 19(2):209256, 2010.
Paul Tseng. An analysis of the em algorithm and entropy-like proximal point methods. Mathematics of Operations Research, 29(1):2744, 2004.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027, 2010.
Martin J Wainwright. Structured regularizers for high-dimensional problems: Statistical and computational issues. Annual Review of Statistics and Its Application, 1:233253, 2014.
Zhaoran Wang, Quanquan Gu, Yang Ning, and Han Liu. High dimensional expectation-maximization algorithm: Statistical optimization and asymptotic normality. arXiv preprint arXiv:1412.8729, 2014.
C.F.Jeff Wu. On the convergence properties of the em algorithm. The Annals of statistics, pages 95103, 1983.
Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi. Alternating minimization for mixed linear regression. arXiv preprint arXiv:1310.3745, 2013.  9
Arash Asadpour, Hamid Nazerzadeh, and Amin Saberi. Stochastic submodular maximization. In Internet and Network Economics, pages 477489. 2008.
Gruia Calinescu and Alexander Zelikovsky. The polymatroid steiner problems. Journal of Combinatorial Optimization, 9(3):281294, 2005.
Nguyen Viet Cuong, Wee Sun Lee, and Nan Ye. Near-optimal Adaptive Pool-based Active Learning with General Loss. In Proc. Uncertainty in Artificial Intelligence, 2014.
Nguyen Viet Cuong, Wee Sun Lee, Nan Ye, Kian Ming A. Chai, and Hai Leong Chieu. Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion. In Advances in Neural Information Processing Systems (NIPS), 2013.
Daniel Golovin and Andreas Krause. Adaptive submodularity: Theory and applications in active learning and stochastic optimization. J. Artificial Intelligence Research, 42(1):427486, 2011.
Daniel Golovin, Andreas Krause, and Debajyoti Ray. Near-optimal bayesian active learning with noisy observations. In Advances in Neural Information Processing Systems (NIPS), pages 766774, 2010.
Andrew Guillory and Jeff Bilmes. Interactive submodular set cover. In International Conference on Machine Learning (ICML), Haifa, Israel, 2010.
Anupam Gupta, Viswanath Nagarajan, and R. Ravi. Approximation Algorithms for Optimal Decision Trees and Adaptive TSP Problems. In Samson Abramsky, Cyril Gavoille, Claude Kirchner, Friedhelm Meyer auf der Heide, and Paul G. Spirakis, editors, Automata, Languages and Programming, number 6198 in Lecture Notes in Computer Science, pages 690 701. Springer Berlin Heidelberg, January 2010.
Leslie Pack Kaelbling, Michael. L Littman, and Anthony R. Cassandra. Planning and acting in partially observable stochastic domains. Artificial Intelligence, 101:99134, January 1998.
Zhan Wei Lim, David Hsu, and Wee Sun Lee. Adaptive informative path planning in metric spaces. In Workshop on the Algorithmic Foundations of Robotics, 2014.
George L. Nemhauser, Laurence A. Wolsey, and Marshall L. Fisher. An analysis of approximations for maximizing submodular set functionsI. Mathematical Programming, 14(1):265 294, 1978.
Sylvie C. W. Ong, Shao Wei Png, David Hsu, and Wee Sun Lee. Planning under uncertainty for robotic tasks with mixed observability. Int. J. Robotics Research, 29(8):10531068, 2010.
David Silver and Joel Veness. Monte-Carlo Planning in Large POMDPs. Advances in Neural Information Processing Systems (NIPS), 2010.
Adhiraj Somani, Nan Ye, David Hsu, and Wee Sun Lee. Despot: Online pomdp planning with regularization. In Advances in Neural Information Processing Systems (NIPS), pages 17721780, 2013.
Alice X. Zheng, Irina Rish, and Alina Beygelzimer. Efficient Test Selection in Active Diagnosis via Entropy Approximation. Proc. Uncertainty in Artificial Intelligence, 2005.  9
Peter Auer, Mark Herbster, and Manfred K Warmuth. Exponentially many local minima for single neurons. Advances in neural information processing systems, pages 316322, 1996.
Yoshua Bengio. Learning deep architectures for AI. Foundations and trends in Machine Learning, 2(1):1127, 2009.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult. Neural Networks, IEEE Transactions on, 5(2):157166, 1994.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Kenji Doya. Bifurcations of recurrent neural networks in gradient descent learning. IEEE Transactions on neural networks, 1:7580, 1993.
Jean-Louis Goffin, Zhi-Quan Luo, and Yinyu Ye. Complexity analysis of an interior cutting plane method for convex feasibility problems. SIAM Journal on Optimization, 6(3):638652, 1996.
Adam Tauman Kalai and Ravi Sastry. The isotron algorithm: High-dimensional isotonic regression. In COLT, 2009.
Qifa Ke and Takeo Kanade. Quasiconvex optimization for robust geometric reconstruction. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 29(10):18341847, 2007.
Rustem F Khabibullin. A method to find a point of a convex set. Issled. Prik. Mat., 4:1522, 1977.
Krzysztof C Kiwiel. Convergence and efficiency of subgradient methods for quasiconvex minimization. Mathematical programming, 90(1):125, 2001.
Igor V Konnov. On convergence properties of a subgradient method. Optimization Methods and Software, 18(1):5362, 2003.
Jean-Jacques Laffont and David Martimort. The theory of incentives: the principal-agent model. Princeton university press, 2009.
James Martens and Ilya Sutskever. Learning recurrent neural networks with hessian-free optimization. In Proceedings of the 28th International Conference on Machine Learning (ICML11), pages 10331040, 2011.
P. McCullagh and JA Nelder. Generalised linear models. London: Chapman and Hall/CRC, 1989.
Yu E Nesterov. Minimization methods for nonsmooth convex and quasiconvex functions. Matekon, 29:519531, 1984.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In Proceedings of The 30th International Conference on Machine Learning, pages 13101318, 2013.
Boris T Polyak. A general method of solving extremum problems. Dokl. Akademii Nauk SSSR, 174(1):33, 1967.
Jarosaw Sikorski. Quasi subgradient algorithms for calculating surrogate constraints. In Analysis and algorithms of optimization problems, pages 203236. Springer, 1986.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 11391147, 2013.
Hal R Varian. Price discrimination and social welfare. The American Economic Review, pages 870875, 1985.
Elmar Wolfstetter. Topics in microeconomics: Industrial organization, auctions, and incentives. Cambridge University Press, 1999.
Yaroslav Ivanovich Zabotin, AI Korablev, and Rustem F Khabibullin. The minimization of quasicomplex functionals. Izv. Vyssh. Uch. Zaved. Mat., (10):2733, 1972.  9
B.D. Anderson and J.B. Moore. Optimal Filtering. Dover, 2005. 1
R.E. Kalman and R.S. Bucy. New results in linear filtering and prediction theory. J. of Basic Eng., Trans. ASME, Series D, 83(1):95108, 1961. 1  8
R.E. Kalman. A new approach to linear filtering and prediction problems. J. Basic Eng., Trans. ASME, Series D., 82(1):3545, 1960. 1
F. Daum. Nonlinear filters: beyond the kalman filter. Aerospace and Electronic Systems Magazine, IEEE, 20(8):5769, 2005. 1
S. Julier, J. Uhlmann, and H. Durrant-Whyte. A new method for the nonlinear transformation of means and covariances in filters and estimators. IEEE Trans. Autom. Control, 45(3):477482, 2000. 1
A. Doucet and A.M. Johansen. A tutorial on particle filtering and smoothing: fifteen years later. In D. Crisan and B. Rozovskii, editors, Handbook of Nonlinear Filtering, pages 656704. Oxford, UK: Oxford University Press, 2009. 1
P. Bremaud. Point Processes and Queues: Martingale Dynamics. Springer, New York, 1981. 1
D.L. Snyder and M.I. Miller. Random Point Processes in Time and Space. Springer, second edition edition, 1991. 1, 2.1
P. Dayan and L.F. Abbott. Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems. MIT Press, 2005. 1
O. Bobrowski, R. Meir, and Y.C. Eldar. Bayesian filtering in spiking neural networks: noise, adaptation, and multisensory integration. Neural Comput, 21(5):12771320, May 2009. 1
Y. Ahmadian, J.W. Pillow, and L. Paninski. Efficient markov chain monte carlo methods for decoding neural spike trains. Neural Comput, 23(1):4696, Jan 2011. 1
A.K. Susemihl, R. Meir, and M. Opper. Analytical results for the error in filtering of gaussian processes. In J. Shawe-Taylor, R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 23032311. 2011. 1
A.K. Susemihl, R. Meir, and M. Opper. Dynamic state estimation based on poisson spike trains towards a theory of optimal encoding. Journal of Statistical Mechanics: Theory and Experiment, 2013(03):P03009, 2013. 1, 3.2, 3.2, 5
P.S. Maybeck. Stochastic Models, Estimation, and Control. Academic Press, 1979. 1
D. Brigo, B. Hanzon, and F. LeGland. A differential geometric approach to nonlinear filtering: the projection filter. Automatic Control, IEEE Transactions on, 43:247252, 1998. 1
M. Opper. A Bayesian approach to online learning. In D. Saad, editor, Online Learning in Neural Networks, pages 363378. Cambridge university press, 1998. 1, 3.2
T.P. Minka. Expectation propagation for approximate bayesian inference. In Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence, pages 362369. Morgan Kaufmann Publishers Inc., 2001. 1
N.S. Harper and D. McAlpine. Optimal neural population coding of an auditory spatial cue. Nature, 430(7000):682686, Aug 2004. n1397b. 1, 4.1, 4.2
M. Bethge, D. Rotermund, and K. Pawelzik. Optimal short-term population coding: when fisher information fails. Neural Comput, 14(10):23172351, Oct 2002. 1
S. Yaeli and R. Meir. Error-based analysis of optimal tuning functions explains phenomena observed in sensory neurons. Front Comput Neurosci, 4:130, 2010. 1, 2.1
D. Ganguli and E.P. Simoncelli. Efficient sensory encoding and bayesian inference with heterogeneous neural populations. Neural Comput, 26(10):21032134, 2014. 1, 4.1
I. Rhodes and D. Snyder. Estimation and control performance for space-time point-process observations. IEEE Transactions on Automatic Control, 22(3):338346, 1977. 2.1, 3.1, 3.1, 1, 3.3, 3.3
A.K. Susemihl, R. Meir, and M. Opper. Optimal Neural Codes for Control and Estimation. Advances in Neural Information Processing Systems, pages 19, 2014. 2.1, 3.1, 3.2, 3.3, 3.3
D. Snyder. Filtering and detection for doubly stochastic Poisson processes. IEEE Transactions on Information Theory, 18(1):91102, January 1972. 3.1
A. Brand, O. Behrend, T. Marquardt, D. McAlpine, and B. Grothe. Precise inhibition is essential for microsecond interaural time difference coding. Nature, 417(6888):543547, 2002. 4.1  9
Venkat Chandrasekaran and Michael I. Jordan. Computational and statistical tradeoffs via convex relaxation. Proceedings of the National Academy of Sciences, 110(13):E1181E1190, 2013.
IM Johnstone. Function estimation and gaussian sequence models. Unpublished manuscript, 2002.
D. L. Donoho. De-noising by soft-thresholding. IEEE Trans. Inf. Theor., 41(3):613627, May 1995.
David L. Donoho and Iain M. Johnstone. Minimax estimation via wavelet shrinkage. Ann. Statist., 26(3):879921, 06 1998.
Quentin Berthet and Philippe Rigollet. Complexity theoretic lower bounds for sparse principal component detection. In COLT 2013 - The 26th Annual Conference on Learning Theory, June 12-14, 2013, Princeton University, NJ, USA, pages 10461066, 2013.
Scott Decatur, Oded Goldreich, and Dana Ron. Computational sample complexity. In Proceedings of the Tenth Annual Conference on Computational Learning Theory, COLT 97, pages 130142, New York, NY, USA, 1997. ACM.
Rocco A. Servedio. Computational sample complexity and attribute-efficient learning. Journal of Computer and System Sciences, 60(1):161  178, 2000.
Amit Daniely, Nati Linial, and Shai Shalev-Shwartz. More data speeds up training time in learning halfspaces over sparse vectors. In Christopher J. C. Burges, Leon Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States., pages 145153, 2013.
C. Gao, Z. Ma, and H. H. Zhou. Sparse CCA: Adaptive Estimation and Computational Barriers. ArXiv e-prints, September 2014.
Jean B. Lasserre. Global optimization with polynomials and the problem of moments. SIAM Journal on Optimization, 11(3):796817, 2001.
Pablo A. Parrilo. Structured Semidefinite Programs and Semialgebraic Geometry Methods in Robustness and Optimization. PhD thesis, California Institute of Technology, 2000.
Dima Grigoriev. Linear lower bound on degrees of positivstellensatz calculus proofs for the parity. Theoretical Computer Science, 259(1):613622, 2001.
Emil Artin. Uber die zerlegung definiter funktionen in quadrate. In Abhandlungen aus dem mathematischen Seminar der Universitat Hamburg, volume 5, pages 100115. Springer, 1927.
Jean-Louis Krivine. Anneaux preordonnes. Journal danalyse mathematique, 1964.
Gilbert Stengle. A nullstellensatz and a positivstellensatz in semialgebraic geometry. Mathematische Annalen, 207(2):8797, 1974.
N.Z. Shor. An approach to obtaining global extremums in polynomial mathematical programming problems. Cybernetics, 23(5):695700, 1987.
Konrad Schmudgen. Thek-moment problem for compact semi-algebraic sets. Mathematische Annalen, 289(1):203206, 1991.
Mihai Putinar. Positive polynomials on compact semi-algebraic sets. Indiana University Mathematics Journal, 42(3):969984, 1993.
Yurii Nesterov. Squared functional systems and optimization problems. In Hans Frenk, Kees Roos, Tams Terlaky, and Shuzhong Zhang, editors, High Performance Optimization, volume 33 of Applied Optimization, pages 405440. Springer US, 2000.
Jean Bernard Lasserre. An introduction to polynomial and semi-algebraic optimization. Cambridge Texts in Applied Mathematics. Cambridge: Cambridge University Press. , 2015.
Monique Laurent. Sums of squares, moment matrices and optimization over polynomials. In Mihai Putinar and Seth Sullivant, editors, Emerging Applications of Algebraic Geometry, volume 149 of The IMA Volumes in Mathematics and its Applications, pages 157270. Springer New York, 2009.
Hanif D. Sherali and Warren P. Adams. A hierarchy of relaxations between the continuous and convex hull representations for zero-one programming problems. SIAM Journal on Discrete Mathematics, 3(3):411 430, 1990.
L. Lovasz and A. Schrijver. Cones of matrices and set-functions and 01 optimization. SIAM Journal on Optimization, 1(2):166190, 1991.  8
Boaz Barak, Jonathan A. Kelner, and David Steurer. Dictionary learning and tensor decomposition via the sum-of-squares method. In Proceedings of the Forty-seventh Annual ACM Symposium on Theory of Computing, STOC 15, 2015.
Boaz Barak, Jonathan A. Kelner, and David Steurer. Rounding sum-of-squares relaxations. In STOC, pages 3140, 2014.
Boaz Barak and Ankur Moitra. Tensor prediction, rademacher complexity and random 3-xor. CoRR, abs/1501.06521, 2015.
Boaz Barak and David Steurer. Sum-of-squares proofs and the quest toward optimal algorithms. In Proceedings of International Congress of Mathematicians (ICM), 2014. To appear.
D. Grigoriev. Complexity of positivstellensatz proofs for the knapsack. computational complexity, 10(2):139154, 2001.
Grant Schoenebeck. Linear level lasserre lower bounds for certain k-csps. In Proceedings of the 2008 49th Annual IEEE Symposium on Foundations of Computer Science, FOCS 08, pages 593602, Washington, DC, USA, 2008. IEEE Computer Society.
Raghu Meka, Aaron Potechin, and Avi Wigderson. Sum-of-squares lower bounds for planted clique. CoRR, abs/1503.06447, 2015.
Z. Wang, Q. Gu, and H. Liu. Statistical Limits of Convex Relaxations. ArXiv e-prints, March 2015.
Iain M. Johnstone. On the distribution of the largest eigenvalue in principal components analysis. Ann. Statist., 29(2):295327, 04 2001.
Zongming Ma. Sparse principal component analysis and iterative thresholding. Ann. Statist., 41(2):772 801, 04 2013.
Vincent Q. Vu and Jing Lei. Minimax sparse principal subspace estimation in high dimensions. Ann. Statist., 41(6):29052947, 12 2013.
U. Alon, N. Barkai, D. A. Notterman, K. Gish, S. Ybarra, D. Mack, and A. J. Levine. Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays. Proceedings of the National Academy of Sciences, 96(12):67456750, 1999.
Iain M. Johnstone and Arthur Yu Lu. On consistency and sparsity for principal components analysis in high dimensions. Journal of the American Statistical Association, 104(486):pp. 682703, 2009.
Xi Chen. Adaptive elastic-net sparse principal component analysis for pathway association testing. Statistical Applications in Genetics and Molecular Biology, 10, 2011.
Rodolphe Jenatton, Guillaume Obozinski, and Francis R. Bach. Structured sparse principal component analysis. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010, pages 366373, 2010.
Vincent Q. Vu and Jing Lei. Minimax rates of estimation for sparse PCA in high dimensions. In Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2012, La Palma, Canary Islands, April 21-23, 2012, pages 12781286, 2012.
Debashis Paul and Iain M Johnstone. Augmented sparse principal component analysis for high dimensional data. arXiv preprint arXiv:1202.1242, 2012.
Quentin Berthet and Philippe Rigollet. Optimal detection of sparse principal components in high dimension. The Annals of Statistics, 41(4):17801815, 2013.
Arash A. Amini and Martin J. Wainwright. High-dimensional analysis of semidefinite relaxations for sparse principal components. Ann. Statist., 37(5B):28772921, 10 2009.
Yash Deshpande and Andrea Montanari. Sparse PCA via covariance thresholding. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 334342, 2014.
Alexandre dAspremont, Laurent El Ghaoui, Michael I. Jordan, and Gert R. G. Lanckriet. A direct formulation for sparse pca using semidefinite programming. SIAM Review, 49(3):434448, 2007.
Robert Krauthgamer, Boaz Nadler, and Dan Vilenchik. Do semidefinite relaxations solve sparse pca up to the information limit? The Annals of Statistics, 43(3):13001322, 2015.
Y. Deshpande and A. Montanari. Improved Sum-of-Squares Lower Bounds for Hidden Clique and Hidden Submatrix Problems. ArXiv e-prints, February 2015.
Prasad Raghavendra and Tselil Schramm. Tight lower bounds for planted clique in the degree-4 SOS program. CoRR, abs/1507.05136, 2015.
Samuel B. Hopkins, Pravesh K. Kothari, and Aaron Potechin. Sos and planted clique: Tight analysis of MPW moments at all degrees and an optimal lower bound at degree four. CoRR, abs/1507.05230, 2015.
Tengyu Ma and Philippe Rigollet. personal communication, 2014.  9
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. Hruschka Jr, and T. M. Mitchell. Toward an architecture for never-ending language learning. In AAAI, 2010.
C. C. Chang and C. J. Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:127:27, 2011.
C. Chen, J. Zhu, and X. Zhang. Robust Bayesian max-margin clustering. In NIPS, 2014.
K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector machines. JMLR, 2:265292, 2002.
A. P. Dawid and A. M. Skene. Maximum likelihood estimation of observer error-rates using the EM algorithm. Applied Statistics, pages 2028, 1979.
M. Dudk, S. J. Phillips, and R. E. Schapire. Maximum entropy density estimation with generalized regularization and an application to species distribution modeling. JMLR, 8(6), 2007.
K. Ganchev, J. Graca, J. Gillenwater, and B. Taskar. Posterior regularization for structured latent variable models. JMLR, 11:20012049, 2010.
Otto C. Liu X. Han, H. and A. Jain. Demographic estimation from face images: Human vs. machine performance. IEEE Trans. on PAMI, 2014.
S. Jagabathula, L. Subramanian, and A. Venkataraman. Reputation-based worker filtering in crowdsourcing. In NIPS, 2014.
D. R. Karger, S. Oh, and D. Shah. Iterative learning for reliable crowdsourcing systems. In NIPS, 2011.
H. Li and B. Yu. Error rate bounds and iterative weighted majority voting for crowdsourcing. arXiv preprint arXiv:1411.4086, 2014.
Q. Liu, J. Peng, and A. Ihler. Variational inference for crowdsourcing. In NIPS, 2012.
J. R. Michael, W. R. Schucany, and R. W. Haas. Generating random variates using transformations with multiple roots. The American Statistician, 30(2):8890, 1976.
N. G. Polson and S. L. Scott. Data augmentation for support vector machines. Bayesian Analysis, 6(1):123, 2011.
V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni, and L. Moy. Learning from crowds. JMLR, 11:12971322, 2010.
T. Shi and J. Zhu. Online Bayesian passive-aggressive learning. In ICML, 2014.
R. Snow, B. OConnor, D. Jurafsky, and A. Y. Ng. Cheap and fastbut is it good?: evaluating non-expert annotations for natural language tasks. In EMNLP, 2008.
T. Tian and J. Zhu. Uncovering the latent structures of crowd labeling. In PAKDD, 2015.
P. Welinder, S. Branson, P. Perona, and S. J. Belongie. The multidimensional wisdom of crowds. In NIPS, 2010.
J. Whitehill, T. F. Wu, J. Bergsma, J. R. Movellan, and P. L. Ruvolo. Whose vote should count more: Optimal integration of labels from labelers of unknown expertise. In NIPS, 2009.
L. Xu and D. Schuurmans. Unsupervised and semi-supervised multi-class support vector machines. In AAAI, 2005.
O. F. Zaidan and C. Callison-Burch. Crowdsourcing translation: Professional quality from non-professionals. In ACL, 2011.
Y. Zhang, X. Chen, D. Zhou, and M. I. Jordan. Spectral methods meet EM: A provably optimal algorithm for crowdsourcing. In NIPS, 2014.
D. Zhou, S. Basu, Y. Mao, and J. C. Platt. Learning from the wisdom of crowds by minimax entropy. In NIPS, 2012.
D. Zhou, Q. Liu, J. Platt, and C. Meek. Aggregating ordinal labels from crowds by minimax conditional entropy. In ICML, 2014.
J. Zhu, N. Chen, H. Perkins, and B. Zhang. Gibbs max-margin topic models with data augmentation. JMLR, 15:10731110, 2014.
J. Zhu, N. Chen, and E. P. Xing. Bayesian inference with posterior regularization and applications to infinite latent svms. JMLR, 15:17991847, 2014. 9
F. Bach and A. Dieuleveut. arXiv:1408.0361, 2014.  Non-parametric stochastic approximation with large step sizes.
P. Bartlett and M. Traskin. Adaboost is consistent. J. Mach. Learn. Res., 8:23472368, 2007.
F. Bauer, S. Pereverzev, and L. Rosasco. On regularization algorithms in learning theory. J. Complexity, 23(1):5272, 2007.
D. P. Bertsekas. A new class of incremental gradient methods for least squares problems. SIAM J. Optim., 7(4):913926, 1997.
G. Blanchard and N. Kramer. Optimal learning rates for kernel conjugate gradient regression. In Advances in Neural Inf. Proc. Systems (NIPS), pages 226234, 2010.  8
L. Bottou and O. Bousquet. The tradeoffs of large scale learning. In Suvrit Sra, Sebastian Nowozin, and Stephen J. Wright, editors, Optimization for Machine Learning, pages 351368. MIT Press, 2011.
P. Buhlmann and B. Yu. Boosting with the l2 loss: Regression and classification. J. Amer. Stat. Assoc., 98:324339, 2003.
A. Caponnetto and E. De Vito. Optimal rates for regularized least-squares algorithm. Found. Comput. Math., 2006.
A. Caponnetto and Y. Yao. Cross-validation based adaptation for regularization operators in learning theory. Anal. Appl., 08:161183, 2010.
N. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning algorithms. IEEE Trans. Information Theory, 50(9):20502057, 2004.
N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press, 2006.
F. Cucker and D. X. Zhou. Learning Theory: An Approximation Theory Viewpoint. Cambridge University Press, 2007.
E. De Vito, L. Rosasco, A. Caponnetto, U. De Giovannini, and F. Odone. Learning from examples as an inverse problem. J.Mach. Learn. Res., 6:883904, 2005.
E. De Vito, L. Rosasco, A. Caponnetto, M. Piana, and A. Verri. Some properties of regularized kernel methods. Journal of Machine Learning Research, 5:13631390, 2004.
H. W. Engl, M. Hanke, and A. Neubauer. Regularization of inverse problems. Kluwer, 1996.
P.-S. Huang, H. Avron, T. Sainath, V. Sindhwani, and B. Ramabhadran. Kernel methods match deep neural networks on timit. In IEEE ICASSP, 2014.
W. Jiang. Process consistency for adaboost. Ann. Stat., 32:1329, 2004.
Y. LeCun, L. Bottou, G. Orr, and K. Muller. Efficient backprop. In G. Orr and Muller K., editors, Neural Networks: Tricks of the trade. Springer, 1998.
A. Nedic and D. P Bertsekas. Incremental subgradient methods for nondifferentiable optimization. SIAM Journal on Optimization, 12(1):109138, 2001.
A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM J. Optim., 19(4):15741609, 2008.
A. Nemirovskii. The regularization properties of adjoint gradient method in ill-posed problems. USSR Computational Mathematics and Mathematical Physics, 26(2):716, 1986.
F. Orabona. Simultaneous model selection and optimization through parameter-free stochastic learning. NIPS Proceedings, 2014.
I. Pinelis. Optimum bounds for the distributions of martingales in Banach spaces. Ann. Probab., 22(4):16791706, 1994.
B. Polyak. Introduction to Optimization. Optimization Software, New York, 1987.
J. Ramsay and B. Silverman. Functional Data Analysis. Springer-Verlag, New York, 2005.
G. Raskutti, M. Wainwright, and B. Yu. Early stopping for non-parametric regression: An optimal datadependent stopping rule. In in 49th Annual Allerton Conference, pages 13181325. IEEE, 2011.
S. Smale and D. Zhou. Shannon sampling II: Connections to learning theory. Appl. Comput. Harmon. Anal., 19(3):285302, November 2005.
S. Smale and D.-X. Zhou. Learning theory estimates via integral operators and their approximations. Constr. Approx., 26(2):153172, 2007.
N. Srebro, K. Sridharan, and A. Tewari. Optimistic rates for learning with a smooth loss. arXiv:1009.3896, 2012.
I. Steinwart and A. Christmann. Support Vector Machines. Springer, 2008.
I. Steinwart, D. R. Hush, and C. Scovel. Optimal rates for regularized least squares regression. In COLT, 2009.
P. Tarres and Y. Yao. Online learning as stochastic approximation of regularization paths: optimality and almost-sure convergence. IEEE Trans. Inform. Theory, 60(9):57165735, 2014.
V. Vapnik. Statistical learning theory. Wiley, New York, 1998.
Y. Yao, L. Rosasco, and A. Caponnetto. On early stopping in gradient descent learning. Constr. Approx., 26:289315, 2007.
Y. Ying and M. Pontil. Online gradient descent learning algorithms. Found. Comput. Math., 8:561596, 2008.
T. Zhang and B. Yu. Boosting with early stopping: Convergence and consistency. Annals of Statistics, pages 15381579, 2005.  9
Borgwardt, K. M. Graph Kernels. PhD thesis, Ludwig-Maximilians-University Munich, 2007.
Borgwardt, K. M., Ong, C. S., Schonauer, S., Vishwanathan, S. V. N., Smola, A. J., and Kriegel, H.-P. Protein function prediction via graph kernels. Bioinformatics, 21(suppl 1):i47i56, 2005.
Brualdi, R. A. The Mutually Beneficial Relationship of Graphs and Matrices. AMS, 2011.
Costa, F. and Grave, K. D. Fast neighborhood subgraph pairwise distance kernel. In Proceedings of the 27th International Conference on Machine Learning (ICML), 255262, 2010.
Gartner, T., Flach, P., and Wrobel, S. On graph kernels: Hardness results and efficient alternatives. In Learning Theory and Kernel Machines (LNCS 2777), 129143, 2003.
Girvan, M. and Newman, M. E. J. Community structure in social and biological networks. Proceedings of the National Academy of Sciences (PNAS), 99(12):78217826, 2002.
Kashima, H., Tsuda, K., and Inokuchi, A. Marginalized kernels between labeled graphs. In Proceedings of the 20th International Conference on Machine Learning (ICML), 321328, 2003.
Katz, L. A new status index derived from sociometric analysis. Psychometrika, 18(1):3943, 1953.
Kriege, N., Neumann, M., Kersting, K., and Mutzel, P. Explicit versus implicit graph feature maps: A computational phase transition for walk kernels. In Proceedings of IEEE International Conference on Data Mining (ICDM), 881886, 2014.
Liben-Nowell, D. and Kleinberg, J. The link-prediction problem for social networks. Journal of the American Society for Information Science and Technology, 58(7):10191031, 2007.
Mahe, P., Ueda, N., Akutsu, T., Perret, J.-L., and Vert, J.-P. Extensions of marginalized graph kernels. In Proceedings of the 21st International Conference on Machine Learning (ICML), 2004.
Shervashidze, N. and Borgwardt, K. M. Fast subtree kernels on graphs. In Advances in Neural Information Processing Systems (NIPS) 22, 16601668, 2009.
Shervashidze, N., Schweitzer, P., van Leeuwen, E. J., Mehlhorn, K., and Borgwardt, K. M. Weisfeiler-Lehman graph kernels. Journal of Machine Learning Research, 12:23592561, 2011.
Vishwanathan, S. V. N., Schraudolph, N. N., Kondor, R., and Borgwardt, K. M. Graph kernels. Journal of Machine Learning Research, 11:12011242, 2010.  9
T. V. Nguyen and E. V. Bonilla. Automated variational inference for Gaussian process models. In NIPS, pages 14041412, 2014.
L. Csato and M. Opper. Sparse on-line Gaussian processes. Neural comp., 14(3):641668, 2002.
E. Snelson and Z. Ghahramani. Sparse Gaussian processes using pseudo-inputs. In NIPS, pages 1257 1264, 2005.
M. K. Titsias. Variational learning of inducing variables in sparse Gaussian processes. In AISTATS, pages 567574, 2009.
M. Lazaro-Gredilla, J. Quinonero-Candela, C. E. Rasmussen, and A. Figueiras-Vidal. Sparse spectrum Gaussian process regression. JMLR, 11:18651881, 2010.
A. Solin and S. Sarkka. Hilbert space methods for reduced-rank Gaussian process regression. arXiv preprint 1401.5508, 2014.
A. G. Wilson, E. Gilboa, A. Nehorai, and J. P. Cunningham. Fast kernel learning for multidimensional pattern extrapolation. In NIPS, pages 36263634. 2014.
S. Sarkka. Bayesian filtering and smoothing, volume 3. Cambridge University Press, 2013.
M. Filippone and R. Engler. Enabling scalable stochastic gradient-based inference for Gaussian processes by employing the Unbiased LInear System SolvEr (ULISSE). ICML 2015, 2015.
A. G. D. G. Matthews, J. Hensman, R. E. Turner, and Z. Ghahramani. On sparse variational methods and the KL divergence between stochastic processes. arXiv preprint 1504.07027, 2015.
I. Murray and R. P. Adams. Slice sampling covariance hyperparameters of latent Gaussian models. In NIPS, pages 17321740, 2010.
M. Filippone, M. Zhong, and M. Girolami. A comparative evaluation of stochastic-based inference methods for Gaussian process models. Mach. Learn., 93(1):93114, 2013.
M. N. Gibbs and D. J. C. MacKay. Variational Gaussian process classifiers. IEEE Trans. Neural Netw., 11(6):14581464, 2000.
M. Opper and C. Archambeau. The variational Gaussian approximation revisited. Neural comp., 21(3): 786792, 2009.
M. Kuss and C. E. Rasmussen. Assessing approximate inference for binary Gaussian process classification. JMLR, 6:16791704, 2005.
H. Nickisch and C. E. Rasmussen. Approximations for binary Gaussian process classification. JMLR, 9: 20352078, 2008.
E. Khan, S. Mohamed, and K. P. Murphy. Fast Bayesian inference for non-conjugate Gaussian process regression. In NIPS, pages 31403148, 2012.
K. M. A. Chai. Variational multinomial logit Gaussian process. JMLR, 13(1):17451808, June 2012.
C. Lloyd, T. Gunter, M. A. Osborne, and S. J. Roberts. Variational inference for Gaussian process modulated poisson processes. ICML 2015, 2015.
J. Hensman, A. Matthews, and Z. Ghahramani. Scalable variational Gaussian process classification. In AISTATS, pages 351360, 2014.
C. K. I. Williams and D. Barber. Bayesian classification with Gaussian processes. IEEE Trans. Pattern Anal. Mach. Intell., 20(12):13421351, 1998.
Michalis K Titsias, Neil Lawrence, and Magnus Rattray. Markov chain monte carlo algorithms for gaussian processes. In D. Barber, A. T. Chiappa, and S. Cemgil, editors, Bayesian time series models. 2011.
S. P. Smith. Differentiation of the cholesky algorithm. J. Comp. Graph. Stat., 4(2):134147, 1995.
J. Quinonero-Candela and C. E. Rasmussen. A unifying view of sparse approximate Gaussian process regression. JMLR, 6:19391959, 2005.
Z. Wang, S. Mohamed, and N. De Freitas. Adaptive Hamiltonian and Riemann manifold Monte Carlo. In ICML, volume 28, pages 14621470, 2013.
J. Vanhatalo and A. Vehtari. Sparse Log Gaussian Processes via MCMC for Spatial Epidemiology. In Gaussian processes in practice, volume 1, pages 7389, 2007.
O. F. Christensen, G. O. Roberts, and J. S. Rosenthal. Scaling limits for the transient phase of local MetropolisHastings algorithms. JRSS:B, 67(2):253268, 2005.
I. Murray, R. P. Adams, and D. J. C. MacKay. Elliptical slice sampling. In AISTATS, volume 9, 2010.
G. Ratsch, T. Onoda, and K-R Muller. Soft margins for adaboost. Mach. Learn., 42(3):287320, 2001.
J. Mller, A. R. Syversveen, and R. P. Waagepetersen. Log Gaussian Cox processes. Scand. stat., 25(3): 451482, 1998.
M. Girolami and S. Rogers. Variational Bayesian multinomial probit regression with Gaussian process priors. Neural Comp., 18:2006, 2005.
H. Kim and Z. Ghahramani. Bayesian Gaussian Process Classification with the EM-EP Algorithm. IEEE TPAMI, 28(12):19481959, 2006.
D. Hernandez-Lobato, J. M. Hernandez-Lobato, and P. Dupont. Robust multi-class Gaussian process classification. In NIPS, pages 280288, 2011.
Y. Gal, M. Van der Wilk, and Rasmussen C. E. Distributed variational inference in sparse Gaussian process regression and latent variable models. In NIPS. 2014.  9
Bernhard Scholkopf and Alexander J. Smola. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond (Adaptive Computation and Machine Learning). MIT Press, 2002.
Alex J. Smola and Bernhard Scholkopf. Sparse Greedy Matrix Approximation for Machine Learning. In ICML, pages 911918. Morgan Kaufmann, 2000.
C. Williams and M. Seeger. Using the Nystrom Method to Speed Up Kernel Machines. In NIPS, pages 682688. MIT Press, 2000.
Ali Rahimi and Benjamin Recht. Random Features for Large-Scale Kernel Machines. In NIPS, pages 11771184. Curran Associates, Inc., 2007.  8
J. Yang, V. Sindhwani, H. Avron, and M. W. Mahoney. Quasi-Monte Carlo Feature Maps for ShiftInvariant Kernels. In ICML, volume 32 of JMLR Proceedings, pages 485493. JMLR.org, 2014.
Quoc V. Le, Tamas Sarlos, and Alexander J. Smola. Fastfood - Computing Hilbert Space Expansions in loglinear time. In ICML, volume 28 of JMLR Proceedings, pages 244252. JMLR.org, 2013.
Si Si, Cho-Jui Hsieh, and Inderjit S. Dhillon. Memory Efficient Kernel Approximation. In ICML, volume 32 of JMLR Proceedings, pages 701709. JMLR.org, 2014.
Yuchen Zhang, John C. Duchi, and Martin J. Wainwright. Divide and Conquer Kernel Ridge Regression. In COLT, volume 30 of JMLR Proceedings, pages 592617. JMLR.org, 2013.
S. Kumar, M. Mohri, and A. Talwalkar. Ensemble Nystrom Method. In NIPS, pages 10601068, 2009.
Mu Li, James T. Kwok, and Bao-Liang Lu. Making Large-Scale Nystrom Approximation Possible. In ICML, pages 631638. Omnipress, 2010.
Kai Zhang, Ivor W. Tsang, and James T. Kwok. Improved Nystrom Low-rank Approximation and Error Analysis. ICML, pages 12321239. ACM, 2008.
Bo Dai, Bo Xie 0002, Niao He, Yingyu Liang, Anant Raj, Maria-Florina Balcan, and Le Song. Scalable Kernel Methods via Doubly Stochastic Gradients. In NIPS, pages 30413049, 2014.
Petros Drineas and Michael W. Mahoney. On the Nystrom Method for Approximating a Gram Matrix for Improved Kernel-Based Learning. JMLR, 6:21532175, December 2005.
A. Gittens and M. W. Mahoney. Revisiting the Nystrom method for improved large-scale machine learning. 28:567575, 2013.
Shusen Wang and Zhihua Zhang. Improving CUR Matrix Decomposition and the Nystrom Approximation via Adaptive Sampling. JMLR, 14(1):27292769, 2013.
Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, and David P. Woodruff. Fast approximation of matrix coherence and statistical leverage. JMLR, 13:34753506, 2012.
Michael B. Cohen, Yin Tat Lee, Cameron Musco, Christopher Musco, Richard Peng, and Aaron Sidford. Uniform Sampling for Matrix Approximation. In ITCS, pages 181190. ACM, 2015.
Shusen Wang and Zhihua Zhang. Efficient Algorithms and Error Analysis for the Modified Nystrom Method. In AISTATS, volume 33 of JMLR Proceedings, pages 9961004. JMLR.org, 2014.
S. Kumar, M. Mohri, and A. Talwalkar. Sampling methods for the Nystrom method. JMLR, 13(1):981 1006, 2012.
Corinna Cortes, Mehryar Mohri, and Ameet Talwalkar. On the Impact of Kernel Approximation on Learning Accuracy. In AISTATS, volume 9 of JMLR Proceedings, pages 113120. JMLR.org, 2010.
R Jin, T. Yang, M. Mahdavi, Y. Li, and Z. Zhou. Improved Bounds for the Nystrom Method With Application to Kernel Classification. Information Theory, IEEE Transactions on, 59(10), Oct 2013.
Tianbao Yang, Yu-Feng Li, Mehrdad Mahdavi, Rong Jin, and Zhi-Hua Zhou. Nystrom Method vs Random Fourier Features: A Theoretical and Empirical Comparison. In NIPS, pages 485493, 2012.
Francis Bach. Sharp analysis of low-rank kernel matrix approximations. In COLT, volume 30, 2013.
A. Alaoui and M. W. Mahoney. Fast randomized kernel methods with statistical guarantees. arXiv, 2014.
I. Steinwart and A. Christmann. Support Vector Machines. Springer New York, 2008.
Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares algorithm. Foundations of Computational Mathematics, 7(3):331368, 2007.
L. Lo Gerfo, Lorenzo Rosasco, Francesca Odone, Ernesto De Vito, and Alessandro Verri. Spectral Algorithms for Supervised Learning. Neural Computation, 20(7):18731897, 2008.
I. Steinwart, D. Hush, and C. Scovel. Optimal rates for regularized least squares regression. In COLT, 2009.
S. Mendelson and J. Neeman. Regularization in kernel learning. The Annals of Statistics, 38(1), 2010.
F. Bauer, S. Pereverzev, and L. Rosasco. On regularization algorithms in learning theory. Journal of complexity, 23(1):5272, 2007.
A. Caponnetto and Yuan Yao. Adaptive rates for regularization operators in learning theory. Analysis and Applications, 08, 2010.
Y. Ying and M. Pontil. Online gradient descent learning algorithms. Foundations of Computational Mathematics, 8(5):561596, 2008.
Alessandro Rudi, Guillermo D. Canas, and Lorenzo Rosasco. On the Sample Complexity of Subspace Learning. In NIPS, pages 20672075, 2013.
Gene H. Golub and Charles F. Van Loan. Matrix computations, volume 3. JHU Press, 2012.  9
C. Andrieu, A. Doucet, and R. Holenstein. Particle Markov chain Monte Carlo methods. Journal of the Royal Statistical Society Series B, 72(3):269342, 2010.
M. J. Beal, Z. Ghahramani, and C. E. Rasmussen. The infinite hidden Markov model. In Advances in Neural Information Processing Systems, volume 14, 2002.
S. J. Fortune, D. M. Gay, B. W. Kernighan, O. Landron, R. A. Valenzuela, and M. H. Wright. WISE design of indoor wireless systems: Practical computation and optimization. IEEE Computing in Science & Engineering, 2(1):5868, March 1995.
E. B. Fox, E. B. Sudderth, M. I. Jordan, and A. S. Willsky. Bayesian nonparametric methods for learning Markov switching processes. IEEE Signal Processing Magazine, 27(6):4354, 2010.
E. B. Fox, E. B. Sudderth, M. I. Jordan, and A. S. Willsky. A sticky HDP-HMM with application to speaker diarization. Annals of Applied Statistics, 5(2A):10201056, 2011.
L. Jiang, S. S. Singh, and S. Yldrm. Bayesian tracking and parameter learning for non-linear multiple target tracking models. arXiv preprint arXiv:1410.2046, 2014.
M. J. Johnson and A. S. Willsky. Bayesian nonparametric hidden semi-Markov models. Journal of Machine Learning Research, 14:673701, February 2013.
M. I. Jordan. Hierarchical models, nested models and completely random measures. Springer, New York, (NY), 2010.
R. E. Kalman. A new approach to linear filtering and prediction problems. ASME Journal of Basic Engineering, 82(Series D):3545, 1960.
D. Knowles and Z. Ghahramani. Nonparametric Bayesian sparse factor models with application to gene expression modeling. The Annals of Applied Statistics, 5(2B):15341552, June 2011.
J Z. Kolter and T. Jaakkola. Approximate inference in additive factorial hmms with application to energy disaggregation. In International conference on artificial intelligence and statistics, pages 14721482, 2012.
J. Lim and U. Chong. Multitarget tracking by particle filtering based on RSS measurement in wireless sensor networks. International Journal of Distributed Sensor Networks, March 2015.
F. Lindsten, M. I. Jordan, and T. B. Schon. Particle Gibbs with ancestor sampling. Journal of Machine Learning Research, 15(1):21452184, 2014.
F. Lindsten and T. B. Schon. Backward simulation methods for Monte Carlo statistical inference. Foundations and Trends in Machine Learning, 6(1):1143, 2013.
S. Makonin, F. Popowich, L. Bartram, B. Gill, and I. V. Bajic. AMPds: A public dataset for load disaggregation and eco-feedback research. In Proceedings of the 2013 IEEE Electrical Power and Energy Conference (EPEC), 2013.
S. Oh, S. Russell, and S. Sastry. Markov chain Monte Carlo data association for general multiple-target tracking problems. In IEEE Conference on Decision and Control, volume 1, pages 735742, Dec 2004.
P. Orbanz and Y. W. Teh. Bayesian nonparametric models. In Encyclopedia of Machine Learning. Springer, 2010.
S. Sarkka, A. Vehtari, and J. Lampinen. Rao-blackwellized particle filter for multiple target tracking. Information Fusion, 8(1):215, 2007.
Y. W. Teh, D. Gorur, and Z. Ghahramani. Stick-breaking construction for the Indian buffet process. In Proceedings of the International Conference on Artificial Intelligence and Statistics, volume 11, 2007.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):15661581, 2006.
F. Thouin, S. Nannuru, and M. Coates. Multi-target tracking for measurement models with additive contributions. In Proceedings of the 14th International Conference on Information Fusion (FUSION), pages 18, July 2011.
M. K. Titsias and C. Yau. Hamming ball auxiliary sampling for factorial hidden Markov models. In Advances in Neural Information Processing Systems 27, 2014.
J. Van Gael, Y. W. Teh, and Z. Ghahramani. The infinite factorial hidden Markov model. In Advances in Neural Information Processing Systems, volume 21, 2009.
M. A. Vazquez and J. Mguez. User activity tracking in DS-CDMA systems. IEEE Transactions on Vehicular Technology, 62(7):31883203, 2013.
N. Whiteley, C. Andrieu, and A. Doucet. Efficient Bayesian inference for switching state-space models using particle Markov chain Monte Carlo methods. Technical report, Bristol Statistics Research Report 10:04, 2010.  9
B. Efron, T. Hastie, I. Johnstone, and R. TIbshirani. Least angle regression. Annals of Statistics, 32(2):407499, 2004.
T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu. The entire regularization path for the support vector machine. Journal of Machine Learning Research, 5:13911415, 2004.
S. Rosset and J. Zhu. Piecewise linear regularized solution paths. Annals of Statistics, 35:10121030, 2007.
J. Giesen, J. Mueller, S. Laue, and S. Swiercy. Approximating Concavely Parameterized Optimization Problems. In Advances in Neural Information Processing Systems, 2012.
J. Giesen, M. Jaggi, and S. Laue. Approximating Parameterized Convex Optimization Problems. ACM Transactions on Algorithms, 9, 2012.
J. Giesen, S. Laue, and Wieschollek P. Robust and Efficient Kernel Hyperparameter Paths with Guarantees. In International Conference on Machine Learning, 2014.
J. Mairal and B. Yu. Complexity analysis of the Lasso reguralization path. In International Conference on Machine Learning, 2012.
V. Vapnik and O. Chapelle. Bounds on Error Expectation for Support Vector Machines. Neural Computation, 12:20132036, 2000.
T. Joachims. Estimating the generalization performance of a SVM efficiently. In International Conference on Machine Learning, 2000.
K. Chung, W. Kao, C. Sun, L. Wang, and C. Lin. Radius margin bounds for support vector machines with the RBF kernel. Neural computation, 2003.
M. Lee, S. Keerthi, C. Ong, and D. DeCoste. An efficient method for computing leave-one-out error in support vector machines with Gaussian kernels. IEEE Transactions on Neural Networks, 15:7507, 2004.
L. El Ghaoui, V. Viallon, and T. Rabbani. Safe feature elimination in sparse supervised learning. Pacific Journal of Optimization, 2012.
Z. Xiang, H. Xu, and P. Ramadge. Learning sparse representations of high dimensional data on large scale dictionaries. In Advances in Neural Information Processing Sysrtems, 2011.
K. Ogawa, Y. Suzuki, and I. Takeuchi. Safe screening of non-support vectors in pathwise SVM computation. In International Conference on Machine Learning, 2013.
J. Liu, Z. Zhao, J. Wang, and J. Ye. Safe Screening with Variational Inequalities and Its Application to Lasso. In International Conference on Machine Learning, volume 32, 2014.
J. Wang, J. Zhou, J. Liu, P. Wonka, and J. Ye. A Safe Screening Rule for Sparse Logistic Regression. In Advances in Neural Information Processing Sysrtems, 2014.
V. Vapnik. The Nature of Statistical Learning Theory. Springer, 1996.
S. Shalev-Shwartz and S. Ben-David. Understanding machine learning. Cambridge University Press, 2014.
J. Snoek, H. Larochelle, and R. Adams. Practical Bayesian Optimization of Machine Learning Algorithms. In Advances in Neural Information Processing Sysrtems, 2012.
J. Bergstra and Y. Bengio. Random Search for Hyper-Parameter Optimization. Journal of Machine Learning Research, 13:281305, 2012.
O. Chapelle, V. Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for support vector machines. Machine Learning, 46:131159, 2002.
P D. Bertsekas. Nonlinear Programming. Athena Scientific, 1999.
C. Chang and C. Lin. LIBSVM : A Library for Support Vector Machines. ACM Transactions on Intelligent Systems and Technology, 2:139, 2011.
O. Chapelle. Training a support vector machine in the primal. Neural computation, 19:11551178, 2007.
C. Lin, R. Weng, and S. Keerthi. Trust Region Newton Method for Large-Scale Logistic Regression. The Journal of Machine Learning Research, 9:627650, 2008.
R. Fan, K. Chang, and C. Hsieh. LIBLINEAR: A library for large linear classification. The Journal of Machine Learning, 9:18711874, 2008.  9
William Beecher Scoville and Brenda Milner. Loss of recent memory after bilateral hippocampal lesions. Journal of neurology, neurosurgery, and psychiatry, 20(1):11, 1957.
Howard Eichenbaum. Memory, amnesia, and the hippocampal system. MIT press, 1993.
John OKeefe and Jonathan Dostrovsky. The hippocampus as a spatial map. preliminary evidence from unit activity in the freely-moving rat. Brain research, 34(1):171175, 1971.
Kazu Nakazawa, Linus D Sun, Michael C Quirk, Laure Rondi-Reig, Matthew A Wilson, and Susumu Tonegawa. Hippocampal CA3 NMDA receptors are crucial for memory acquisition of one-time experience. Neuron, 38(2):305 315, 2003.
Toshiaki Nakashiba, Jennie Z Young, Thomas J McHugh, Derek L Buhl, and Susumu Tonegawa. Transgenic inhibition of synaptic transmission reveals role of ca3 output in hippocampal learning. Science, 319(5867):12601264, 2008.
Tobias Bast, Iain A Wilson, Menno P Witter, and Richard GM Morris. From rapid place learning to behavioral performance: a key role for the intermediate hippocampus. PLoS biology, 7(4):e1000089, 2009.
Alexei Samsonovich and Bruce L McNaughton. Path integration and cognitive mapping in a continuous attractor neural network model. The Journal of Neuroscience, 17(15):59005920, 1997.
Kirsten Brun Kjelstrup, Trygve Solstad, Vegard Heimly Brun, Torkel Hafting, Stefan Leutgeb, Menno P Witter, Edvard I Moser, and May-Britt Moser. Finite scale of spatial representation in the hippocampus. Science, 321(5885):140143, 2008.
Brad E Pfeiffer and David J Foster. Hippocampal place-cell sequences depict future paths to remembered goals. Nature, 497(7447):7479, 2013.
Andrew M Wikenheiser and A David Redish. Hippocampal theta sequences reflect current goals. Nature neuroscience, 2015.
Louis-Emmanuel Martinet, Denis Sheynikhovich, Karim Benchenane, and Angelo Arleo. Spatial learning and action planning in a prefrontal cortical network model. PLoS computational biology, 7(5):e1002045, 2011.
Filip Ponulak and John J Hopfield. Rapid, parallel path planning by propagating wavefronts of spiking neural activity. Frontiers in computational neuroscience, 7, 2013.
Peter Dayan. Improving generalization for temporal difference learning: The successor representation. Neural Computation, 5(4):613624, 1993.
Kimberly L Stachenfeld, Matthew Botvinick, and Samuel J Gershman. Design principles of the hippocampal cognitive map. In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 25282536. Curran Associates, Inc., 2014.
Mathias Franzius, Henning Sprekeler, and Laurenz Wiskott. Slowness and sparseness lead to place, head-direction, and spatial-view cells. PLoS Computational Biology, 3(8):e166, 2007.
Fabian Schoenfeld and Laurenz Wiskott. Modeling place field activity with hierarchical slow feature analysis. Frontiers in Computational Neuroscience, 9:51, 2015.
Richard S Sutton and Andrew G Barto. Introduction to reinforcement learning. MIT Press, 1998.
Ronald R Coifman and Stephane Lafon. Diffusion maps. Applied and computational harmonic analysis, 21(1):530, 2006.
Sridhar Mahadevan. Learning Representation and Control in Markov Decision Processes, volume 3. Now Publishers Inc, 2009.
Henning Sprekeler. On the relation of slow feature analysis and laplacian eigenmaps. Neural computation, 23(12): 32873302, 2011.
John Conklin and Chris Eliasmith. A controlled attractor network model of path integration in the rat. Journal of computational neuroscience, 18(2):183203, 2005.
Nicholas J Gustafson and Nathaniel D Daw. Grid cells, place cells, and geodesic generalization for spatial reinforcement learning. PLoS computational biology, 7(10):e1002235, 2011.
Chris Eliasmith and C Charles H Anderson. Neural engineering: Computation, representation, and dynamics in neurobiological systems. MIT Press, 2004.
Henning Sprekeler, Christian Michaelis, and Laurenz Wiskott. Slowness: an objective for spike-timing-dependent plasticity. PLoS Comput Biol, 3(6):e112, 2007.
Patrick J Drew and LF Abbott. Extending the effects of spike-timing-dependent plasticity to behavioral timescales. Proceedings of the National Academy of Sciences, 103(23):88768881, 2006.
Phillip Larimer and Ben W Strowbridge. Representing information in cell assemblies: persistent activity mediated by semilunar granule cells. Nature neuroscience, 13(2):213222, 2010.
Robert Urbanczik and Walter Senn. Learning by the dendritic prediction of somatic spiking. Neuron, 81(3):521528, 2014.  9
Ellen Riloff and Michael Thelen. A rule-based question answering system for reading comprehension tests. In Proceedings of the ANLP/NAACL Workshop on Reading Comprehension Tests As Evaluation for Computer-based Language Understanding Sytems.
Hoifung Poon, Janara Christensen, Pedro Domingos, Oren Etzioni, Raphael Hoffmann, Chloe Kiddon, Thomas Lin, Xiao Ling, Mausam, Alan Ritter, Stefan Schoenmackers, Stephen Soderland, Dan Weld, Fei Wu, and Congle Zhang. Machine reading at the University of Washington. In Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading.
Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. CoRR, abs/1410.3916, 2014.
Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. CoRR, abs/1503.08895, 2015.
Terry Winograd. Understanding Natural Language. Academic Press, Inc., Orlando, FL, USA, 1972.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.
Volodymyr Mnih, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu. Recurrent models of visual attention. In Advances in Neural Information Processing Systems 27.
Karol Gregor, Ivo Danihelka, Alex Graves, and Daan Wierstra. DRAW: A recurrent neural network for image generation. CoRR, abs/1502.04623, 2015.
Matthew Richardson, Christopher J. C. Burges, and Erin Renshaw. Mctest: A challenge dataset for the open-domain machine comprehension of text. In Proceedings of EMNLP.
Krysta Svore, Lucy Vanderwende, and Christopher Burges. Enhancing single-document summarization by combining RankNet and third-party sources. In Proceedings of EMNLP/CoNLL.
Kristian Woodsend and Mirella Lapata. Automatic generation of story highlights. In Proceedings of ACL, 2010.
Wilson L Taylor. Cloze procedure: a new tool for measuring readability. Journalism Quarterly, 30:415433, 1953.
Dipanjan Das, Desai Chen, Andre F. T. Martins, Nathan Schneider, and Noah A. Smith. Framesemantic parsing. Computational Linguistics, 40(1):956, 2013.
Karl Moritz Hermann, Dipanjan Das, Jason Weston, and Kuzman Ganchev. Semantic frame identification with distributed word representations. In Proceedings of ACL, June 2014.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network for modelling sentences. In Proceedings of ACL, 2014.
Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:24932537, November 2011.
Ilya Sutskever, Oriol Vinyals, and Quoc V. V Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems 27.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):17351780, November 1997.
Alex Graves. Supervised Sequence Labelling with Recurrent Neural Networks, volume 385 of Studies in Computational Intelligence. Springer, 2012.
T. Tieleman and G. Hinton. Lecture 6.5RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.  9
Lopes M, Jacob L, Wainwright M (2011) A More Powerful Two-Sample Test in High Dimensions using Random Projection. NIPS : 12061214.
Clemmensen L, Hastie T, Witten D, Ersb ll B (2011) Sparse Discriminant Analysis. Technometrics 53: 406413.
van der Vaart AW, Wellner JA (1996) Weak Convergence and Empirical Processes. Springer.
Gibbs AL, Su FE (2002) On Choosing and Bounding Probability Metrics. International Statistical Review 70: 419435.
Wei S, Lee C, Wichers L, Marron JS (2015) Direction-Projection-Permutation for High Dimensional Hypothesis Tests. Journal of Computational and Graphical Statistics .
Rosenbaum PR (2005) An exact distribution-free test comparing two multivariate distributions based on adjacency. Journal of the Royal Statistical Society Series B 67: 515530.
Szekely G, Rizzo M (2004) Testing for equal distributions in high dimension. InterStat 5.
Gretton A, Borgwardt KM, Rasch MJ, Scholkopf B, Smola A (2012) A Kernel Two-Sample Test. The Journal of Machine Learning Research 13: 723773.
Cramer H, Wold H (1936) Some Theorems on Distribution Functions. Journal of the London Mathematical Society 11: 290294.
Cuesta-Albertos JA, Fraiman R, Ransford T (2007) A sharp form of the CramerWold theorem. Journal of Theoretical Probability 20: 201209.
Jirak M (2011) On the maximum of covariance estimators. Journal of Multivariate Analysis 102: 1032 1046.
Tibshirani R (1996) Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society Series B : 267288.
Bradley PS, Mangasarian OL (1998) Feature Selection via Concave Minimization and Support Vector Machines. ICML : 8290.
DAspremont A, El Ghaoui L, Jordan MI, Lanckriet GR (2007) A direct formulation for sparse PCA using semidefinite programming. SIAM Review : 434448.
Amini AA, Wainwright MJ (2009) High-dimensional analysis of semidefinite relaxations for sparse principal components. The Annals of Statistics 37: 28772921.
Good P (1994) Permutation Tests: A Practical Guide to Resampling Methods for Testing Hypotheses. Spring-Verlag.
Duchi J, Hazan E, Singer Y (2011) Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research 12: 21212159.
Wright SJ (2010) Optimization Algorithms in Machine Learning. NIPS Tutorial .
Sandler R, Lindenbaum M (2011) Nonnegative Matrix Factorization with Earth Movers Distance Metric for Image Analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence 33: 15901602.
Levina E, Bickel P (2001) The Earth Movers distance is the Mallows distance: some insights from statistics. ICCV 2: 251256.
Wang Z, Lu H, Liu H (2014) Tighten after Relax: Minimax-Optimal Sparse PCA in Polynomial Time. NIPS 27: 33833391.
Bertsekas DP (1998) Network Optimization: Continuous and Discrete Models. Athena Scientific.
Bertsekas DP, Eckstein J (1988) Dual coordinate step methods for linear network flow problems. Mathematical Programming 42: 203243.
Bertsekas DP (2011) Incremental gradient, subgradient, and proximal methods for convex optimization: A survey. In: Optimization for Machine Learning, MIT Press. pp. 85119.
Beck A, Teboulle M (2009) A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences 2: 183202.
Guyon I, Gunn S, Nikravesh M, Zadeh LA (2006) Feature Extraction: Foundations and Applications. Secaucus, NJ, USA: Springer-Verlag.
Zou H, Hastie T, Tibshirani R (2005) Sparse Principal Component Analysis. Journal of Computational and Graphical Statistics 67: 301320.
Geiler-Samerotte KA, Bauer CR, Li S, Ziv N, Gresham D, et al. (2013) The details in the distributions: why and how to study phenotypic variability. Current opinion in biotechnology 24: 7529.
Zeisel A, Munoz-Manchado AB, Codeluppi S, Lonnerberg P, La Manno G, et al. (2015) Cell types in the mouse cortex and hippocampus revealed by single-cell RNA-seq. Science 347: 11381142.  9
E. Altman, B. Gaujal, and A. Hordijk. Multimodularity, convexity, and optimization properties. Mathematics of Operations Research, 25(2):324347, 2000.
E. Altman and S. Stidham Jr. Optimality of monotonic policies for two-action Markovian decision processes, with applications to control of queues with delayed information. Queueing Systems, 21(3-4):267 291, 1995.
M. Araya, O. Buffet, V. Thomas, and F. Charpillet. A POMDP extension with belief-dependent rewards. In Neural Information Processing Systems, pages 6472, 2010.
A. Badanidiyuru, B. Mirzasoleiman, A. Karbasi, and A. Krause. Streaming submodular maximization: Massive data summarization on the fly. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 671680, 2014.
J. Berstel, A. Lauve, C. Reutenauer, and F. Saliola. Combinatorics on Words: Christoffel Words and Repetitions in Words. CRM Monograph Series, 2008.
S. Bubeck and N. Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems, Foundation and Trends in Machine Learning, Vol. 5. NOW, 2012.
Y. Chen, H. Shioi, C. Montesinos, L. P. Koh, S. Wich, and A. Krause. Active detection via adaptive submodularity. In Proceedings of The 31st International Conference on Machine Learning, pages 5563, 2014.
J. Gittins, K. Glazebrook, and R. Weber. Multi-armed bandit allocation indices. John Wiley & Sons, 2011.
R. Graham, D. Knuth, and O. Patashnik. Concrete Mathematics: A Foundation for Computer Science. Addison-Wesley, 1994.
S. Guha, K. Munagala, and P. Shi. Approximation algorithms for restless bandit problems. Journal of the ACM, 58(1):3, 2010.
B. La Scala and B. Moran. Optimal target tracking with restless bandits. Digital Signal Processing, 16(5):479487, 2006.
J. Le Ny, E. Feron, and M. Dahleh. Scheduling continuous-time Kalman filters. IEEE Trans. Automatic Control, 56(6):13811394, 2011.
M. Lothaire. Algebraic combinatorics on words. Cambridge University Press, 2002.
A. Marshall, I. Olkin, and B. Arnold. Inequalities: Theory of majorization and its applications. Springer Science & Business Media, 2010.
L. Meier, J. Peschon, and R. Dressler. Optimal control of measurement subsystems. IEEE Trans. Automatic Control, 12(5):528536, 1967.
J. Nino-Mora and S. Villar. Multitarget tracking via restless bandit marginal productivity indices and Kalman filter in discrete time. In Proceedings of the 48th IEEE Conference on Decision and Control, pages 29052910, 2009.
R. Ortner, D. Ryabko, P. Auer, and R. Munos. Regret bounds for restless Markov bandits. In Algorithmic Learning Theory, pages 214228. Springer, 2012.
B. Rajpathak, H. Pillai, and S. Bandyopadhyay. Analysis of stable periodic orbits in the one dimensional linear piecewise-smooth discontinuous map. Chaos, 22(3):033126, 2012.
T. Thiele. Sur la compensation de quelques erreurs quasi-systematiques par la methode des moindres carres. CA Reitzel, 1880.
I. Verloop. Asymptotic optimal control of multi-class restless bandits. CNRS Technical Report, hal00743781, 2014.
S. Villar. Restless bandit index policies for dynamic sensor scheduling optimization. PhD thesis, Statistics Department, Universidad Carlos III de Madrid, 2012.
E. Vul, G. Alvarez, J. B. Tenenbaum, and M. J. Black. Explaining human multiple object tracking as resource-constrained approximate inference in a dynamic probabilistic model. In Neural Information Processing Systems, pages 19551963, 2009.
R. R. Weber and G. Weiss. On an index policy for restless bandits. Journal of Applied Probability, pages 637648, 1990.
P. Whittle. Restless bandits: Activity allocation in a changing world. Journal of Applied Probability, pages 287298, 1988.  9
S. A. Andersson, D. Madigan, and M. D. Perlman. A characterization of Markov equivalence classes for acyclic digraphs. Annals of Statistics, 25:505541, 1997.
J. Bell. On the Einstein Podolsky Rosen paradox. Physics, 1(3):195200, 1964.
Z. Cai, M. Kuroki, J. Pearl, and J. Tian. Bounds on direct effects in the presence of confounded intermediate variables. Biometrics, 64:695  701, 2008.
M. Drton. Discrete chain graph models. Bernoulli, 15(3):736753, 2009.
R. J. Evans and T. S. Richardson. Maximum likelihood fitting of acyclic directed mixed graphs to binary data. In Proceedings of the Twenty Sixth Conference on Uncertainty in Artificial Intelligence, volume 26, 2010.
R. J. Evans and T. S. Richardson. Markovian acyclic directed mixed graphs for discrete data. Annals of Statistics, pages 130, 2014.
J. T. A. Koster. Marginalizing and conditioning in graphical models. Bernoulli, 8(6):817840, 2002.
S. L. Lauritzen. Graphical Models. Oxford, U.K.: Clarendon, 1996.
S. L. Lauritzen and T. S. Richardson. Chain graph models and their causal interpretations (with discussion). Journal of the Royal Statistical Society: Series B, 64:321361, 2002.
E. L. Ogburn and T. J. VanderWeele. Causal diagrams for interference. Statistical Science, 29(4):559578, 2014.
J. Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan and Kaufmann, San Mateo, 1988.
J. Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, 2 edition, 2009.
T. Richardson and P. Spirtes. Ancestral graph Markov models. Annals of Statistics, 30:9621030, 2002.
T. S. Richardson. Markov properties for acyclic directed mixed graphs. Scandinavial Journal of Statistics, 30(1):145157, 2003.
K. Sadeghi and S. Lauritzen. Markov properties for mixed graphs. Bernoulli, 20(2):676696, 2014.
I. Shpitser, R. J. Evans, T. S. Richardson, and J. M. Robins. Introduction to nested Markov models. Behaviormetrika, 41(1):339, 2014.
M. Studeny. Bayesian networks from the point of view of chain graphs. In Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI-98), pages 496503. Morgan Kaufmann, San Francisco, CA, 1998.
M. J. van der Laan. Causal inference for networks. Working paper, 2012.
M. J. van der Laan. Causal inference for a population of causally connected units. Journal of Causal Inference, 2(1):1374, 2014.
T. J. VanderWeele, E. J. T. Tchetgen, and M. E. Halloran. Components of the indirect effect in vaccine trials: identification of contagion and infectiousness effects. Epidemiology, 23(5):751761, 2012.
T. S. Verma and J. Pearl. Equivalence and synthesis of causal models. Technical Report R-150, Department of Computer Science, University of California, Los Angeles, 1990.
N. Wermuth. Probability distributions with summary graph structure. Bernoulli, 17(3):845879, 2011.  9
K. P. Bennett. Global tree optimization: A non-greedy decision tree algorithm. Computing Science and Statistics, pages 156156, 1994.
K. P. Bennett and J.A. Blue. A support vector machine approach to decision trees. In Department of Mathematical Sciences Math Report No. 97-100, Rensselaer Polytechnic Institute, pages 23962401, 1997.
K. P. Bennett, N. Cristianini, J. Shawe-Taylor, and D. Wu. Enlarging the margins in perceptron decision trees. Machine Learning, 41(3):295313, 2000.
L. Breiman. Random forests. Machine Learning, 45(1):532, 2001.
L. Breiman, J. Friedman, R. A. Olshen, and C. J. Stone. Classification and regression trees. Chapman & Hall/CRC, 1984.
C. C. Chang and C. J. Lin. LIBSVM: a library for support vector machines, 2001.
A. Criminisi and J. Shotton. Decision Forests for Computer Vision and Medical Image Analysis. Springer, 2013.
Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of Statistics, pages 11891232, 2001.
J. Gall, A. Yao, N. Razavi, L. Van Gool, and V. Lempitsky. Hough forests for object detection, tracking, and action recognition. IEEE Trans. PAMI, 33(11):21882202, 2011.
T. Hastie, R. Tibshirani, and J. Friedman. The elements of statistical learning (Ed. 2). Springer, 2009.
L. Hyafil and R. L. Rivest. Constructing optimal binary decision trees is NP-complete. Information Processing Letters, 5(1):1517, 1976.
J. Jancsary, S. Nowozin, and C. Rother. Loss-specific training of non-parametric image restoration models: A new state of the art. ECCV, 2012.
M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural Comput., 6(2):181214, 1994.
E. Konukoglu, B. Glocker, D. Zikic, and A. Criminisi. Neighbourhood approximation forests. In Medical Image Computing and Computer-Assisted InterventionMICCAI 2012, pages 7582. Springer, 2012.
B. Lakshminarayanan, D. M. Roy, and Y. H. Teh. Mondrian forests: Efficient online random forests. In Advances in Neural Information Processing Systems, pages 31403148, 2014.
J. Mingers. An empirical comparison of pruning methods for decision tree induction. Machine Learning, 4(2):227243, 1989.
S. K. Murthy and S. L. Salzberg. On growing better decision trees from data. PhD thesis, John Hopkins University, 1995.
M. Norouzi, M. D. Collins, D. J. Fleet, and P. Kohli. Co2 forest: Improved random forest by continuous optimization of oblique splits. arXiv:1506.06155, 2015.
M. Norouzi and D. J. Fleet. Minimal Loss Hashing for Compact Binary Codes. ICML, 2011.
S. Nowozin. Improved information gain estimates for decision tree induction. ICML, 2012.
J. R. Quinlan. Induction of decision trees. Machine learning, 1(1):81106, 1986.
J. Shotton, R. Girshick, A. Fitzgibbon, T. Sharp, M. Cook, M. Finocchio, R. Moore, P. Kohli, A. Criminisi, A. Kipman, et al. Efficient human pose estimation from single depth images. IEEE Trans. PAMI, 2013.
B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks. NIPS, 2003.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine learning for interdependent and structured output spaces. ICML, 2004.
C. N. J. Yu and T. Joachims. Learning structural SVMs with latent variables. ICML, 2009.  9
J.B. Tenenbaum, V. De Silva, and J.C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):23192323, 2000.
S.T. Roweis and L.K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):23232326, 2000.
M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, volume 14, pages 585591, 2001.
M. Chen, J. Silva, J. Paisley, C. Wang, D.B. Dunson, and L. Carin. Compressive sensing on manifolds using a nonparametric mixture of factor analyzers: Algorithm and performance bounds. Signal Processing, IEEE Transactions on, 58(12):61406155, 2010.
Y. Wang, A. Canale, and D.B. Dunson. Scalable multiscale density estimation. arXiv preprint arXiv:1410.7692, 2014.
N. Lawrence. Probabilistic non-linear principal component analysis with gaussian process latent variable models. The Journal of Machine Learning Research, 6:17831816, 2005.
M. Titsias and N. Lawrence. Bayesian gaussian process latent variable model. The Journal of Machine Learning Research, 9:844851, 2010.
Neil D Lawrence and Joaquin Quinonero-Candela. Local distance preservation in the GP-LVM through back constraints. In Proceedings of the 23rd international conference on Machine learning, pages 513520. ACM, 2006.
Raquel Urtasun, David J Fleet, Andreas Geiger, Jovan Popovic, Trevor J Darrell, and Neil D Lawrence. Topologically-constrained latent variable models. In Proceedings of the 25th international conference on Machine learning, pages 10801087. ACM, 2008.
T. Hastie and W. Stuetzle. Principal curves. Journal of the American Statistical Association, 84(406):502516, 1989.
V. Rao, R.P. Adams, and D.B. Dunson. Bayesian inference for matern repulsive processes. arXiv preprint arXiv:1308.1136, 2013.
J.B. Hough, M. Krishnapur, Y. Peres, et al. Zeros of Gaussian analytic functions and determinantal point processes, volume 51. American Mathematical Soc., 2009.
K.Q. Weinberger and L.K. Saul. An introduction to nonlinear dimensionality reduction by maximum variance unfolding. In AAAI, volume 6, pages 16831686, 2006.
A. Buades, B. Coll, and J.M. Morel. A non-local algorithm for image denoising. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 2, pages 6065. IEEE, 2005.
P. Perona and J. Malik. Scale-space and edge detection using anisotropic diffusion. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 12(7):629639, 1990.  9
P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proc. ICML, 2004.
M. Babes-Vroman, V. Marivate, K. Subramanian, and M. Littman. Apprenticeship learning about multiple intentions. In Proc. ICML, pages 897904, 2011.
J. Bilmes. A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov models. Technical Report ICSI-TR-97-02, University of California, Berkeley, 1998.
J. Chen, N. Cao, K. H. Low, R. Ouyang, C. K.-Y. Tan, and P. Jaillet. Parallel Gaussian process regression with low-rank covariance matrix approximations. In Proc. UAI, pages 152161, 2013.
J. Choi and K. Kim. Inverse reinforcement learning in partially observable environments. JMLR, 12:691 730, 2011.
J. Choi and K. Kim. Nonparametric Bayesian inverse reinforcement learning for multiple reward functions. In Proc. NIPS, pages 314322, 2012.
K. Dvijotham and E. Todorov. Inverse optimal control with linearly-solvable MDPs. In Proc. ICML, pages 335342, 2010.
T. N. Hoang, Q. M. Hoang, and K. H. Low. A unifying framework of anytime sparse Gaussian process regression models with stochastic variational inference for big data. In Proc. ICML, pages 569578, 2015.
T. N. Hoang and K. H. Low. A general framework for interacting Bayes-optimally with self-interested agents using arbitrary parametric model and model prior. In Proc. IJCAI, pages 13941400, 2013.
T. N. Hoang and K. H. Low. Interactive POMDP Lite: Towards practical planning to predict and exploit intentions for interacting with self-interested agents. In Proc. IJCAI, pages 22982305, 2013.
T. N. Hoang, K. H. Low, P. Jaillet, and M. Kankanhalli. Nonmyopic -Bayes-optimal active learning of Gaussian processes. In Proc. ICML, pages 739747, 2014.
S. Levine, Z. Popovic, and V. Koltun. Nonlinear inverse reinforcement learning with Gaussian processes. In Proc. NIPS, pages 1927, 2011.
K. H. Low, J. Chen, T. N. Hoang, N. Xu, and P. Jaillet. Recent advances in scaling up Gaussian process predictive models for large spatiotemporal data. In S. Ravela and A. Sandu, editors, Proc. Dynamic Data-driven Environmental Systems Science Conference (DyDESS14). LNCS 8964, Springer, 2015.
K. H. Low, N. Xu, J. Chen, K. K. Lim, and E. B. Ozgul. Generalized online sparse Gaussian processes with application to persistent mobile robot localization. In Proc. ECML/PKDD Nectar Track, 2014.
K. H. Low, J. Yu, J. Chen, and P. Jaillet. Parallel Gaussian process regression for big data: Low-rank representation meets Markov approximation. In Proc. AAAI, pages 28212827, 2015.
G. Neu and C. Szepesvari. Apprenticeship learning using inverse reinforcement learning and gradient methods. In Proc. UAI, pages 295302, 2007.
G. Neu and C. Szepesvari. Training parsers by inverse reinforcement learning. Machine Learning, 77(2 3):303337, 2009.
P. Newson and J. Krumm. Hidden Markov map matching through noise and sparseness. In Proc. 17th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, pages 336343, 2009.
A. Y. Ng and S. Russell. Algorithms for inverse reinforcement learning. In Proc. ICML, 2000.
L. R. Rabiner. A tutorial on hidden Markov models and selected applications in speech recognition. Proc. IEEE, 77(2):257286, 1989.
D. Ramachandran and E. Amir. Bayesian inverse reinforcement learning. In Proc. IJCAI, pages 2586 2591, 2007.
S. Russell. Learning agents for uncertain environments. In Proc. COLT, pages 101103, 1998.
U. Syed, M. Bowling, and R. E. Schapire. Apprenticeship learning using linear programming. In Proc. ICML, pages 10321039, 2008.
U. Syed and R. E. Schapire. A game-theoretic approach to apprenticeship learning. In Proc. NIPS, pages 14491456, 2007.
N. Xu, K. H. Low, J. Chen, K. K. Lim, and E. B. Ozgul. GP-Localize: Persistent mobile robot localization using online sparse Gaussian process observation model. In Proc. AAAI, pages 25852592, 2014.
J. Yu, K. H. Low, A. Oran, and P. Jaillet. Hierarchical Bayesian nonparametric approach to modeling and learning the wisdom of crowds of urban traffic route planning agents. In Proc. IAT, pages 478485, 2012.
B. D. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey. Maximum entropy inverse reinforcement learning. In Proc. AAAI, pages 14331438, 2008.  9
A. Agarwal, O. Chapelle, M. Dudk, and J. Langford. A reliable effective terascale linear learning system. CoRR, abs/1110.4198, 2011.
M.-F. Balcan, A. Blum, S. Fine, and Y. Mansour. Distributed learning, communication complexity and privacy. In COLT, 2012.
M.-F. Balcan, V. Kanchanapally, Y. Liang, and D. Woodruff. Improved distributed principal component analysis. In NIPS, 2014.
R. Bekkerman, M. Bilenko, and J. Langford. Scaling up machine learning: Parallel and distributed approaches. Cambridge University Press, 2011.
S.P. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via ADMM. Foundations and Trends in Machine Learning, 3(1):1122, 2011.
K. Clarkson and D. Woodruff. Numerical linear algebra in the streaming model. In STOC, 2009.
A. Cotter, O. Shamir, N. Srebro, and K. Sridharan. Better mini-batch algorithms via accelerated gradient methods. In NIPS, 2011.
O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Optimal distributed online prediction using mini-batches. Journal of Machine Learning Research, 13:165202, 2012.
J. Duchi, A. Agarwal, and M. Wainwright. Dual averaging for distributed optimization: Convergence analysis and network scaling. IEEE Trans. Automat. Contr., 57(3):592606, 2012.
R. Frostig, R. Ge, S. Kakade, and A. Sidford. Competing with the empirical risk minimizer in a single pass. arXiv preprint arXiv:1412.6606, 2014.
M. Jaggi, V. Smith, M. Takac, J. Terhorst, S. Krishnan, T. Hofmann, and M. Jordan. Communication-efficient distributed dual coordinate ascent. In NIPS, 2014.
J. Lee, T. Ma, and Q. Lin. Distributed stochastic variance reduced gradient methods. CoRR, 1507.07595, 2015.
D. Mahajan, S. Keerthy, S. Sundararajan, and L. Bottou. A parallel SGD method with strong convergence. CoRR, abs/1311.0636, 2013.
Y. Nesterov. Introductory lectures on convex optimization: A basic course. Springer, 2004.
Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical programming, 103(1):127152, 2005.
B. Recht, C. Re, S. Wright, and F. Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In NIPS, 2011.
P. Richtarik and M. Takac. Distributed coordinate descent method for learning with big data. CoRR, abs/1310.2059, 2013.
O. Shamir. Fundamental limits of online and distributed algorithms for statistical learning and estimation. In NIPS, 2014.
O. Shamir and N. Srebro. On distributed stochastic optimization and learning. In Allerton Conference on Communication, Control, and Computing, 2014.
O. Shamir, N. Srebro, and T. Zhang. Communication-efficient distributed optimization using an approximate newton-type method. In ICML, 2014.
T. Tao. Topics in random matrix theory, volume 132. American Mathematical Soc., 2012.
J. Tsitsiklis and Z.-Q. Luo. Communication complexity of convex optimization. J. Complexity, 3(3):231243, 1987.
T. Yang. Trading computation for communication: Distributed SDCA. In NIPS, 2013.
Y.-L. Yu. Better approximation and faster algorithm using proximal average. In NIPS, 2013.
Y. Zhang, J. Duchi, and M. Wainwright. Communication-efficient algorithms for statistical optimization. Journal of Machine Learning Research, 14:33213363, 2013.
Y. Zhang and L. Xiao. Communication-efficient distributed optimization of self-concordant empirical loss. In ICML, 2015.
M. Zinkevich, M. Weimer, A. Smola, and L. Li. Parallelized stochastic gradient descent. In NIPS, 2010.  9
A. Asuncion, M. Welling, P. Smyth, and Y. W. Teh. On smoothing and inference for topic models. In Proc. UAI, pages 2734, 2009.
A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31(3):167175, 2003.
C. M. Bishop and J. Lasserre. Generative or discriminative? getting the best of both worlds. Bayesian Statistics, 8:324, 2007.
D. M. Blei and J. D. Mcauliffe. Supervised topic models. In Proc. NIPS, pages 121128, 2007.
D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. JMLR, 3:9931022, 2003.
J. Blitzer, M. Dredze, and F. Pereira. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Proc. ACL, volume 7, pages 440447, 2007.
G. Bouchard and B. Triggs. The tradeoff between generative and discriminative classifiers. In Proc. COMPSTAT, pages 721728, 2004.
J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:21212159, Jul. 2011.
T. L. Griffiths and M. Steyvers. Finding scientific topics. Proc. of the National Academy of Sciences, pages 52285235, 2004.
J. R. Hershey, J. L. Roux, and F. Weninger. Deep unfolding: Model-based inspiration of novel deep architectures. arXiv:1409.2574, 2014.
G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Process. Mag., 29(6):8297, 2012.
A. Holub and P. Perona. A discriminative framework for modelling object classes. In Proc. IEEE CVPR, volume 1, pages 664671, 2005.
P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck. Learning deep structured semantic models for web search using clickthrough data. In Proc. CIKM, pages 23332338, 2013.
S. Kapadia. Discriminative Training of Hidden Markov Models. PhD thesis, University of Cambridge, 1998.
S. Lacoste-Julien, F. Sha, and M. I. Jordan. DiscLDA: Discriminative learning for dimensionality reduction and classification. In Proc. NIPS, pages 897904, 2008.
J. J. McAuley and J. Leskovec. From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews. In Proc. WWW, pages 897908, 2013.
Andrew Kachites McCallum. http://mallet.cs.umass.edu, 2002.  MALLET: A Machine Learning for Language Toolkit.
D. B. Nemirovsky. A. S., Yudin. Problem Complexity and Method Efficiency in Optimization. Wiley, New York, 1983.
D. Sontag and D. Roy. Complexity of inference in latent dirichlet allocation. In Proc. NIPS, pages 10081016, 2011.
V. Stoyanov, A. Ropson, and J. Eisner. Empirical risk minimization of graphical model parameters given approximate inference, decoding, and model structure. In Proc. AISTATS, pages 725733, 2011.
P. Tseng. On accelerated proximal gradient methods for convex-concave optimization. SIAM Journal on Optimization, 2008.
H. M. Wallach, D. M. Mimno, and A. McCallum. Rethinking LDA: Why priors matter. In Proc. NIPS, pages 19731981, 2009.
H. M. Wallach, I. Murray, R. Salakhutdinov, and D. Mimno. Evaluation methods for topic models. In Proc. ICML, pages 11051112, 2009.
Y. Wang and J. Zhu. Spectral methods for supervised topic models. In Proc. NIPS, pages 15111519, 2014.
Oksana Yakhnenko, Adrian Silvescu, and Vasant Honavar. Discriminatively trained Markov model for sequence classification. In Proc. IEEE ICDM, 2005.
J. Zhu, A. Ahmed, and E. P. Xing. MedLDA: maximum margin supervised topic models. JMLR, 13(1):22372278, 2012.
J. Zhu, N. Chen, H. Perkins, and B. Zhang. Gibbs max-margin topic models with data augmentation. JMLR, 15(1):10731110, 2014.  9
C. Boutsidis, M. W. Mahoney, and P. Drineas. An improved approximation algorithm for the column subset selection problem. In SODA, pages 968977, New York, NY, 2009.
A. Das and D. Kempe. Algorithms for subset selection in linear regression. In STOC, pages 4554, Victoria, Canada, 2008.
A. Das and D. Kempe. Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection. In ICML, pages 10571064, Bellevue, WA, 2011.
G. Davis, S. Mallat, and M. Avellaneda. Adaptive greedy approximations. Constructive Approximation, 13(1):5798, 1997.
J. Demsar. Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research, 7:130, 2006.
G. Diekhoff. Statistics for the Social and Behavioral Sciences: Univariate, Bivariate, Multivariate. William C Brown Pub, 1992.
D. L. Donoho, M. Elad, and V. N. Temlyakov. Stable recovery of sparse overcomplete representations in the presence of noise. IEEE Transactions on Information Theory, 52(1):618, 2006.
J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96(456):13481360, 2001.
A. C. Gilbert, S. Muthukrishnan, and M. J. Strauss. Approximation of functions over redundant dictionaries using coherence. In SODA, pages 243252, Baltimore, MD, 2003.
I. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene selection for cancer classification using support vector machines. Machine Learning, 46(1-3):389422, 2002.
R. A. Johnson and D. W. Wichern. Applied Multivariate Statistical Analysis. Pearson, 6th edition, 2007.
A. Miller. Subset Selection in Regression. Chapman and Hall/CRC, 2nd edition, 2002.
B. K. Natarajan. Sparse approximate solutions to linear systems. SIAM Journal on Computing, 24(2):227 234, 1995.
C. Qian, Y. Yu, and Z.-H. Zhou. An analysis on recombination in multi-objective evolutionary optimization. Artificial Intelligence, 204:99119, 2013.
C. Qian, Y. Yu, and Z.-H. Zhou. On constrained Boolean Pareto optimization. In IJCAI, pages 389395, Buenos Aires, Argentina, 2015.
C. Qian, Y. Yu, and Z.-H. Zhou. Pareto ensemble pruning. In AAAI, pages 29352941, Austin, TX, 2015.
M. Tan, I. Tsang, and L. Wang. Matching pursuit LASSO Part I: Sparse recovery over big dictionary. IEEE Transactions on Signal Processing, 63(3):727741, 2015.
R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological), 58(1):267288, 1996.
J. A. Tropp. Greed is good: Algorithmic results for sparse approximation. IEEE Transactions on Information Theory, 50(10):22312242, 2004.
J. A. Tropp, A. C. Gilbert, S. Muthukrishnan, and M. J. Strauss. Improved sparse approximation over quasiincoherent dictionaries. In ICIP, pages 3740, Barcelona, Spain, 2003.
L. Xiao and T. Zhang. A proximal-gradient homotopy method for the sparse least-squares problem. SIAM Journal on Optimization, 23(2):10621091, 2013.
Y. Yu, X. Yao, and Z.-H. Zhou. On the approximation ability of evolutionary optimization with application to minimum set cover. Artificial Intelligence, 180-181:2033, 2012.
Y. Yu and Z.-H. Zhou. On the usefulness of infeasible solutions in evolutionary search: A theoretical study. In IEEE CEC, pages 835840, Hong Kong, China, 2008.
C.-H. Zhang. Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics, 38(2):894942, 2010.
T. Zhang. On the consistency of feature selection using greedy least squares regression. Journal of Machine Learning Research, 10:555568, 2009.
T. Zhang. Adaptive forward-backward greedy algorithm for learning sparse representations. IEEE Transactions on Information Theory, 57(7):46894708, 2011.
H. Zhou. Matlab SparseReg Toolbox Version 0.0.1. Available Online, 2013.
H. Zhou, A. Armagan, and D. Dunson. Path following and empirical Bayes model selection for sparse regression. arXiv:1201.3528, 2012.
H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2):301320, 2005.  9
Lafferty, J. D.; McCallum, A.; Pereira, F. C. N. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. 2001; pp 282289.
McCullagh, P.; Nelder, J. A. Generalized linear models; Chapman and Hall, 1989.
Devlin, J.; Zbib, R.; Huang, Z.; Lamar, T.; Schwartz, R.; Makhoul, J. Fast and robust neural network joint models for statistical machine translation. Proceedings of the Annual Meeting of the Association for Computational Linguistics. 2014.
Vaswani, A.; Zhao, Y.; Fossum, V.; Chiang, D. Decoding with large-scale neural language models improves translation. Proceedings of the Conference on Empirical Methods in Natural Language Processing. 2013.
Andreas, J.; Klein, D. When and why are log-linear models self-normalizing? Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics. 2014.
Bartlett, P. L. IEEE Transactions on Information Theory 1998, 44, 525536.
Anthony, M.; Bartlett, P. Neural network learning: theoretical foundations; Cambridge University Press, 2009.
Gutmann, M.; Hyvarinen, A. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. Proceedings of the International Conference on Artificial Intelligence and Statistics. 2010; pp 297304.
OHagan, A. Journal of statistical planning and inference 1991, 29, 245260.
Chen, Y.; Welling, M.; Smola, A. Proceedings of the Conference on Uncertainty in Artificial Intelligence 2010, 109116.
Doucet, A.; De Freitas, N.; Gordon, N. An introduction to sequential Monte Carlo methods; Springer, 2001.
Morin, F.; Bengio, Y. Proceedings of the International Conference on Artificial Intelligence and Statistics 2005, 246.
Yang, E.; Allen, G.; Liu, Z.; Ravikumar, P. K. Graphical models via generalized linear models. Advances in Neural Information Processing Systems. 2012; pp 13581366.  9
Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. Inf. Comput., 108(2):212261, February 1994.
T. L. Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics, 6(1):422, 1985.
Robert D. Kleinberg and Frank Thomson Leighton. The value of knowing a demand curve: Bounds on regret for online posted-price auctions. In FOCS, pages 594605, 2003.
Alekh Agarwal, Peter L. Bartlett, and Max Dama. Optimal allocation strategies for the dark pool problem. In AISTATS, pages 916, 2010.
Nicolo Cesa-Bianchi, Gabor Lugosi, and Gilles Stoltz. Minimizing regret with label efficient prediction. IEEE Transactions on Information Theory, 51(6):21522162, 2005.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In ICML, pages 928936, 2003.
Varsha Dani, Thomas P. Hayes, and Sham M. Kakade. Stochastic linear optimization under bandit feedback. In COLT, pages 355366, 2008.
Antonio Piccolboni and Christian Schindelhauer. Discrete prediction games with arbitrary feedback and loss. In COLT, pages 208223, 2001.
Nicolo Cesa-Bianchi, Gabor Lugosi, and Gilles Stoltz. Regret minimization under partial monitoring. Math. Oper. Res., 31(3):562580, 2006.
Gabor Bartok, David Pal, and Csaba Szepesvari. Minimax regret of finite partial-monitoring games in stochastic environments. In COLT, pages 133154, 2011.
Gabor Bartok, Navid Zolghadr, and Csaba Szepesvari. An adaptive algorithm for finite stochastic partial monitoring. In ICML, 2012.
Gabor Bartok. A near-optimal algorithm for finite partial-monitoring games against adversarial opponents. In COLT, pages 696710, 2013.
Hastagiri P. Vanchinathan, Gabor Bartok, and Andreas Krause. Efficient partial monitoring with prior information. In NIPS, pages 16911699, 2014.
Peter Auer, Nicolo Cesa-bianchi, and Paul Fischer. Finite-time Analysis of the Multiarmed Bandit Problem. Machine Learning, 47:235256, 2002.
Aurelien Garivier and Olivier Cappe. The KL-UCB algorithm for bounded stochastic bandits and beyond. In COLT, pages 359376, 2011.
Amir Dembo and Ofer Zeitouni. Large deviations techniques and applications. Applications of mathematics. Springer, New York, Berlin, Heidelberg, 1998.
Junya Honda and Akimichi Takemura. An Asymptotically Optimal Bandit Algorithm for Bounded Support Models. In COLT, pages 6779, 2010.
Andreas Wachter and Carl D. Laird. Interior point optimizer (IPOPT).
Gurobi Optimization Inc. Gurobi optimizer.
S. Ito, Y. Liu, and K. L. Teo. A dual parametrization method for convex semi-infinite programming. Annals of Operations Research, 98(1-4):189213, 2000.
Anthony V. Fiacco. Introduction to sensitivity and stability analysis in nonlinear programming. Academic Press, New York, 1983.  9
C. Alos-Ferrer. A simple characterization of approval voting. Social Choice and Welfare, 27(3):621625, 2006.
H. Azari Soufiani, W. Z. Chen, D. C. Parkes, and L. Xia. Generalized method-of-moments for rank aggregation. In Proc. of 27th NIPS, pages 27062714, 2013.
H. Azari Soufiani, D. C. Parkes, and L. Xia. Random utility theory for social choice. In Proc. of 26th NIPS, pages 126134, 2012.
H. Azari Soufiani, D. C. Parkes, and L. Xia. Computing parametric ranking models via rankbreaking. In Proc. of 31st ICML, pages 360368, 2014.
J. Bartholdi, C. A. Tovey, and M. A. Trick. Voting schemes for which it can be difficult to tell who won the election. Social Choice and Welfare, 6:157165, 1989.
D. Baumeister, G. Erdelyi, E. Hemaspaandra, L. A. Hemaspaandra, and J. Rothe. Computational aspects of approval voting. In Handbook on Approval Voting, pages 199251. Springer, 2010.
S. J. Brams. Mathematics and democracy: Designing better voting and fair-division procedures. Princeton University Press, 2007.
S. J. Brams and P. C. Fishburn. Approval Voting. Springer, 2nd edition, 2007.
I. Caragiannis, A. D. Procaccia, and N. Shah. When do noisy votes reveal the truth? In Proc. of 14th EC, pages 143160, 2013.
I. Caragiannis, A. D. Procaccia, and N. Shah. Modal ranking: A uniquely robust voting rule. In Proc. of 28th AAAI, pages 616622, 2014.
E. Elkind and N. Shah. Electing the most probable without eliminating the irrational: Voting over intransitive domains. In Proc. of 30th UAI, pages 182191, 2014.
G. Erdelyi, M. Nowak, and J. Rothe. Sincere-strategy preference-based approval voting fully resists constructive control and broadly resists destructive control. Math. Log. Q., 55(4):425 443, 2009.
P. C. Fishburn. Axioms for approval voting: Direct proof. Journal of Economic Theory, 19(1):180185, 1978.
P. C. Fishburn and S. J. Brams. Approval voting, condorcets principle, and runoff elections. Public Choice, 36(1):89114, 1981.
A. Goel, A. K. Krishnaswamy, S. Sakshuwong, and T. Aitamurto. Knapsack voting. In Proc. of Collective Intelligence, 2015.
J. Lee, W. Kladwang, M. Lee, D. Cantu, M. Azizyan, H. Kim, A. Limpaecher, S. Yoon, A. Treuille, and R. Das. RNA design rules from a massive open laboratory. Proceedings of the National Academy of Sciences, 111(6):21222127, 2014.
G. Little, L. B. Chilton, M. Goldman, and R. C. Miller. TurKit: Human computation algorithms on Mechanical Turk. In Proc. of 23rd UIST, pages 5766, 2010.
T. Lu and C. Boutilier. Learning Mallows models with pairwise preferences. In Proc. of 28th ICML, pages 145152, 2011.
C. L. Mallows. Non-null ranking models. Biometrika, 44:114130, 1957.
A. Mao, A. D. Procaccia, and Y. Chen. Better human computation through principled voting. In Proc. of 27th AAAI, pages 11421148, 2013.
A. D. Procaccia, S. J. Reddi, and N. Shah. A maximum likelihood approach for selecting sets of alternatives. In Proc. of 28th UAI, pages 695704, 2012.
M. R. Sertel. Characterizing approval voting. Journal of Economic Theory, 45(1):207211, 1988.
N. Shah, D. Zhou, and Y. Peres. Approval voting and incentives in crowdsourcing. In Proc. of 32nd ICML, pages 1019, 2015.
H. P. Young. Condorcets theory of voting. 82(4):12311244, 1988. 9  The American Political Science Review,
Jason V. Davis, Brian Kulis, Prateek Jain, Suvrit Sra, and Inderjit S. Dhillon. Informationtheoretic metric learning. In Proc. of ICML, pages 209216, 2007.
Jacob Goldberger, Sam T. Roweis, Geoffrey E. Hinton, and Ruslan Salakhutdinov. Neighbourhood components analysis. In Proc. of NIPS, pages 513520, 2004.
Aurelien Bellet, Amaury Habrard, and Marc Sebban. Metric Learning. Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers, 2015.
Brian Kulis. Metric learning: A survey. Foundations and Trends in Machine Learning, 5(4):287364, 2013.
Kilian Q. Weinberger, John Blitzer, and Lawrence K. Saul. Distance metric learning for large margin nearest neighbor classification. In Proc. of NIPS, pages 14731480, 2005.
Amir Globerson and Sam T. Roweis. Metric learning by collapsing classes. In Proc. of NIPS, pages 451458, 2005.
Kilian Q. Weinberger and Lawrence K. Saul. Distance metric learning for large margin nearest neighbor classification. JMLR, 10:207244, 2009.
Dor Kedem, Stephen Tyree, Kilian Q. Weinberger, Fei Sha, and Gert R. G. Lanckriet. Nonlinear metric learning. In Proc. of NIPS, pages 25822590, 2012.
Bernhard Scholkopf, Alex J. Smola, and Klaus-Robert Muller. Kernel principal component analysis. In Proc. of ICANN, pages 583588, 1997.
Jason Weston, Olivier Chapelle, Andre Elisseeff, Bernhard Scholkopf, and Vladimir Vapnik. Kernel dependency estimation. In Proc. of NIPS, pages 873880, 2002.
Corinna Cortes, Mehryar Mohri, and Jason Weston. A general regression technique for learning transductions. In Proc. of ICML, pages 153160, 2005.
Hachem Kadri, Mohammad Ghavamzadeh, and Philippe Preux. A generalized kernel approach to structured output learning. In Proc. of ICML, pages 471479, 2013.
Rong Jin, Shijun Wang, and Yang Zhou. Regularized distance metric learning: Theory and algorithm. In Proc. of NIPS, pages 862870, 2009.
Olivier Bousquet and Andre Elisseeff. Stability and generalization. JMLR, 2:499526, 2002.
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008.
Purushottam Kar and Prateek Jain. Similarity-based learning via data driven embeddings. In Proc. of NIPS, pages 19982006, 2011.
Nicolas Courty, Remi Flamary, and Devis Tuia. Domain adaptation with regularized optimal transport. In Proc. of ECML/PKDD, pages 274289, 2014.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Proc. of NIPS, pages 22922300, 2013.
M. Lichman. UCI machine learning repository, 2013.
Yuan Shi, Aurelien Bellet, and Fei Sha. Sparse compositional metric learning. In Proc. of AAAI Conference on Artificial Intelligence, pages 20782084, 2014.
Aurelien Bellet, Amaury Habrard, and Marc Sebban. Similarity learning for provably accurate sparse linear classification. In Proc. of ICML, 2012.
The closed-form implementation of RVML is freely available on the authors website.
Maria-Florina Balcan, Avrim Blum, and Nathan Srebro. Improved guarantees for learning via similarity functions. In Proc. of COLT, pages 287298, 2008.  9
E. J. Candes, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? Journal of the ACM (JACM), 58(3):11, 2011.
V. Chandrasekaran, S. Sanghavi, P. A. Parrilo, and A. S. Willsky. Rank-sparsity incoherence for matrix decomposition. SIAM Journal on Optimization, 21(2):572596, 2011.
Y. Chen, A. Jalali, S. Sanghavi, and C. Caramanis. Low-rank matrix recovery from errors and erasures. IEEE Transactions on Information Theory, 59(7):43244337, 2013.
Y. Chen. Incoherence-optimal matrix completion. 61(5):29092923, May 2015.  IEEE Transactions on Information Theory,
E. J. Candes and B. Recht. Exact matrix completion via convex optimization. Foundations of Computational mathematics, 9(6):717772, 2009.
E. J. Candes and T. Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE Transactions on Information Theory, 56(5):20532080, 2010.
D. Gross. Recovering low-rank matrices from few coefficients in any basis. IEEE Transactions on Information Theory, 57(3):15481566, 2011.
B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM Review, 52(3):471501, 2010.
Y. Chen, S. Bhojanapalli, S. Sanghavi, and R. Ward. Completing any low-rank matrix, provably. arXiv preprint arXiv:1306.2979, 2013.
D. Hsu, S. M. Kakade, and T. Zhang. Robust matrix decomposition with sparse corruptions. IEEE Transactions on Information Theory, 57(11):72217234, 2011.
A. Ganesh, J. Wright, X. Li, E. J. Candes, and Y. Ma. Dense error correction for low-rank matrices via principal component pursuit. In IEEE International Symposium on Information Theory (ISIT), pages 15131517, Austin, TX, US, June 2010.
X. Li. Compressed sensing and matrix completion with constant proportion of corruptions. Constructive Approximation, 37(1):7399, 2013.
S. Oymak and B. Hassibi. Finding dense clusters via low rank+ sparse decomposition. arXiv preprint arXiv:1104.5186, 2011.
Y. Chen, S. Sanghavi, and H. Xu. Clustering sparse graphs. In Advances in Neural Information Processing Systems (NIPS), pages 22042212, Lake Tahoe, Nevada, US, December 2012.
Y. Chen, S. Sanghavi, and H. Xu. Improved graph clustering. IEEE Transactions on Information Theory, 60(10):64406455, Oct 2014.
Y. Chen, A. Jalali, S. Sanghavi, and H. Xu. Clustering partially observed graphs via convex optimization. Journal of Machine Learning Research, 15(1):22132238, 2014.
Z. Lin, M. Chen, and Y. Ma. The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices. arXiv preprint arXiv:1009.5055, 2010.
N. Srebro and R. R. Salakhutdinov. Collaborative filtering in a non-uniform world: Learning with the weighted trace norm. In Advances in Neural Information Processing Systems (NIPS), pages 20562064, Hyatt Regency, Vancouver, Canada, 2010. December.
R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv:1011.3027, 2010.  arXiv preprint
J. A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics, 12(4):389434, 2012.  9
Ilya Sutskever, Oriol Vinyals, and Quoc V. V Le. Sequence to sequence learning with neural networks. In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 31043112. Curran Associates, Inc., 2014.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
GZ Sun, C Lee Giles, HH Chen, and YC Lee. The neural network pushdown automaton: Model, stack and learning simulations. 1998.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, abs/1410.5401, 2014.
Alex Graves. Supervised Sequence Labelling with Recurrent Neural Networks, volume 385 of Studies in Computational Intelligence. Springer, 2012.
Markus Dreyer, Jason R. Smith, and Jason Eisner. Latent-variable modeling of string transductions with finite-state methods. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP 08, pages 10801089, Stroudsburg, PA, USA, 2008. Association for Computational Linguistics.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. OpenFST: A general and efficient weighted finite-state transducer library. In Implementation and Application of Automata, volume 4783 of Lecture Notes in Computer Science, pages 1123. Springer Berlin Heidelberg, 2007.
Dekai Wu. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational linguistics, 23(3):377403, 1997.
Alex Graves. Sequence transduction with recurrent neural networks. In Representation Learning Worksop, ICML. 2012.
Sreerupa Das, C Lee Giles, and Guo-Zheng Sun. Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory. In Proceedings of The Fourteenth Annual Conference of Cognitive Science Society. Indiana University, 1992.
Sreerupa Das, C Lee Giles, and Guo-Zheng Sun. Using prior knowledge in a {NNPDA} to learn context-free languages. Advances in neural information processing systems, 1993.
Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. Weakly supervised memory networks. CoRR, abs/1503.08895, 2015.
Wojciech Zaremba and Ilya Sutskever. Reinforcement learning neural turing machines. arXiv preprint arXiv:1505.00521, 2015.
Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. arXiv preprint arXiv:1503.01007, 2015.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807814, 2010.
Alfred V Aho and Jeffrey D Ullman. The theory of parsing, translation, and compiling. Prentice-Hall, Inc., 1972.
Dekai Wu and Hongsing Wong. Machine translation with a stochastic grammatical channel. In Proceedings of the 17th international conference on Computational linguistics-Volume 2, pages 14081415. Association for Computational Linguistics, 1998.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4, 2012.
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. Understanding the exploding gradient problem. Computing Research Repository (CoRR) abs/1211.5063, 2012.
Zhi-Hua Zhou, Jianxin Wu, and Wei Tang. Ensembling neural networks: many could be better than all. Artificial intelligence, 137(1):239263, 2002. 9
Y. Altun, I. Tsochantaridis, and T. Hofmann. Hidden Markov support vector machines. In ICML, 2003.
F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. Goodfellow, A. Bergeron, N. Bouchard, D. WardeFarley, and Y. Bengio. Theano: new features and speed improvements. In Deep Learning and Unsupervised Feature Learning NIPS Workshop, 2012.
Y. Bengio, E. Laufer, G. Alain, and J. Yosinski. Deep generative stochastic networks trainable by backprop. In ICML, 2014.
N. Chen, J. Zhu, F. Sun, and E. P. Xing. Large-margin predictive latent subspace learning for multi-view data analysis. IEEE Trans. on PAMI, 34(12):23652378, 2012.  8
C. Cortes and V. Vapnik. Support-vector networks. Journal of Machine Learning, 20(3):273297, 1995.
A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning to generate chairs with convolutional neural networks. arXiv:1411.5928, 2014.
I. J. Goodfellow, J. P. Abadie, M. Mirza, B. Xu, D. W. Farley, S.ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, 2014.
I. J. Goodfellow, D.Warde-Farley, M. Mirza, A. C. Courville, and Y. Bengio. Maxout networks. In ICML, 2013.
K. Gregor, I. Danihelka, A. Mnih, C. Blundell, and D. Wierstra. Deep autoregressive networks. In ICML, 2014.
D. P. Kingma and J. L. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
D. P. Kingma, D. J. Rezende, S. Mohamed, and M. Welling. Semi-supervised learning with deep generative models. In NIPS, 2014.
D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In ICLR, 2014.
H. Larochelle and I. Murray. The neural autoregressive distribution estimator. In AISTATS, 2011.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. In Proceedings of the IEEE, 1998.
C. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-supervised nets. In AISTATS, 2015.
H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In ICML, 2009.
M. Lin, Q. Chen, and S. Yan. Network in network. In ICLR, 2014.
R. J. Little and D. B. Rubin. Statistical analysis with missing data. JMLR, 539, 1987.
L. V. Matten and G. Hinton. Visualizing data using t-SNE. JMLR, 9:25792605, 2008.
K. Miller, M. P. Kumar, B. Packer, D. Goodman, and D. Koller. Max-margin min-entropy models. In AISTATS, 2012.
A. Mnih and K. Gregor. Neural variational inference and learning in belief networks. In ICML, 2014.
Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised feature learning. NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011.
M. Ranzato, J. Susskind, V. Mnih, and G. E. Hinton. On deep generative models with applications to recognition. In CVPR, 2011.
D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In ICML, 2014.
R. Salakhutdinov and G. E. Hinton. Deep Boltzmann machines. In AISTATS, 2009.
L. Saul, T. Jaakkola, and M. Jordan. Mean field theory for sigmoid belief networks. Journal of AI Research, 4:6176, 1996.
P. Sermanet, S. Chintala, and Y. Lecun. Convolutional neural networks applied to house numbers digit classification. In ICPR, 2012.
S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter. Pegasos: Primal estimated sub-gradient solver for SVM. Mathematical Programming, Series B, 2011.
Y. Tang. Deep learning using linear support vector machines. In Challenges on Representation Learning Workshop, ICML, 2013.
B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks. In NIPS, 2003.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine learning for interdependent and structured output spaces. In ICML, 2004.
C. J. Yu and T. Joachims. Learning structural SVMs with latent variables. In ICML, 2009.
M. D. Zeiler and R. Fergus. Stochastic pooling for regularization of deep convolutional neural networks. In ICLR, 2013.
J. Zhu, A. Ahmed, and E. P. Xing. MedLDA: Maximum margin supervised topic models. JMLR, 13:2237 2278, 2012.
J. Zhu, N. Chen, H. Perkins, and B. Zhang. Gibbs max-margin topic models with data augmentation. JMLR, 15:10731110, 2014.
J. Zhu, N. Chen, and E. P. Xing. Bayesian inference with posterior regularization and applications to infinite latent SVMs. JMLR, 15:17991847, 2014.
J. Zhu, E.P. Xing, and B. Zhang. Partially observed maximum entropy discrimination Markov networks. In NIPS, 2008.  9
Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine Learning, 20(3):273297, 1995.
T. Joachims. Training linear svms in linear time. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 217226. ACM, 2006.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. Liblinear: A library for large linear classification. The Journal of Machine Learning Research, 9:18711874, 2008.
Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cotter. Pegasos: Primal estimated subgradient solver for SVM. Mathematical Programming, 127(1):330, 2011.
Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in neural information processing systems, pages 11771184, 2007.
Salomon Bochner. Harmonic analysis and the theory of probability. Dover Publications, 1955.
Subhransu Maji and Alexander C Berg. Max-margin additive classifiers for detection. In International Conference on Computer Vision, pages 4047. IEEE, 2009.
V Sreekanth, Andrea Vedaldi, Andrew Zisserman, and C Jawahar. Generalized RBF feature maps for efficient detection. In British Machine Vision Conference, 2010.
Fuxin Li, Catalin Ionescu, and Cristian Sminchisescu. Random fourier approximations for skewed multiplicative histogram kernels. In Pattern Recognition, pages 262271. Springer, 2010.
A. Vedaldi and A. Zisserman. Efficient additive kernels via explicit feature maps. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(3):480492, 2012.
Jiyan Yang, Vikas Sindhwani, Quanfu Fan, Haim Avron, and Michael Mahoney. Random laplace feature maps for semigroup kernels on histograms. In Computer Vision and Pattern Recognition (CVPR), pages 971978. IEEE, 2014.
Hideki Isozaki and Hideto Kazawa. Efficient support vector classifiers for named entity recognition. In Proceedings of the 19th International Conference on Computational Linguistics-Volume 1, pages 17. Association for Computational Linguistics, 2002.
Kwang In Kim, Keechul Jung, and Hang Joon Kim. Face recognition using kernel principal component analysis. Signal Processing Letters, IEEE, 9(2):4042, 2002.
Purushottam Kar and Harish Karnick. Random feature maps for dot product kernels. In International Conference on Artificial Intelligence and Statistics, pages 583591, 2012.
Ninh Pham and Rasmus Pagh. Fast and scalable polynomial kernels via explicit feature maps. In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 239247. ACM, 2013.
Raffay Hamid, Ying Xiao, Alex Gittens, and Dennis Decoste. Compact random feature maps. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1927, 2014.
Ping Li, Trevor J Hastie, and Kenneth W Church. Improving random projections using marginal information. In Learning Theory, pages 635649. Springer, 2006.
Isaac J Schoenberg. Metric spaces and completely monotone functions. Annals of Mathematics, pages 811841, 1938.
EE Kummer. De integralibus quibusdam definitis et seriebus infinitis. Journal fur die reine und angewandte Mathematik, 17:228242, 1837.
Felix X Yu, Sanjiv Kumar, Henry Rowley, and Shih-Fu Chang. Compact nonlinear maps and circulant extensions. arXiv preprint arXiv:1503.03893, 2015.
Dmitry Storcheus, Mehryar Mohri, and Afshin Rostamizadeh. Foundations of coupled nonlinear dimensionality reduction. arXiv preprint arXiv:1509.08880, 2015.
Bo Dai, Bo Xie, Niao He, Yingyu Liang, Anant Raj, Maria-Florina F Balcan, and Le Song. Scalable kernel methods via doubly stochastic gradients. In Advances in Neural Information Processing Systems, pages 30413049, 2014.  9
G. E. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504507, 2006.
Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. In B. Scholkopf, J. C. Platt, and T. Hoffman, editors, NIPS, pages 153160. MIT Press, 2007.
J. Schmidhuber. Deep learning in neural networks: An overview. Neural Networks, 61:85117, 2015.
Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436444, 2015.
V. Nair and G. E. Hinton. Rectified linear units improve restricted Boltzmann machines. In ICML, pages 807814. Omnipress 2010, ISBN 978-1-60558-907-7, 2010.
X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier neural networks. In AISTATS, volume 15, pages 315323, 2011.
N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15:19291958, 2014.
S. Hochreiter, U. Bodenhofer, et al. FABIA: factor analysis for bicluster acquisition. Bioinformatics, 26(12):15201527, 2010.
S. Hochreiter. HapFABIA: Identification of very short segments of identity by descent characterized by rare variants in large sequencing data. Nucleic Acids Res., 41(22):e202, 2013.
B. J. Frey and G. E. Hinton. Variational learning in nonlinear Gaussian belief networks. Neural Computation, 11(1):193214, 1999.
M. Harva and A. Kaban. Variational learning for rectified factor analysis. Signal Processing, 87(3):509 527, 2007.
K. Ganchev, J. Graca, J. Gillenwater, and B. Taskar. Posterior regularization for structured latent variable models. Journal of Machine Learning Research, 11:20012049, 2010.
J. Palmer, D. Wipf, K. Kreutz-Delgado, and B. Rao. Variational EM algorithms for non-Gaussian latent variable models. In NIPS, volume 18, pages 10591066, 2006.
D. P. Bertsekas. On the Goldstein-Levitin-Polyak gradient projection method. IEEE Trans. Automat. Control, 21:174184, 1976.
C. T. Kelley. Iterative Methods for Optimization. Society for Industrial and Applied Mathematics (SIAM), Philadelphia, 1999.
D. P. Bertsekas. Projected Newton methods for optimization problems with simple constraints. SIAM J. Control Optim., 20:221246, 1982.
J. Abadie and J. Carpentier. Optimization, chapter Generalization of the Wolfe Reduced Gradient Method to the Case of Nonlinear Constraints. Academic Press, 1969.
J. B. Rosen. The gradient projection method for nonlinear programming. part ii. nonlinear constraints. Journal of the Society for Industrial and Applied Mathematics, 9(4):514532, 1961.
E. J. Haug and J. S. Arora. Applied optimal design. J. Wiley & Sons, New York, 1979.
A. Ben-Tal and A. Nemirovski. Interior Point Polynomial Time Methods for Linear Programming, Conic Quadratic Programming, and Semidefinite Programming, chapter 6, pages 377442. Society for Industrial and Applied Mathematics, 2001.
A. Gunawardana and W. Byrne. Convergence theorems for generalized alternating minimization procedures. Journal of Machine Learning Research, 6:20492073, 2005.
W. I. Zangwill. Nonlinear Programming: A Unified Approach. Prentice Hall, Englewood Cliffs, N.J., 1969.
N. Srebro. Learning with Matrix Factorizations. PhD thesis, Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, 2004.
A. Hyvarinen and E. Oja. A fast fixed-point algorithm for independent component analysis. Neural Comput., 9(7):14831492, 1999.
Y. LeCun, F.-J. Huang, and L. Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Press, 2004.
P. Vincent, H. Larochelle, et al. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. JMLR, 11:33713408, 2010.
H. Larochelle, D. Erhan, et al. An empirical evaluation of deep architectures on problems with many factors of variation. In ICML, pages 473480, 2007.
A. Krizhevsky. Learning multiple layers of features from tiny images. Masters thesis, Deptartment of Computer Science, University of Toronto, 2009.
B. Verbist, G. Klambauer, et al. Using transcriptomics to guide lead optimization in drug discovery projects: Lessons learned from the {QSTAR} project. Drug Discovery Today, 20(5):505  513, 2015.
S. Hochreiter, D.-A. Clevert, and K. Obermayer. A new summarization method for Affymetrix probe level data. Bioinformatics, 22(8):943949, 2006.  9
M. Bartlett and J. Cussens. Integer linear programming for the Bayesian network structure learning problem. Artificial Intelligence, 2015. in press.
D. M. Chickering, C. Meek, and D. Heckerman. Large-sample learning of Bayesian networks is hard. In Proceedings of the 19st Conference on Uncertainty in Artificial Intelligence, UAI03, pages 124133. Morgan Kaufmann, 2003.
G. F. Cooper and E. Herskovits. A Bayesian method for the induction of probabilistic networks from data. Machine Learning, 9(4):309347, 1992.
J. Cussens. Bayesian network learning with cutting planes. In Proceedings of the 27st Conference Annual Conference on Uncertainty in Artificial Intelligence, UAI-11, pages 153160. AUAI Press, 2011.
J. Cussens, B. Malone, and C. Yuan. IJCAI 2013 tutorial on optimal algorithms for learning Bayesian networks (https://sites.google.com/site/ijcai2013bns/slides), 2013.
C. P. de Campos and Q. Ji. Efficient structure learning of Bayesian networks using constraints. Journal of Machine Learning Research, 12:663689, 2011.
C. P. de Campos, Z. Zeng, and Q. Ji. Structure learning of Bayesian networks using constraints. In Proceedings of the 26st Annual International Conference on Machine Learning, ICML-09, pages 113120, 2009.
J. V. Haaren and J. Davis. Markov network structure learning: A randomized feature generation approach. In Proceedings of the 26st AAAI Conference on Artificial Intelligence, 2012.
D. Heckerman, D. Geiger, and D.M. Chickering. Learning Bayesian networks: The combination of knowledge and statistical data. Machine Learning, 20:197243, 1995.
T. Jaakkola, D. Sontag, A. Globerson, and M. Meila. Learning Bayesian Network Structure using LP Relaxations. In Proceedings of the 13st International Conference on Artificial Intelligence and Statistics, AISTATS-10, pages 358365, 2010.
M. Koivisto. Parent assignment is hard for the MDL, AIC, and NML costs. In Proceedings of the 19st annual conference on Learning Theory, pages 289303. Springer-Verlag, 2006.
M. Koivisto and K. Sood. Exact Bayesian Structure Discovery in Bayesian Networks. Journal of Machine Learning Research, 5:549573, 2004.
D. Lowd and J. Davis. Learning Markov network structure with decision trees. In Geoffrey I. Webb, Bing Liu 0001, Chengqi Zhang, Dimitrios Gunopulos, and Xindong Wu, editors, Proceedings of the 10st Int. Conference on Data Mining (ICDM2010), pages 334343, 2010.
W. J. McGill. Multivariate information transmission. Psychometrika, 19(2):97116, 1954.
A. Moore and W. Wong. Optimal reinsertion: A new search operator for accelerated and more accurate Bayesian network structure learning. In T. Fawcett and N. Mishra, editors, Proceedings of the 20st International Conference on Machine Learning, ICML-03, pages 552 559, Menlo Park, California, August 2003. AAAI Press.
A. E. Raftery. Bayesian model selection in social research. Sociological methodology, 25:111 164, 1995.
T. Silander and P. Myllymaki. A simple approach for finding the globally optimal Bayesian network structure. In Proceedings of the 22nd Conference on Uncertainty in Artificial Intelligence, UAI-06, pages 445452, 2006.
M. Teyssier and D. Koller. Ordering-based search: A simple and effective algorithm for learning Bayesian networks. In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence, UAI-05, pages 584590, 2005.
C. Yuan and B. Malone. An improved admissible heuristic for learning optimal Bayesian networks. In Proceedings of the 28st Conference on Uncertainty in Artificial Intelligence, UAI-12, 2012.
C. Yuan and B. Malone. Learning optimal Bayesian networks: A shortest path perspective. Journal of Artificial Intelligence Research, 48:2365, 2013.  9
Prem Melville and Vikas Sindhwani. Recommender systems. In Encyclopedia of machine learning. Springer, 2010.
Mihai Cucuringu. Graph realization and low-rank matrix completion. PhD thesis, Princeton University, 2012.
Benjamin Recht. A simpler approach to matrix completion. JMLR, 12:34133430, 2011.
Emmanuel J Candes and Benjamin Recht. Exact matrix completion via convex optimization. FOCM, 9(6):717772, 2009.
Emmanuel J Candes and Yaniv Plan. Matrix completion with noise. Proceedings of the IEEE, 98(6):925 936, 2010.
Sahand Negahban and Martin J Wainwright. Restricted strong convexity and weighted matrix completion: Optimal bounds with noise. The Journal of Machine Learning Research, 13(1):16651697, 2012.
Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few entries. Information Theory, IEEE Transactions on, 56(6):29802998, 2010.
David Gross. Recovering low-rank matrices from few coefficients in any basis. Information Theory, IEEE Transactions on, 57(3):15481566, 2011.
Jon Dattorro. Convex optimization & Euclidean distance geometry. Lulu. com, 2010.
Bart Vandereycken. Low-rank matrix completion by riemannian optimization. SIAM Journal on Optimization, 23(2):12141236, 2013.
Mingkui Tan, Ivor W Tsang, Li Wang, Bart Vandereycken, and Sinno J Pan. Riemannian pursuit for big matrix recovery. In ICML, pages 15391547, 2014.
Zheng Wang, Ming-Jun Lai, Zhaosong Lu, Wei Fan, Hasan Davulcu, and Jieping Ye. Rank-one matrix pursuit for matrix completion. In ICML, pages 9199, 2014.
Zaiwen Wen, Wotao Yin, and Yin Zhang. Solving a low-rank factorization model for matrix completion by a nonlinear successive over-relaxation algorithm. Mathematical Programming Computation, 2012.
Brian Eriksson, Laura Balzano, and Robert Nowak. High-rank matrix completion. In AISTATS, 2012.
Congyuan Yang, Daniel Robinson, and Rene Vidal. Sparse subspace clustering with missing entries. In ICML, 2015.
Mahdi Soltanolkotabi, Emmanuel J Candes, et al. A geometric analysis of subspace clustering with outliers. The Annals of Statistics, 40(4):21952238, 2012.
Ehsan Elhamifar and Rene Vidal. Sparse subspace clustering: Algorithm, theory, and applications. TPAMI, 2013.
Aarti Singh, Akshay Krishnamurthy, Sivaraman Balakrishnan, and Min Xu. Completion of high-rank ultrametric matrices using selective entries. In SPCOM, pages 15. IEEE, 2012.
Oluwasanmi Koyejo, Sreangsu Acharyya, and Joydeep Ghosh. Retargeted matrix factorization for collaborative filtering. In Proceedings of the 7th ACM conference on Recommender systems, pages 4956. ACM, 2013.
Mark A Davenport, Yaniv Plan, Ewout van den Berg, and Mary Wootters. 1-bit matrix completion. Information and Inference, 3(3):189223, 2014.
Sham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai. Efficient learning of generalized linear and single index models with isotonic regression. In NIPS, 2011.
Adam Tauman Kalai and Ravi Sastry. The isotron algorithm: High-dimensional isotonic regression. In COLT, 2009.
Hidehiko Ichimura. Semiparametric least squares (sls) and weighted sls estimation of single-index models. Journal of Econometrics, 58(1):71120, 1993.
Joel L Horowitz and Wolfgang Hardle. Direct semiparametric estimation of single-index models with discrete covariates. Journal of the American Statistical Association, 91(436):16321640, 1996.
Alekh Agarwal, Sham Kakade, Nikos Karampatziakis, Le Song, and Gregory Valiant. Least squares revisited: Scalable approaches for multi-class prediction. In ICML, pages 541549, 2014.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027, 2010.
Stephen Becker, E Candes, and M Grant. Tfocs: Flexible first-order methods for rank minimization. In Low-rank Matrix Optimization Symposium, SIAM Conference on Optimization, 2011.  9
Gentner, D., Holyoak, K.J., Kokinov, B.N.: The analogical mind: Perspectives from cognitive science. MIT press (2001)
Shelley, C.: Multiple analogies in science and philosophy. John Benjamins Publishing (2003)
Juthe, A.: Argument by analogy. Argumentation (2005)
Aubry, M., Maturana, D., Efros, A., Russell, B., Sivic, J.: Seeing 3d chairs: exemplar part-based 2d-3d alignment using a large dataset of cad models. In: CVPR. (2014)
Turney, P.D.: Similarity of semantic relations. Comput. Linguist. (2006)
Turney, P.D., Littman, M.L.: Corpus-based learning of analogies and semantic relations. CoRR (2005)
Baroni, M., Lenci, A.: Distributional memory: A general framework for corpus-based semantics. Comput. Linguist. (2010)
Jurgens, D.A., Turney, P.D., Mohammad, S.M., Holyoak, K.J.: Semeval-2012 task 2: Measuring degrees of relational similarity, ACL (2012)
Turney, P.D., Pantel, P.: From frequency to meaning: Vector space models of semantics. J. Artif. Int. Res. (2010)
Levy, O., Goldberg, Y.: Linguistic regularities in sparse and explicit word representations. In: CoNLL, ACL (2014)
Barbella, D.M., Forbus, K.D.: Analogical dialogue acts: Supporting learning by reading analogies in instructional texts. In: AAAI. (2011)
Chang, M.D., Forbus, K.D.: Using analogy to cluster hand-drawn sketches for sketch-based educational software. AI Magazine (2014)
Forbus, K.D., Usher, J.M., Tomai, E.: Analogical learning of visual/conceptual relationships in sketches. In: AAAI. (2005)
Forbus, K., Usher, J., Lovett, A., Lockwood, K., Wetzel, J.: Cogsketch: Sketch understanding for cognitive science research and for education. Topics in Cognitive Science (2011)
Chang, M.D., Wetzel, J.W., Forbus, K.D.: Spatial reasoning in comparative analyses of physics diagrams. In: Spatial Cognition IX. (2014)
Hertzmann, A., Jacobs, C.E., Oliver, N., Curless, B., Salesin, D.H.: Image analogies. In: SIGGRAPH, ACM (2001)
Tenenbaum, J.B., Freeman, W.T.: Separating style and content with bilinear models. Neural computation (2000)
Chopra, S., Hadsell, R., LeCun, Y.: Learning a similarity metric discriminatively, with application to face verification. In: CVPR. (2005)
Farhadi, A., Endres, I., Hoiem, D., Forsyth, D.: Describing objects by their attributes. In: CVPR. (2009)
Parikh, D., Grauman, K.: Relative attributes. In: ICCV. (2011)
Hwang, S.J., Grauman, K., Sha, F.: Analogy-preserving semantic embedding for visual object categorization. In: ICML. (2013)
Kiros, R., Salakhutdinov, R., Zemel, R.S.: Unifying visual-semantic embeddings with multimodal neural language models. arXiv preprint arXiv:1411.2539 (2014)
Mikolov, T., Yih, W.t., Zweig, G.: Linguistic regularities in continuous space word representations. In: HLT-NAACL. (2013)
Geman, D., Geman, S., Hallonquist, N., Younes, L.: Visual turing test for computer vision systems. PNAS (2015)
Malinowski, M., Fritz, M.: A multi-world approach to question answering about real-world scenes based on uncertain input. In: NIPS. (2014)
Sadeghi, F., Kumar Divvala, S., Farhadi, A.: VisKE: Visual Knowledge Extraction and Question Answering by Visual Verification of Relation Phrases. In: CVPR. (2015)
Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh, D.: VQA: Visual question answering. In: ICCV. (2015)
Yu, L., Park, E., Berg, A.C., Berg, T.L.: Visual madlibs: Fill in the blank description generation and question answering. In: ICCV. (2015)
Malinowski, M., Rohrbach, M., Fritz, M.: Ask your neurons: A neural-based approach to answering questions about images. In: ICCV. (2015)
Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: NIPS. (2012)
Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., Darrell, T.: Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093 (2014)  9
T. S. Ferguson. A Bayesian analysis of some nonparametric problems. The Annals of Statistics, 1(2):209230, 1973.
A. Lijoi, R. H. Mena, and I. Prunster. Hierarchical mixture modeling with normalized inverseGaussian priors. Journal of the American Statistical Association, 100(472):12781291, 2005.
A. Brix. Generalized Gamma measures and shot-noise Cox processes. Advances in Applied Probability, 31:929953, 1999.
A. Lijoi, R. H. Mena, and I. Prunster. Controlling the reinforcement in Bayesian nonparametric mixture models. Journal of the Royal Statistical Society B, 69:715740, 2007.
E. Regazzini, A. Lijoi, and I. Prunster. Distriubtional results for means of normalized random measures with independent increments. The Annals of Statistics, 31(2):560585, 2003.
C. Chen, N. Ding, and W. Buntine. Dependent hierarchical normalized random measures for dynamic topic modeling. In Proceedings of the International Conference on Machine Learning (ICML), Edinburgh, UK, 2012.
C. Chen, V. Rao, W. Buntine, and Y. W. Teh. Dependent normalized random measures. In Proceedings of the International Conference on Machine Learning (ICML), Atlanta, Georgia, USA, 2013.
L. F. James. Bayesian Poisson process partition calculus with an application to Bayesian Levy moving averages. The Annals of Statistics, 33(4):17711799, 2005.
L. F. James, A. Lijoi, and I. Prunster. Posterior analysis for normalized random measures with independent increments. Scandinavian Journal of Statistics, 36(1):7697, 2009.
S. Favaro and Y. W. Teh. MCMC for normalized random measure mixture models. Statistical Science, 28(3):335359, 2013.
C. E. Antoniak. Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems. The Annals of Statistics, 2(6):11521174, 1974.
S. Jain and R. M. Neal. A split-merge Markov chain Monte Carlo procedure for the Dirichlet process mixture model. Journal of Computational and Graphical Statistics, 13:158182, 2000.
J. E. Griffin and S. G. Walkera. Posterior simulation of normalized random measure mixtures. Journal of Computational and Graphical Statistics, 20(1):241259, 2011.
J. Lee and S. Choi. Incremental tree-based inference with dependent normalized random measures. In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS), Reykjavik, Iceland, 2014.
K. A. Heller and Z. Ghahrahmani. Bayesian hierarchical clustering. In Proceedings of the International Conference on Machine Learning (ICML), Bonn, Germany, 2005.  9
N. Bansal, U. Feige, R. Krauthgamer, K. Makarychev, V. Nagarajan, J. SeffiNaor, and R. Schwartz. Min-max graph partitioning and small set expansion. SIAM J. on Computing, 43(2):872904, 2014.
F. Bourse, M. Lelarge, and M. Vojnovic. Balanced graph edge partition. In Proc. of ACM KDD, 2014.
Y. Chen, S. Sanghavi, and H. Xu. Clustering sparse graphs. In Proc. of NIPS, 2012.
Y. Cheng and G. M. Church. Biclustering of expression data. In Ismb, volume 8, pages 93103, 2000.
F. Chung, S. Handjani, and D. Jungreis. Generalizations of Polyas urn problem. Annals of Combinatorics, (7):141153, 2003.
I. S. Dhillon. Co-clustering documents and words using bipartite spectral graph partitioning. In Proc. of ACM KDD, 2001.
I. S. Dhillon, S. Mallela, and D. S. Modha. Information-theoretic co-clustering. In Proc. of ACM KDD, 2003.
S. Fortunato. Community detection in graphs. Physics Reports, 486(75), 2010.
M. Hein, S. Setzer, L. Jost, and S. S. Rangapuram. The total variation on hypergraphs - learning hypergraphs revisited. In Proc. of NIPS, 2013.
T. Karagiannis, C. Gkantsidis, D. Narayanan, and A. Rowstron. Hermes: clustering users in large-scale e-mail services. In Proc. of ACM SoCC, 2010.
G. Karypis and V. Kumar. Multilevel k-way hypergraph partitioning. VLSI Design, 11(3), 2000.
R. Krauthgamer, J. S. Naor, and R. Schwartz. Partitioning graphs into balanced components. 2009.
M. Li, D. G. Andersen, and A. J. Smola. Graph partitioning via parallel submodular approximation to accelerate distributed machine learning. arXiv preprint arXiv:1505.04636, 2015.
L. Massoulie. Community detection thresholds and the weak Ramanujan property. In Proc. of ACM STOC, 2014.
I. Mitliagkas, C. Caramanis, and P. Jain. Memory limited, streaming PCA. In Proc. of NIPS, 2013.
E. Mossel, J. Neeman, and A. Sly. Reconstruction and estimation in the planted partition model. Probability Theory and Related Fields, pages 131, 2014.
L. OConnor and S. Feizi. Biclustering using message passing. In Proc. of NIPS, 2014.
J. M. Pujol et al. The little engine(s) that could: Scaling online social networks. IEEE/ACM Trans. Netw., 20(4):11621175, 2012.
I. Stanton. Streaming balanced graph partitioning algorithms for random graphs. In Proc. of ACM-SIAM SODA, 2014.
I. Stanton and G. Kliot. Streaming graph partitioning for large distributed graphs. In Proc. of ACM KDD, 2012.
C. E. Tsourakakis, C. Gkantsidis, B. Radunovic, and M. Vojnovic. FENNEL: streaming graph partitioning for massive scale graphs. In Proc. of ACM WSDM, 2014.
S.-Y. Yun, M. Lelarge, and A. Proutiere. Streaming, memory limited algorithms for community detection. In Proc. of NIPS, 2014.
Z. Z. Svitkina and E. Tardos. Min-max multiway cut. In K. Jansen, S. Khanna, J. Rolim, and D. Ron, editors, Proc. of APPROX/RANDOM, pages 207218. 2004.
B. Zong, C. Gkantsidis, and M. Vojnovic. Herding small streaming queries. In Proc. of ACM DEBS, 2015.  9
Daniel McFadden. Conditional logit analysis of qualitative choice behavior. 1973.
Louis L Thurstone. A law of comparative judgment. Psychological review, 34(4):273, 1927.
Jacob Marschak. Binary-choice constraints and random utility indicators. In Proceedings of a symposium on mathematical methods in the social sciences, volume 7, pages 1938, 1960.
D. R. Luce. Individual Choice Behavior. Wiley, New York, 1959.
Yu Lu and Sahand N Negahban. Individualized rank aggregation using nuclear norm regularization. arXiv preprint arXiv:1410.0860, 2014.
Dohyung Park, Joe Neeman, Jin Zhang, Sujay Sanghavi, and Inderjit S Dhillon. Preference completion: Large-scale collaborative ranking from pairwise comparisons. 2015.
Isobel Claire Gormley and Thomas Brendan Murphy. A grade of membership model for rank data. Bayesian Analysis, 4(2):265295, 2009.
Tie-Yan Liu. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval, 3(3):225331, 2009.
D. R. Hunter. Mm algorithms for generalized bradley-terry models. Annals of Statistics, pages 384406, 2004.
John Guiver and Edward Snelson. Bayesian inference for plackett-luce ranking models. In proceedings of the 26th annual international conference on machine learning, pages 377384. ACM, 2009.
Francois Caron and Arnaud Doucet. Efficient bayesian inference for generalized bradleyterry models. Journal of Computational and Graphical Statistics, 21(1):174196, 2012.
B. Hajek, S. Oh, and J. Xu. Minimax-optimal inference from partial rankings. In Advances in Neural Information Processing Systems, pages 14751483, 2014.
S. Negahban, S. Oh, and D. Shah. Iterative ranking from pair-wise comparisons. In NIPS, pages 2483 2491, 2012.
S. Oh and D. Shah. Learning mixed multinomial logit model from ordinal data. In Advances in Neural Information Processing Systems, pages 595603, 2014.
W. Ding, P. Ishwar, and V. Saligrama. A topic modeling approach to rank aggregation. Boston University Center for Info. and Systems Engg. Technical Report http://www.bu.edu/systems/publications, 2014.
A. Ammar, S. Oh, D. Shah, and L. Voloch. Whats your choice? learning the mixed multi-nomial logit model. In Proceedings of the ACM SIGMETRICS/international conference on Measurement and modeling of computer systems, 2014.
Rui Wu, Jiaming Xu, R Srikant, Laurent Massoulie, Marc Lelarge, and Bruce Hajek. Clustering and inference from pairwise comparisons. arXiv preprint arXiv:1502.04631, 2015.
H. Azari Soufiani, H. Diao, Z. Lai, and D. C. Parkes. Generalized random utility models with multiple types. In Advances in Neural Information Processing Systems, pages 7381, 2013.
H. A. Soufiani, D. C. Parkes, and L. Xia. Random utility theory for social choice. In NIPS, pages 126134, 2012.
B. Recht, M. Fazel, and P. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM review, 52(3):471501, 2010.
E. J. Candes and B. Recht. Exact matrix completion via convex optimization. Foundations of Computational Mathematics, 9(6):717772, 2009.
S. Negahban and M. J. Wainwright. Restricted strong convexity and (weighted) matrix completion: Optimal bounds with noise. Journal of Machine Learning Research, 2012.
Stephane Boucheron, Gabor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymptotic theory of independence. Oxford University Press, 2013.
A. Agarwal, S. Negahban, and M. Wainwright. Fast global convergence rates of gradient methods for high-dimensional statistical recovery. In In NIPS, pages 3745, 2010.
J. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Comput. Math., 2011.
S. Van De Geer. Empirical Processes in M-estimation, volume 6. Cambridge university press, 2000.
M. Ledoux. The concentration of measure phenomenon. Number 89. American Mathematical Soc., 2005.  9
Adelson, E. H. and Bergen, J. R. (1985). Spatiotemporal energy models for the perception of motion. Journal of Optical Society of America, A., 2(2):28499.
Dong, D. (2010). Maximizing causal information of natural scenes in motion. In Ilg, U. J. and Masson, G. S., editors, Dynamics of Visual Motion Processing, pages 261282. Springer US.
Doretto, G., Chiuso, A., Wu, Y. N., and Soatto, S. (2003). Dynamic textures. International Journal of Computer Vision, 51(2):91109.
Field, D. J. (1987). Relations between the statistics of natural images and the response properties of cortical cells. J. Opt. Soc. Am. A, 4(12):23792394.
Galerne, B. (2011). Stochastic image models and texture synthesis. PhD thesis, ENS de Cachan.
Galerne, B., Gousseau, Y., and Morel, J. M. (2011). Micro-Texture synthesis by phase randomization. Image Processing On Line, 1.
Gregory, R. L. (1980). Perceptions as hypotheses. Philosophical Transactions of the Royal Society B: Biological Sciences, 290(1038):181197.
Jogan, M. and Stocker, A. A. (2015). Signal integration in human visual speed perception. The Journal of Neuroscience, 35(25):93819390.
Nestares, O., Fleet, D., and Heeger, D. (2000). Likelihood functions and confidence bounds for total-least-squares problems. In IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000, volume 1, pages 523530. IEEE Comput. Soc.
Sanz-Leon, P., Vanzetta, I., Masson, G. S., and Perrinet, L. U. (2012). Motion clouds: modelbased stimulus synthesis of natural-like random textures for the study of motion perception. Journal of Neurophysiology, 107(11):32173226.
Simoncini, C., Perrinet, L. U., Montagnini, A., Mamassian, P., and Masson, G. S. (2012). More is not always better: adaptive gain control explains dissociation between perception and action. Nature Neurosci, 15(11):15961603.
Sotiropoulos, G., Seitz, A. R., and Series, P. (2014). Contrast dependency and prior expectations in human speed perception. Vision Research, 97(0):16  23.
Stocker, A. A. and Simoncelli, E. P. (2006). Noise characteristics and prior expectations in human visual speed perception. Nature Neuroscience, 9(4):578585.
Unser, M. and Tafti, P. (2014). An Introduction to Sparse Stochastic Processes. Cambridge University Press, Cambridge, UK. 367 p.
Unser, M., Tafti, P. D., Amini, A., and Kirshner, H. (2014). A unified formulation of gaussian versus sparse stochastic processes - part II: Discrete-Domain theory. IEEE Transactions on Information Theory, 60(5):30363051.
Wei, L. Y., Lefebvre, S., Kwatra, V., and Turk, G. (2009). State of the art in example-based texture synthesis. In Eurographics 2009, State of the Art Report, EG-STAR. Eurographics Association.
Wei, X.-X. and Stocker, A. A. (2012). Efficient coding provides a direct link between prior and likelihood in perceptual bayesian inference. In Bartlett, P. L., Pereira, F. C. N., Burges, C. J. C., Bottou, L., and Weinberger, K. Q., editors, NIPS, pages 13131321.
Weiss, Y. and Fleet, D. J. (2001). Velocity likelihoods in biological and machine vision. In In Probabilistic Models of the Brain: Perception and Neural Function, pages 81100.
Weiss, Y., Simoncelli, E. P., and Adelson, E. H. (2002). Motion illusions as optimal percepts. Nature Neuroscience, 5(6):598604.
Xia, G. S., Ferradans, S., Peyre, G., and Aujol, J. F. (2014). Synthesizing and mixing stationary gaussian texture models. SIAM Journal on Imaging Sciences, 7(1):476508.
Young, R. A. and Lesperance, R. M. (2001). The gaussian derivative model for spatial-temporal vision: II. cortical data. Spatial vision, 14(3):321390.  9
A. J. Bell and T. J. Sejnowski. The independent components of natural scenes are edge filters. Vision Research, 37(23):33273338, 1997.
P. Brodatz. Textures: A Photographic Album for Artists and Designers. Dover, New York, 1966. URL http://www.ux.uis.no/tranden/brodatz.html.
T. Cover and J. Thomas. Elements of Information Theory. Wiley, 2nd edition, 2006.
E. Denton, S. Chintala, A. Szlam, and R. Fergus. Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks. In Advances in Neural Information Processing Systems 28, 2015.
J. Domke, A. Karapurkar, and Y. Aloimonos. Who killed the directed model? In CVPR, 2008.
J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. DeCAF: A deep convolutional activation feature for generic visual recognition. In ICML 31, 2014.
H. E. Gerhard, L. Theis, and M. Bethge. Modeling natural image statistics. In Biologically-inspired Computer VisionFundamentals and Applications. Wiley VCH, 2015.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, 2014.
A. Graves and J. Schmidhuber. Offline handwriting recognition with multidimensional recurrent neural networks. In Advances in Neural Information Processing Systems 22, 2009.
K. Gregor, I. Danihelka, A. Mnih, C. Blundell, and D. Wierstra. Deep AutoRegressive Networks. In Proceedings of the 31st International Conference on Machine Learning, 2014.
K. Gregor, I. Danihelka, A. Graves, and D. Wierstra. DRAW: A recurrent neural network for image generation. In Proceedings of the 32nd International Conference on Machine Learning, 2015.
N. Heess, C. Williams, and G. E. Hinton. Learning generative texture models with extended fields-ofexperts. In BMCV, 2009.
G. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep belief nets. Neural Comp., 2006.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8), 1997.
R. Hosseini, F. Sinz, and M. Bethge. Lower bounds on the redundancy of natural images. Vis. Res., 2010.
A. Hyvarinen and P. O. Hoyer. Emergence of phase and shift invariant features by decomposition of natural images into independent feature subspaces. Neural Computation, 12(7):1705-1720, 2000.  8
Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding, 2014. arXiv:1408.5093.
D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In ICLR, 2014.
D. P. Kingma, D. J. Rezende, S. Mohamed, and M. Welling. Semi-supervised learning with deep generative models. In Advances in Neural Information Processing Systems 27, 2014.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, 2012.
H. Larochelle and I. Murray. The neural autoregressive distribution estimator. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, 2011.
A. B. Lee, D. Mumford, and J. Huang. Occlusion models for natural images: A statistical study of a scale-invariant dead leaves model. International Journal of Computer Vision, 2001.
H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In ICML 26, 2009.
Y. Li, K. Swersky, and R. Zemel. Generative moment matching networks. In ICML 32, 2015.
D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In ICCV, 2001.
G. Matheron. Modele sequential de partition aleatoire. Technical report, CMM, 1968.
R. M. Neal. Connectionist learning of belief networks. Artificial Intelligence, 56:71113, 1992.
J. Ngiam, Z. Chen, P. W. Koh, and A. Y. Ng. Learning deep energy models. In ICML 28, 2011.
S. Osindero and G. E. Hinton. Modelling image patches with a directed hierarchy of markov random fields. In Advances In Neural Information Processing Systems 20, 2008.
M. A. Ranzato, J. Susskind, V. Mnih, and G. E. Hinton. On deep generative models with applications to recognition. In IEEE Conference on Computer Vision and Pattern Recognition, 2011.
M. A. Ranzato, A. Szlam, J. Bruna, M. Mathieu, R. Collobert, and S. Chopra. Video (language) modeling: a baseline for generative models of natural videos, 2015. arXiv:1412.6604v2.
A. J. Robinson and F. Fallside. The utility driven dynamic error propagation network. Technical report, Cambridge University, 1987.
S. Roth and M. J. Black. Fields of experts. International Journal of Computer Vision, 82(2), 2009.
K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Represenations, 2015.
J. Sohl-Dickstein, E. A. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML 32, 2015.
N. Srivastava and R. Salakhutdinov. Multimodal learning with deep Boltzmann machines. JMLR, 2014.
N. Srivastava, E. Mansimov, and R. Salakhutdinov. Unsupervised learning of video representations using LSTMs. In Proceedings of the 32nd International Conference on Machine Learning, 2015.
M. Sundermeyer, R. Schluter, and H. Ney. LSTM neural networks for language modeling. In INTERSPEECH, 2010.
I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems 27, 2014.
L. Theis, S. Gerwinn, F. Sinz, and M. Bethge. In all likelihood, deep belief is not enough. JMLR, 2011.
L. Theis, R. Hosseini, and M. Bethge. Mixtures of conditional Gaussian scale mixtures applied to multiscale image representations. PLoS ONE, 7(7), 2012.
L. Theis, J. Sohl-Dickstein, and M. Bethge. Training sparse natural image models with a fast Gibbs sampler of an extended state space. In Advances in Neural Information Processing Systems 25, 2012.
L. Tierney. Markov chains for exploring posterior distributions. The Annals of Statistics, 1994.
B. Uria, I. Murray, and H. Larochelle. RNADE: the real-valued neural autoregressive density-estimator. In Advances in Neural Information Processing Systems 26, 2013.
B. Uria, I. Murray, and H. Larochelle. A deep and tractable density estimator. In ICML 31, 2014.
A. van den Oord and B. Schrauwen. The student-t mixture as a natural image patch prior with application to image compression. Journal of Machine Learning Research, 15(1):20612086, 2014.
A. van den Oord and B. Schrauwen. Factoring variations in natural images with deep Gaussian mixture models. In Advances in Neural Information Processing Systems 27, 2014.
J. H. van Hateren and A. van der Schaaf. Independent component filters of natural images compared with simple cells in primary visual cortex. Proc. of the Royal Society B: Biological Sciences, 265(1394), 1998.
D. Zoran and Y. Weiss. From learning models of natural image patches to whole image restoration. In IEEE International Conference on Computer Vision, 2011.
D. Zoran and Y. Weiss. Natural images, Gaussian mixtures and dead leaves. In NIPS 25, 2012.  9
Alekh Agarwal, Sahand Negahban, Martin J Wainwright, et al. Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions. The Annals of Statistics, 40(2):1171 1197, 2012.
Peter J Bickel, Yaacov Ritov, and Alexandre B Tsybakov. Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, pages 17051732, 2009.
Emmanuel J Cands, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis? Journal of the ACM (JACM), 58(3):11, 2011.
Rina Foygel, Ohad Shamir, Nati Srebro, and Ruslan R Salakhutdinov. Learning with the weighted trace-norm under arbitrary sampling distributions. In Advances in Neural Information Processing Systems, pages 21332141, 2011.
Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM review, 53(2):217288, 2011.
Jun He, Laura Balzano, and John Lui. Online robust subspace tracking from partial information. arXiv preprint arXiv:1109.3827, 2011.
Jun He, Laura Balzano, and Arthur Szlam. Incremental gradient on the grassmannian for online foreground and background separation in subsampled video. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 15681575. IEEE, 2012.
Odalric Maillard and Rmi Munos. Compressed least-squares regression. In Advances in Neural Information Processing Systems, pages 12131221, 2009.
Praneeth Netrapalli, UN Niranjan, Sujay Sanghavi, Animashree Anandkumar, and Prateek Jain. Non-convex robust PCA. In Advances in Neural Information Processing Systems, pages 11071115, 2014.
John Wright, Arvind Ganesh, Shankar Rao, Yigang Peng, and Yi Ma. Robust principal component analysis: Exact recovery of corrupted low-rank matrices via convex optimization. In Advances in Neural Information Processing Systems, pages 20802088, 2009.
Huan Xu, Constantine Caramanis, and Sujay Sanghavi. Robust PCA via outlier pursuit. In Advances in Neural Information Processing Systems, pages 24962504, 2010.
Shuheng Zhou, John Lafferty, and Larry Wasserman. Compressed and privacy-sensitive sparse regression. IEEE Transactions on Information Theory, 55(2):846866, 2009.
Tianyi Zhou and Dacheng Tao. Godec: Randomized low-rank & sparse matrix decomposition in noisy case. In Proceedings of the 28th International Conference on Machine Learning, pages 3340, 2011.  9
David Aldous. Random walks on finite groups and rapidly mixing markov chains. In Seminaire de Probabilites XVII. Springer, 1983.
Russ Bubley and Martin Dyer. Path coupling: A technique for proving rapid mixing in markov chains. In Symposium on Foundations of Computer Science, 1997.
Russ Bubley, Martin Dyer, and Catherine Greenhill. Beating the 2d bound for approximately counting colourings: A computer-assisted proof of rapid mixing. In Symposium on Discrete Algorithms, 1998.
Michele Conforti and Gerard Cornuejols. Submodular set functions, matroids and the greedy algorithm: Tight worst-case bounds and some generalizations of the rado-edmonds theorem. Disc. App. Math., 1984.
Persi Diaconis and Daniel Stroock. Geometric bounds for eigenvalues of markov chains. The Annals of Applied Probability, 1991.
Josip Djolonga and Andreas Krause. From MAP to marginals: Variational inference in bayesian submodular models. In Neural Information Processing Systems, 2014.
Martin Dyer, Leslie Ann Goldberg, and Mark Jerrum. Matrix norms and rapid mixing for spin systems. Annals of Applied Probability, 2009.
Martin Dyer and Catherine Greenhill. On markov chains for independent sets. J. of Algorithms, 2000.
Uriel Feige, Vahab S. Mirrokni, and Jan Vondrak. Maximizing non-monotone submodular functions. In Symposium on Foundations of Computer Science, 2007.
Satoru Fujishige. Submodular Functions and Optimization. Elsevier Science, 2005.
Andrew Gelman and Kenneth Shirley. Innovation and intellectual property rights. In Handbook of Markov Chain Monte Carlo. CRC Press, 2011.
Daniel Golovin and Andreas Krause. Adaptive submodularity: Theory and applications in active learning and stochastic optimization. Journal of Artificial Intelligence Research, 2011.
Rishabh Iyer and Jeff Bilmes. Submodular point processes with applications in machine learning. In International Conference on Artificial Intelligence and Statistics, 2015.
Mark Jerrum. A very simple algorithm for estimating the number of k-colorings of a low-degree graph. Random Structures and Algorithms, 1995.
Mark Jerrum. Counting, Sampling and Integrating: Algorithms and Complexity. Birkhauser, 2003.
Mark Jerrum and Alistair Sinclair. Approximating the permanent. SIAM Journal on Computing, 1989.
Mark Jerrum and Alistair Sinclair. Polynomial-time approximation algorithms for the Ising model. SIAM Journal on Computing, 1993.
Mark Jerrum, Alistair Sinclair, and Eric Vigoda. A polynomial-time approximation algorithm for the permanent of a matrix with non-negative entries. Journal of the ACM, 2004.
David Kempe, Jon Kleinberg, and Eva Tardos. Maximizing the spread of influence through a social network. In Conference on Knowledge Discovery and Data Mining, 2003.
Daphne Koller and Nir Friedman. Probabilistic Graphical Models: Principles and Techniques. The MIT Press, 2009.
Andreas Krause, Carlos Guestrin, Anupam Gupta, and Jon Kleinberg. Near-optimal sensor placements: Maximizing information while minimizing communication cost. In Information Processing in Sensor Networks, 2006.
Andreas Krause, Jure Leskovec, Carlos Guestrin, Jeanne Vanbriesen, and Christos Faloutsos. Efficient sensor placement optimization for securing large water distribution networks. Journal of Water Resources Planning and Management, 2008.
Alex Kulesza and Ben Taskar. Determinantal point processes for machine learning. Foundations and Trends in Machine Learning, 2012.
David A. Levin, Yuval Peres, and Elizabeth L. Wilmer. Markov Chains and Mixing Times. American Mathematical Society, 2008.
Hui Lin and Jeff Bilmes. A class of submodular functions for document summarization. In Human Language Technologies, 2011.
George L. Nemhauser, Laurence A. Wolsey, and Marshall L. Fisher. An analysis of approximations for maximizing submodular set functions. Mathematical Programming, 1978.
Patrick Rebeschini and Amin Karbasi. Fast mixing for discrete point processes. In Conference on Learning Theory, 2015.
Alistair Sinclair. Improved bounds for mixing rates of markov chains and multicommodity flow. Combinatorics, Probability and Computing, 1992.  9
H. Kwak, C. Lee, H. Park, and others. What is Twitter, a social network or a news media? WWW, 2010.
J. Cheng, L. Adamic, P. A. Dow, and others. Can cascades be predicted? WWW, 2014.
D. Antoniades and C. Dovrolis. Co-evolutionary dynamics in social networks: A case study of twitter. arXiv:1309.6001, 2013.
S. Myers and J. Leskovec. The bursty dynamics of the twitter information network. WWW, 2014.
L. Weng, J. Ratkiewicz, N. Perra, B. Goncalves, C. Castillo, F. Bonchi, R. Schifanella, F. Menczer, and A. Flammini. The role of information diffusion in the evolution of social networks. KDD, 2013.
N. Du, L. Song, M. Gomez-Rodriguez, and H. Zha. Scalable influence estimation in continuous-time diffusion networks. NIPS, 2013.
M. Gomez-Rodriguez, D. Balduzzi, and B. Scholkopf. Uncovering the temporal dynamics of diffusion networks. ICML, 2011.
M. Gomez-Rodriguez, J. Leskovec, A. Krause. Inferring networks of diffusion and influence. KDD, 2010.
D. Chakrabarti, Y. Zhan, and C. Faloutsos. R-mat: A recursive model for graph mining. Computer Science Department, page 541, 2004.
J. Leskovec, D. Chakrabarti, J. Kleinberg, C. Faloutsos, and J. Leskovec. Kronecker graphs: An approach to modeling networks. JMLR, 2010.
J. Leskovec, L. Backstrom, R. Kumar, and others. Microscopic evolution of social networks. KDD, 2008.
T.J. Liniger. Multivariate Hawkes Processes. PhD thesis, ETHZ, 2009.
C. Blundell, J. Beck, K. Heller. Modelling reciprocating relationships with hawkes processes. NIPS, 2012.
M. Farajtabar, N. Du, M. Gomez-Rodriguez, I. Valera, H. Zha, and L. Song. Shaping social activity by incentivizing users. NIPS, 2014.
T. Iwata, A. Shah, and Z. Ghahramani. Discovering latent influence in online social activities via shared cascade poisson processes. KDD, 2013.
S. Linderman and R. Adams. Discovering latent network structure in point process data. ICML, 2014.
I. Valera, M. Gomez-Rodriguez, Modeling adoption of competing products and conventions in social media. ICDM, 2015.
K. Zhou, H. Zha, and L. Song. Learning social infectivity in sparse low-rank networks using multidimensional hawkes processes. AISTATS, 2013.
K. Zhou, H. Zha, and L. Song. Learning triggering kernels for multi-dimensional hawkes processes. ICML, 2013.
D. Hunter, P. Smyth, D. Q. Vu, and others. Dynamic egocentric models for citation networks. ICML, 2011.
D. Q. Vu, D. Hunter, P. Smyth, and A. Asuncion. Continuous-time regression models for longitudinal networks. NIPS, 2011.
J. Leskovec, J. Kleinberg, and C. Faloutsos. Graphs over time: densification laws, shrinking diameters and possible explanations. KDD, 2005.
S. Goel, D. J. Watts, and D. G. Goldstein. The structure of online diffusion networks. EC, 2012.
O. Aalen, O. Borgan, and H. Gjessing. Survival and event history analysis: a process point of view, 2008.
D. Kempe, J. Kleinberg, and E. Tardos. Maximizing the spread of influence through a social network. KDD, 2003.
Y. Ogata. On lewis simulation method for point processes. IEEE TIT, 27(1):2331, 1981.
P. Erdos and A Renyi. On the evolution of random graphs. Hungar. Acad. Sci, 5:1761, 1960. L. Backstrom, P. Boldi, M. Rosa, J. Ugander, and S. Vigna. Four degrees of separation. WebSci, 2012. M. Granovetter. The strength of weak ties. American journal of sociology, pages 13601380, 1973. D. Romero and J. Kleinberg. The directed closure process in hybrid social-information networks, with an analysis of link formation on twitter. ICWSM, 2010.
J. Ugander, L. Backstrom, and J. Kleinberg. Subgraph frequencies: Mapping the empirical and extremal geography of large graph collections. WWW, 2013.
D.J. Watts and S.H. Strogatz. Collective dynamics of small-world networks. Nature, 1998.
T. Gross and B. Blasius. Adaptive coevolutionary networks: a review. Royal Society Interface, 2008.
P. Singer, C. Wagner, and M. Strohmaier. Factors influencing the co-evolution of social and content networks in online social media. Modeling and Mining Ubiquitous Social Media, pages 4059. Springer, 2012.  9
Leonard E. Baum, Ted Petrie, George Soules, and Norman Weiss. A maximization technique occurring in the statistical analysis of probabilistic functions of markov chains. The Annals of Mathematical Statistics, 41(1):pp. 164171, 1970.
W. R. Gilks, S. Richardson, and D. J. Spiegelhalter. Markov Chain Monte Carlo in Practice. Chapman and Hall, London, 1996 (ISBN: 0-412-05551-1). 8  This book thoroughly summarizes the uses of MCMC in Bayesian analysis. It is a core book for Bayesian studies.
Daniel Hsu, Sham M. Kakade, and Tong Zhang. A spectral algorithm for learning hidden markov models. In COLT, 2009.
Judea Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, New York, NY, USA, 2000.
J.H. Stock and M.W. Watson. Introduction to Econometrics. Addison-Wesley series in economics. Addison-Wesley, 2011.
Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor decompositions for learning latent variable models. The Journal of Machine Learning Research, 15(1):27732832, 2014.
Matthew Rosencrantz and Geoff Gordon. Learning low dimensional predictive representations. In ICML 04: Twenty-first international conference on Machine learning, pages 695 702, 2004.
P. van Overschee and L.R. de Moor. Subspace identification for linear systems: theory, implementation, applications. Kluwer Academic Publishers, 1996.
Byron Boots, Arthur Gretton, and Geoffrey J. Gordon. Hilbert Space Embeddings of Predictive State Representations. In Proc. 29th Intl. Conf. on Uncertainty in Artificial Intelligence (UAI), 2013.
Kenji Fukumizu, Le Song, and Arthur Gretton. Kernel bayes rule: Bayesian inference with positive definite kernels. Journal of Machine Learning Research, 14(1):37533783, 2013.
Byron Boots. Spectral Approaches to Learning Predictive Representations. PhD thesis, Carnegie Mellon University, December 2012.
Sajid Siddiqi, Byron Boots, and Geoffrey J. Gordon. Reduced-rank hidden Markov models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS-2010), 2010.
Byron Boots, Sajid Siddiqi, and Geoffrey Gordon. Closing the learning planning loop with predictive state representations. In I. J. Robotic Research, volume 30, pages 954956, 2011.
Byron Boots and Geoffrey Gordon. An online spectral learning algorithm for partially observable nonlinear dynamical systems. In Proceedings of the 25th National Conference on Artificial Intelligence (AAAI-2011), 2011.
Borja Balle, William Hamilton, and Joelle Pineau. Methods of moments for learning stochastic languages: Unified presentation and empirical comparison. In Tony Jebara and Eric P. Xing, editors, Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 13861394. JMLR Workshop and Conference Proceedings, 2014.
L. Song, B. Boots, S. M. Siddiqi, G. J. Gordon, and A. J. Smola. Hilbert space embeddings of hidden Markov models. In Proc. 27th Intl. Conf. on Machine Learning (ICML), 2010.
Byron Boots and Geoffrey Gordon. Two-manifold problems with applications to nonlinear system identification. In Proc. 29th Intl. Conf. on Machine Learning (ICML), 2012.
John Langford, Ruslan Salakhutdinov, and Tong Zhang. Learning nonlinear dynamic models. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML 2009, Montreal, Quebec, Canada, June 14-18, 2009, pages 593600, 2009.
Albert T. Corbett and John R. Anderson. Knowledge tracing: Modelling the acquisition of procedural knowledge. User Model. User-Adapt. Interact., 4(4):253278, 1995.
Kenneth R. Koedinger, R. S. J. Baker, K. Cunningham, A. Skogsholm, B. Leber, and John Stamper. A data repository for the EDM community: The PSLC DataShop. Handbook of Educational Data Mining, pages 4355, 2010.  9
Michael Bowling, Neil Burch, Michael Johanson, and Oskari Tammelin. Heads-up limit holdem poker is solved. Science, 347(6218):145149, 2015.
Noam Brown, Sam Ganzfried, and Tuomas Sandholm. Hierarchical abstraction, distributed equilibrium computation, and post-processing, with application to a champion no-limit texas holdem agent. In Proceedings of the 2015 international conference on Autonomous agents and multi-agent systems. International Foundation for Autonomous Agents and Multiagent Systems, 2015.
Andrew Gilpin, Javier Pena, and Tuomas Sandholm. First-order algorithm with O(ln(1/)) convergence for -equilibrium in two-person zero-sum games. Mathematical Programming, 133(12):279298, 2012. Conference version appeared in AAAI-08.
Andrew Gilpin and Tuomas Sandholm. Lossless abstraction of imperfect information games. Journal of the ACM, 54(5), 2007. Early version Finding equilibria in large sequential games of imperfect information appeared in the Proceedings of the ACM Conference on Electronic Commerce (EC), pages 160169, 2006.
Samid Hoda, Andrew Gilpin, Javier Pena, and Tuomas Sandholm. Smoothing techniques for computing Nash equilibria of sequential games. Mathematics of Operations Research, 35(2):494512, 2010. Conference version appeared in WINE-07.
Eric Griffin Jackson. A time and space efficient algorithm for approximately solving large imperfect information games. In AAAI Workshop on Computer Poker and Imperfect Information, 2014.
Michael Johanson, Nolan Bard, Neil Burch, and Michael Bowling. Finding optimal abstract strategies in extensive-form games. In AAAI Conference on Artificial Intelligence (AAAI), 2012.
Christian Kroer, Kevin Waugh, Fatma Klnc-Karzan, and Tuomas Sandholm. Faster firstorder methods for extensive-form game solving. In Proceedings of the ACM Conference on Economics and Computation (EC), 2015.
Marc Lanctot, Kevin Waugh, Martin Zinkevich, and Michael Bowling. Monte Carlo sampling for regret minimization in extensive games. In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS), pages 10781086, 2009.
Francois Pays. An interior point approach to large games of incomplete information. In AAAI Computer Poker Workshop, 2014.
Tuomas Sandholm. The state of solving large incomplete-information games, and application to poker. AI Magazine, pages 1332, Winter 2010. Special issue on Algorithmic Game Theory.
Finnegan Southey, Michael Bowling, Bryce Larson, Carmelo Piccione, Neil Burch, Darse Billings, and Chris Rayner. Bayes bluff: Opponent modelling in poker. In Proceedings of the 21st Annual Conference on Uncertainty in Artificial Intelligence (UAI), pages 550558, July 2005.
Oskari Tammelin. Solving large imperfect information games using CFR+. arXiv preprint arXiv:1407.5042, 2014.
Oskari Tammelin, Neil Burch, Michael Johanson, and Michael Bowling. Solving heads-up limit texas holdem. In IJCAI, volume 2015, 2015.
Kevin Waugh, David Schnizlein, Michael Bowling, and Duane Szafron. Abstraction pathologies in extensive games. In International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), 2009.
Martin Zinkevich, Michael Bowling, Michael Johanson, and Carmelo Piccione. Regret minimization in games with incomplete information. In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS), 2007.  9
V. Alba Fernandez, M. Jimenez-Gamero, and J. Munoz Garcia. A test for the two-sample problem based on empirical characteristic functions. Computational Statistics and Data Analysis, 52:37303748, 2008.
P. Baldi, P. Sadowski, and D. Whiteson. Searching for exotic particles in high-energy physics with deep learning. Nature Communications, 5, 2014.
L Baringhaus and C Franz. On a new multivariate two-sample test. J mult anal, 88(1):190206, 2004.
Alain Berlinet and Christine Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and statistics, volume 3. Kluwer Academic Boston, 2004.
K.M. Borgwardt, A. Gretton, M.J. Rasch, H.-P. Kriegel, B. Scholkopf, and A. Smola. Integrating structured biological data by kernel maximum mean discrepancy. Bioinformatics, 22(14):e49e57, 2006.
K. R. Davidson. Pointwise limits of analytic functions. Am math mon, pages 391394, 1983.
T.W. Epps and K.J. Singleton. An omnibus test for the two-sample problem using the empirical characteristic function. Journal of Statistical Computation and Simulation., 26(3-4):177203, 1986.
A. Gretton, K. Borgwardt, M. Rasch, B. Scholkopf, and A. Smola. A kernel two-sample test. JMLR, 13:723773, 2012.
A. Gretton, K. Fukumizu, Z. Harchaoui, and B. Sriperumbudur. A fast, consistent kernel two-sample test. In NIPS, 2009.
A. Gretton, B. Sriperumbudur, D. Sejdinovic, H. Strathmann, S. Balakrishnan, M. Pontil, and K. Fukumizu. Optimal kernel choice for large-scale two-sample tests. In NIPS, 2012.
Z. Harchaoui, F.R. Bach, and E. Moulines. Testing for Homogeneity with Kernel Fisher Discriminant Analysis. In NIPS. 2008.
CE Heathcote. A test of goodness of fit for symmetric random variables. Aust J stat, 14(2):172181, 1972.
CR Heathcote. The integrated squared error estimation of parameters. Biometrika, 64(2):255264, 1977.
H.-C. Ho and G. Shieh. Two-stage U-statistics for hypothesis testing. Scandinavian Journal of Statistics, 33(4):861873, 2006.
H. Hotelling. The generalization of students ratio. Ann. Math. Statist., 2(3):360378, 1931.
Q. Le, T. Sarlos, and A. Smola. Fastfood - computing Hilbert space expansions in loglinear time. In ICML, volume 28, pages 244252, 2013.
M. Lichman. UCI machine learning repository, 2013.
J.R. Lloyd and Z. Ghahramani. Statistical model criticism using kernel two sample tests. Technical report, 2014.
Tomas Pevny and Jessica Fridrich. Benchmarking for steganography. In Information Hiding, pages 251 267. Springer, 2008.
A. Rahimi and B. Recht. Random features for large-scale kernel machines. In NIPS, 2007.
A. Ramdas, S. Reddi, B. Poczos, A. Singh, and L. Wasserman. On the decreasing power of kernel- and distance-based nonparametric hypothesis tests in high dimensions. AAAI, 2015.
S. Reddi, A. Ramdas, B. Poczos, A. Singh, and L. Wasserman. On the high-dimensional power of lineartime kernel two-sample testing under mean-difference alternatives. AISTATS, 2015.
Walter Rudin. Real and complex analysis. Tata McGraw-Hill Education, 1987.
D. Sejdinovic, B. Sriperumbudur, A. Gretton, and K. Fukumizu. Equivalence of distance-based and RKHS-based statistics in hypothesis testing. Annals of Statistics, 41(5):22632291, 2013.
B. Sriperumbudur, K. Fukumizu, and G. Lanckriet. Universality, characteristic kernels and RKHS embedding of measures. JMLR, 12:23892410, 2011.
B. Sriperumbudur, A. Gretton, K. Fukumizu, G. Lanckriet, and B. Scholkopf. Hilbert space embeddings and metrics on probability measures. JMLR, 11:15171561, 2010.
I. Steinwart and A. Christmann. Support vector machines. Springer Science & Business Media, 2008.
I. Steinwart, D. Hush, and C. Scovel. An explicit description of the reproducing kernel hilbert spaces of gaussian rbf kernels. Information Theory, IEEE Transactions on, 52(10):46354643, 2006.
Hong-Wei Sun and Ding-Xuan Zhou. Reproducing kernel hilbert spaces associated with analytic translation-invariant mercer kernels. Journal of Fourier Analysis and Applications, 14(1):89101, 2008.
GJ Szekely. E-statistics: The energy of statistical samples. Technical report, 2003.
W. Zaremba, A. Gretton, and M. Blaschko. B-test: A non-parametric, low variance kernel two-sample test. In NIPS, 2013.
Ji Zhao and Deyu Meng. FastMMD: Ensemble of circular discrepancy for efficient two-sample test. Neural computation, (27):13451372, 2015.
AA Zinger, AV Kakosyan, and LB Klebanov. A characterization of distributions by mean values of statistics and certain probabilistic metrics. Journal of Mathematical Sciences, 59(4):914920, 1992.  9
B. Alexe, T. Deselaers, and V. Ferrari. Measuring the objectness of image windows. PAMI, 2012. 2
N. Chavali, H. Agrawal, A. Mahendru, and D. Batra. Object-proposal evaluation protocol is gameable. arXiv:1505.05836, 2015. 8
L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Semantic image segmentation with deep convolutional nets and fully connected crfs. ICLR, 2015. 2
N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005. 1
J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 1, 3
D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable object detection using deep neural networks. In CVPR, 2014. 2
M. Everingham, L. V. Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL visual object classes (VOC) challenge. IJCV, 2010. 1, 2, 6
P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part-based models. PAMI, 2010. 1
R. Girshick. Fast R-CNN. arXiv:1504.08083, 2015. 2, 8
R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014. 1, 2, 6
B. Hariharan, P. Arbelaez, R. Girshick, and J. Malik. Hypercolumns for object segmentation and finegrained localization. In CVPR, 2015. 2
K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV, 2014. 2
J. Hosang, R. Benenson, P. Dollar, and B. Schiele. What makes for effective detection proposals? arXiv:1502.05082, 2015. 2, 6
A. Humayun, F. Li, and J. M. Rehg. RIGOR: Reusing Inference in Graph Cuts for generating Object Regions. In CVPR, 2014. 2, 6, 7, 8
H. Kaiming, Z. Xiangyu, R. Shaoqing, and S. Jian. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV, 2014. 1
P. Krahenbuhl and V. Koltun. Geodesic object proposals. In ECCV, 2014. 2, 6, 7, 8
P. Krahenbuhl and V. Koltun. Learning to propose objects. In CVPR, 2015. 2
A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012. 2
W. Kuo, B. Hariharan, and J. Malik. Deepbox: Learning objectness with convolutional networks. In arXiv:505.02146v1, 2015. 2, 8
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 1998. 1
T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, C. L. Zitnick, and P. Dollar. Microsoft COCO: Common objects in context. arXiv:1405.0312, 2015. 1, 2, 5, 6
M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Is object localization for free?  Weakly-supervised learning with convolutional neural networks. In CVPR, 2015. 2
P. O. Pinheiro and R. Collobert. Recurrent conv. neural networks for scene labeling. In ICML, 2014. 2
J. Pont-Tuset, P. Arbelaez, J. Barron, F. Marques, and J. Malik. Multiscale combinatorial grouping for image segmentation and object proposal generation. In arXiv:1503.00848, 2015. 2, 6, 7, 8
S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In arXiv:1506.01497, 2015. 2
P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014. 5
K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. 2, 3, 5
N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. JMLR, 2014. 5
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In CVPR, 2015. 2
C. Szegedy, S. Reed, D. Erhan, and D. Anguelov. Scalable, high-quality object detection. In arXiv:1412.1441, 2014. 2
J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders. Selective search for object recog. IJCV, 2013. 2, 6, 7, 8
P. Viola and M. J. Jones. Robust real-time face detection. IJCV, 2004. 1
Z. Y. Zhu, R. Urtasun, R. Salakhutdinov, and S. Fidler. segdeepm: Exploiting segmentation and context in deep neural networks for object detection. In CVPR, 2015. 1
C. L. Zitnick and P. Dollar. Edge boxes: Locating object proposals from edges. In ECCV, 2014. 2, 6, 7, 8  9
H. Akaike. A new look at the statistical model identification. IEEE Transactions on Automatic Control, 19(6):716723, 1974.
M. A. Alvarez, L. Rosasco, and N. D. Lawrence. Kernels for vector-valued functions: a review. Foundations and Trends in Machine Learning, 4(3):195266, 2012.
M. J. Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis, University College London.
R. Caruana. Multitask learning. Machine Learning, 28(1):4175, 1997.
C. R. Dietrich and G. N. Newsam. Fast and exact simulation of stationary Gaussian processes through circulant embedding of the covariance matrix. SIAM Journal on Scientific Computing, 18(4):10881107, 1997.
K. Dzirasa, R. Fuentes, S. Kumar, J. M. Potes, and M. A. L. Nicolelis. Chronic in vivo multi-circuit neurophysiological recordings in mice. Journal of Neuroscience Methods, 195(1):3646, 2011.
K. Dzirasa, S. Ribeiro, R. Costa, L. M. Santos, S. C. Lin, A. Grosmark, T. D. Sotnikova, R. R. Gainetdinov, M. G. Caron, and M. A. L. Nicolelis. Dopaminergic control of sleepwake states. The Journal of Neuroscience, 26(41):1057710589, 2006.
R. G. Gallager. Principles of digital communication. pages 229232, 2008.
M. Gonen and E. Alpaydn. Multiple kernel learning algorithms. JMLR, 12:22112268, 2011.
P. Goovaerts. Geostatistics for Natural Resources Evaluation. Oxford University Press, 1997.
G. G. Gregoriou, S. J. Gotts, H. Zhou, and R. Desimone. High-frequency, long-range coupling between prefrontal and visual cortex during attention. Science, 324(5931):12071210, 2009.
M. Lazaro-Gredilla, J. Quinonero Candela, C. E. Rasmussen, and A. R. Figueiras-Vidal. Sparse spectrum Gaussian process regression. JMLR, (11):18651881, 2010.
J. R. Lloyd, D. Duvenaud, R. Grosse, J. B. Tenenbaum, and Z. Ghahramani. Automatic construction and natural-language description of nonparametric regression models. AAAI, 2014.
D. J. C. MacKay. Ensemble learning for hidden Markov models. Technical report, 1997.
D. Pfaff, A. Ribeiro, J. Matthews, and L. Kow. Concepts and mechanisms of generalized central nervous system arousal. ANYAS, 2008.
C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. 2006.
P. Sauseng and W. Klimesch. What does phase information of oscillatory brain activity tell us about cognitive processes? Neuroscience and Biobehavioral Reviews, 32:10011013, 2008.
C. M. Sweeney-Reed, T. Zaehle, J. Voges, F. C. Schmitt, L. Buentjen, K. Kopitzki, C. Esslinger, H. Hinrichs, H. J. Heinze, R. T. Knight, and A. Richardson-Klavehn. Corticothalamic phase synchrony and cross-frequency coupling predict human memory formation. eLIFE, 2014.
Y. W. Teh, M. Seeger, and M. I. Jordan. Semiparametric latent factor models. AISTATS, 10:333340, 2005.
M. A. Tucker, Y. Hirota, E. J. Wamsley, H. Lau, A. Chaklader, and W. Fishbein. A daytime nap containing solely non-REM sleep enhances declarative but not procedural memory. Neurobiology of Learning and Memory, 86(2):2417, 2006.
K. Ulrich, D. E. Carlson, W. Lian, J. S. Borg, K. Dzirasa, and L. Carin. Analysis of brain states from multi-region LFP time-series. NIPS, 2014.
P. D. Welch. The use of fast Fourier transform for the estimation of power spectra: A method based on time averaging over short, modified periodograms. IEEE Transactions on Audio and Electroacoustics, 15(2):7073, 1967.
A. G. Wilson. Covariance kernels for fast automatic pattern discovery and extrapolation with Gaussian processes. PhD thesis, University of Cambridge, 2014.
A. G. Wilson and R. P. Adams. Gaussian process kernels for pattern discovery and extrapolation. ICML, 2013.
A. G. Wilson, E. Gilboa, A. Nehorai, and J. P. Cunningham. Fast kernel learning for multidimensional pattern extrapolation. NIPS, 2014.
A. G. Wilson and D. A. Knowles. Gaussian process regression networks. ICML, 2012.
Z. Yang, A. J. Smola, L. Song, and A. G. Wilson. A la carte  learning fast kernels. AISTATS, 2015.  9
Emmanuel Abbe, Amir Khandani, and W Andrew. Lo. 2011.privacy-preserving methods for sharing financial risk exposures.. The American Economic Review, 102:6570.
Amos Beimel, Kobbi Nissim, and Eran Omri. Distributed private data analysis: Simultaneously solving how and what. In Advances in CryptologyCRYPTO 2008, pages 451468. Springer, 2008.
Michael Ben-Or, Shafi Goldwasser, and Avi Wigderson. Completeness theorems for non-cryptographic fault-tolerant distributed computation. In Proceedings of the twentieth annual ACM symposium on Theory of computing, pages 110. ACM, 1988.
D. Blackwell. Equivalent comparisons of experiments. The annals of mathematical statistics, 24(2):265 272, 1953.
A. Blum, C. Dwork, F. McSherry, and K. Nissim. Practical privacy: the sulq framework. In Proceedings of the twenty-fourth symposium on Principles of database systems, pages 128138. ACM, 2005.
H. Brenner and K. Nissim. Impossibility of differentially private universally optimal mechanisms. In Foundations of Computer Science, 2010 51st Annual IEEE Symposium on, pages 7180. IEEE, 2010.
J. A. Calandrino, A. Kilzer, A. Narayanan, E. W. Felten, and V. Shmatikov.  you might also like: privacy risks of collaborative filtering. In Security and Privacy (SP), 2011 IEEE Symposium on, pages 231246. IEEE, 2011.
K. Chaudhuri, A. Sarwate, and K. Sinha. Near-optimal differentially private principal components. In Advances in Neural Information Processing Systems, pages 989997, 2012.
K. Chaudhuri, A. D. Sarwate, and K. Sinha. A near-optimal algorithm for differentially-private principal components. Journal of Machine Learning Research, 14:29052943, 2013.
Kamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate. Differentially private empirical risk minimization. The Journal of Machine Learning Research, 12:10691109, 2011.
David Chaum, Claude Crepeau, and Ivan Damgard. Multiparty unconditionally secure protocols. In Proceedings of the twentieth annual ACM symposium on Theory of computing, pages 1119. ACM, 1988.
T. M. Cover and J. A. Thomas. Elements of information theory. John Wiley & Sons, 2012.
J. C. Duchi, M. I. Jordan, and M. J. Wainwright. Local privacy and statistical minimax rates. In Foundations of Computer Science (FOCS), 2013 IEEE 54th Annual Symposium on, pages 429438. IEEE, 2013.  8
C. Dwork. Differential privacy. In Automata, languages and programming, pages 112. Springer, 2006.
C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography, pages 265284. Springer, 2006.
Cynthia Dwork. Differential privacy: A survey of results. In Theory and Applications of Models of Computation, pages 119. Springer, 2008.
Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data, ourselves: Privacy via distributed noise generation. In Advances in Cryptology-EUROCRYPT 2006, pages 486503. Springer, 2006.
Quan Geng and Pramod Viswanath. The optimal mechanism in differential privacy. arXiv preprint arXiv:1212.1186, 2012.
Quan Geng and Pramod Viswanath. The optimal mechanism in differential privacy: Multidimensional setting. arXiv preprint arXiv:1312.0655, 2013.
A. Ghosh, T. Roughgarden, and M. Sundararajan. Universally utility-maximizing privacy mechanisms. SIAM Journal on Computing, 41(6):16731693, 2012.
O. Goldreich, S. Micali, and A. Wigderson. How to play any mental game. In Proceedings of the Nineteenth Annual ACM Symposium on Theory of Computing, STOC 87, pages 218229, New York, NY, USA, 1987. ACM.
V. Goyal, I. Mironov, O. Pandey, and A. Sahai. Accuracy-privacy tradeoffs for two-party differentially private protocols. In Advances in CryptologyCRYPTO 2013, pages 298315. Springer, 2013.
Mangesh Gupte and Mukund Sundararajan. Universally optimal privacy mechanisms for minimax agents. In Proceedings of the twenty-ninth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pages 135146. ACM, 2010.
M. Hardt and A. Roth. Beating randomized response on incoherent matrices. In Proceedings of the forty-fourth annual ACM symposium on Theory of computing, pages 12551268. ACM, 2012.
N. Homer, S. Szelinger, M. Redman, D. Duggan, W. Tembe, J. Muehling, J. V. Pearson, D. A. Stephan, S. F. Nelson, and D. W. Craig. Resolving individuals contributing trace amounts of dna to highly complex mixtures using high-density snp genotyping microarrays. PLoS genetics, 4(8):e1000167, 2008.
P. Kairouz, S. Oh, and P. Viswanath. Extremal mechanisms for local differential privacy. In Advances in neural information processing systems, 2014.
M. Kapralov and K. Talwar. On differentially private low rank approximation. In Proceedings of the Twenty-Fourth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 13951414. SIAM, 2013.
Shiva Prasad Kasiviswanathan, Homin K Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. What can we learn privately? SIAM Journal on Computing, 40(3):793826, 2011.
Joe Kilian. More general completeness theorems for secure two-party computation. In Proceedings of the thirty-second annual ACM symposium on Theory of computing, pages 316324. ACM, 2000.
R. Kunzler, J. Muller-Quade, and D. Raub. Secure computability of functions in the it setting with dishonest majority and applications to long-term security. In Theory of Cryptography, pages 238255. Springer, 2009.
Andrew McGregor, Ilya Mironov, Toniann Pitassi, Omer Reingold, Kunal Talwar, and Salil Vadhan. The limits of two-party differential privacy. In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE Symposium on, pages 8190. IEEE, 2010.
F. McSherry and K. Talwar. Mechanism design via differential privacy. In Foundations of Computer Science, 2007. FOCS07. 48th Annual IEEE Symposium on, pages 94103. IEEE, 2007.
A. Narayanan and V. Shmatikov. Robust de-anonymization of large sparse datasets. In Security and Privacy, 2008. SP 2008. IEEE Symposium on, pages 111125. IEEE, 2008.
Sewoong Oh and Pramod Viswanath. The composition theorem for differential privacy. arXiv preprint arXiv:1311.0776, 2013.
Manoj M Prabhakaran and Vinod M Prabhakaran. On secure multiparty sampling for more than two parties. In Information Theory Workshop (ITW), 2012 IEEE, pages 99103. IEEE, 2012.
Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM review, 52(3):471501, 2010.
L. Sweeney. Simple demographics often identify people uniquely. Health, 671:134, 2000.
S. L. Warner. Randomized response: A survey technique for eliminating evasive answer bias. Journal of the American Statistical Association, 60(309):6369, 1965.
Andrew C Yao. Protocols for secure computations. In 2013 IEEE 54th Annual Symposium on Foundations of Computer Science, pages 160164. IEEE, 1982.  9
J. Ba, V. Mnih, and K. Kavukcuoglu. Multiple object recognition with visual attention. ICLR, 2015.
S. Branson, G. Van Horn, S. Belongie, and P. Perona. Bird species categorization using pose normalized deep convolutional nets. BMVC., 2014.
J. Bruna and S. Mallat. Invariant scattering convolution networks. IEEE PAMI, 35(8):18721886, 2013.
M. Cimpoi, S. Maji, and A. Vedaldi. Deep filter banks for texture recognition and segmentation. In CVPR, 2015.
T. S. Cohen and M. Welling. Transformation properties of learned visual representations. ICLR, 2015.
D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable object detection using deep neural networks. In CVPR, 2014.
B. J. Frey and N. Jojic. Fast, large-scale transformation-invariant clustering. In NIPS, 2001.
R. Gens and P. M. Domingos. Deep symmetry networks. In NIPS, 2014.
R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.
I. J. Goodfellow, Y. Bulatov, J. Ibarz, S. Arnoud, and V. Shet. Multi-digit number recognition from street view imagery using deep convolutional neural networks. arXiv:1312.6082, 2013.
K. Gregor, I. Danihelka, A. Graves, and D. Wierstra. Draw: A recurrent neural network for image generation. ICML, 2015.
G. E. Hinton. A parallel computation that assigns canonical object-based frames of reference. In IJCAI, 1981.
G. E. Hinton, A. Krizhevsky, and S. D. Wang. Transforming auto-encoders. In ICANN. 2011.
G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580, 2012.
S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. ICML, 2015.
M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman. Synthetic data and artificial neural networks for natural scene text recognition. NIPS DLW, 2014.
A. Kanazawa, A. Sharma, and D. Jacobs. Locally scale-invariant convolutional neural networks. In NIPS, 2014.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, 1998.
K. Lenc and A. Vedaldi. Understanding image representations by measuring their equivariance and equivalence. CVPR, 2015.
T. Lin, A. RoyChowdhury, and S. Maji. Bilinear CNN models for fine-grained visual recognition. arXiv:1504.07889, 2015.
Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised feature learning. In NIPS DLW, 2011.
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. arXiv:1409.0575, 2014.
P. Sermanet, A. Frome, and E. Real. Attention for fine-grained categorization. arXiv:1412.7054, 2014.
M. Simon and E. Rodner. Neural activation constellations: Unsupervised part model discovery with convolutional networks. arXiv:1504.08289, 2015.
K. Sohn and H. Lee. Learning invariant representations with local transformations. arXiv:1206.6418, 2012.
M. F. Stollenga, J. Masci, F. Gomez, and J. Schmidhuber. Deep networks with internal selective attention through feedback connections. In NIPS, 2014.
T. Tieleman. Optimizing Neural Networks that Generate Images. PhD thesis, University of Toronto, 2014.
C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 dataset. 2011.
K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio. Show, attend and tell: Neural image caption generation with visual attention. ICML, 2015.
X. Zhang, J. Zou, X. Ming, K. He, and J. Sun. Efficient and accurate approximations of nonlinear convolutional networks. arXiv:1411.4229, 2014.  9
Michael Trusov, Randolph E Bucklin, and Koen Pauwels. Effects of word-of-mouth versus traditional marketing: Findings from an internet social networking site. Journal of marketing, 73(5):90102, 2009.
David Kempe, Jon Kleinberg, and Eva Tardos. Maximizing the spread of influence through a social network. In Proceedings of the 9th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 137146. ACM, 2003.
Wei Chen, Yajun Wang, and Siyu Yang. Efficient influence maximization in social networks. In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 199208. ACM, 2009.
Wei Chen, Chi Wang, and Yajun Wang. Scalable influence maximization for prevalent viral marketing in large-scale social networks. In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 10291038. ACM, 2010.
Manuel Gomez-Rodriguez, David Balduzzi, and Bernhard Scholkopf. Uncovering the temporal dynamics of diffusion networks. In Proceedings of the 28th International Conference on Machine Learning, pages 561568, 2011.
Nan Du, Le Song, Hyenkyun Woo, and Hongyuan Zha. Uncover topic-sensitive information diffusion networks. In Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics, pages 229237, 2013.
Alan G Hawkes and David Oakes. A cluster process representation of a self-exciting process. Journal of Applied Probability, pages 493503, 1974.
Ke Zhou, Hongyuan Zha, and Le Song. Learning triggering kernels for multi-dimensional hawkes processes. In Proceedings of the 30th International Conference on Machine Learning, pages 13011309, 2013.
Remi Lemonnier and Nicolas Vayatis. Nonparametric markovian learning of triggering kernels for mutually exciting and mutually inhibiting multivariate hawkes processes. In Machine Learning and Knowledge Discovery in Databases, pages 161176. Springer, 2014.
Mehrdad Farajtabar, Nan Du, Manuel Gomez-Rodriguez, Isabel Valera, Hongyuan Zha, and Le Song. Shaping social activity by incentivizing users. In Advances in Neural Information Processing Systems, pages 24742482, 2014.
Manuel Gomez-Rodriguez and Bernhard Scholkopf. Influence maximization in continuous time diffusion networks. In Proceedings of the 29th International Conference on Machine Learning, pages 313320, 2012.
Nan Du, Le Song, Manuel Gomez-Rodriguez, and Hongyuan Zha. Scalable influence estimation in continuous-time diffusion networks. In Advances in Neural Information Processing Systems, pages 3147 3155, 2013.
Manuel Gomez-Rodriguez, Le Song, Hadi Daneshmand, and B. Schoelkopf. Estimating diffusion networks: Recovery conditions, sample complexity & soft-thresholding algorithm. Journal of Machine Learning Research, 2015.
Jean Pouget-Abadie and Thibaut Horel. Inferring graphs from cascades: A sparse recovery framework. In Proceedings of the 32nd International Conference on Machine Learning, pages 977986, 2015.
Moez Draief, Ayalvadi Ganesh, and Laurent Massoulie. Thresholds for virus spread on networks. Annals of Applied Probability, 18(2):359378, 2008.
Bela Bollobas, Svante Janson, and Oliver Riordan. The phase transition in inhomogeneous random graphs. Random Structures & Algorithms, 31(1):3122, 2007.
Remi Lemonnier, Kevin Scaman, and Nicolas Vayatis. Tight bounds for influence in diffusion networks and application to bond percolation and epidemiology. In Advances in Neural Information Processing Systems, pages 846854, 2014.
William O Kermack and Anderson G McKendrick. Contributions to the mathematical theory of epidemics. ii. the problem of endemicity. Proceedings of the Royal society of London. Series A, 138(834):55 83, 1932.
Jure Leskovec, Andreas Krause, Carlos Guestrin, Christos Faloutsos, Jeanne VanBriesen, and Natalie Glance. Cost-effective outbreak detection in networks. In Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 420429. ACM, 2007.
Mark Newman. Networks: An Introduction. Oxford University Press, Inc., New York, NY, USA, 2010.
Mathew Penrose. Random geometric graphs, volume 5. Oxford University Press Oxford, 2003.  9
T. Zhang, Class-size independent generalization analsysis of some discriminative multi-category classification, in Advances in Neural Information Processing Systems, pp. 16251632, 2004.
T. Hofmann, L. Cai, and M. Ciaramita, Learning with taxonomies: Classifying documents and words, in NIPS workshop on syntax, semantics, and statistics, 2003.  8
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, Imagenet: A large-scale hierarchical image database, in Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248255, IEEE, 2009.
A. Beygelzimer, J. Langford, Y. Lifshits, G. Sorkin, and A. Strehl, Conditional probability tree estimation analysis and algorithms, in Proceedings of UAI, pp. 5158, AUAI Press, 2009.
S. Bengio, J. Weston, and D. Grangier, Label embedding trees for large multi-class tasks, in Advances in Neural Information Processing Systems, pp. 163171, 2010.
P. Jain and A. Kapoor, Active learning for large multi-class problems, in Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 762769, IEEE, 2009.
O. Dekel and O. Shamir, Multiclass-multilabel classification with more classes than examples, in International Conference on Artificial Intelligence and Statistics, pp. 137144, 2010.
M. R. Gupta, S. Bengio, and J. Weston, Training highly multiclass classifiers, The Journal of Machine Learning Research, vol. 15, no. 1, pp. 14611492, 2014.
T. Zhang, Statistical analysis of some multi-category large margin classification methods, The Journal of Machine Learning Research, vol. 5, pp. 12251251, 2004.
A. Tewari and P. L. Bartlett, On the consistency of multiclass classification methods, The Journal of Machine Learning Research, vol. 8, pp. 10071025, 2007.
T. Glasmachers, Universal consistency of multi-class support vector classification, in Advances in Neural Information Processing Systems, pp. 739747, 2010.
M. Mohri, A. Rostamizadeh, and A. Talwalkar, Foundations of machine learning. MIT press, 2012.
V. Kuznetsov, M. Mohri, and U. Syed, Multi-class deep boosting, in Advances in Neural Information Processing Systems, pp. 25012509, 2014.
V. Koltchinskii and D. Panchenko, Empirical margin distributions and bounding the generalization error of combined classifiers, Annals of Statistics, pp. 150, 2002.
Y. Guermeur, Combining discriminant models with new multi-class svms, Pattern Analysis & Applications, vol. 5, no. 2, pp. 168179, 2002.
L. Oneto, D. Anguita, A. Ghio, and S. Ridella, The impact of unlabeled patterns in rademacher complexity theory for kernel classifiers, in Advances in Neural Information Processing Systems, pp. 585593, 2011.
V. Koltchinskii and D. Panchenko, Rademacher processes and bounding the risk of function learning, in High Dimensional Probability II, pp. 443457, Springer, 2000.
C. Cortes, M. Mohri, and A. Rostamizadeh, Multi-class classification with maximum margin multiple kernel, in ICML-13, pp. 4654, 2013.
M. Kloft, U. Brefeld, S. Sonnenburg, and A. Zien, Lp-norm multiple kernel learning, The Journal of Machine Learning Research, vol. 12, pp. 953997, 2011.
K. Crammer and Y. Singer, On the algorithmic implementation of multiclass kernel-based vector machines, The Journal of Machine Learning Research, vol. 2, pp. 265292, 2002.
P. L. Bartlett and S. Mendelson, Rademacher and gaussian complexities: Risk bounds and structural results, J. Mach. Learn. Res., vol. 3, pp. 463482, 2002.
M. Ledoux and M. Talagrand, Probability in Banach Spaces: isoperimetry and processes, vol. 23. Berlin: Springer, 1991.
S. I. Hill and A. Doucet, A framework for kernel-based multi-category classification., J. Artif. Intell. Res.(JAIR), vol. 30, pp. 525564, 2007.
C. A. Micchelli and M. Pontil, Learning the kernel function via regularization, Journal of Machine Learning Research, pp. 10991125, 2005.
S. S. Keerthi, S. Sundararajan, K.-W. Chang, C.-J. Hsieh, and C.-J. Lin, A sequential dual method for large scale multi-class linear svms, in 14th ACM SIGKDD, pp. 408416, ACM, 2008.
J. D. Rennie and R. Rifkin, Improving multiclass text classification with the support vector machine, tech. rep., AIM-2001-026, MIT, 2001.
K. Lang, Newsweeder: Learning to filter netnews, in Proceedings of the 12th international conference on machine learning, pp. 331339, 1995.
D. D. Lewis, Y. Yang, T. G. Rose, and F. Li, Rcv1: A new benchmark collection for text categorization research, The Journal of Machine Learning Research, vol. 5, pp. 361397, 2004.
P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona, Caltech-UCSD Birds 200, Tech. Rep. CNS-TR-2010-001, California Institute of Technology, 2010.
Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell, Caffe: Convolutional architecture for fast feature embedding, arXiv preprint arXiv:1408.5093, 2014.  9
J. P. Cunningham and B. M Yu, Dimensionality reduction for large-scale neural recordings, Nature neuroscience, vol. 17, no. 71, pp. 15001509, 2014.
L. Paninski, Maximum likelihood estimation of cascade point-process neural encoding models, Network: Computation in Neural Systems, vol. 15, no. 4, pp. 243262, 2004.
W. Truccolo, U. T. Eden, M. R. Fellows, J. P. Donoghue, and E. N. Brown, A point process framework for relating neural spiking activity to spiking history, neural ensemble, and extrinsic covariate effects, Journal of neurophysiology, vol. 93, no. 2, pp. 10741089, 2005.
J. W. Pillow, J. Shlens, L. Paninski, A. Sher, A. M. Litke, E. Chichilnisky, and E. P. Simoncelli, Spatiotemporal correlations and visual signalling in a complete neuronal population, Nature, vol. 454, no. 7207, pp. 995999, 2008.
M. Vidne, Y. Ahmadian, J. Shlens, J. W. Pillow, J. Kulkarni, A. M. Litke, E. Chichilnisky, E. Simoncelli, and L. Paninski, Modeling the impact of common noise inputs on the network activity of retinal ganglion cells, Journal of computational neuroscience, vol. 33, no. 1, pp. 97121, 2012.  8
J. E. Kulkarni and L. Paninski, Common-input models for multiple neural spike-train data, Network: Computation in Neural Systems, vol. 18, no. 4, pp. 375407, 2007.
B. M Yu, J. P. Cunningham, G. Santhanam, S. I. Ryu, K. V. Shenoy, and M. Sahani, Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity, in NIPS, pp. 1881 1888, 2009.
J. H. Macke, L. Buesing, J. P. Cunningham, B. M Yu, K. V. Shenoy, and M. Sahani, Empirical models of spiking in neural populations, in NIPS, pp. 13501358, 2011.
B. Petreska, B. M Yu, J. P. Cunningham, G. Santhanam, S. I. Ryu, K. V. Shenoy, and M. Sahani, Dynamical segmentation of single trials from population neural data, in NIPS, pp. 756764, 2011.
D. Pfau, E. A. Pnevmatikakis, and L. Paninski, Robust learning of low-dimensional dynamics from large neural ensembles, in NIPS, pp. 23912399, 2013.
L. Buesing, T. A. Machado, J. P. Cunningham, and L. Paninski, Clustered factor analysis of multineuronal spike data, in NIPS, pp. 35003508, 2014.
M. M. Churchland, B. M Yu, J. P. Cunningham, L. P. Sugrue, M. R. Cohen, G. S. Corrado, W. T. Newsome, A. M. Clark, P. Hosseini, B. B. Scott, et al., Stimulus onset quenches neural variability: a widespread cortical phenomenon, Nature neuroscience, vol. 13, no. 3, pp. 369378, 2010.
J. P. Cunningham, B. M Yu, K. V. Shenoy, and S. Maneesh, Inferring neural firing rates from spike trains using gaussian processes, in NIPS, pp. 329336, 2007.
R. P. Adams, I. Murray, and D. J. MacKay, Tractable nonparametric bayesian inference in poisson processes with gaussian process intensities, in ICML, pp. 916, ACM, 2009.
S. Koyama, On the spike train variability characterized by variance-to-mean power relationship, Neural computation, 2015.
R. L. Goris, J. A. Movshon, and E. P. Simoncelli, Partitioning neuronal variability, Nature neuroscience, vol. 17, no. 6, pp. 858865, 2014.
J. Scott and J. W. Pillow, Fully bayesian inference for neural models with negative-binomial spiking, in NIPS, pp. 18981906, 2012.
S. W. Linderman, R. Adams, and J. Pillow, Inferring structured connectivity from spike trains under negative-binomial generalized linear models, COSYNE, 2015.
J. del Castillo and M. Perez-Casany, Overdispersed and underdispersed poisson generalizations, Journal of Statistical Planning and Inference, vol. 134, no. 2, pp. 486500, 2005.
M. Emtiyaz Khan, A. Aravkin, M. Friedlander, and M. Seeger, Fast dual variational inference for nonconjugate latent gaussian models, in ICML, pp. 951959, 2013.
C. R. Rao, On discrete distributions arising out of methods of ascertainment, Sankhya: The Indian Journal of Statistics, Series A, pp. 311324, 1965.
D. Lambert, Zero-inflated poisson regression, with an application to defects in manufacturing, Technometrics, vol. 34, no. 1, pp. 114, 1992.
J. Singh, A characterization of positive poisson distribution and its statistical application, SIAM Journal on Applied Mathematics, vol. 34, no. 3, pp. 545548, 1978.
K. F. Sellers and G. Shmueli, A flexible regression model for count data, The Annals of Applied Statistics, pp. 943961, 2010.
C. V. Ananth and D. G. Kleinbaum, Regression models for ordinal responses: a review of methods and applications., International journal of epidemiology, vol. 26, no. 6, pp. 13231333, 1997.
L. Paninski, J. Pillow, and J. Lewi, Statistical models for neural encoding, decoding, and optimal stimulus design, Progress in brain research, vol. 165, pp. 493507, 2007.
S. Boyd and L. Vandenberghe, Convex optimization. Cambridge university press, 2009.
L. Buesing, J. H. Macke, and M. Sahani, Learning stable, regularised latent models of neural population dynamics, Network: Computation in Neural Systems, vol. 23, no. 1-2, pp. 2447, 2012.
L. Buesing, J. H. Macke, and M. Sahani, Estimating state and parameters in state-space models of spike trains, in Advanced State Space Methods for Neural and Clinical Data, Cambridge Univ Press., 2015.
V. Lawhern, W. Wu, N. Hatsopoulos, and L. Paninski, Population decoding of motor cortical activity using a generalized linear model with hidden states, Journal of neuroscience methods, vol. 189, no. 2, pp. 267280, 2010.
M. M. Churchland, J. P. Cunningham, M. T. Kaufman, J. D. Foster, P. Nuyujukian, S. I. Ryu, and K. V. Shenoy, Neural population dynamics during reaching, Nature, vol. 487, no. 7405, pp. 5156, 2012.
M. R. Cohen and A. Kohn, Measuring and interpreting neuronal correlations, Nature neuroscience, vol. 14, no. 7, pp. 811819, 2011.  9
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. CVPR (to appear), 2015.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 2015.
Marco Cuturi and Arnaud Doucet. Fast Computation of Wasserstein Barycenters. ICML, 2014.
Justin Solomon, Raif M Rustamov, Leonidas J Guibas, and Adrian Butscher. Wasserstein Propagation for Semi-Supervised Learning. In ICML, pages 306314, 2014.
Michael H Coen, M Hidayath Ansari, and Nathanael Fillmore. Comparing Clusterings in Space. ICML, pages 231238, 2010.
Lorenzo Rosasco Mauricio A. Alvarez and Neil D. Lawrence. Kernels for vector-valued functions: A review. Foundations and Trends in Machine Learning, 4(3):195266, 2011.
Leonid I Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based noise removal algorithms. Physica D: Nonlinear Phenomena, 60(1):259268, 1992.
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Semantic image segmentation with deep convolutional nets and fully connected crfs. In ICLR, 2015.
Marco Cuturi, Gabriel Peyre, and Antoine Rolet. A Smoothed Dual Approach for Variational Wasserstein Problems. arXiv.org, March 2015.
Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The earth movers distance as a metric for image retrieval. IJCV, 40(2):99121, 2000.
Kristen Grauman and Trevor Darrell. Fast contour matching using approximate earth movers distance. In CVPR, 2004.
S Shirdhonkar and D W Jacobs. Approximate earth movers distance in linear time. In CVPR, 2008.
Herbert Edelsbrunner and Dmitriy Morozov. Persistent homology: Theory and practice. In Proceedings of the European Congress of Mathematics, 2012.
Federico Bassetti, Antonella Bodini, and Eugenio Regazzini. On minimum kantorovich distance estimators. Stat. Probab. Lett., 76(12):12981302, 1 July 2006.
Cedric Villani. Optimal Transport: Old and New. Springer Berlin Heidelberg, 2008.
Vladimir I Bogachev and Aleksandr V Kolesnikov. The Monge-Kantorovich problem: achievements, connections, and perspectives. Russian Math. Surveys, 67(5):785, 10 2012.
Dimitris Bertsimas, John N. Tsitsiklis, and John Tsitsiklis. Introduction to Linear Optimization. Athena Scientific, Boston, third printing edition, 1997.
Marco Cuturi. Sinkhorn Distances: Lightspeed Computation of Optimal Transport. NIPS, 2013.
Philip A Knight and Daniel Ruiz. A fast algorithm for matrix balancing. IMA Journal of Numerical Analysis, 33(3):drs0191047, October 2012.
Lenaic Chizat, Gabriel Peyre, Bernhard Schmitzer, and Francois-Xavier Vialard. Unbalanced Optimal Transport: Geometry and Kantorovich Formulation. arXiv.org, August 2015.
Ofir Pele and Michael Werman. Fast and robust Earth Movers Distances. ICCV, pages 460467, 2009.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. JMLR, 3:463482, March 2003.
Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. The new data and new challenges in multimedia research. arXiv preprint arXiv:1503.01817, 2015.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In NIPS, 2013.
A. Vedaldi and K. Lenc. abs/1412.4564, 2014.  MatConvNet  Convolutional Neural Networks for MATLAB.  CoRR,
M. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes. Classics in Mathematics. Springer Berlin Heidelberg, 2011.
Clark R. Givens and Rae Michael Shortt. A class of wasserstein metrics for probability distributions. Michigan Math. J., 31(2):231240, 1984.  9
T. Blumensath and M. Davies. Iterative hard thresholding for compressed sensing. Applied and Computational Harmonic Analysis, 27:265274, 2009.
P. Boufounos and R. Baraniuk. 1-bit compressive sensing. In Information Science and Systems, 2008.
E. Candes and T. Tao. The Dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35:23132351, 2007.
S. Chen and A. Banerjee. One-bit Compressed Sensing with the k-Support Norm. In AISTATS, 2015.
D. Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52:12891306, 2006.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:18711874, 2008.
C. Genovese, J. Jin, L. Wasserman, and Z. Yao. A Comparison of the Lasso and Marginal Regression. Journal of Machine Learning Research, 13:21072143, 2012.
S. Gopi, P. Netrapalli, P. Jain, and A. Nori. One-bit Compressed Sensing: Provable Support and Vector Recovery. In ICML, 2013.
L. Jacques, K. Degraux, and C. De Vleeschouwer. Quantized iterative hard thresholding: Bridging 1-bit and high-resolution quantized compressed sensing. arXiv:1305.1786, 2013.
L. Jacques, D. Hammond, and M. Fadili. Dequantizing compressed sensing: When oversampling and non-gaussian constraints combine. IEEE Transactions on Information Theory, 57:559571, 2011.
L. Jacques, J. Laska, P. Boufounos, and R. Baraniuk. Robust 1-bit Compressive Sensing via Binary Stable Embeddings of Sparse Vectors. IEEE Transactions on Information Theory, 59:20822102, 2013.
J. Kieffer. Uniqueness of locally optimal quantizer for log-concave density and convex error weighting function. IEEE Transactions on Information Theory, 29:4247, 1983.
J. Laska and R. Baraniuk. Regime change: Bit-depth versus measurement-rate in compressive sensing. arXiv:1110.3450, 2011.
J. Laska, P. Boufounos, M. Davenport, and R. Baraniuk. Democracy in action: Quantization, saturation, and compressive sensing. Applied and Computational Harmonic Analysis, 31:429443, 2011.
P. Li. Binary and Multi-Bit Coding for Stable Random Projections. arXiv:1503.06876, 2015.
P. Li. One scan 1-bit compressed sensing. Technical report, arXiv:1503.02346, 2015.
P. Li, C.-H. Zhang, and T. Zhang. Compressed counting meets compressed sensing. In COLT, 2014.
J. Liu and S. Wright. Robust dequantized compressive sensing. Applied and Computational Harmonic Analysis, 37:325346, 2014.
S. Lloyd. Least Squares Quantization in PCM. IEEE Transactions on Information Theory, 28:129137, 1982.
J. Max. Quantizing for Minimum Distortion. IRE Transactions on Information Theory, 6:712, 1960.
D. Needell and J. Tropp. CoSaMP: Iterative signal recovery from incomplete and inaccurate samples. Applied and Computational Harmonic Analysis, 26:301321, 2008.
Y. Plan and R. Vershynin. One-bit compressed sensing by linear programming. Communications on Pure and Applied Mathematics, 66:12751297, 2013.
Y. Plan and R. Vershynin. Robust 1-bit compressed sensing and sparse logistic regression: a convex programming approach. IEEE Transactions on Information Theory, 59:482494, 2013.
R. Zhu and Q. Gu. Towards a Lower Sample Complexity for Robust One-bit Compressed Sensing. In ICML, 2015.
R. Vershynin. In: Compressed Sensing: Theory and Applications, chapter Introduction to the nonasymptotic analysis of random matrices. Cambridge University Press, 2012.
M. Wainwright. Sharp thresholds for noisy and high-dimensional recovery of sparsity using 1 constrained quadratic programming (Lasso). IEEE Transactions on Information Theory, 55:21832202, 2009.
C.-H. Zhang and T. Zhang. A general theory of concave regularization for high-dimensional sparse estimation problems. Statistical Science, 27:576593, 2013.
L. Zhang, J. Yi, and R. Jin. Efficient algorithms for robust one-bit compressive sensing. In ICML, 2014.
T. Zhang. Adaptive Forward-Backward Greedy Algorithm for Learning Sparse Representations. IEEE Transactions on Information Theory, 57:46894708, 2011.  9
Shun-ichi Amari. Natural gradient works efficiently in learning. Neural Computation, 1998.
Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In NIPS. 2014.
Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Oper. Res. Lett., 2003.
P. L. Combettes and J.-C. Pesquet. Proximal Splitting Methods in Signal Processing. ArXiv e-prints, December 2009.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. In JMLR. 2011.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, May 2010.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. ICML, 2015.
Roger Grosse James Martens. Optimizing neural networks with kronecker-factored approximate curvature. In ICML, June 2015.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Masters thesis, University of Toronto, 2009.
Yann LeCun, Leon Bottou, Genevieve B. Orr, and Klaus-Robert Muller. Efficient backprop. In Neural Networks, Tricks of the Trade, Lecture Notes in Computer Science LNCS 1524. Springer Verlag, 1998.
Yann Lecun, Lon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. In Proceedings of the IEEE, pages 22782324, 1998.
James Martens. Deep learning via Hessian-free optimization. In ICML, June 2010.
K.-R. Muller and G. Montavon. Deep boltzmann machines and the centering trick. In K.-R. Muller, G. Montavon, and G. B. Orr, editors, Neural Networks: Tricks of the Trade. Springer, 2013.
Yann Ollivier. Riemannian metrics for neural networks. arXiv, abs/1303.0818, 2013.
Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. In ICLR, 2014.
Daniel Povey, Xiaohui Zhang, and Sanjeev Khudanpur. Parallel training of deep neural networks with natural gradient and parameter averaging. ICLR workshop, 2015.
T. Raiko, H. Valpola, and Y. LeCun. Deep learning made easier by linear transformations in perceptrons. In AISTATS, 2012.
G. Raskutti and S. Mukherjee. The Information Geometry of Mirror Descent. arXiv, October 2013.
Ruslan Salakhutdinov Roger B. Grosse. Scaling up natural gradient by sparsely factorizing the inverse fisher matrix. In ICML, June 2015.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 2015.
Nicol N. Schraudolph. Accelerated gradient descent by factor-centering decomposition. Technical Report IDSIA-33-98, Istituto Dalle Molle di Studi sullIntelligenza Artificiale, 1998.
K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, 2015.
Jascha Sohl-Dickstein. The natural gradient by analogy to signal whitening, and recipes and tricks for its use. arXiv, 2012.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 2014.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. arXiv, 2014.
Philip S Thomas, William C Dabney, Stephen Giguere, and Sridhar Mahadevan. Projected natural actorcritic. In Advances in Neural Information Processing Systems 26. 2013.
Tijmen Tieleman and Geoffrey Hinton. Rmsprop: Divide the gradient by a running average of its recent magnitude. coursera: Neural networks for machine learning. 2012.
Tommi Vatanen, Tapani Raiko, Harri Valpola, and Yann LeCun. Pushing stochastic gradient towards second-order methods  backpropagation learning with transformations in nonlinearities. ICONIP, 2013.  9
Ahn, S., Korattikara, A., Liu, N., Rajan, S., and Welling, M. (2015). Large scale distributed Bayesian matrix factorization using stochastic gradient MCMC. In KDD.
Ahn, S., Shahbaba, B., and Welling, M. (2014). Distributed stochastic gradient MCMC. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 10441052.
Beaumont, M. A., Cornuet, J.-M., Marin, J.-M., and Robert, C. P. (2009). Adaptive approximate Bayesian computation. Biometrika, 96(4):983990.
Blum, M. G. and Francois, O. (2010). Non-linear regression models for approximate Bayesian computation. Statistics and Computing, 20(1):6373.
Bonassi, F. V. and West, M. (2015). Sequential Monte Carlo with adaptive weights for approximate Bayesian computation. Bayesian Analysis, 10(1).
Del Moral, P., Doucet, A., and Jasra, A. (2006). Sequential Monte Carlo samplers. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(3):411436.
Drovandi, C. C., Pettitt, A. N., and Faddy, M. J. (2011). Approximate Bayesian computation using indirect inference. Journal of the Royal Statistical Society: Series C (Applied Statistics), 60(3):317337.
Fearnhead, P. and Prangle, D. (2012). Constructing summary statistics for approximate Bayesian computation: semi-automatic approximate Bayesian computation. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74(3):419474.
Forneron, J.-J. and Ng, S. (2015a). The ABC of simulation estimation with auxiliary statistics. arXiv preprint arXiv:1501.01265v2.
Forneron, J.-J. and Ng, S. (2015b). A likelihood-free reverse sampler of the posterior distribution. arXiv preprint arXiv:1506.04017v1.
Gourieroux, C., Monfort, A., and Renault, E. (1993). Indirect inference. Journal of applied econometrics, 8(S1):S85S118.
Gutmann, M. U. and Corander, J. (2015). Bayesian optimization for likelihood-free inference of simulator-based statistical models. Journal of Machine Learning Research, preprint arXiv:1501.03291. In press.
Jones, D. R., Schonlau, M., and Welch, W. J. (1998). Efficient global optimization of expensive black-box functions. Journal of Global optimization, 13(4):455492.
Maclaurin, D. and Duvenaud, D. (2015). Autograd. github.com/HIPS/autograd.
Meeds, E., Leenders, R., and Welling, M. (2015). Hamiltonian ABC. Uncertainty in AI, 31.
Neal, P. (2012). Efficient likelihood-free Bayesian computation for household epidemics. Statistical Computing, 22:12391256.
Paige, B., Wood, F., Doucet, A., and Teh, Y. W. (2014). Asynchronous anytime Sequential Monte Carlo. In Advances in Neural Information Processing Systems, pages 34103418.
Shestopaloff, A. Y. and Neal, R. M. (2013). On Bayesian inference for the M/G/1 queue with efficient MCMC sampling. Technical Report, Dept. of Statistics, University of Toronto.
Sisson, S., Fan, Y., and Tanaka, M. M. (2007). Sequential Monte Carlo without likelihoods. Proceedings of the National Academy of Sciences, 104(6).
Sisson, S., Fan, Y., and Tanaka, M. M. (2009). Sequential Monte Carlo without likelihoods: Errata. Proceedings of the National Academy of Sciences, 106(16).
Snoek, J., Larochelle, H., and Adams, R. P. (2012). Practical Bayesian optimization of machine learning algorithms. Advances in Neural Information Processing Systems 25.
Spall, J. C. (1992). Multivariate stochastic approximation using a simultaneous perturbation gradient approximation. Automatic Control, IEEE Transactions on, 37(3):332341.  9
R. Glowinski and A. Marroco. Sur lapproximation, par elements finis dordre un, et la resolution, par penalisation-dualite dune classe de problemes de Dirichlet non lineaires. Rev. Francaise dAutomat. Inf. Recherche Operationelle, 9(2):4176, 1975.
Roland Glowinski and Patrick Le Tallec. Augmented Lagrangian and Operator-Splitting Methods in Nonlinear Mechanics. Society for Industrial and Applied Mathematics, Philadephia, PA, 1989.
Tom Goldstein and Stanley Osher. The Split Bregman method for `1 regularized problems. SIAM J. Img. Sci., 2(2):323343, April 2009.
Ernie Esser, Xiaoqun Zhang, and Tony F. Chan. A general framework for a class of first order primal-dual algorithms for convex optimization in imaging science. SIAM Journal on Imaging Sciences, 3(4):1015 1046, 2010.
Antonin Chambolle and Thomas Pock. A first-order primal-dual algorithm for convex problems with applications to imaging. Convergence, 40(1):149, 2010.
Yuyuan Ouyang, Yunmei Chen, Guanghui Lan, and Eduardo Pasiliao Jr. An accelerated linearized alternating direction method of multipliers. arXiv preprint arXiv:1401.6607, 2014.
B. He, H. Yang, and S.L. Wang. Alternating direction method with self-adaptive penalty parameters for monotone variational inequalities. Journal of Optimization Theory and Applications, 106(2):337356, 2000.
L.D. Popov. A modification of the arrow-hurwicz method for search of saddle points. Mathematical notes of the Academy of Sciences of the USSR, 28:845848, 1980.
Mingqiang Zhu and Tony Chan. An efficient primal-dual hybrid gradient algorithm for total variation image restoration. UCLA CAM technical report, 08-34, 2008.
T. Pock, D. Cremers, H. Bischof, and A. Chambolle. An algorithm for minimizing the mumford-shah functional. In Computer Vision, 2009 IEEE 12th International Conference on, pages 11331140, 2009.
Bingsheng He and Xiaoming Yuan. Convergence analysis of primal-dual algorithms for a saddle-point problem: From contraction perspective. SIAM J. Img. Sci., 5(1):119149, January 2012.
Laurent Condat. A primal-dual splitting method for convex optimization involving lipschitzian, proximable and linear composite terms. Journal of Optimization Theory and Applications, 158(2):460479, 2013.
Silvia Bonettini and Valeria Ruggiero. On the convergence of primaldual hybrid gradient algorithms for total variation image restoration. Journal of Mathematical Imaging and Vision, 44(3):236253, 2012.
S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers. Foundations and Trends in Machine Learning, 2010.
A. Belloni, Victor Chernozhukov, and L. Wang. Square-root lasso: pivotal recovery of sparse signals via conic programming. Biometrika, 98(4):791806, 2011.
Tingni Sun and Cun-Hui Zhang. Scaled sparse linear regression. Biometrika, 99(4):879898, 2012.
L Rudin, S Osher, and E Fatemi. Nonlinear total variation based noise removal algorithms. Physica. D., 60:259268, 1992.
Tom Goldstein, Lina Xu, Kevin Kelly, and Richard Baraniuk. The STONE transform: Multi-resolution image enhancement and real-time compressive video. Preprint available at Arxiv.org (arXiv:1311.34056), 2013.
M. Lustig, D. Donoho, and J. Pauly. Sparse MRI: The application of compressed sensing for rapid MR imaging. Magnetic Resonance in Medicine, 58:11821195, 2007.
Xiaoqun Zhang and J. Froment. Total variation based fourier reconstruction and regularization for computer tomography. In Nuclear Science Symposium Conference Record, 2005 IEEE, volume 4, pages 23322336, Oct 2005.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B, 58:267288, 1994.  9
A. Agarwal, A. Anandkumar, P. Jain, and P. Netrapalli. Learning sparsely used overcomplete dictionaries via alternating minimization. In Proceedings of The 27th Conference on Learning Theory (COLT), 2013.
A. Anandkumar, D. Hsu, A. Javanmard, and S. Kakade. Learning latent bayesian networks and topic models under expansion constraints. In Proceedings of the 30th International Conference on Machine Learning (ICML), 2013. 3  We stress we want to analyze whether variational inference will work or not. Handling common words algorithmically is easy: they can be detected and filtered out initially. Then we can perform the variational inference updates over the rest of the words only. This is in fact often done in practice. 4 See supplementary material.  8
A. Anandkumar, S. Kakade, D. Foster, Y. Liu, and D. Hsu. Two svds suffice: Spectral decompositions for probabilistic topic modeling and latent dirichlet allocation. Technical report, 2012.
S. Arora, R. Ge, Y. Halpern, D. Mimno, A. Moitra, D. Sontag, Y. Wu, and M. Zhu. A practical algorithm for topic modeling with provable guarantees. In Proceedings of the 30th International Conference on Machine Learning (ICML), 2013.
S. Arora, R. Ge, R. Kanna, and A. Moitra. Computing a nonnegative matrix factorization provably. In Proceedings of the forty-fourth annual ACM symposium on Theory of Computing, pages 145162. ACM, 2012.
S. Arora, R. Ge, T. Ma, and A. Moitra. Simple, efficient, and neural algorithms for sparse coding. In Proceedings of The 28th Conference on Learning Theory (COLT), 2015.
S. Arora, R. Ge, and A. Moitra. Learning topic models  going beyond svd. In Proceedings of the 53rd Annual IEEE Symposium on Foundations of Computer Science (FOCS), 2012.
S. Arora, R. Ge, and A. Moitra. New algorithms for learning incoherent and overcomplete dictionaries. In Proceedings of The 27th Conference on Learning Theory (COLT), 2014.
S. Balakrishnan, M.J. Wainwright, and B. Yu. Statistical guarantees for the em algorithm: From population to sample-based analysis. arXiv preprint arXiv:1408.2156, 2014.
T. Bansal, C. Bhattacharyya, and R. Kannan. A provable svd-based algorithm for learning topics in dominant admixture corpus. In Advances in Neural Information Processing Systems (NIPS), 2014.
D. Blei and J.D. Lafferty. Topic models. Text mining: classification, clustering, and applications, 10:71, 2009.
D. Blei, A. Ng, , and M. Jordan. Latent dirichlet allocation. Journal of Machine Learning Research, 3:9931022, 2003.
S. Dasgupta and L. Schulman. A two-round variant of em for gaussian mixtures. In Proceedings of Uncertainty in Artificial Intelligence (UAI), 2000.
S. Dasgupta and L. Schulman. A probabilistic analysis of em for mixtures of separated, spherical gaussians. Journal of Machine Learning Research, 8:203226, 2007.
A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society, Series B, 39:138, 1977.
W. Ding, M.H. Rohban, P. Ishwar, and V. Saligrama. Topic discovery through data dependent and random projections. arXiv preprint arXiv:1303.3664, 2013.
W. Ding, M.H. Rohban, P. Ishwar, and V. Saligrama. Efficient distributed topic modeling with provable guarantees. In Proceedings ot the 17th International Conference on Artificial Intelligence and Statistics, pages 167175, 2014.
M. Hoffman, D. Blei, J. Paisley, and C. Wan. Stochastic variational inference. Journal of Machine Learning Research, 14:13031347, 2013.
M. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. An introduction to variational methods for graphical models. Machine learning, 37(2):183233, 1999.
A. Kumar and R. Kannan. Clustering with spectral norm and the k-means algorithm. In Proceedings of Foundations of Computer Science (FOCS), 2010.
D. Lee and S. Seung. Algorithms for non-negative matrix factorization. In Advances in Neural Information Processing Systems (NIPS), 2000.
P. Netrapalli, P. Jain, and S. Sanghavi. Phase retrieval using alternating minimization. In Advances in Neural Information Processing Systems (NIPS), 2013.
D. Sontag and D. Roy. Complexity of inference in latent dirichlet allocation. In Advances in Neural Information Processing Systems (NIPS), 2000.
R. Sundberg. Maximum likelihood from incomplete data via the em algorithm. Scandinavian Journal of Statistics, 1:4958, 1974.
M. Telgarsky. Dirichlet draws are sparse with high probability. Manuscript, 2013.  9
Jacob Abernethy, Francis Bach, Theodoros Evgeniou, and Jean-Philippe Vert. Low-rank matrix factorization with attributes. arXiv preprint cs/0611124, 2006.
Francis Bach, Julien Mairal, and Jean Ponce. Convex sparse matrix factorizations. CoRR, abs/0812.1869, 2008.
Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering. In NIPS, volume 14, pages 585591, 2001.
Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural computation, 15(6):13731396, 2003.
Deng Cai, Xiaofei He, Jiawei Han, and Thomas S Huang. Graph regularized nonnegative matrix factorization for data representation. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 33(8): 15481560, 2011.
Jian-Feng Cai, Emmanuel J Candes, and Zuowei Shen. A singular value thresholding algorithm for matrix completion. SIAM Journal on Optimization, 20(4):19561982, 2010.
Venkat Chandrasekaran, Benjamin Recht, Pablo A Parrilo, and Alan S Willsky. The convex geometry of linear inverse problems. Foundations of Computational Mathematics, 12(6):805849, 2012.
Gideon Dror, Noam Koenigstein, Yehuda Koren, and Markus Weimer. The yahoo! music dataset and kdd-cup11. In KDD Cup, pages 818, 2012.
Benjamin Haeffele, Eric Young, and Rene Vidal. Structured low-rank matrix factorization: Optimality, algorithm, and applications to image processing. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 20072015, 2014.
Prateek Jain and Inderjit S Dhillon. Provable inductive matrix completion. arXiv preprint arXiv:1306.0626, 2013.
Mohsen Jamali and Martin Ester. A matrix factorization technique with trust propagation for recommendation in social networks. In Proceedings of the Fourth ACM Conference on Recommender Systems, RecSys 10, pages 135142, 2010.
Vassilis Kalofolias, Xavier Bresson, Michael Bronstein, and Pierre Vandergheynst. Matrix completion on graphs. (EPFL-CONF-203064), 2014.
Wu-Jun Li and Dit-Yan Yeung. Relation regularized matrix factorization. In 21st International Joint Conference on Artificial Intelligence, 2009.
Hao Ma, Dengyong Zhou, Chao Liu, Michael R. Lyu, and Irwin King. Recommender systems with social regularization. In Proceedings of the fourth ACM international conference on Web search and data mining, WSDM 11, pages 287296, Hong Kong, China, 2011.
Paolo Massa and Paolo Avesani. Trust-aware bootstrapping of recommender systems. ECAI Workshop on Recommender Systems, pages 2933, 2006.
Sahand Negahban and Martin J Wainwright. Restricted strong convexity and weighted matrix completion: Optimal bounds with noise. The Journal of Machine Learning Research, 13(1):16651697, 2012.
Sahand N Negahban, Pradeep Ravikumar, Martin J Wainwright, and Bin Yu. A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers. Statistical Science, 27(4): 538557, 2012.
Nikhil Rao, Parikshit Shah, and Stephen Wright. Conditional gradient with enhancement and truncation for atomic-norm regularization. NIPS workshop on Greedy Algorithms, 2013.
Benjamin Recht. A simpler approach to matrix completion. The Journal of Machine Learning Research, 12:34133430, 2011.
Alexander J Smola and Risi Kondor. Kernels and regularization on graphs. In Learning theory and kernel machines, pages 144158. Springer, 2003.
Nathan Srebro and Ruslan R Salakhutdinov. Collaborative filtering in a non-uniform world: Learning with the weighted trace norm. In Advances in Neural Information Processing Systems, pages 20562064, 2010.
Ambuj Tewari, Pradeep K Ravikumar, and Inderjit S Dhillon. Greedy algorithms for structurally constrained high dimensional problems. In Advances in Neural Information Processing Systems, pages 882 890, 2011.
Roman Vershynin. A note on sums of independent random matrices after ahlswede-winter. Lecture notes, 2009.
Miao Xu, Rong Jin, and Zhi-Hua Zhou. Speedup matrix completion with side information: Application to multi-label learning. In Advances in Neural Information Processing Systems, pages 23012309, 2013.
Yangyang. Xu and Wotao Yin. A block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor factorization and completion. SIAM Journal on Imaging Sciences, 6(3):17581789, 2013.
Zhou Zhao, Lijun Zhang, Xiaofei He, and Wilfred Ng. Expert finding for question answering via graph regularized matrix completion. Knowledge and Data Engineering, IEEE Transactions on, PP(99), 2014.
Tinghui Zhou, Hanhuai Shan, Arindam Banerjee, and Guillermo Sapiro. Kernelized probabilistic matrix factorization: Exploiting graphs and side information. In SDM, volume 12, pages 403414. SIAM, 2012.  9
Herbert Robbins. Some aspects of the sequential design of experiments. In Herbert Robbins Selected Papers, pages 169177. Springer, 1985.
Sebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1222, 2012.
Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, learning, and games, volume 1. Cambridge University Press Cambridge, 2006.
Nicolo Cesa-Bianchi and Gabor Lugosi. Combinatorial bandits. Journal of Computer and System Sciences, 78(5):14041422, 2012.
Aurelien Garivier and Olivier Cappe. The KL-UCB algorithm for bounded stochastic bandits and beyond. In Proc. of COLT, 2011.
Venkatachalam Anantharam, Pravin Varaiya, and Jean Walrand. Asymptotically efficient allocation rules for the multiarmed bandit problem with multiple plays-part i: iid rewards. Automatic Control, IEEE Transactions on, 32(11):968976, 1987.
Branislav Kveton, Zheng Wen, Azin Ashkan, Hoda Eydgahi, and Brian Eriksson. Matroid bandits: Fast combinatorial optimization with learning. In Proc. of UAI, 2014.
Yi Gai, Bhaskar Krishnamachari, and Rahul Jain. Learning multiuser channel allocations in cognitive radio networks: A combinatorial multi-armed bandit formulation. In Proc. of IEEE DySpan, 2010.
Yi Gai, Bhaskar Krishnamachari, and Rahul Jain. Combinatorial network optimization with unknown variables: Multi-armed bandits with linear rewards and individual observations. IEEE/ACM Trans. on Networking, 20(5):14661478, 2012.
Wei Chen, Yajun Wang, and Yang Yuan. Combinatorial multi-armed bandit: General framework and applications. In Proc. of ICML, 2013.
Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesvari. Tight regret bounds for stochastic combinatorial semi-bandits. In Proc. of AISTATS, 2015.
Zheng Wen, Azin Ashkan, Hoda Eydgahi, and Branislav Kveton. Efficient learning in large-scale combinatorial semi-bandits. In Proc. of ICML, 2015.
Jean-Yves Audibert, Sebastien Bubeck, and Gabor Lugosi. Regret in online combinatorial optimization. Mathematics of Operations Research, 39(1):3145, 2013.
Andras Gyorgy, Tamas Linder, Gabor Lugosi, and Gyorgy Ottucsak. The on-line shortest path problem under partial monitoring. Journal of Machine Learning Research, 8(10), 2007.
Satyen Kale, Lev Reyzin, and Robert Schapire. Non-stochastic bandit slate problems. Advances in Neural Information Processing Systems, pages 10541062, 2010.
Nir Ailon, Kohei Hatano, and Eiji Takimoto. Bandit online optimization over the permutahedron. In Algorithmic Learning Theory, pages 215229. Springer, 2014.
Gergely Neu. First-order regret bounds for combinatorial semi-bandits. In Proc. of COLT, 2015.
Sebastien Bubeck, Nicolo Cesa-Bianchi, and Sham M. Kakade. Towards minimax policies for online linear optimization with bandit feedback. Proc. of COLT, 2012.
Todd L. Graves and Tze Leung Lai. Asymptotically efficient adaptive choice of control laws in controlled markov chains. SIAM J. Control and Optimization, 35(3):715743, 1997.
Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics, 6(1):422, 1985.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite time analysis of the multiarmed bandit problem. Machine Learning, 47(2-3):235256, 2002.
Stefan Magureanu, Richard Combes, and Alexandre Proutiere. Lipschitz bandits: Regret lower bounds and optimal algorithms. Proc. of COLT, 2014.
I. Csiszar and P.C. Shields. Information theory and statistics: A tutorial. Now Publishers Inc, 2004.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
H. D. Sherali. A constructive proof of the representation theorem for polyhedral sets based on fundamental definitions. American Journal of Mathematical and Management Sciences, 7(3-4):253270, 1987.
David P. Helmbold and Manfred K. Warmuth. Learning permutations with exponential weights. Journal of Machine Learning Research, 10:17051736, 2009.  9
Barber, D. and Agakov, F. The IM algorithm: a variational approach to information maximization. In NIPS, volume 16, pp. 201, 2004.
Brunel, N. and Nadal, J. Mutual information, Fisher information, and population coding. Neural Computation, 10(7):17311757, 1998.
Buhmann, J. M., Chehreghani, M. H., Frank, M., and Streich, A. P. Information theoretic model selection for pattern analysis. Workshop on Unsupervised and Transfer Learning, 2012.
Cover, T. M. and Thomas, J. A. Elements of information theory. John Wiley & Sons, 1991.
Duchi, J., Hazan, E., and Singer, Y. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:21212159, 2011.
Gao, S., Steeg, G. V., and Galstyan, A. Efficient estimation of mutual information for strongly dependent variables. arXiv:1411.2003, 2014.
Gretton, A., Herbrich, R., and Smola, A. J. The kernel mutual information. In ICASP, volume 4, pp. IV880, 2003.
Itti, L. and Baldi, P. F. Bayesian surprise attracts human attention. In NIPS, pp. 547554, 2005.
Jaakkola, T. S. and Jordan, M. I. Improving the mean field approximation via the use of mixture distributions. In Learning in graphical models, pp. 163173. 1998.
Jung, T., Polani, D., and Stone, P. Empowerment for continuous agent-environment systems. Adaptive Behavior, 19(1):1639, 2011.
Klyubin, A. S., Polani, D., and Nehaniv, C. L. Empowerment: A universal agent-centric measure of control. In IEEE Congress on Evolutionary Computation, pp. 128135, 2005.
Koutnk, J., Schmidhuber, J., and Gomez, F. Evolving deep unsupervised convolutional networks for vision-based reinforcement learning. In GECCO, pp. 541548, 2014.
LeCun, Y. and Bengio, Y. Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks, 3361:310, 1995.
Little, D. Y. and Sommer, F. T. Learning and exploration in action-perception loops. Frontiers in neural circuits, 7, 2013.
Mnih, V., Kavukcuoglu, K., and Silver, D., et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529533, 2015.
Nelson, J. D. Finding useful questions: on Bayesian diagnosticity, probability, impact, and information gain. Psychological review, 112(4):979, 2005.
Ng, Andrew Y, Harada, Daishi, and Russell, Stuart. Policy invariance under reward transformations: Theory and application to reward shaping. In ICML, 1999.
Oudeyer, P. and Kaplan, F. How can we define intrinsic motivation? In International conference on epigenetic robotics, 2008.
Rubin, J., Shamir, O., and Tishby, N. Trading value and information in MDPs. In Decision Making with Imperfect Decision Makers, pp. 5774. 2012.
Salge, C., Glackin, C., and Polani, D. Changing the environment based on empowerment as intrinsic motivation. Entropy, 16(5):27892819, 2014.
Salge, C., Glackin, C., and Polani, D. Empowermentan introduction. In Guided SelfOrganization: Inception, pp. 67114. 2014.
Schmidhuber, J. Formal theory of creativity, fun, and intrinsic motivation (19902010). IEEE Trans. Autonomous Mental Development, 2(3):230247, 2010.
Singh, S. P., Barto, A. G., and Chentanez, N. Intrinsically motivated reinforcement learning. In NIPS, 2005.
Still, S. and Precup, D. An information-theoretic approach to curiosity-driven reinforcement learning. Theory in Biosciences, 131(3):139148, 2012.
Sutton, R. S. and Barto, A. G. Introduction to reinforcement learning. MIT Press, 1998.
Tishby, N., Pereira, F. C., and Bialek, W. The information bottleneck method. In Allerton Conference on Communication, Control, and Computing, 1999.
Todorov, E., Erez, T., and Tassa, Y. Mujoco: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), pp. 50265033, 2012.
Wissner-Gross, A. D. and Freer, C. E. Causal entropic forces. Phys. Rev. Let., 110(16), 2013.
Yeung, R. W. The Blahut-Arimoto algorithms. In Information Theory and Network Coding, pp. 211228. 2008.  9
K. M. Borgwardt and H.-P. Kriegel. Shortest-path kernels on graphs. In ICML, pages 7481, 2005.
K. M. Borgwardt, C. S. Ong, S. Schonauer, S. V. N. Vishwanathan, A. J. Smola, and H.-P. Kriegel. Protein function prediction via graph kernels. In ISMB, Detroit, USA, 2005.
S. F. Chen and J. Goodman. An empirical study of smoothing techniques for language modeling. In ACL, pages 310318, 1996.
D. Croce, A. Moschitti, and R. Basili. Structured lexical similarity via convolution kernels on dependency trees. In Proceedings of EMNLP, pages 10341046. Association for Computational Linguistics, 2011.
A. K. Debnath, R. L. Lopez de Compadre, G. Debnath, A. J. Shusterman, and C. Hansch. Structureactivity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. J. Med. Chem, 34:786797, 1991.
A. Feragen, N. Kasenburg, J. Petersen, M. de Bruijne, and K. Borgwardt. Scalable kernels for graphs with continuous attributes. In NIPS, pages 216224, 2013.
T. Gartner, P. Flach, and S. Wrobel. On graph kernels: Hardness results and efficient alternatives. In COLT, pages 129143, 2003.
S. Goldwater, T. Griffiths, and M. Johnson. Interpolating between types and tokens by estimating powerlaw generators. NIPS, 2006.
S. Goldwater, T. L. Griffiths, and M. Johnson. Producing power-law distributions and damping word frequencies with two-stage language models. JMLR, 12:23352382, 2011.
D. Haussler. Convolution kernels on discrete structures. Technical Report UCS-CRL-99-10, UC Santa Cruz, 1999.
J. Kandola, T. Graepel, and J. Shawe-Taylor. Reducing kernel matrix diagonal dominance using semidefinite programming. In COLT, volume 2777 of Lecture Notes in Computer Science, pages 288302, Washington, DC, 2003.
R. Kneser and H. Ney. Improved backing-off for M-gram language modeling. In ICASSP, 1995.
B. D. McKay. Nauty users guide (version 2.4). Australian National University, 2007.
M. Neumann, R. Garnett, P. Moreno, N. Patricia, and K. Kersting. Propagation kernels for partially labeled graphs. In ICML2012 Workshop on Mining and Learning with Graphs, Edinburgh, UK, 2012.
H. Ney, U. Essen, and R. Kneser. On structuring probabilistic dependences in stochastic language modeling. In Computer Speech and Language, pages 138, 1994.
J. Pitman and M. Yor. The two-parameter poisson-dirichlet distribution derived from a stable subordinator. Annals of Probability, 25(2):855900, 1997.
N. Przulj. Biological network comparison using graphlet degree distribution. In ECCB, 2006.
J. Ramon and T. Gartner. Expressivity versus efficiency of graph kernels. Technical report, First International Workshop on Mining Graphs, Trees and Sequences (held with ECML/PKDD03), 2003.
B. Scholkopf and A. J. Smola. Learning with Kernels. 2002.
A. Severyn and A. Moschitti. Fast support vector machines for convolution tree kernels. Data Mining and Knowledge Discovery, 25(2):325357, 2012.
N. Shervashidze and K. Borgwardt. Fast subtree kernels on graphs. In NIPS, 2010.
N. Shervashidze, S. V. N. Vishwanathan, T. Petri, K. Mehlhorn, and K. Borgwardt. Efficient graphlet kernels for large graph comparison. In AISTATS, 2009.
N. Shervashidze, P. Schweitzer, E. J. Van Leeuwen, K. Mehlhorn, and K. M. Borgwardt. Weisfeilerlehman graph kernels. JMLR, 12:25392561, 2011.
A. J. Smola and R. Kondor. Kernels and regularization on graphs. In COLT, pages 144158, 2003.
Y. W. Teh. A hierarchical bayesian language model based on pitman-yor processes. In ACL, 2006.
H. Toivonen, A. Srinivasan, R. D. King, S. Kramer, and C. Helma. Statistical evaluation of the predictive toxicology challenge 2000-2001. Bioinformatics, 19(10):11831193, July 2003.
S. V. N. Vishwanathan, N. N. Schraudolph, I. R. Kondor, and K. M. Borgwardt. Graph kernels. JMLR, 2010.
N. Wale, I. A. Watson, and G. Karypis. Comparison of descriptor spaces for chemical compound retrieval and classification. Knowledge and Information Systems, 14(3):347375, 2008.
P. Yanardag and S. Vishwanathan. Deep graph kernels. In KDD, pages 13651374. ACM, 2015.
C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2):179214, 2004.  9
William A. Gale and Geoffrey Sampson. Good-turing frequency estimation without tears. Journal of Quantitative Linguistics, 2(3):217237, 1995.
S. F. Chen and J. Goodman. An empirical study of smoothing techniques for language modeling. In ACL, 1996.
Liam Paninski. Variational minimax estimation of discrete distributions under KL loss. In NIPS, 2004.
Hermann Ney, Ute Essen, and Reinhard Kneser. On structuring probabilistic dependences in stochastic language modelling. Computer Speech & Language, 8(1):138, 1994.
Fredrick Jelinek and Robert L. Mercer. Probability distribution estimation from sparse data. IBM Tech. Disclosure Bull., 1984.
Irving J. Good. The population frequencies of species and the estimation of population parameters. Biometrika, 40(3-4):237264, 1953.
Thomas M. Cover and Joy A. Thomas. Elements of information theory (2. ed.). Wiley, 2006.
Raphail E. Krichevsky. The performance of universal encoding. Transactions on Information Theory, 44(1):296303, January 1998.
Sudeep Kamath, Alon Orlitsky, Dheeraj Pichapati, and Ananda Theertha Suresh. On learning distributions from their samples. In COLT, 2015.
Dietrich Braess and Thomas Sauer. Bernstein polynomials and learning theory. Journal of Approximation Theory, 128(2):187206, 2004.
David A. McAllester and Robert E. Schapire. On the convergence rate of Good-Turing estimators. In COLT, 2000.
Evgeny Drukh and Yishay Mansour. Concentration bounds for unigrams language model. In COLT, 2004.
Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, and Ananda Theertha Suresh. Optimal probability estimation with applications to prediction and classification. In COLT, 2013.
Alon Orlitsky, Narayana P. Santhanam, and Junan Zhang. Always Good Turing: Asymptotically optimal probability estimation. In FOCS, 2003.
Boris Yakovlevich Ryabko. Twice-universal coding. Problemy Peredachi Informatsii, 1984.
Boris Yakovlevich Ryabko. Fast adaptive coding algorithm. Problemy Peredachi Informatsii, 26(4):24 37, 1990.
Dominique Bontemps, Stephane Boucheron, and Elisabeth Gassiat. About adaptive coding on countable alphabets. IEEE Transactions on Information Theory, 60(2):808821, 2014.
Stephane Boucheron, Elisabeth Gassiat, and Mesrob I. Ohannessian. About adaptive coding on countable alphabets: Max-stable envelope classes. CoRR, abs/1402.6305, 2014.
David L Donoho and Jain M Johnstone. Ideal spatial adaptation by wavelet shrinkage. Biometrika, 81(3):425455, 1994.
Felix Abramovich, Yoav Benjamini, David L Donoho, and Iain M Johnstone. Adapting to unknown sparsity by controlling the false discovery rate. The Annals of Statistics, 2006.
Peter J Bickel, Chris A Klaassen, YAAcov Ritov, and Jon A Wellner. Efficient and adaptive estimation for semiparametric models. Johns Hopkins University Press Baltimore, 1993.
Andrew Barron, Lucien Birge, and Pascal Massart. Risk bounds for model selection via penalization. Probability theory and related fields, 113(3):301413, 1999.
Alexandre B. Tsybakov. Introduction to Nonparametric Estimation. Springer, 2004.
Jayadev Acharya, Hirakendu Das, Ashkan Jafarpour, Alon Orlitsky, and Shengjun Pan. Competitive closeness testing. COLT, 2011.
Jayadev Acharya, Hirakendu Das, Ashkan Jafarpour, Alon Orlitsky, Shengjun Pan, and Ananda Theertha Suresh. Competitive classification and closeness testing. In COLT, 2012.
Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, and Ananda Theertha Suresh. A competitive test for uniformity of monotone distributions. In AISTATS, 2013.
Gregory Valiant and Paul Valiant. An automatic inequality prover and instance optimal identity testing. In FOCS, 2014.
Gregory Valiant and Paul Valiant. Instance optimal learning. CoRR, abs/1504.05321, 2015.
Michael Mitzenmacher and Eli Upfal. Probability and computing: Randomized algorithms and probabilistic analysis. Cambridge University Press, 2005.  9
P. Bartlett, M. Jordan, and J. Mcauliffe. Convexity, Classification, and Risk Bounds. Journal of American Statistical Association, 101(473):138156, 2006.
A. Beygelzimer, J. Langford, and P. Ravikumar. Multiclass classification with filter trees. 2007.
R. Busa-Fekete, D. Benbouzid, and B. Kgl. Fast classification using sparse decision dags. In Proceedings of the 29th International Conference on Machine Learning, 2012.
M. Chen, Z. Xu, K. Weinberger, O. Chapelle, and D. Kedem. Classifier cascade: Tradeoff between accuracy and feature evaluation cost. In International Conference on Artificial Intelligence and Statistics, 2012.
G. Dulac-Arnold, L. Denoyer, P. Preux, and P. Gallinari. Datum-wise classification: a sequential approach to sparsity. In Machine Learning and Knowledge Discovery in Databases, pages 375390. 2011.
T. Gao and D. Koller. Active classification based on value of classifier. In Advances in Neural Information Processing Systems, volume 24, pages 10621070, 2011.
H. He, H. Daume III, and J. Eisner. Imitation learning by coaching. In Advances In Neural Information Processing Systems, pages 31583166, 2012.
S. Ji and L. Carin. Cost-sensitive feature acquisition and classification. Pattern Recognition, 40(5), 2007.
P. Kanani and P. Melville. Prediction-time active feature-value acquisition for cost-effective customer targeting. In Advances In Neural Information Processing Systems, 2008.
S. Karayev, M. Fritz, and T. Darrell. Dynamic feature selection for classification on a budget. In International Conference on Machine Learning: Workshop on Prediction with Sequential Models, 2013.
M. Kusner, W. Chen, Q. Zhou, Z. Xu, K. Weinberger, and Y. Chen. Feature-cost sensitive learning with submodular trees of classifiers. In Twenty-Eighth AAAI Conference on Artificial Intelligence, 2014.
J. Leskovec, A. Krause, C. Guestrin, C. Faloutsos, J. VanBriesen, and N. Glance. Cost-effective outbreak detection in networks. In International Conference on Knowledge Discovery and Data Mining, 2007.
L. Maaten, M. Chen, S. Tyree, and K. Q. Weinberger. Learning with marginalized corrupted features. In Proceedings of the 30th International Conference on Machine Learning, 2013.
F. Nan, J. Wang, and V. Saligrama. Feature-budgeted random forest. In Proceedings of the 32nd International Conference on Machine Learning, 2015.
F. Nan, J. Wang, K. Trapeznikov, and V. Saligrama. Fast margin-based cost-sensitive classification. In International Conference on Acoustics, Speech and Signal Processing, 2014.
G. Nemhauser, L. Wolsey, and M. Fisher. An analysis of approximations for maximizing submodular set functionsi. Mathematical Programming, 14(1):265294, 1978.
V. S. Sheng and C. X. Ling. Feature value acquisition in testing: A sequential batch test algorithm. In Proceedings of the 23rd International Conference on Machine Learning, pages 809816, 2006.
I. Steinwart. Consistency of support vector machines and other regularized kernel classifiers. Information Theory, IEEE Transactions on, 51(1):128142, 2005.
K. Trapeznikov and V. Saligrama. Supervised sequential classification under budget constraints. In International Conference on Artificial Intelligence and Statistics, pages 581589, 2013.
J. Wang, T. Bolukbasi, K. Trapeznikov, and V. Saligrama. Model selection by linear programming. In European Conference on Computer Vision, pages 647662, 2014.
J. Wang and V. Saligrama. Local supervised learning through space partitioning. In Advances in Neural Information Processing Systems, pages 9199. 2012.
J. Wang and V. Saligrama. Locally-linear learning machines (L3M). In Asian Conference on Machine Learning, pages 451466, 2013.
J. Wang, K. Trapeznikov, and V. Saligrama. An lp for sequential learning under budgets. In International Conference on Artificial Intelligence and Statistics, pages 987995, 2014.
Z. Xu, O. Chapelle, and K. Weinberger. The greedy miser: Learning under test-time budgets. In Proceedings of the 29th International Conference on Machine Learning, 2012.
Z. Xu, M. Kusner, M. Chen, and K. Weinberger. Cost-sensitive tree of classifiers. In Proceedings of the 30th International Conference on Machine Learning, pages 133141, 2013.
C. Zhang and Z. Zhang. A Survey of Recent Advances in Face Detection. Technical report, Microsoft Research, 2010.  9
De Blasi, P., Favaro, S., Lijoi, A., Mena, R. H., Pruenster, I., & Ruggiero, M. 2015. Are Gibbs-type priors the most natural generalization of the Dirichlet process? Pages 212229 of: IEEE Transactions on Pattern Analysis & Machine Intelligence, vol. 37.
Devroye, L. 1986. Non-Uniform Random Variate Generation. Springer-Verlag.
Devroye, L. 2009. Random variate generation for exponentially and polynomially tilted Stable distributions. ACM Transactions on Modelling and Computer Simulation, 19, 120.
Escobar, M. D. 1994. Estimating normal means with a Dirichlet process prior. Journal of the American Statistical Association, 89, 268277.
Escobar, M. D., & West, M. 1995. Bayesian density estimation and inference using mixtures. Journal of the American Statistical Association, 90, 577588.
Favaro, S., & Teh, Y. W. 2013. MCMC for Normalized Random Measure Mixture Models. Statistical Science, 28(3), 335359.
Favaro, S., & Walker, S. G. 2012. Slice sampling -Stable Poisson-Kingman mixture models. Journal of Computational and Graphical Statistics, 22, 830847.
Favaro, S., Lomeli, M., Nipoti, B., & Teh, Y. W. 2014. On the Stick-Breaking representation of -Stable Poisson-Kingman models. Electronic Journal of Statistics, 8, 10631085.
Ghahramani, Z. 2015. Probabilistic Machine Learning and Artificial Inteligence. Nature, 521, 452459.
Gnedin, A., & Pitman, J. 2006. Exchangeable Gibbs partitions and Stirling triangles. Journal of Mathematical Sciences, 138, 56745684.
Hofert, M. 2011. Efficiently sampling nested Archimedean copulas. Comput. Statist. Data Anal., 55, 5770.
Ishwaran, H., & James, L. F. 2001. Gibbs Sampling Methods for Stick-Breaking Priors. Journal of the American Statistical Association, 96(453), 161173.
James, L. F. 2002. Poisson process partition calculus with applications to exchangeable models and Bayesian nonparametrics. ArXiv:math/0205093.
Kanter, M. 1975. Stable densities under change of scale and total variation inequalities. Annals of Probability, 3, 697707.
Kingman, J. F. C. 1967. Completely Random Measures. Pacific Journal of Mathematics, 21, 5978.
Kingman, J. F. C. 1978. The representation of partition structures. Journal of the London Mathematical Society, 18, 374380.
Lomeli, M., Favaro, S., & Teh, Y. W. 2015. A marginal sampler for -stable Poisson-Kingman mixture models. Journal of Computational and Graphical Statistics (To appear).
Neal, R. M. 1998. Markov Chain Sampling Methods for Dirichlet Process Mixture Models. Tech. rept. 9815. Department of Statistics, University of Toronto.
Neal, R. M. 2003. Slice sampling. Annals of Statistics, 31, 705767.
Papaspiliopoulos, O., & Roberts, G. O. 2008. Retrospective Markov chain Monte Carlo methods for Dirichlet process hierarchical models. Biometrika, 95, 169186.
Perman, M., Pitman, J., & Yor, M. 1992. Size-biased sampling of Poisson point processes and excursions. Probability Theory and Related Fields, 92, 2139.
Pitman, J. 1996. Random discrete distributions invariant under size-biased permutation. Advances in Applied Probability, 28, 525539.  8
Pitman, J. 2003. Poisson-Kingman Partitions. Pages 134 of: Goldstein, D. R. (ed), Statistics and Science: a Festschrift for Terry Speed. Institute of Mathematical Statistics.
Pitman, J. 2006. Combinatorial Stochastic Processes. Lecture Notes in Mathematics. Springer-Verlag, Berlin.
Regazzini, E., Lijoi, A., & Pruenster, I. 2003. Distributional results for means of normalized random measures with independent increments. Annals of Statistics, 31, 560585.
Roeder, K. 1990. Density estimation with confidence sets exemplified by super-clusters and voids in the galaxies. Journal of the American Statistical Association, 85, 617624.
von Renesse, M., Yor, M., & Zambotti, L. 2008. Quasi-invariance properties of a class of subordinators. Stochastic Processes and their Applications, 118, 20382057.
Walker, Stephen G. 2007. Sampling the Dirichlet Mixture Model with Slices. Communications in Statistics - Simulation and Computation, 36, 45.  9
D. Angluin. Computational learning theory: survey and selected bibliography. In Proceedings of the twenty-fourth annual ACM symposium on Theory of computing, pages 351369. ACM, 1992.
M. Bouvel, V. Grebinski, and G. Kucherov. Combinatorial search on graphs motivated by bioinformatics applications: A brief survey. In Graph-Theoretic Concepts in Computer Science, pages 1627. Springer, 2005.
N. Bshouty and Y. Mansour. Simple learning algorithms for decision trees and multivariate polynomials. In Foundations of Computer Science, 1995. Proceedings., 36th Annual Symposium on, pages 304311, Oct 1995.
N. H. Bshouty and H. Mazzawi. Optimal query complexity for reconstructing hypergraphs. In 27th International Symposium on Theoretical Aspects of Computer Science-STACS 2010, pages 143154, 2010.
S.-S. Choi, K. Jung, and J. H. Kim. Almost tight upper bound for finding fourier coefficients of bounded pseudo-boolean functions. Journal of Computer and System Sciences, 77(6):1039 1053, 2011.
S. A. Goldman. Computational learning theory. In Algorithms and theory of computation handbook, pages 2626. Chapman & Hall/CRC, 2010.
J. Jackson. An efficient membership-query algorithm for learning dnf with respect to the uniform distribution. In Foundations of Computer Science, 1994 Proceedings., 35th Annual Symposium on, pages 4253. IEEE, 1994.
M. J. Kearns. The computational complexity of machine learning. MIT press, 1990.
M. Kocaoglu, K. Shanmugam, A. G. Dimakis, and A. Klivans. Sparse polynomial learning and graph sketching. In Advances in Neural Information Processing Systems, pages 31223130, 2014.
E. Kushilevitz and Y. Mansour. Learning decision trees using the fourier spectrum. SIAM Journal on Computing, 22(6):13311348, 1993.
Y. Mansour. Learning boolean functions via the fourier transform. In Theoretical advances in neural computation and learning, pages 391424. Springer, 1994.
Y. Mansour. Randomized interpolation and approximation of sparse polynomials. SIAM Journal on Computing, 24(2):357368, 1995.
H. Mazzawi. Reconstructing Graphs Using Edge Counting Queries. PhD thesis, TechnionIsrael Institute of Technology, Faculty of Computer Science, 2011.
S. Negahban and D. Shah. Learning sparse boolean polynomials. In Communication, Control, and Computing (Allerton), 2012 50th Annual Allerton Conference on, pages 20322036. IEEE, 2012.
T. Richardson and R. Urbanke. Modern coding theory. Cambridge University Press, 2008.
R. Scheibler, S. Haghighatshoar, and M. Vetterli. A fast hadamard transform for signals with sub-linear sparsity. arXiv preprint arXiv:1310.1803, 2013.
B. Settles. Active learning literature survey. University of Wisconsin, Madison, 52:5566, 2010.
P. Stobbe and A. Krause. Learning fourier sparse set functions. In International Conference on Artificial Intelligence and Statistics, pages 11251133, 2012.
Yahoo. Yahoo! webscope dataset ydata-ymessenger-user-communication-pattern-v1 0.  9
Dominik Csiba, Zheng Qu, and Peter Richtarik. Stochastic dual coordinate ascent with adaptive probabilities. arXiv preprint arXiv:1502.08053, 2015.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, pages 315323, 2013. 1  On one of the new datasets, SVRG with a ratio of step-size to Lavg more aggressive than theory suggests stopped converging; hence we changed all runs to use the permissible 1/8. No other parameters were changed adapted to the dataset.  8
Qihang Lin, Zhaosong Lu, and Lin Xiao. An accelerated proximal coordinate gradient method and its application to regularized empirical risk minimization. arXiv preprint arXiv:1407.1296, 2014.
Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic average gradient. arXiv preprint arXiv:1309.2388, 2013.
Shai Shalev-Shwartz and Tong Zhang. Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization. Mathematical Programming, pages 141, 2013.
Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss. The Journal of Machine Learning Research, 14(1):567599, 2013.
Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM Journal on Optimization, 24(4):20572075, 2014.
Yuchen Zhang and Lin Xiao. Stochastic primal-dual coordinate method for regularized empirical risk minimization. arXiv preprint arXiv:1409.3257, 2014.
Peilin Zhao and Tong Zhang. Stochastic optimization with importance sampling. arXiv preprint arXiv:1401.2753, 2014.
Peilin Zhao and Tong Zhang. Stochastic optimization with importance sampling for regularized loss minimization. Proceedings of The 32nd International Conference on Machine Learning, 2015.  9
J.J. Koenderink. The structure of images. Biological cybernetics, 50(5):363370, 1984.  8
T. Lindeberg. Scale-space theory: A basic tool for analyzing structures at different scales. Journal of applied statistics, 21(1-2):225270, 1994.
T. Kadir and M. Brady. Saliency, scale and image description. IJCV, 45(2):83105, 2001.
M. Jagersand. Saliency maps and attention selection in scale and spatial coordinates: An information theoretic approach. In ICCV 1995, pages 195202. IEEE, 1995.
K. Mikolajczyk et al. A comparison of affine region detectors. IJCV, 65(1-2):4372, 2005.
N.D.B. Bruce and J.K. Tsotsos. Saliency based on information maximization. NIPS 2005, pages 155162, 2005.
M. Toews and W.M. Wells. A mutual-information scale-space for image feature detection and featurebased classification of volumetric brain images. In CVPR Workshops, pages 111116. IEEE, 2010.
A. Borji, D. Sihite, and L. Itti. Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study. IEEE TIP, 22(1):5569, 2013.
P. Perona, T. Shiota, and J. Malik. Anisotropic diffusion. In Geometry-driven diffusion in computer vision, pages 7392. Springer, 1994.
A. Buades, B. Coll, and J-M Morel. A non-local algorithm for image denoising. In CVPR 2005, volume 2, pages 6065. IEEE, 2005.
S. Paris and F. Durand. A fast approximation of the bilateral filter using a signal processing approach. In ECCV 2006, pages 568580. Springer, 2006.
LMJ Florack, BM Ter Haar Romeny, Jan J Koenderink, and Max A Viergever. General intensity transformations and differential invariants. Journal of Mathematical Imaging and Vision, 4(2):171187, 1994.
F. Mokhtarian and R. Suomela. Robust image corner detection through curvature scale space. IEEE T PAMI, 20(12):13761381, 1998.
J. Harel, C. Koch, and P. Perona. Graph-based visual saliency. NIPS 2006, 19:545, 2007.
Y. Li, X. Hou, C. Koch, J.M. Rehg, and A.L. Yuille. The secrets of salient object segmentation. CVPR 2014, pages 280287, 2014.
P. Arbelez, J. Pont-Tuset, J. Barron, F. Marques, and J. Malik. Multiscale combinatorial grouping. CVPR 2014, pages 328335, 2014.
J. Carreira and C. Sminchisescu. Cpmc: Automatic object segmentation using constrained parametric min-cuts. IEEE TPAMI, 34(7):13121328, 2012.
L. Itti, C. Koch, and E. Niebur. A model of saliency-based visual attention for rapid scene analysis. IEEE T PAMI, 20(11):12541259, 1998.
X. Hou and L. Zhang. Dynamic visual attention: Searching for coding length increments. NIPS 2008, pages 681688, 2009.
L. Zhang, M.H. Tong, T.K. Marks, H. Shan, and G.W. Cottrell. Sun: A bayesian framework for saliency using natural statistics. Journal of Vision, 8(7), 2008.
X. Hou, J. Harel, and C. Koch. Image signature: Highlighting sparse salient regions. IEEE TPAMI, 34(1):194201, 2012.
A. Garcia-Diaz, V. Lebor n, X.R. Fdez-Vidal, and X.M. Pardo. On the relationship between optical variability, visual saliency, and eye fixations: A computational approach. Journal of Vision, 12(6), 2012.
R. Achanta, S. Hemamiz, F. Estrada, and S. Susstrunk. Frequency-tuned salient region detection. CVPR 2009 Workshops, pages 15971604, 2009.
M.-M. Cheng, N.J. Mitra, X. Huang, P.H.S. Torr, and S.-M. Hu. Global contrast based salient region detection. IEEE TPAMI, 37(3):569582, 2015.
F. Perazzi, P. Krahenbuhl, Y. Pritch, and A. Hornung. Saliency filters: Contrast based filtering for salient region detection. CVPR 2012, pages 733740, 2012.
Tal A. Zelnik-Manor L. Margolin, R.What makes a patch distinct? CVPR 2013, pages 11391146, 2013.
A. Andreopoulos and J.K. Tsotsos. On sensor bias in experimental methods for comparing interest-point, saliency, and recognition algorithms. IEEE TPAMI, 34(1):110126, 2012.
C-H Lee, A. Varshney, and D.W. Jacobs. Mesh saliency. ACM SIGGRAPH 2005, pages 659666, 2005.
N.D.B. Bruce and J.K. Tsotsos. Saliency, attention, and visual search: An information theoretic approach. Journal of vision, 9(3):5, 2009.
D. Gao and N. Vasconcelos. Decision-theoretic saliency: computational principles, biological plausibility, and implications for neurophysiology and psychophysics. Neural computation, 21(1):239271, 2009.
M. Jiang et al. Salicon: Saliency in context. CVPR 2015, pages 10721080, 2015.  9
J. Abernethy, E. Hazan, and A. Rakhlin. Interior-point methods for full-information and bandit
online learning. IEEE Transactions on Information Theory, 58(7):41644175, 2012.
J. Abernethy, C. Lee, A. Sinha, and A. Tewari. Online linear optimization via smoothing. In COLT,
pages 807823, 2014.
J.-Y. Audibert and S. Bubeck. Minimax policies for adversarial and stochastic bandits. In COLT,
pages 217226, 2009.
J.-Y. Audibert, S. Bubeck, and G. Lugosi. Minimax policies for combinatorial prediction games. In
COLT, 2011.
P. Auer. Using confidence bounds for exploitation-exploration trade-offs. The Journal of Machine
Learning Research, 3:397422, 2003.
P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. Gambling in a rigged casino: The adversarial multi-arm bandit problem. In FOCS, 1995.
P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem.
Machine learning, 47(2-3):235256, 2002.
P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit
problem. SIAM Journal of Computuataion, 32(1):4877, 2003. ISSN 0097-5397.
D. P. Bertsekas. Stochastic optimization problems with nondifferentiable cost functionals. Journal
of Optimization Theory and Applications, 12(2):218231, 1973. ISSN 0022-3239.
N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press,
V. Dani and T. P. Hayes. Robbing the bandit: less regret in online geometric optimization against an
adaptive adversary. In SODA, pages 937943, 2006.
V. Dani, T. Hayes, and S. Kakade. The price of bandit information for online optimization. In NIPS,
L. Devroye, G. Lugosi, and G. Neu. Prediction by random-walk perturbation. In Conference on
Learning Theory, pages 460473, 2013.
E. Elsayed. Reliability Engineering. Wiley Series in Systems Engineering and Management.
Wiley, 2012. ISBN 9781118309544. URL https://books.google.com/books?id=
NdjF5G6tfLQC.
P. Embrechts, C. Kluppelberg, and T. Mikosch. Modelling Extremal Events: For Insurance and
Finance. Applications of mathematics. Springer, 1997. ISBN 9783540609315. URL https:
//books.google.com/books?id=BXOI2pICfJUC.
A. D. Flaxman, A. T. Kalai, and H. B. McMahan. Online convex optimization in the bandit setting:
gradient descent without a gradient. In SODA, pages 385394, 2005. ISBN 0-89871-585-7.
J. Gittins. Quantitative methods in the planning of pharmaceutical research. Drug Information
Journal, 30(2):479487, 1996.
J. Gittins, K. Glazebrook, and R. Weber. Multi-armed bandit allocation indices. John Wiley & Sons,
J. Hannan. Approximation to bayes risk in repeated play. In M. Dresher, A. W. Tucker, and P. Wolfe,
editors, Contributions to the Theory of Games, volume III, pages 97139, 1957.
A. Kalai and S. Vempala. Efficient algorithms for online decision problems. Journal of Computer
and System Sciences, 71(3):291307, 2005.
T. Kocak, G. Neu, M. Valko, and R. Munos. Efficient learning by implicit exploration in bandit
problems with side observations. In NIPS, pages 613621. Curran Associates, Inc., 2014.
J. Kujala and T. Elomaa. On following the perturbed leader in the bandit setting. In Algorithmic
Learning Theory, pages 371385. Springer, 2005.
T. L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied
Mathematics, 6(1):422, 1985.
N. Littlestone and M. K. Warmuth. The weighted majority algorithm. Information and Computation,
108(2):212261, 1994. ISSN 0890-5401.
H. B. McMahan and A. Blum. Online geometric optimization in the bandit setting against an adaptive adversary. In COLT, pages 109123, 2004.
G. Neu and G. Bartok. An efficient algorithm for learning with semi-bandit feedback. In Algorithmic
Learning Theory, pages 234248. Springer, 2013.
M. Pacula, J. Ansel, S. Amarasinghe, and U.-M. OReilly. Hyperparameter tuning in bandit-based
adaptive operator selection. In Applications of Evolutionary Computation, pages 7382. Springer,
J.-P. Penot. Sub-hessians, super-hessians and conjugation. Nonlinear Analysis: Theory, Methods & Applications, 23(6):689702, 1994. URL http://www.sciencedirect.com/
science/article/pii/0362546X94902127.
S. Rakhlin, O. Shamir, and K. Sridharan. Relax and randomize: From value to algorithms. In
Advances in Neural Information Processing Systems, pages 21412149, 2012.
H. Robbins. Some aspects of the sequential design of experiments. Bull. Amer. Math. Soc., 58(5):
527535, 1952.
C. Tsallis. Possible generalization of boltzmann-gibbs statistics. Journal of Statistical Physics, 52
(1-2):479487, 1988.
G. Van den Broeck, K. Driessens, and J. Ramon. Monte-carlo tree search in poker using expected
reward distributions. In Advances in Machine Learning, pages 367381. Springer, 2009.
T. Van Erven, W. Kotlowski, and M. K. Warmuth. Follow the leader with dropout perturbations. In
COLT, 2014.
R. Adamczak, A. E. Litvak, A. Pajor, and N. Tomczak-Jaegermann. Restricted isometry property of matrices with independent columns and neighborly polytopes by random sampling. Constructive Approximation, 34(1):6188, 2011.
A. Banerjee, S. Chen, F. Fazayeli, and V. Sivakumar. Estimation with Norm Regularization. In NIPS, 2014.
P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. Annals of Statistics, 37(4):17051732, 2009.
E. J. Candes, J. Romberg, and T. Tao. Robust Uncertainty Principles : Exact Signal Reconstruction from Highly Incomplete Frequency Information. IEEE Transactions on Information Theory, 52(2):489509, 2006.
E. J. Candes and T. Tao. Decoding by Linear Programming. IEEE Transactions on Information Theory, 51(12):42034215, 2005.
E. J. Candes and T. Tao. The Dantzig selector : statistical estimation when p is much larger than n. Annals of Statistics, 35(6):23132351, 2007.
V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky. The Convex Geometry of Linear Inverse Problems. Foundations of Computational Mathematics, 12(6):805849, 2012.
S. Chatterjee, S. Chen, and A. Banerjee. Generalized Dantzig Selector: Application to the k-support norm. In NIPS, 2014.
D. Hsu and S. Sabato. Heavy-tailed regression with a generalized median-of-means. In ICML, 2014.
V. Koltchinskii and S. Mendelson. Bounding the smallest singular value of a random matrix without concentration. arXiv:1312.3580, 2013.
G. Lecue and S. Mendelson. Sparse recovery under weak moment assumptions. arXiv:1401.2188, 2014.
M. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes. Springer Berlin, 1991.
N. Meinshausen and B. Yu. Lasso-type recovery of sparse representations for high-dimensional data. Annals of Statistics, 37(1):246270, 2009.
S. Mendelson. Learning without concentration. Journal of the ACM, To appear, 2015.
S. Mendelson and G. Paouris. On generic chaining and the smallest singular value of random matrices with heavy tails. Journal of Functional Analysis, 262(9):37753811, 2012.
S. N. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A Unified Framework for HighDimensional Analysis of $M$-Estimators with Decomposable Regularizers. Statistical Science, 27(4):538557, 2012.
R. I. Oliveira. The lower tail of random quadratic forms, with applications to ordinary least squares and restricted eigenvalue properties. arXiv:1312.2903, 2013.
M. Rudelson and S. Zhou. Reconstruction from anisotropic random measurements. IEEE Transaction on Information Theory, 59(6):34343447, 2013.
M. Talagrand. The Generic Chaining. Springer Berlin, 2005.
J. A. Tropp. Convex recovery of a structured signal from independent random linear measurements. In Sampling Theory - a Renaissance. To appear, 2015.
R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Y Eldar and G. Kutyniok, editors, Compressed Sensing, pages 210268. Cambridge University Press, Cambridge, 2012.
M. J Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using L1 -constrained quadratic programmming ( Lasso ). IEEE Transaction on Information Theory, 55(5):21832201, 2009.
P. Zhao and B. Yu. On Model Selection Consistency of Lasso. Journal of Machine Learning Research, 7:25412563, 2006.  9
R. K. Ando and T. Zhang. Learning on graph with Laplacian regularization. In NIPS, 2007.
N. Balcan and A. Blum. An augmented PAC model for semi-supervised learning. In O. Chapelle, B. Scholkopf, and A. Zien, editors, Semi-supervised learning. MIT press Cambridge, 2006.
J. P. Boyle and R. L. Dykstra. A Method for Finding Projections onto the Intersection of Convex Sets in Hilbert Spaces. In Advances in Order Restricted Statistical Inference, volume 37 of Lecture Notes in Statistics, pages 2847. Springer New York, 1986.
T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to algorithms, volume 2. MIT press Cambridge, 2001.
M. Eisenberg-Nagy, M. Laurent, and A. Varvitsiotis. Forbidden minor characterizations for low-rank optimal solutions to semidefinite programs over the elliptope. J. Comb. Theory, Ser. B, 108:4080, 2014.
A. Erdem and M. Pelillo. Graph transduction as a Non-Cooperative Game. Neural Computation, 24(3):700723, 2012.
M. X. Goemans. Semidefinite programming in combinatorial optimization. Mathematical Programming, 79(1-3):143161, 1997.
V. Jethava, A. Martinsson, C. Bhattacharyya, and D. P. Dubhashi. The Lovasz  function, SVMs and finding large dense subgraphs. In NIPS, pages 11691177, 2012.
R. Johnson and T. Zhang. On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning. JMLR, 8(7):14891517, 2007.
Y. LeCun and C. Cortes. The MNIST database of handwritten digits, 1998.
M. Leordeanu, A. Zanfir, and C. Sminchisescu. Semi-supervised learning and optimization for hypergraph matching. In ICCV, pages 22742281. IEEE, 2011.
M. Lichman. UCI machine learning repository, 2013.
L. Lovasz. On the shannon capacity of a graph. Information Theory, IEEE Transactions on, 25(1):17, 1979.
N. Parikh and S. Boyd. Proximal algorithms. Foundations and Trends in optimization, 1(3):123231, 2013.
M. Schmidt, N. L. Roux, and F. R. Bach. Convergence rates of inexact proximal-gradient methods for convex optimization. In NIPS, pages 14581466, 2011.
R. Shivanna and C. Bhattacharyya. Learning on graphs using Orthonormal Representation is Statistically Consistent. In NIPS, pages 36353643, 2014.
L. Tran. Application of three graph Laplacian based semi-supervised learning methods to protein function prediction problem. IJBB, 2013.
S. Villa, S. Salzo, L. Baldassarre, and A. Verri. Accelerated and Inexact Forward-Backward Algorithms. SIAM Journal on Optimization, 23(3):16071633, 2013.
T. Zhang and R. K. Ando. Analysis of spectral kernel design based semi-supervised learning. NIPS, 18:1601, 2005.
D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Scholkopf. Learning with local and global consistency. NIPS, 16(16):321328, 2004.
D. Zhou and C. J. C. Burges. Spectral clustering and transductive learning with multiple views. In ICML, pages 11591166. ACM, 2007.  9
Frederic Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud Bergeron, Nicolas Bouchard, and Yoshua Bengio. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.
George E. Dahl, Navdeep Jaitly, and Ruslan Salakhutdinov. Multi-task neural networks for QSAR predictions. arXiv preprint arXiv:1406.1231, 2014.
John S. Delaney. ESOL: Estimating aqueous solubility directly from molecular structure. Journal of Chemical Information and Computer Sciences, 44(3):10001005, 2004.
Francisco-Javier Gamo, Laura M Sanz, Jaume Vidal, Cristina de Cozar, Emilio Alvarez, Jose-Luis Lavandera, Dana E Vanderwall, Darren VS Green, Vinod Kumar, Samiul Hasan, et al. Thousands of chemical starting points for antimalarial lead identification. Nature, 465(7296):305310, 2010.
Robert C. Glem, Andreas Bender, Catrin H. Arnby, Lars Carlsson, Scott Boyer, and James Smith. Circular fingerprints: flexible molecular descriptors with applications from physical chemistry to ADME. IDrugs: the investigational drugs journal, 9(3):199204, 2006.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing machines. arXiv preprint arXiv:1410.5401, 2014.
Johannes Hachmann, Roberto Olivares-Amaya, Sule Atahan-Evrenk, Carlos Amador-Bedolla, Roel S Sanchez-Carrera, Aryeh Gold-Parker, Leslie Vogt, Anna M Brockway, and Alan Aspuru-Guzik. The Harvard clean energy project: large-scale computational screening and design of organic photovoltaics on the world community grid. The Journal of Physical Chemistry Letters, 2(17):22412251, 2011.
John R Hershey, Jonathan Le Roux, and Felix Weninger. Deep unfolding: Model-based inspiration of novel deep architectures. arXiv preprint arXiv:1409.2574, 2014. 8
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):17351780, 1997.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network for modelling sentences. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, June 2014.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Yann LeCun and Yoshua Bengio. Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks, 3361, 1995.
Alessandro Lusci, Gianluca Pollastri, and Pierre Baldi. Deep architectures and deep learning in chemoinformatics: the prediction of aqueous solubility for drug-like molecules. Journal of chemical information and modeling, 53(7):15631575, 2013.
Alessio Micheli. Neural network for graphs: A contextual constructive approach. Neural Networks, IEEE Transactions on, 20(3):498511, 2009.
H.L. Morgan. The generation of a unique machine description for chemical structure. Journal of Chemical Documentation, 5(2):107113, 1965.
Travis E Oliphant. Python for scientific computing. Computing in Science & Engineering, 9(3):1020, 2007.
Bharath Ramsundar, Steven Kearnes, Patrick Riley, Dale Webster, David Konerding, and Vijay Pande. Massively multitask networks for drug discovery. arXiv:1502.02072, 2015.
RDKit: Open-source cheminformatics. www.rdkit.org.
David Rogers and Mathew Hahn. Extended-connectivity fingerprints. Journal of Chemical Information and Modeling, 50(5):742754, 2010.
F. Scarselli, M. Gori, Ah Chung Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network model. Neural Networks, IEEE Transactions on, 20(1):6180, Jan 2009.
Richard Socher, Eric H Huang, Jeffrey Pennin, Christopher D Manning, and Andrew Y Ng. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing Systems, pages 801809, 2011.
Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151161. Association for Computational Linguistics, 2011.
Kai Sheng Tai, Richard Socher, and Christopher D Manning. Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075, 2015.
Tox21 Challenge. National center for advancing translational sciences. http://tripod. nih.gov/tox21/challenge, 2014.
Thomas Unterthiner, Andreas Mayr, Gunter Klambauer, and Sepp Hochreiter. Toxicity prediction using deep learning. arXiv preprint arXiv:1503.01445, 2015.
Thomas Unterthiner, Andreas Mayr, G unter Klambauer, Marvin Steijaert, Jorg Wenger, Hugo Ceulemans, and Sepp Hochreiter. Deep learning as an opportunity in virtual screening. In Advances in Neural Information Processing Systems, 2014.
Li Wan, Matthew Zeiler, Sixin Zhang, Yann L. Cun, and Rob Fergus. Regularization of neural networks using dropconnect. In International Conference on Machine Learning, 2013.
David Weininger. SMILES, a chemical language and information system. Journal of chemical information and computer sciences, 28(1):3136, 1988.  9
D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding. In SODA, 2007.
A. Asadpour and A. Saberi. An approximation algorithm for max-min fair allocation of indivisible goods. In SICOMP, 2010.
S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 2011.
N. Buchbinder, M. Feldman, J. Naor, and R. Schwartz. A tight linear time (1/2)-approximation for unconstrained submodular maximization. In FOCS, 2012.
C. Chekuri and A. Ene. Approximation algorithms for submodular multiway partition. In FOCS, 2011.
C. Chekuri and A. Ene. Submodular cost allocation problem and applications. In Automata, Languages and Programming, pages 354366. Springer, 2011.
A. Ene, J. Vondrak, and Y. Wu. Local distribution and the symmetry gap: Approximability of multiway partitioning problems. In SODA, 2013.
M. Fisher, G. Nemhauser, and L. Wolsey. An analysis of approximations for maximizing submodular set functionsII. In Polyhedral combinatorics, 1978.
S. Fujishige. Submodular functions and optimization, volume 58. Elsevier, 2005.
L. A. Garca-Escudero, A. Gordaliza, C. Matran, and A. Mayo-Iscar. A review of robust clustering methods. Advances in Data Analysis and Classification, 4(2-3):89109, 2010.
M. Goemans, N. Harvey, S. Iwata, and V. Mirrokni. Approximating submodular functions everywhere. In SODA, 2009.
D. Golovin. Max-min fair allocation of indivisible goods. Technical Report CMU-CS-05-144, 2005.
R. Iyer, S. Jegelka, and J. Bilmes. Monotone closure of relaxed constraints in submodular optimization: Connections between minimization and maximization: Extended version.
R. Iyer, S. Jegelka, and J. Bilmes. Fast semidifferential based submodular function optimization. In ICML, 2013.
S. Jegelka and J. Bilmes. Submodularity beyond submodular energies: coupling edges in graph cuts. In CVPR, 2011.
S. Khot and A. Ponnuswami. Approximation algorithms for the max-min allocation problem. In APPROX, 2007.
V. Kolmogorov and R. Zabin. What energy functions can be minimized via graph cuts? In TPAMI, 2004.
A. Krause, B. McMahan, C. Guestrin, and A. Gupta. Robust submodular observation selection. In JMLR, 2008.
J. K. Lenstra, D. B. Shmoys, and E. Tardos. Approximation algorithms for scheduling unrelated parallel machines. In Mathematical programming, 1990.
M. Li, D. Andersen, and A. Smola. Graph partitioning via parallel submodular approximation to accelerate distributed machine learning. In arXiv preprint arXiv:1505.04636, 2015.
M. Minoux. Accelerated greedy algorithms for maximizing submodular set functions. In Optimization Techniques, 1978.
M. Narasimhan, N. Jojic, and J. A. Bilmes. Q-clustering. In NIPS, 2005.
J. Orlin. A faster strongly polynomial time algorithm for submodular function minimization. Mathematical Programming, 2009.
D. Povey, X. Zhang, and S. Khudanpur. Parallel training of deep neural networks with natural gradient and parameter averaging. arXiv preprint arXiv:1410.7455, 2014.
Z. Svitkina and L. Fleischer. Submodular approximation: Sampling-based algorithms and lower bounds. In FOCS, 2008.
J. Vondrak. Optimal approximation for the submodular welfare problem in the value oracle model. In STOC, 2008.
K. Wei, R. Iyer, and J. Bilmes. Submodularity in data subset selection and active learning. In ICML, 2015.
K. Wei, R. Iyer, S. Wang, W. Bai, and J. Bilmes. Mixed robust/average submodular partitioning: Fast algorithms, guarantees, and applications: NIPS 2015 Extended Supplementary.
L. Zhao, H. Nagamochi, and T. Ibaraki. On generalized greedy splitting algorithms for multiway partition problems. Discrete applied mathematics, 143(1):130143, 2004.  9
P. Domingos, M. Niepert, and D. Lowd (Eds.). In ICML Workshop on Learning Tractable Probabilistic Models, 2014.
F. R.. Bach and M. I. Jordan. Thin junction trees. In Proceedings of NIPS, pages 569576, 2001.
N. L. Zhang. Hierarchical latent class models for cluster analysis. JMLR, 5:697723, 2004.
M. Narasimhan and J. Bilmes. PAC-learning bounded tree-width graphical models. In Proc. UAI, 2004.
A. Chechetka and C. Guestrin. Efficient principled learning of thin junction trees. In Proceedings of NIPS, pages 273280, 2007.
M. Chavira and A. Darwiche. On probabilistic inference by weighted model counting. AIJ, 172(67): 772799, 2008.
M. Niepert and G. Van den Broeck. Tractability through exchangeability: A new perspective on efficient probabilistic inference. In Proceedings of AAAI, 2014.
A. Darwiche. A differential approach to inference in Bayesian networks. JACM, 50(3):280305, 2003.
D. Lowd and A. Rooshenas. Learning Markov networks with arithmetic circuits. In Proc. AISTATS, pages 406414, 2013.
T. Rahman, P. Kothalkar, and V. Gogate. Cutset networks: A simple, tractable, and scalable approach for improving the accuracy of Chow-Liu trees. In Proceedings of ECML PKDD, pages 630645, 2014.
R. Gens and P. Domingos. Learning the structure of sum-product networks. In Proceedings of ICLM, pages 873880, 2013.
A. Rooshenas and D. Lowd. Learning sum-product networks with direct and indirect variable interactions. In Proceedings ICML, pages 710718, 2014.
M. Niepert and P. Domingos. Exchangeable variable models. In Proceedings of ICML, 2014.
J. Van Haaren, G. Van den Broeck, W. Meert, and J. Davis. Lifted generative learning of markov logic networks. Machine Learning, 2015. (to appear).
A. Kulesza and B. Taskar. Determinantal point processes for machine learning. Foundations and Trends in Machine Learning, 2012.
J. D. Park. Map complexity results and approximation methods. In Proceedings of UAI, 2002.
S. J. Chen, A. Choi, and A. Darwiche. Algorithms and applications for the same-decision probability. JAIR, pages 601633, 2014.
C. Krause, A.and Guestrin. Optimal nonmyopic value of information in graphical models - efficient algorithms and theoretical limits. In Proceedings of IJCAI, 2005.
L. C. van der Gaag, H. L. Bodlaender, and A. Feelders. Monotonicity in bayesian networks. In Proceedings of UAI, pages 569576, 2004.
D. D. Maua, C. P. De Campos, and M. Zaffalon. On the complexity of solving polytree-shaped limited memory influence diagrams with binary variables. AIJ, 205:3038, 2013.
I. Parberry and G. Schnitger. Relating Boltzmann machines to conventional models of computation. Neural Networks, 2(1):5967, 1989.
D. Buchman and D. Poole. Representing aggregators in relational probabilistic models. In Proceedings of AAAI, 2015.
A. Darwiche. SDD: A new canonical representation of propositional knowledge bases. In Proceedings of IJCAI, pages 819826, 2011.
S. Della Pietra, V. Della Pietra, and J. Lafferty. Inducing Features of Random Fields. IEEE TPAMI, 19: 380392, 1997.
Daniel Lowd and Jesse Davis. Improving Markov network structure learning using decision trees. The Journal of Machine Learning Research, 15(1):501532, 2014.
D. Kisa, G. Van den Broeck, A. Choi, and A. Darwiche. Probabilistic sentential decision diagrams. In KR, 2014.
A. Choi, G. Van den Broeck, and A. Darwiche. Tractable learning for structured probability spaces: A case study in learning preference distributions. In Proceedings of IJCAI, 2015.  9
N. Le Roux, M. Schmidt, and F. Bach, A stochastic gradient method with an exponential convergence rate for strongly-convex optimization with finite training sets, Advances in neural information processing systems (NIPS), 2012.
S. Shalev-Schwartz and T. Zhang, Stochastic dual coordinate ascent methods for regularized loss minimization, Journal of Machine Learning Research, vol. 14, pp. 567599, 2013.
J. Mairal, Optimization with first-order surrogate functions, International Conference on Machine Learning (ICML), 2013.
A. Defazio, F. Bach, and S. Lacoste-Julien, Saga: A fast incremental gradient method with support for non-strongly convex composite objectives, Advances in neural information processing systems (NIPS), 2014.
M. Mahdavi, L. Zhang, and R. Jin, Mixed optimization for smooth functions, Advances in neural information processing systems (NIPS), 2013.
R. Johnson and T. Zhang, Accelerating stochastic gradient descent using predictive variance reduction, Advances in neural information processing systems (NIPS), 2013.
L. Zhang, M. Mahdavi, and R. Jin, Linear convergence with condition number independent access of full gradients, Advances in neural information processing systems (NIPS), 2013.
J. Konecny and P. Richtarik, Semi-stochastic gradient descent methods, arXiv preprint, 2013.
M. Schmidt, N. Le Roux, and F. Bach, Convergence rates of inexact proximal-gradient methods for convex optimization, Advances in neural information processing systems (NIPS), 2011.
C. Hu, J. Kwok, and W. Pan, Accelerated gradient methods for stochastic optimization and online learning, Advances in neural information processing systems (NIPS), 2009.
L. Xiao and T. Zhang, A proximal stochastic gradient method with progressive variance reduction, SIAM Journal on Optimization, vol. 24, no. 2, pp. 20572075, 2014.
S. Lohr, Sampling: design and analysis. Cengage Learning, 2009.
M. P. Friedlander and M. Schmidt, Hybrid deterministic-stochastic methods for data fitting, SIAM Journal of Scientific Computing, vol. 34, no. 3, pp. A1351A1379, 2012.
A. Aravkin, M. P. Friedlander, F. J. Herrmann, and T. Van Leeuwen, Robust inversion, dimensionality reduction, and randomized sampling, Mathematical Programming, vol. 134, no. 1, pp. 101125, 2012.
S. Rosset and J. Zhu, Piecewise linear regularized solution paths, The Annals of Statistics, vol. 35, no. 3, pp. 10121030, 2007.
T. Joachims, Making large-scale SVM learning practical, in Advances in Kernel Methods Support Vector Learning (B. Scholkopf, C. Burges, and A. Smola, eds.), ch. 11, pp. 169184, Cambridge, MA: MIT Press, 1999.
N. Usunier, A. Bordes, and L. Bottou, Guarantees for approximate incremental svms, International Conference on Artificial Intelligence and Statistics (AISTATS), 2010.
J. Konecny, J. Liu, P. Richtarik, and M. Takac, ms2gd: Mini-batch semi-stochastic gradient descent in the proximal setting, arXiv preprint, 2014.
L. Bottou and O. Bousquet, The tradeoffs of large scale learning, Advances in neural information processing systems (NIPS), 2007.
R. H. Byrd, G. M. Chin, J. Nocedal, and Y. Wu, Sample size selection in optimization methods for machine learning, Mathematical programming, vol. 134, no. 1, pp. 127155, 2012.
K. van den Doel and U. Ascher, Adaptive and stochastic algorithms for EIT and DC resistivity problems with piecewise constant solutions and many measurements, SIAM J. Scient. Comput, vol. 34, 2012.  9
G. A. Miller, The magical number seven, plus or minus two: Some limits on our capacity for processing information, The Psychological Review, pp. 8197, March 1956.
D. M. Blei, A. Y. Ng, and M. I. Jordan, Latent Dirichlet allocation, JMLR, pp. 3:9931022, 2003.
H. Zou, T. Hastie, and R. Tibshirani, Sparse principal component analysis, Journal of Computational and Graphical Statistics, vol. 15, p. 2006, 2004.
K. Than and T. B. Ho, Fully sparse topic models, in ECML-PKDD, pp. 490505, 2012.
S. Williamson, C. Wang, K. Heller, and D. Blei, The ibp compound dirichlet process and its application to focused topic modeling, ICML, 2010.
E. Elhamifar and R. Vidal, Sparse subspace clustering: Algorithm, theory, and applications, IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 11, pp. 27652781, 2013.
R. Agrawal, J. Gehrke, D. Gunopulos, and P. Raghavan, Automatic subspace clustering of high dimensional data for data mining applications, SIGMOD Rec., vol. 27, pp. 94105, June 1998.
B. Kim, C. Rudin, and J. A. Shah, The Bayesian Case Model: A generative approach for case-based reasoning and prototype classification, in NIPS, 2014.
R. K, Differential diagnosis in primary care, JAMA, vol. 307, no. 14, pp. 15331534, 2012.
Z. J and Y. KF, Whats the relative risk?: A method of correcting the odds ratio in cohort studies of common outcomes, JAMA, vol. 280, no. 19, pp. 16901691, 1998.
S. Alelyani, J. Tang, and H. Liu, Feature selection for clustering: A review., Data Clustering: Algorithms and Applications, vol. 29, 2013.
S. Guerif, Unsupervised variable selection: when random rankings sound as irrelevancy., in FSDM, 2008.
P. Mitra, C. Murthy, and S. K. Pal, Unsupervised feature selection using feature similarity, IEEE transactions on pattern analysis and machine intelligence, vol. 24, no. 3, pp. 301312, 2002.
M. Dash and H. Liu, Feature selection for clustering, in KDD: Current Issues and New Applications, pp. 110121, 2000.
K. Tsuda, M. Kawanabe, and K.-R. Mller, Clustering with the fisher score, in NIPS, 2003.
J. G. Dy and C. E. Brodley, Feature selection for unsupervised learning, JMLR, pp. 5:845889, 2004.
Y. Guan, J. G. Dy, and M. I. Jordan, A unified probabilistic model for global and local unsupervised feature selection., in ICML, pp. 10731080, 2011.
W. Fan and N. Bouguila, Online learning of a dirichlet process mixture of generalized dirichlet distributions for simultaneous clustering and localized feature selection., in ACML, pp. 113128, 2012.
G. Yu, R. Huang, and Z. Wang, Document clustering via dirichlet process mixture model with feature selection, in KDD, pp. 763772, ACM, 2010.
A. A. Freitas, Comprehensible classification models: a position paper, ACM SIGKDD Explorations Newsletter, 2014.
G. Death and K. E. Fabricius, Classification and regression trees: a powerful yet simple technique for ecological data analysis, Ecology, vol. 81, no. 11, pp. 31783192, 2000.
M. J. Wainwright and M. I. Jordan, Graphical models, exponential families, and variational inference, Foundations and Trends in Machine Learning, vol. 1, no. 1-2, pp. 1305, 2008.
M. Lichman, UCI machine learning repository, 2013.
C. Kemp and J. B. Tenenbaum, The discovery of structural form, PNAS, 2008.
F. Doshi-Velez, Y. Ge, and I. Kohane, Comorbidity clusters in autism spectrum disorders: an electronic health record time-series analysis, Pediatrics, vol. 133, no. 1, pp. e54e63, 2014.
A. Kulesza, Learning with Determinantal Point Processes. PhD thesis, University of Pennsylvania, 2012.
A. Kulesza and B. Taskar, Structured determinantal point processes, in NIPS, 2010.
J. Y. Zou and R. P. Adams, Priors for diversity in generative latent variable models, in NIPS, 2012.
N. K. Batmanghelich, G. Quon, A. Kulesza, M. Kellis, P. Golland, and L. Bornn, Diversifying sparsity using variational determinantal point processes, CoRR, 2014.
J. Eisenstein, A. Ahmed, and E. P. Xing, Sparse additive generative models of text, ICML, 2011.
J. Y. Zou, D. J. Hsu, D. C. Parkes, and R. P. Adams, Contrastive learning using spectral methods, in NIPS, 2013.
J. Zhu, A. Ahmed, and E. P. Xing, Medlda: maximum margin supervised topic models for regression and classification, in ICML, pp. 12571264, ACM, 2009.  9
David H Hubel. Eye, brain, and vision. Scientific American Library/Scientific American Books, 1995.
E Oja. Simplified neuron model as a principal component analyzer. J Math Biol, 15(3):267273, 1982.
KI Diamantaras and SY Kung. Principal component neural networks: theory and applications. John Wiley & Sons, Inc., 1996.
B Yang. Projection approximation subspace tracking. IEEE Trans. Signal Process., 43(1):95107, 1995.
T Hu, ZJ Towfic, C Pehlevan, A Genkin, and DB Chklovskii. A neuron as a signal processing device. In Asilomar Conference on Signals, Systems and Computers, pages 362366. IEEE, 2013.
E Oja. Principal components, minor components, and linear neural networks. Neural Networks, 5(6):927 935, 1992.
R Arora, A Cotter, K Livescu, and N Srebro. Stochastic optimization for pca and pls. In Allerton Conf. on Communication, Control, and Computing, pages 861868. IEEE, 2012.
J Goes, T Zhang, R Arora, and G Lerman. Robust stochastic principal component analysis. In Proc. 17th Int. Conf. on Artificial Intelligence and Statistics, pages 266274, 2014.
Todd K Leen. Dynamics of learning in recurrent feature-discovery networks. NIPS, 3, 1990.  8
P Foldiak. Adaptive network for optimal linear feature extraction. In Int. Joint Conf. on Neural Networks, pages 401405. IEEE, 1989.
TD Sanger. Optimal unsupervised learning in a single-layer linear feedforward neural network. Neural networks, 2(6):459473, 1989.
J Rubner and P Tavan. A self-organizing network for principal-component analysis. EPL, 10:693, 1989.
MD Plumbley. A hebbian/anti-hebbian network which optimizes information capacity by orthonormalizing the principal subspace. In Proc. 3rd Int. Conf. on Artificial Neural Networks, pages 8690, 1993.
MD Plumbley. A subspace network that determines its own output dimension. Tech. Rep., 1994.
MD Plumbley. Information processing in negative feedback neural networks. Network-Comp Neural, 7(2):301305, 1996.
BA Olshausen and DJ Field. Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision Res, 37(23):33113325, 1997.
AA Koulakov and D Rinberg. Sparse incomplete representations: a potential role of olfactory granule cells. Neuron, 72(1):124136, 2011.
S Druckmann, T Hu, and DB Chklovskii. A mechanistic model of early sensory processing based on subtracting sparse representations. In NIPS, pages 19791987, 2012.
P Vertechi, W Brendel, and CK Machens. Unsupervised learning of an efficient short-term memory network. In NIPS, pages 36533661, 2014.
AL Fairhall, GD Lewen, W Bialek, and RRR van Steveninck. Efficiency and ambiguity in an adaptive neural code. Nature, 412(6849):787792, 2001.
SE Palmer, O Marre, MJ Berry, and W Bialek. Predictive information in a sensory population. PNAS, 112(22):69086913, 2015.
E Doi, JL Gauthier, GD Field, J Shlens, et al. Efficient coding of spatial information in the primate retina. J Neurosci, 32(46):1625616264, 2012.
R Linsker. Self-organization in a perceptual network. Computer, 21(3):105117, 1988.
C Pehlevan, T Hu, and DB Chklovskii. A hebbian/anti-hebbian neural network for linear subspace learning: A derivation from multidimensional scaling of streaming data. Neural Comput, 27:14611495, 2015.
G Young and AS Householder. Discussion of a set of points in terms of their mutual distances. Psychometrika, 3(1):1922, 1938.
WS Torgerson. Multidimensional scaling: I. theory and method. Psychometrika, 17(4):401419, 1952.
HG Barrow and JML Budd. Automatic gain control by a basic neural circuit. Artificial Neural Networks, 2:433436, 1992.
EJ Candes and B Recht. Exact matrix completion via convex optimization. Found Comput Math, 9(6):717772, 2009.
J Mairal, F Bach, J Ponce, and G Sapiro. Online learning for matrix factorization and sparse coding. JMLR, 11:1960, 2010.
S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.
HS Seung, TJ Richardson, JC Lagarias, and JJ Hopfield. excitatory-inhibitory networks. NIPS, 10:329335, 1998.  Minimax and hamiltonian dynamics of
P Gao and S Ganguli. On simplicity and complexity in the brave new world of large-scale neuroscience. Curr Opin Neurobiol, 32:148155, 2015.
M Zhu and CJ Rozell. Modeling inhibitory interneurons in efficient sensory coding models. PLoS Comput Biol, 11(7):e1004353, 2015.
PD King, J Zylberberg, and MR DeWeese. Inhibitory interneurons decorrelate excitatory cells to drive sparse code formation in a spiking model of v1. J Neurosci, 33(13):54755485, 2013.
N Kriegeskorte, M Mur, DA Ruff, R Kiani, et al. Matching categorical object representations in inferior temporal cortex of man and monkey. Neuron, 60(6):11261141, 2008.
R Kiani, H Esteky, K Mirpour, and K Tanaka. Object category structure in response patterns of neuronal population in monkey inferior temporal cortex. J Neurophysiol, 97(6):42964309, 2007.
G Tkacik, E Granot-Atedgi, R Segev, and E Schneidman. Retinal metric: a stimulus distance measure derived from population neural responses. PRL, 110(5):058104, 2013.
C Pehlevan and DB Chklovskii. Optimization theory of hebbian/anti-hebbian networks for pca and whitening. In Allerton Conf. on Communication, Control, and Computing, 2015.  9
T. Chen, E. B. Fox, and C. Guestrin. Stochastic gradient Hamiltonian Monte Carlo. In ICML, 2014.
N. Ding, Y. Fang, R. Babbush, C. Chen, R. D. Skeel, and H. Neven. Bayesian sampling using stochastic gradient thermostats. In NIPS, 2014.
H. Risken. The Fokker-Planck equation. Springer-Verlag, New York, 1989.
M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient Langevin dynamics. In ICML, 2011.
Y. W. Teh, A. H. Thiery, and S. J. Vollmer. Consistency and fluctuations for stochastic gradient Langevin dynamics. Technical Report arXiv:1409.0578, University of Oxford, UK, Sep. 2014. URL http://arxiv.org/abs/1409.0578.
S. J. Vollmer, K. C. Zygalakis, and Y. W. Teh. (Non-)asymptotic properties of stochastic gradient Langevin dynamics. Technical Report arXiv:1501.00438, University of Oxford, UK, January 2015. URL http://arxiv.org/abs/1501.00438.
M. Betancourt. The fundamental incompatibility of Hamiltonian Monte Carlo and data subsampling. In ICML, 2015.
I. Sato and H. Nakagawa. Approximation analysis of stochastic gradient Langevin dynamics by using Fokker-Planck equation and Ito process. In ICML, 2014.
B. Leimkuhler and X. Shang. Adaptive thermostats for noisy gradient systems. Technical Report arXiv:1505.06889v1, University of Edinburgh, UK, May 2015. URL http: //arxiv.org/abs/1505.06889.
A. Abdulle, G. Vilmart, and K. C. Zygalakis. Long time accuracy of LieTrotter splitting methods for Langevin dynamics. SIAM J. NUMER. ANAL., 53(1):116, 2015.
R. Hasminskii. Stochastic stability of differential equations. Springer-Verlag Berlin Heidelberg, 2012.
J. C. Mattingly, A. M. Stuart, and M. V. Tretyakov. Construction of numerical time-average and stationary measures via Poisson equations. SIAM J. NUMER. ANAL., 48(2):552577, 2010.
P. Giesl. Construction of global Lyapunov functions using radial basis functions. Springer Berlin Heidelberg, 2007.
N. N. Bogoliubov and N. M. Krylov. La theorie generalie de la mesure dans son application a letude de systemes dynamiques de la mecanique non-lineaire. Ann. Math. II (in French), 38 (1):65113, 1937.
B. Leimkuhler and C. Matthews. Rational construction of stochastic numerical methods for molecular sampling. AMRX, 2013(1):3456, 2013.
D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. JMLR, 2003.
M. D. Hoffman, D. M. Blei, and F. Bach. Online learning for latent Dirichlet allocation. In NIPS, 2010.
Z. Gan, C. Chen, R. Henao, D. Carlson, and L. Carin. Scalable deep Poisson factor analysis for topic modeling. In ICML, 2015.
S. Patterson and Y. W. Teh. Stochastic gradient Riemannian Langevin dynamics on the probability simplex. In NIPS, 2013.
Z. Gan, R. Henao, D. Carlson, and L. Carin. Learning deep sigmoid belief networks with data augmentation. In AISTATS, 2015.
R. Salakhutdinov and I. Murray. On the quantitative analysis of deep belief networks. In ICML, 2008.
A. Mnih and K. Gregor. Neural variational inference and learning in belief networks. In ICML, 2014.
A. Debussche and E. Faou. Weak backward error analysis for SDEs. SIAM J. NUMER. ANAL., 50(3):17341752, 2012.
M. Kopec. Weak backward error analysis for overdamped Langevin processes. IMA J. NUMER. ANAL., 2014. 9
R. Albert. Scale-free networks in cell biology. Journal of cell science, 118(21):49474957, 2005.
C. Ambroise, J. Chiquet, C. Matias, et al. Inferring sparse gaussian graphical models with latent structure. Electronic Journal of Statistics, 3:205238, 2009.
N. Aronszajn. Theory of reproducing kernels. Transactions of the American mathematical society, pages 337404, 1950.
S. Canu and A. Smola. Kernel methods and the exponential family. Neurocomputing, 69(7):714720, 2006.
A. Defazio and T. S. Caetano. A convex formulation for learning scale-free networks via submodular relaxation. In Advances in Neural Information Processing Systems, pages 12501258, 2012.
J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 9(3):432441, 2008.
J. Friedman, T. Hastie, and R. Tibshirani. A note on the group lasso and a sparse group lasso. arXiv preprint arXiv:1001.0736, 2010.
K. Fukumizu, F. R. Bach, and A. Gretton. Statistical consistency of kernel canonical correlation analysis. The Journal of Machine Learning Research, 8:361383, 2007.
S. Geman and C. Graffigne. Markov random field image models and their applications to computer vision. In Proceedings of the International Congress of Mathematicians, volume 1, page 2, 1986.
A. Hyvarinen. Estimation of non-normalized statistical models by score matching. In Journal of Machine Learning Research, pages 695709, 2005.
A. Hyvarinen. Some extensions of score matching. Computational statistics & data analysis, 51(5):2499 2512, 2007.
Y. Jeon and Y. Lin. An effective method for high-dimensional log-density anova estimation, with application to nonparametric graphical model building. Statistica Sinica, 16(2):353, 2006.
R. Kindermann, J. L. Snell, et al. Markov random fields and their applications, volume 1. American Mathematical Society Providence, RI, 1980.
D. Koller and N. Friedman. Probabilistic graphical models: principles and techniques. MIT press, 2009.
Y. A. Kourmpetis, A. D. Van Dijk, M. C. Bink, R. C. van Ham, and C. J. ter Braak. Bayesian markov random field analysis for protein function prediction based on network data. PloS one, 5(2):e9293, 2010.
J. Lafferty, A. McCallum, and F. C. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. 2001.
S. Z. Li. Markov random field modeling in Image Analysis. 2011.
H. Liu, J. Lafferty, and L. Wasserman. The nonparanormal: Semiparametric estimation of high dimensional undirected graphs. The Journal of Machine Learning Research, 10:22952328, 2009.
Q. Liu and A. T. Ihler. Learning scale free networks by reweighted l1 regularization. In International Conference on Artificial Intelligence and Statistics, pages 4048, 2011.
C. D. Manning and H. Schutze. Foundations of statistical natural language processing. MIT press, 1999.
L. Meier, S. Van De Geer, and P. Buhlmann. The group lasso for logistic regression. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70(1):5371, 2008.
N. Meinshausen and P. Buhlmann. High-dimensional graphs and variable selection with the lasso. The Annals of Statistics, pages 14361462, 2006.
P. Ravikumar, M. J. Wainwright, and J. Lafferty. High-dimensional graphical model selection using l1regularized logistic regression. 2008.
P. Ravikumar, M. J. Wainwright, J. D. Lafferty, et al. High-dimensional ising model selection using 1-regularized logistic regression. The Annals of Statistics, 38(3):12871319, 2010.
R. T. Rockafellar. Convex analysis. Number 28. Princeton university press, 1970.
B. Sriperumbudur, K. Fukumizu, R. Kumar, A. Gretton, and A. Hyvarinen. Density estimation in infinite dimensional exponential families. arXiv preprint arXiv:1312.3516, 2013.
S. Sun, H. Wang, and J. Xu. Inferring block structure of graphical models in exponential families. In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, pages 939947, 2015.
Z. Wei and H. Li. A markov random field model for network-based analysis of genomic data. Bioinformatics, 23(12):15371544, 2007.
E. Yang, G. Allen, Z. Liu, and P. K. Ravikumar. Graphical models via generalized linear models. In Advances in Neural Information Processing Systems, pages 13581366, 2012.
M. Yuan and Y. Lin. Model selection and estimation in the gaussian graphical model. Biometrika, 94(1):1935, 2007.
T. Zhao, H. Liu, K. Roeder, J. Lafferty, and L. Wasserman. The huge package for high-dimensional undirected graph estimation in r. The Journal of Machine Learning Research, 13(1):10591062, 2012.  9
S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh. Vqa: Visual question answering. arXiv preprint arXiv:1505.00468, 2015.
J. P. Bigham, C. Jayant, H. Ji, G. Little, A. Miller, R. C. Miller, R. Miller, A. Tatarowicz, B. White, S. White, et al. Vizwiz: nearly real-time answers to visual questions. In ACM symposium on User interface software and technology, pages 333342, 2010.
L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Semantic image segmentation with deep convolutional nets and fully connected crfs. ICLR, 2015.
X. Chen and C. L. Zitnick. Learning a recurrent visual representation for image caption generation. In CVPR, 2015.
K. Cho, B. van Merrienboer, C. Gulcehre, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using rnn encoderdecoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell. Long-term recurrent convolutional networks for visual recognition and description. In CVPR, 2015.
J. L. Elman. Finding structure in time. Cognitive science, 14(2):179211, 1990.
H. Fang, S. Gupta, F. Iandola, R. Srivastava, L. Deng, P. Dollar, J. Gao, X. He, M. Mitchell, J. Platt, et al. From captions to visual concepts and back. In CVPR, 2015.
D. Geman, S. Geman, N. Hallonquist, and L. Younes. Visual turing test for computer vision systems. PNAS, 112(12):36183623, 2015.
R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.
M. Grubinger, P. Clough, H. Muller, and T. Deselaers. The iapr tc-12 benchmark: A new evaluation resource for visual information systems. In International Workshop OntoImage, pages 1323, 2006.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):17351780, 1997.
N. Kalchbrenner and P. Blunsom. Recurrent continuous translation models. In EMNLP, pages 17001709, 2013.
A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In CVPR, 2015.
R. Kiros, R. Salakhutdinov, and R. S. Zemel. Unifying visual-semantic embeddings with multimodal neural language models. TACL, 2015.
B. Klein, G. Lev, G. Sadeh, and L. Wolf. Fisher vectors derived from hybrid gaussian-laplacian mixture models for image annotation. arXiv preprint arXiv:1411.7399, 2014.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.
A. Lavie and A. Agarwal. Meteor: An automatic metric for mt evaluation with high levels of correlation with human judgements. In Workshop on Statistical Machine Translation, pages 228231. Association for Computational Linguistics, 2007.
R. Lebret, P. O. Pinheiro, and R. Collobert. Simple image description generator via a linear phrase-based approach. arXiv preprint arXiv:1412.8419, 2014.
Y. A. LeCun, L. Bottou, G. B. Orr, and K.-R. Muller. Efficient backprop. In Neural networks: Tricks of the trade, pages 948. 2012.
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick. Microsoft coco: Common objects in context. arXiv preprint arXiv:1405.0312, 2014.
M. Malinowski and M. Fritz. A multi-world approach to question answering about real-world scenes based on uncertain input. In Advances in Neural Information Processing Systems, pages 16821690, 2014.
M. Malinowski, M. Rohrbach, and M. Fritz. Ask your neurons: A neural-based approach to answering questions about images. arXiv preprint arXiv:1505.01121, 2015.
J. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and A. Yuille. Deep captioning with multimodal recurrent neural networks (m-rnn). In ICLR, 2015.
J. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and A. Yuille. Learning like a child: Fast novel visual concept learning from sentence descriptions of images. arXiv preprint arXiv:1504.06692, 2015.
J. Mao, W. Xu, Y. Yang, J. Wang, and A. L. Yuille. Explain images with multimodal recurrent neural networks. NIPS DeepLearning Workshop, 2014.
T. Mikolov, A. Joulin, S. Chopra, M. Mathieu, and M. Ranzato. Learning longer memory in recurrent neural networks. arXiv preprint arXiv:1412.7753, 2014.
T. Mikolov, M. Karafiat, L. Burget, J. Cernocky, and S. Khudanpur. Recurrent neural network based language model. In INTERSPEECH, pages 10451048, 2010.
T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS, pages 31113119, 2013.
V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In ICML, pages 807814, 2010.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evaluation of machine translation. In ACL, pages 311318, 2002.
M. Ren, R. Kiros, and R. Zemel. Image question answering: A visual semantic embedding model and a new dataset. arXiv preprint arXiv:1505.02074, 2015.
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge, 2014.
K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.
I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In NIPS, pages 31043112, 2014.
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. arXiv preprint arXiv:1409.4842, 2014.
K. Tu, M. Meng, M. W. Lee, T. E. Choe, and S.-C. Zhu. Joint video and text parsing for understanding events and answering queries. MultiMedia, IEEE, 21(2):4270, 2014.
A. M. Turing. Computing machinery and intelligence. Mind, pages 433460, 1950.
R. Vedantam, C. L. Zitnick, and D. Parikh. Cider: Consensus-based image description evaluation. In CVPR, 2015.
O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. In CVPR, 2015.
Z. Wu and M. Palmer. Verbs semantics and lexical selection. In ACL, pages 133138, 1994.
K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio. Show, attend and tell: Neural image caption generation with visual attention. arXiv preprint arXiv:1502.03044, 2015.
P. Young, A. Lai, M. Hodosh, and J. Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. In ACL, pages 479488, 2014.
J. Zhu, J. Mao, and A. L. Yuille. Learning from weakly supervised data by the expectation loss svm (e-svm) algorithm. In NIPS, pages 11251133, 2014.  9
A. Andoni and P. Indyk. Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. Commun. ACM, 51(1):117122, 2008.
L. Bottou. Large-scale machine learning with stochastic gradient descent. In COMPSTAT, pages 177186. Springer, 2010.
S. Dasgupta and K. Sinha. Randomized partition trees for nearest neighbor search. Algorithmica, 72(1):237263, 2015.
A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages 16461654, 2014.
R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, pages 315323, 2013.
J. Konecny and P. Richtarik. Semi-stochastic gradient descent methods. arXiv preprint arXiv:1312.1666, 2013.
H. Robbins and S. Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400407, 1951.
M. Schmidt. Convergence rate of stochastic gradient with constant step size. UBC Technical Report, 2014.
M. Schmidt, N. L. Roux, and F. Bach. Minimizing finite sums with the stochastic average gradient. arXiv preprint arXiv:1309.2388, 2013.
S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter. Pegasos: Primal estimated sub-gradient solver for SVM. Mathematical programming, 127(1):330, 2011.
S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss. The Journal of Machine Learning Research, 14(1):567599, 2013.  9
D.P. Bertsekas and J.N. Tsitsiklis. Neuro-dynamic programming (optimization and neural computation series, 3). Athena Scientific, 7:1523, 1996.
A.G. Barto, W. Powell, J. Si, and D.C. Wunsch. Handbook of learning and approximate dynamic programming. 2004.
W.H. Fleming. Exit probabilities and optimal stochastic control. Applied Math. Optim, 9:329346, 1971.
W. H. Fleming and H. M. Soner. Controlled Markov processes and viscosity solutions. Applications of mathematics. Springer, New York, 1st edition, 1993.
H. J. Kappen. Linear theory for control of nonlinear stochastic systems. Phys Rev Lett, 95:200201, 2005.
H. J. Kappen. Path integrals and symmetry breaking for optimal control theory. Journal of Statistical Mechanics: Theory and Experiment, 11:P11011, 2005.
H. J. Kappen. An introduction to stochastic control theory, path integrals and reinforcement learning. AIP Conference Proceedings, 887(1), 2007.
S. Thijssen and H. J. Kappen. 91:032104, Mar 2015.  Path integral control and state-dependent feedback.  Phys. Rev. E,
E. Todorov. Efficient computation of optimal actions. Proceedings of the national academy of sciences, 106(28):1147811483, 2009.
E. Theodorou, J. Buchli, and S. Schaal. A generalized path integral control approach to reinforcement learning. The Journal of Machine Learning Research, 11:31373181, 2010.
F. Stulp and O. Sigaud. Path integral policy improvement with covariance matrix adaptation. In Proceedings of the 29th International Conference on Machine Learning (ICML), pages 281288. ACM, 2012.
K. Rawlik, M. Toussaint, and S. Vijayakumar. Path integral control by reproducing kernel hilbert space embedding. In Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence, IJCAI13, pages 16281634, 2013.
Y. Pan and E. Theodorou. Nonparametric infinite horizon kullback-leibler stochastic control. In 2014 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL), pages 18. IEEE, 2014.
V. Gomez, H.J. Kappen, J. Peters, and G. Neumann. Policy search for path integral control. In Machine Learning and Knowledge Discovery in Databases, pages 482497. Springer, 2014.
K. Dvijotham and E Todorov. Linearly solvable optimal control. Reinforcement learning and approximate dynamic programming for feedback control, pages 119141, 2012.
M.P. Deisenroth, G. Neumann, and J. Peters. A survey on policy search for robotics. Foundations and Trends in Robotics, 2(1-2):1142, 2013.
M. Deisenroth, D. Fox, and C. Rasmussen. Gaussian processes for data-efficient learning in robotics and control. IEEE Transsactions on Pattern Analysis and Machine Intelligence, 27:7590, 2015.
E. Theodorou and E. Todorov. Relative entropy and free energy dualities: Connections to path integral and kl control. In 51st IEEE Conference on Decision and Control, pages 14661473, 2012.
Y. Pan and E. Theodorou. Probabilistic differential dynamic programming. In Advances in Neural Information Processing Systems (NIPS), pages 19071915, 2014.
S. Levine and P. Abbeel. Learning neural network policies with guided policy search under unknown dynamics. In Advances in Neural Information Processing Systems (NIPS), pages 10711079, 2014.
S. Levine and V. Koltun. Learning complex neural network policies with trajectory optimization. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 829837, 2014.
J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel. Trust region policy optimization. arXiv preprint arXiv:1502.05477, 2015.
P. Hennig. Optimal reinforcement learning for gaussian systems. In Advances in Neural Information Processing Systems (NIPS), pages 325333, 2011.
J. Quinonero Candela, A. Girard, J. Larsen, and C. E. Rasmussen. Propagation of uncertainty in bayesian kernel models-application to multiple-step ahead forecasting. In IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003.
C.K.I Williams and C.E. Rasmussen. Gaussian processes for machine learning. MIT Press, 2006.
E. Todorov. Compositionality of optimal control laws. In Advances in Neural Information Processing Systems (NIPS), pages 18561864, 2009.
M.P. Deisenroth, P. Englert, J. Peters, and D. Fox. Multi-task policy search for robotics. In Proceedings of 2014 IEEE International Conference on Robotics and Automation (ICRA), 2014.  9
Sungjin Ahn, Babak Shahbaba, and Max Welling. Distributed stochastic gradient mcmc. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 10441052, 2014.
Remi Bardenet, Arnaud Doucet, and Chris Holmes. Towards scaling up markov chain monte carlo: an adaptive subsampling approach. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 405413, 2014.
Matthew D. Hoffman, David M. Blei, Chong Wang, and John William Paisley. Stochastic variational inference. Journal of Machine Learning Research, 14(1):13031347, 2013.
Jose Miguel Hernandez-Lobato and Ryan P. Adams. Probabilistic backpropagation for scalable learning of bayesian neural networks. arXiv:1502.05336, 2015.
Andrew Gelman, Aki Vehtari, Pasi Jylnki, Christian Robert, Nicolas Chopin, and John P. Cunningham. Expectation propagation as a way of life. arXiv:1412.4869, 2014.
Minjie Xu, Balaji Lakshminarayanan, Yee Whye Teh, Jun Zhu, and Bo Zhang. Distributed bayesian posterior sampling via moment sharing. In NIPS, 2014.
Thomas P. Minka. Expectation propagation for approximate Bayesian inference. In Uncertainty in Artificial Intelligence, volume 17, pages 362369, 2001.
Manfred Opper and Ole Winther. Expectation consistent approximate inference. The Journal of Machine Learning Research, 6:21772204, 2005.
Malte Kuss and Carl Edward Rasmussen. Assessing approximate inference for binary gaussian process classification. The Journal of Machine Learning Research, 6:16791704, 2005.
Simon Barthelme and Nicolas Chopin. Expectation propagation for likelihood-free inference. Journal of the American Statistical Association, 109(505):315333, 2014.
John P Cunningham, Philipp Hennig, and Simon Lacoste-Julien. Gaussian probabilities and expectation propagation. arXiv preprint arXiv:1111.6832, 2011.
Thomas P. Minka. Power EP. Technical Report MSR-TR-2004-149, Microsoft Research, Cambridge, 2004.
John M Winn and Christopher M Bishop. Variational message passing. In Journal of Machine Learning Research, pages 661694, 2005.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to variational methods for graphical models. Machine learning, 37(2):183233, 1999.
Matthew James Beal. Variational algorithms for approximate Bayesian inference. PhD thesis, University of London, 2003.
Richard E. Turner and Maneesh Sahani. Two problems with variational expectation maximisation for time-series models. In D. Barber, T. Cemgil, and S. Chiappa, editors, Bayesian Time series models, chapter 5, pages 109130. Cambridge University Press, 2011.
Richard E. Turner and Maneesh Sahani. Probabilistic amplitude and frequency demodulation. In J. Shawe-Taylor, R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 981989. 2011.
Ralf Herbrich, Tom Minka, and Thore Graepel. Trueskill: A bayesian skill rating system. In Advances in Neural Information Processing Systems, pages 569576, 2006.
Peter S. Maybeck. Stochastic models, estimation and control. Academic Press, 1982.
Yuan Qi, Ahmed H Abdel-Gawad, and Thomas P Minka. Sparse-posterior gaussian processes for general likelihoods. In Uncertainty and Artificial Intelligence (UAI), 2010.
Shun-ichi Amari and Hiroshi Nagaoka. Methods of information geometry, volume 191. Oxford University Press, 2000.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400407, 1951.
Guillaume Dehaene and Simon Barthelme. arXiv:1503.08060, 2015.  Expectation propagation in the large-data limit.
Thomas Minka. Divergence measures and message passing. Technical Report MSR-TR-2005-173, Microsoft Research, Cambridge, 2005.
Matthew D Hoffman and Andrew Gelman. The no-u-turn sampler: Adaptively setting path lengths in hamiltonian monte carlo. The Journal of Machine Learning Research, 15(1):15931623, 2014.  9
A. Globerson and T. S. Jaakkola. Fixing max-product: Convergent message passing algorithms for MAP LP-relaxations. In Proc. 21st Neural Information Processing Systems (NIPS), Vancouver, B.C., Canada, 2007.
T. Werner. A linear programming approach to max-sum problem: A review. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 29(7):11651179, 2007.
N. Ruozzi and S. Tatikonda. Message-passing algorithms: Reparameterizations and splittings. IEEE Transactions on Information Theory, 59(9):58605881, Sept. 2013.
Y. Wald and A. Globerson. Tightness results for local consistency relaxations in continuous MRFs. In Proc. 30th Uncertainty in Artifical Intelligence (UAI), Quebec City, Quebec, Canada, 2014.
T. P. Minka. Expectation propagation for approximate Bayesian inference. In Proceedings of the Seventeenth conference on Uncertainty in Artificial Intelligence (UAI), pages 362369, 2001.
N. Ruozzi and S. Tatikonda. Message-passing algorithms for quadratic minimization. Journal of Machine Learning Research, 14:22872314, 2013.
D. M. Malioutov, J. K. Johnson, and A. S. Willsky. Walk-sums and belief propagation in Gaussian graphical models. Journal of Machine Learning Research, 7:20312064, 2006.
C. C. Moallemi and B. Van Roy. Convergence of min-sum message passing for quadratic optimization. Information Theory, IEEE Transactions on, 55(5):2413 2423, May 2009.
C. C. Moallemi and B. Van Roy. Convergence of min-sum message-passing for convex optimization. Information Theory, IEEE Transactions on, 56(4):2041 2050, April 2010.
M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. R in Machine Learning, 1(1-2):1305, 2008. Foundations and Trends
M. Bayati, C. Borgs, J. Chayes, and R. Zecchina. Belief propagation for weighted b-matchings on arbitrary graphs and its relation to linear programs with integer solutions. SIAM Journal on Discrete Mathematics, 25(2):9891011, 2011.
V. Kolmogorov and R. Zabih. What energy functions can be minimized via graph cuts? In Computer VisionECCV 2002, pages 6581. Springer, 2002.
S. Sanghavi, D. M. Malioutov, and A. S. Willsky. Belief propagation and LP relaxation for weighted matching in general graphs. Information Theory, IEEE Transactions on, 57(4):2203 2212, April 2011.
S. Sanghavi, D. Shah, and A. S. Willsky. Message passing for maximum weight independent set. Information Theory, IEEE Transactions on, 55(11):48224834, Nov. 2009.
M. J. Wainwright, T. S. Jaakkola, and A. S. Willsky. MAP estimation via agreement on (hyper)trees: Message-passing and linear programming. Information Theory, IEEE Transactions on, 51(11):3697 3717, Nov. 2005.
David Sontag, Talya Meltzer, Amir Globerson, Yair Weiss, and Tommi Jaakkola. Tightening LP relaxations for MAP using message-passing. In 24th Conference in Uncertainty in Artificial Intelligence, pages 503510. AUAI Press, 2008.
P. O. Vontobel. Counting in graph covers: A combinatorial characterization of the Bethe entropy function. Information Theory, IEEE Transactions on, Jan. 2013.
P. O. Vontobel and R. Koetter. Graph-cover decoding and finite-length analysis of message-passing iterative decoding of LDPC codes. CoRR, abs/cs/0512078, 2005.
S. Iwata, L. Fleischer, and S. Fujishige. A strongly polynomial-time algorithm for minimizing submodular functions. Journal of The ACM, 1999.
A. Schrijver. A combinatorial algorithm minimizing submodular functions in strongly polynomial time. Journal of Combinatorial Theory, Series B, 80(2):346  355, 2000.
S. Karlin and Y. Rinott. Classes of orderings of measures and related correlation inequalities. i. multivariate totally positive distributions. Journal of Multivariate Analysis, 10(4):467  498, 1980.
Y. Weiss and W. T. Freeman. Correctness of belief propagation in Gaussian graphical models of arbitrary topology. Neural Comput., 13(10):21732200, Oct. 2001.
D. M. Malioutov. Approximate inference in Gaussian graphical models. Ph.D. thesis, EECS, MIT, 2008.  9
A. Anandkumar, D. P. Foster, D. Hsu, S. M. Kakade, and Y.-K. Liu. Two svds suffice: Spectral decompositions for probabilistic topic modeling and latent dirichlet allocation. CoRR, abs/1204.6703, 2012.
R. Arora, A. Cotter, and N. Srebro. Stochastic optimization of pca with capped msg. In Advances in Neural Information Processing Systems, pages 18151823, 2013.
A. Balsubramani, S. Dasgupta, and Y. Freund. The fast convergence of incremental pca. In Advances in Neural Information Processing Systems, pages 31743182, 2013.
T. T. Cai and H. H. Zhou. Optimal rates of convergence for sparse covariance matrix estimation. The Annals of Statistics, 40(5):23892420, 2012.
T.-J. Chin and D. Suter. Incremental kernel principal component analysis. IEEE Transactions on Image Processing, 16(6):16621674, 2007.
B. Dai, B. Xie, N. He, Y. Liang, A. Raj, M.-F. F. Balcan, and L. Song. Scalable kernel methods via doubly stochastic gradients. In Advances in Neural Information Processing Systems, pages 30413049, 2014.
M. Hardt and E. Price. The noisy power method: A meta algorithm with applications. In Advances in Neural Information Processing Systems, pages 28612869, 2014.
P. Honeine. Online kernel principal component analysis: A reduced-order model. IEEE Trans. Pattern Anal. Mach. Intell., 34(9):18141826, 2012.
K. Kim, M. O. Franz, and B. Scholkopf. Iterative kernel principal component analysis for image modeling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(9):13511366, 2005.
M. Kim and V. Pavlovic. Covariance operator based dimensionality reduction with extension to semisupervised settings. In International Conference on Artificial Intelligence and Statistics, pages 280287, 2009.
Q. Le, T. Sarlos, and A. J. Smola. Fastfood  computing hilbert space expansions in loglinear time. In International Conference on Machine Learning, 2013.
D. Lopez-Paz, S. Sra, A. Smola, Z. Ghahramani, and B. Scholkopf. Randomized nonlinear component analysis. In International Conference on Machine Learning (ICML), 2014.
F. Meier, P. Hennig, and S. Schaal. Incremental local gaussian regression. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 972980. Curran Associates, Inc., 2014.
G. Montavon, K. Hansen, S. Fazli, M. Rupp, F. Biegler, A. Ziehe, A. Tkatchenko, A. von Lilienfeld, and K.-R. Muller. Learning invariant representations of molecules for atomization energy prediction. In Neural Information Processing Systems, pages 449457, 2012.
E. Oja. A simplified neuron model as a principal component analyzer. J. Math. Biology, 15:267273, 1982.
E. Oja. Subspace methods of pattern recognition. John Wiley and Sons, New York, 1983.
A. Rahimi and B. Recht. Random features for large-scale kernel machines. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20. MIT Press, Cambridge, MA, 2008.
A. Rahimi and B. Recht. Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning. In Neural Information Processing Systems, 2009.
T. D. Sanger. Optimal unsupervised learning in a single-layer linear feedforward network. Neural Networks, 2:459473, 1989.
N. N. Schraudolph, S. Gunter, and S. V. N. Vishwanathan. Fast iterative kernel PCA. In B. Scholkopf, J. Platt, and T. Hofmann, editors, Advances in Neural Information Processing Systems 19, Cambridge MA, June 2007. MIT Press.
O. Shamir. A stochastic pca algorithm with an exponential convergence rate. arXiv:1409.2848, 2014.  arXiv preprint
L. Song, A. Anamdakumar, B. Dai, and B. Xie. Nonparametric estimation of multi-view latent variable models. In International Conference on Machine Learning (ICML), 2014.
R. Vershynin. How close is the sample covariance matrix to the actual covariance matrix? Journal of Theoretical Probability, 25(3):655686, 2012.
C. K. I. Williams and M. Seeger. The effect of the input density distribution on kernel-based classifiers. In P. Langley, editor, Proc. Intl. Conf. Machine Learning, pages 11591166, San Francisco, California, 2000. Morgan Kaufmann Publishers.  9
Raef Bassily, Adam Smith, Thomas Steinke, and Jonathan Ullman. More general queries and less generalization error in adaptive data analysis. CoRR, abs/1503.04843, 2015.
Avrim Blum and Moritz Hardt. The ladder: A reliable leaderboard for machine learning competitions. CoRR, abs/1502.04585, 2015.
Olivier Bousquet and Andr Elisseeff. Stability and generalization. JMLR, 2:499526, 2002.
Gavin C. Cawley and Nicola L. C. Talbot. On over-fitting in model selection and subsequent selection bias in performance evaluation. Journal of Machine Learning Research, 11:20792107, 2010.
Chuong B. Do, Chuan-Sheng Foo, and Andrew Y. Ng. Efficient multiple hyperparameter learning for log-linear models. In NIPS, pages 377384, 2007.
Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth. Preserving statistical validity in adaptive data analysis. CoRR, abs/1411.2664, 2014. Extended abstract in STOC 2015.
Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth. Generalization in adaptive data analysis and holdout reuse. CoRR, abs/1506, 2015.
Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data, ourselves: Privacy via distributed noise generation. In EUROCRYPT, pages 486503, 2006.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography, pages 265284. Springer, 2006.
David A. Freedman. A note on screening regression equations. The American Statistician, 37(2):152155, 1983.
Moritz Hardt and Jonathan Ullman. Preventing false discovery in interactive data analysis is hard. In FOCS, pages 454463, 2014.
Trevor Hastie, Robert Tibshirani, and Jerome H. Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer series in statistics. Springer, 2009.
John Langford. Clever methods of overfitting. http://hunch.net/?p=22, 2005.
Andrew Y. Ng. Preventing "overfitting" of cross-validation data. In ICML, pages 245253, 1997.
Kobbi Nissim and Uri Stemmer. On the generalization properties of differential privacy. CoRR, abs/1504.05800, 2015.
Tomaso Poggio, Ryan Rifkin, Sayan Mukherjee, and Partha Niyogi. General conditions for predictivity in learning theory. Nature, 428(6981):419422, 2004.
R. Bharat Rao and Glenn Fung. On the dangers of cross-validation. an experimental evaluation. In International Conference on Data Mining, pages 588596. SIAM, 2008.
Juha Reunanen. Overfitting in making comparisons between variable selection methods. Journal of Machine Learning Research, 3:13711382, 2003.
Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability and uniform convergence. The Journal of Machine Learning Research, 11:26352670, 2010.
Thomas Steinke and Jonathan Ullman. Interactive fingerprinting codes and the hardness of preventing false discovery. arXiv preprint arXiv:1410.1228, 2014.
Yu-Xiang Wang, Jing Lei, and Stephen E. Fienberg. Learning with differential privacy: Stability, learnability and the sufficiency and necessity of ERM principle. CoRR, abs/1502.06309, 2015.  9
Jacob Abernethy, Sindhu Kutty, Sebastien Lahaie, and Rahul Sami. Information aggregation in
exponential family markets. In Proc. ACM Conference on Economics and Computation, pages
395412. ACM, 2014.
Alina Beygelzimer, John Langford, and David M Pennock. Learning performance of prediction
markets with Kelly bettors. In Proc. AAMAS, pages 13171318, 2012.
Robert F. Bordley. A multiplicative formula for aggregating probability assessments. Management
Science, 28(10):11371148, 1982.
Stephane Boucheron, Gabor Lugosi, and Olivier Bousquet. Concentration inequalities. In Advanced
Lectures on Machine Learning, pages 208240. Springer, 2004.
Aseem Brahma, Mithun Chakraborty, Sanmay Das, Allen Lavoie, and Malik Magdon-Ismail. A
Bayesian market maker. In Proc. ACM Conference on Electronic Commerce, pages 215232.
Yiling Chen and David M. Pennock. A utility framework for bounded-loss market makers. In Proc.
UAI-07, 2007.
Yiling Chen and Jennifer Wortman Vaughan. A new understanding of prediction markets via noregret learning. In Proc. ACM Conference on Electronic Commerce, pages 189198. ACM, 2010.
Dave Cliff and Janet Bruten. Zero is not enough: On the lower limit of agent intelligence for
continuous double auction markets. Technical report, HPL-97-141, Hewlett-Packard Laboratories
Bristol. 105, 1997.
Bo Cowgill and Eric Zitzewitz. Corporate prediction markets: Evidence from Google, Ford, and
Koch industries. Technical report, Working paper, 2013.
J. Doyne Farmer, Paolo Patelli, and Ilija I. Zovko. The predictive power of zero intelligence in
financial markets. PNAS, 102(6):22542259, 2005.
Rafael M. Frongillo, Nicolas D. Penna, and Mark D. Reid. Interpreting prediction markets: A
stochastic approach. In Proc. NIPS, pages 32663274, 2012.
Ashutosh Garg, T.S. Jayram, Shivakumar Vaithyanathan, and Huaiyu Zhu. Generalized opinion
pooling. In Proc. 8th Intl. Symp. on Artificial Intelligence and Mathematics. Citeseer, 2004.
Christian Genest and James V. Zidek. Combining probability distributions: A critique and an annotated bibliography. Statistical Science, pages 114135, 1986.
Tilmann Gneiting and Adrian E. Raftery. Strictly proper scoring rules, prediction, and estimation.
Journal of the American Statistical Association, 102(477):359378, 2007.
Robin D. Hanson. Combinatorial information market design. Information Systems Frontiers, 5(1):
107119, 2003.
Jinli Hu and Amos J. Storkey. Multi-period trading prediction markets with connections to machine
learning. Proc. ICML, 2014.
Krishnamurthy Iyer, Ramesh Johari, and Ciamac C. Moallemi. Information aggregation and allocative efficiency in smooth markets. Management Science, 60(10):25092524, 2014.
Andreu Mas-Colell, Michael D. Whinston, and Jerry R. Green. Microeconomic theory, volume 1.
New York: Oxford University Press, 1995.
Jono Millin, Krzysztof Geras, and Amos J. Storkey. Isoelastic agents and wealth updates in machine
learning markets. In Proc. ICML, pages 18151822, 2012.
Michael Ostrovsky. Information aggregation in dynamic markets with strategic traders. Econometrica, 80(6):25952647, 2012.
David M. Pennock. Aggregating probabilistic beliefs: Market mechanisms and graphical representations. PhD thesis, The University of Michigan, 1999.
David M. Pennock and Lirong Xia. Price updating in combinatorial prediction markets with
Bayesian networks. In Proc. UAI, pages 581588, 2011.
Rajiv Sethi and Jennifer Wortman Vaughan. Belief aggregation with automated market makers.
Available at SSRN 2288670, 2013.
Amos J. Storkey, Zhanxing Zhu, and Jinli Hu. A continuum from mixtures to products: Aggregation under bias. Preprint made available for the ICML Workshop on Divergence Methods for
Probabilistic Inference, 2014.
James Surowiecki. The wisdom of crowds. Anchor, 2005.
Justin Wolfers and Eric Zitzewitz. Prediction markets. J. Econ. Perspectives, 18(2):107126, 2004.
J. Zhu, S. Rosset, T. Hastie, and R. Tibshirani. 1-norm support vector machines. NIPS, 2004.
D. Koller and N. Friedman. Probabilistic graphical models: principles and techniques. MIT press, 2009.
N. Gillis and R. Luce. Robust near-separable nonnegative matrix factorization using linear optimization. JMLR, 2014.
A. Nellore and R. Ward. Recovery guarantees for exemplar-based clustering. arXiv., 2013.
I. Yen, X. Lin, K. Zhong, P. Ravikumar, and I. Dhillon. A convex exemplar-based approach to MADBayes Dirichlet process mixture models. In ICML, 2015.
M. Yuan. High dimensional inverse covariance matrix estimation via linear programming. JMLR, 2010.
D. Bello and G. Riano. Linear programming solvers for Markov decision processes. In Systems and Information Engineering Design Symposium, pages 9095, 2006.
T. Joachims. Training linear svms in linear time. In KDD. ACM, 2006.
C. Hsieh, K. Chang, C. Lin, S.S. Keerthi, and S. Sundararajan. A dual coordinate descent method for large-scale linear SVM. In ICML, volume 307. ACM, 2008.
G. Yuan, C. Hsieh K. Chang, and C. Lin. A comparison of optimization methods and software for largescale l1-regularized linear classification. JMLR, 11, 2010.
J. Nocedal and S.J. Wright. Numerical Optimization. Springer, 2006.
J. Gondzio. Interior point methods 25 years later. EJOR, 2012.
J. Gondzio. Matrix-free interior point method. Computational Optimization and Applications, 2012.
V.Eleuterio and D.Lucia. Finding approximate solutions for large scale linear programs. Thesis, 2009.
Evtushenko, Yu. G, Golikov, AI, and N. Mollaverdy. Augmented lagrangian method for large-scale linear programming problems. Optimization Methods and Software, 20(4-5):515524, 2005.
F. Delbos and J.C. Gilbert. Global linear convergence of an augmented lagrangian algorithm for solving convex quadratic optimization problems. 2003.
O. Guler. Augmented lagrangian algorithms for linear programming. Journal of optimization theory and applications, 75(3):445470, 1992.
J. More J and G. Toraldo. On the solution of large quadratic programming problems with bound constraints. SIAM Journal on Optimization, 1(1):93113, 1991.
M. Hong and Z. Luo. On linear convergence of alternating direction method of multipliers. arXiv, 2012.
H. Wang, A. Banerjee, and Z. Luo. Parallel direction method of multipliers. In NIPS, 2014.
C.Chen, B.He, Y.Ye, and X.Yuan. The direct extension of admm for multi-block convex minimization problems is not necessarily convergent. Mathematical Programming, 2014.
I.Dhillon, P.Ravikumar, and A.Tewari. Nearest neighbor based greedy coordinate descent. In NIPS, 2011.
I. Yen, C. Chang, T. Lin, S. Lin, and S. Lin. Indexed block coordinate descent for large-scale linear classification with limited memory. In KDD. ACM, 2013.
I. Yen, S. Lin, and S. Lin. A dual-augmented block minimization framework for learning with limited memory. In NIPS, 2015.
K. Zhong, I. Yen, I. Dhillon, and P. Ravikumar. Proximal quasi-Newton for computationally intensive l1-regularized m-estimators. In NIPS, 2014.
P. Richtarik and M. Takac. Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function. Mathematical Programming, 144(1-2):138, 2014.
Y. Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341362, 2012.
P. Wang and C. Lin. Iteration complexity of feasible descent methods for convex optimization. The Journal of Machine Learning Research, 15(1):15231548, 2014.
I. Yen, C. Hsieh, P. Ravikumar, and I.S. Dhillon. Constant nullspace strong convexity and fast convergence of proximal methods under high-dimensional settings. In NIPS, 2014.
B. Meindl and M. Templ. Analysis of commercial and free and open source solvers for linear optimization problems. Eurostat and Statistics Netherlands, 2012.
A.J. Hoffman. On approximate solutions of systems of linear inequalities. Journal of Research of the National Bureau of Standards, 49(4):263265, 1952.
A. Rahimi and B. Recht. Random features for large-scale kernel machines. In NIPS, 2007.
I. Yen, T. Lin, S. Lin, P. Ravikumar, and I. Dhillon. Sparse random feature algorithm as coordinate descent in Hilbert space. In NIPS, 2014.  9
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, 2012.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. arXiv:1409.4842
, September 2014.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv:1409.1556
, September 2014.
DC Ciresan, Ueli Meier, Jonathan Masci, Luca M Gambardella, and Jurgen Schmidhuber. Flexible, high performance convolutional neural networks for image classification. In IJCAI, 2011.
Dan Ciresan, Ueli Meier, and Jurgen Schmidhuber. Multi-column deep neural networks for image classification. In IEEE Conference on Computer Vision and Pattern Recognition, 2012.
Dong Yu, Michael L. Seltzer, Jinyu Li, Jui-Ting Huang, and Frank Seide. Feature learning in deep neural networks-studies on speech recognition tasks. arXiv preprint arXiv:1301.3605, 2013.
Sepp Hochreiter and Jurgen Schmidhuber. Bridging long time lags by weight guessing and long shortterm memory. Spatiotemporal models in biological and artificial systems, 37:6572, 1996.
Johan Hastad. Computational limitations of small-depth circuits. MIT press, 1987.
Johan Hastad and Mikael Goldmann. On the power of small-depth threshold circuits. Computational Complexity, 1(2):113129, 1991.
Monica Bianchini and Franco Scarselli. On the complexity of neural network classifiers: A comparison between shallow and deep architectures. IEEE Transactions on Neural Networks, 2014.  8
Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In Advances in Neural Information Processing Systems. 2014.
James Martens and Venkatesh Medabalimi. On the expressive efficiency of sum product networks. arXiv:1411.7717
, November 2014.
James Martens and Ilya Sutskever. Training deep and recurrent networks with hessian-free optimization. Neural Networks: Tricks of the Trade, pages 158, 2012.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. pages 11391147, 2013.
Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Advances in Neural Information Processing Systems 27, pages 29332941. 2014.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics, pages 249256, 2010.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. arXiv:1502.01852
, February 2015.
David Sussillo and L. F. Abbott. Random walk initialization for training very deep feedforward networks. arXiv:1412.6558
, December 2014.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv:1312.6120
, December 2013.
Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout networks. arXiv:1302.4389
, February 2013.
Rupesh K. Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino Gomez, and Jurgen Schmidhuber. Compete to compute. In Advances in Neural Information Processing Systems, pages 23102318, 2013.
Tapani Raiko, Harri Valpola, and Yann LeCun. Deep learning made easier by linear transformations in perceptrons. In International Conference on Artificial Intelligence and Statistics, pages 924932, 2012.
Alex Graves. Generating sequences with recurrent neural networks. arXiv:1308.0850, 2013.
Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeply-supervised nets. pages 562570, 2015.
Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. FitNets: Hints for thin deep nets. arXiv:1412.6550
, December 2014.
Jurgen Schmidhuber. Learning complex, extended sequences using the principle of history compression. Neural Computation, 4(2):234242, March 1992.
Geoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief nets. Neural computation, 18(7):15271554, 2006.
Sepp Hochreiter. Untersuchungen zu dynamischen neuronalen Netzen. Masters thesis, Technische Universitat Munchen, Munchen, 1991.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735 1780, November 1997.
Felix A. Gers, Jurgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction with LSTM. In ICANN, volume 2, pages 850855, 1999.
Rupesh Kumar Srivastava, Klaus Greff, and Jurgen Schmidhuber. Highway networks. arXiv:1505.00387
, May 2015.
Nal Kalchbrenner, Ivo Danihelka, and Alex Graves. Grid long Short-Term memory. arXiv:1507.01526
, July 2015.
Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv:1408.5093
Benjamin Graham. Spatially-sparse convolutional neural networks. arXiv:1409.6070, September 2014.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv:1312.4400, 2014.
Marijn F Stollenga, Jonathan Masci, Faustino Gomez, and Jurgen Schmidhuber. Deep networks with internal selective attention through feedback connections. In NIPS. 2014.
Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. arXiv:1412.6806
, December 2014.
Rupesh Kumar Srivastava, Jonathan Masci, Faustino Gomez, and Jurgen Schmidhuber. Understanding locally competitive networks. In International Conference on Learning Representations, 2015.  9
I. Kononenko. Machine Learning for Medical Diagnosis: History, State of the Art and Perspective. Artificial Intelligence in Medicine, 23(1):89109, 2001.
S. Saria, A. K. Rajani, J. Gould, D. L. Koller, and A. A. Penn. Integration of Early Physiological Responses Predicts Later Illness Severity in Preterm Infants. Science Translational Medicine, 2(48):48ra65, 2010.  8
T. C. Bailey, Y. Chen, Y. Mao, C. Lu, G. Hackmann, S. T. Micek, K. M. Heard, K. M. Faulkner, and M. H. Kollef. A Trial of a Real-Time Alert for Clinical Deterioration in Patients Hospitalized on General Medical Wards. Journal of Hospital Medicine, 8(5):236242, 2013.
J. Shargorodsky, S. G. Curhan, G. C. Curhan, and R. Eavey. Change in Prevalence of Hearing Loss in US Adolescents. Journal of the American Medical Association, 304(7):772778, 2010.
R. Carhart and J. Jerger. Preferred Method for Clinical Determination of Pure-Tone Thresholds. Journal of Speech and Hearing Disorders, 24(4):330345, 1959.
W. Hughson and H. Westlake. Manual for program outline for rehabilitation of aural casualties both military and civilian. Transactions of the American Academy of Ophthalmology and Otolaryngology, 48 (Supplement):115, 1944.
M. Don, J. J. Eggermont, and D. E. Brackmann. Reconstruction of the audiogram using brain stem responses and high-pass noise masking. Annals of Otology, Rhinology and Laryngology., 88(3 Part 2, Supplement 57):120, 1979.
J. R. Gardner, X. Song, K. Q. Weinberger, D. Barbour, and J. P. Cunningham. Psychophysical Detection Testing with Bayesian Active Learning. In UAI, 2015.
D. J. MacKay. Information Theory, Inference, and Learning Algorithms. Cambridge University Press, 2003.
N. Houlsby, F. Huszar, Z. Ghahramani, and J. M. Hernandez-Lobato. Collaborative Gaussian Processes for Preference Learning. In NIPS, pages 20962104, 2012.
R. Garnett, M. A. Osborne, and P. Hennig. Active Learning of Linear Embeddings for Gaussian Processes. In UAI, pages 230239, 2014.
J. M. Hernandez-Lobato, M. W. Hoffman, and Z. Ghahramani. Predictive Entropy Search for Efficient Global Optimization of Black-box Functions. In NIPS, pages 918926, 2014.
N. Houlsby, J. M. Hernandez-Lobato, and Z. Ghahramani. Cold-start Active Learning with Robust Ordinal Matrix Factorization. In ICML, pages 766774, 2014.
A. Ali, R. Caruana, and A. Kapoor. Active Learning with Model Selection. In AAAI, 2014.
J. Kulick, R. Lieck, and M. Toussaint. Active Learning of Hyperparameters: An Expected Cross Entropy Criterion for Active Model Selection. CoRR, abs/1409.7552, 2014.
C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.
D. Duvenaud. Automatic Model Construction with Gaussian Processes. PhD thesis, Computational and Biological Learning Laboratory, University of Cambridge, 2014.
D. Duvenaud, J. R. Lloyd, R. Grosse, J. B. Tenenbaum, and Z. Ghahramani. Structure Discovery in Nonparametric Regression through Compositional Kernel Search. In ICML, pages 11661174, 2013.
A. G. Wilson, E. Gilboa, A. Nehorai, and J. P. Cunningham. Fast Kernel Learning for Multidimensional Pattern Extrapolation. In NIPS, pages 36263634, 2014.
C. Williams and C. Rasmussen. Gaussian processes for regression. In NIPS, 1996.
A. E. Raftery. Approximate Bayes Factors and Accounting for Model Uncertainty in Generalised Linear Models. Biometrika, 83(2):251266, 1996.
J. Kuha. AIC and BIC: Comparisons of Assumptions and Performance. Sociological Methods and Research, 33(2):188229, 2004.
G. Schwarz. Estimating the Dimension of a Model. Annals of Statistics, 6(2):461464, 1978.
K. P. Murphy. Machine Learning: A Probabilistic Perspective. MIT Press, 2012.
M. Kuss and C. E. Rasmussen. Assessing Approximate Inference for Binary Gaussian Process Classification. Journal of Machine Learning Research, 6:16791704, 2005.
T. P. Minka. Expectation Propagation for Approximate Bayesian Inference. In UAI, pages 362369, 2001.
D. McBride and S. Williams. Audiometric notch as a sign of noise induced hearing loss. Occupational Environmental Medicine, 58(1):4651, 2001.
D. I. Nelson, R. Y. Nelson, M. Concha-Barrientos, and M. Fingerhut. The global burden of occupational noise-induced hearing loss. American Journal of Industrial Medicine, 48(6):446458, 2005.  9
Andrieu, C., Doucet, A., and Holenstein, R. (2010). Particle Markov chain Monte Carlo methods.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(3):269342.
Beal, M. J., Ghahramani, Z., and Rasmussen, C. E. (2001). The infinite hidden Markov model. In
Advances in neural information processing systems, pages 577584.
Bishop, C. M. (2006). Pattern recognition and machine learning, volume 4. Springer New York.
Fox, E. B., Sudderth, E. B., Jordan, M. I., and Willsky, A. S. (2008). An HDPHMM for systems
with state persistence. In Proceedings of the 25th international conference on Machine learning,
pages 312319. ACM.
Lindsten, F., Jordan, M. I., and Schon, T. B. (2014). Particle Gibbs with ancestor sampling. The
Journal of Machine Learning Research, 15(1):21452184.
Neal, R. M. (2000). Markov chain sampling methods for Dirichlet process mixture models. Journal
of Computational and Graphical Statistics, 9:249265.
Palla, K., Knowles, D. A., and Ghahramani, Z. (2014). A reversible infinite hmm using normalised
random measures. arXiv preprint arXiv:1403.4206.
Rabiner, L. (1989). A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257286.
Rosenstein, J. K., Ramakrishnan, S., Roseman, J., and Shepard, K. L. (2013). Single ion channel
recordings with cmos-anchored lipid membranes. Nano letters, 13(6):26822686.
Scott, S. L. (2002). Bayesian methods for hidden Markov models. Journal of the American Statistical Association, 97(457).
Teh, Y. W., Jordan, M. I., Beal, M. J., and Blei, D. M. (2006). Hierarchical Dirichlet processes.
Journal of the American Statistical Association, 101(476):15661581.
Van Gael, J., Saatci, Y., Teh, Y. W., and Ghahramani, Z. (2008). Beam sampling for the infinite
hidden Markov model. In Proceedings of the International Conference on Machine Learning,
The Alzheimers Disease Neuroimaging Initiative, https://ida.loni.usc.edu/
Allassonniere, S., Kuhn, E., Trouve, A.: Construction of bayesian deformable models via a stochastic approximation algorithm: a convergence study. Bernoulli 16(3), 641678 (2010)
Braak, H., Braak, E.: Staging of alzheimers disease-related neurofibrillary changes. Neurobiology of aging 16(3), 271278 (1995)
Delyon, B., Lavielle, M., Moulines, E.: Convergence of a stochastic approximation version of the em algorithm. Annals of statistics pp. 94128 (1999)
Dempster, A.P., Laird, N.M., Rubin, D.B.: Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society. Series B (methodological) pp. 138 (1977)
Diggle, P., Heagerty, P., Liang, K.Y., Zeger, S.: Analysis of longitudinal data. Oxford University Press (2002)
Donohue, M.C., Jacqmin-Gadda, H., Le Goff, M., Thomas, R.G., Raman, R., Gamst, A.C., Beckett, L.A., Jack, C.R., Weiner, M.W., Dartigues, J.F., Aisen, P.S., the Alzheimers Disease Neuroimaging Initiative: Estimating long-term multivariate progression from short-term data. Alzheimers & Dementia 10(5), 400 410 (2014)
Durrleman, S., Pennec, X., Trouve, A., Braga, J., Gerig, G., Ayache, N.: Toward a comprehensive framework for the spatiotemporal statistical analysis of longitudinal shape data. International Journal of Computer Vision 103(1), 2259 (2013)
Fonteijn, H.M., Modat, M., Clarkson, M.J., Barnes, J., Lehmann, M., Hobbs, N.Z., Scahill, R.I., Tabrizi, S.J., Ourselin, S., Fox, N.C., et al.: An event-based model for disease progression and its application in familial alzheimers disease and huntingtons disease. NeuroImage 60(3), 18801889 (2012)
Girolami, M., Calderhead, B.: Riemann manifold langevin and hamiltonian monte carlo methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 73(2), 123214 (2011)
Hirsch, M.W.: Differential topology. Springer Science & Business Media (2012)
Hyvarinen, A., Karhunen, J., Oja, E.: Independent component analysis, vol. 46. John Wiley & Sons (2004)
Jack, C.R., Knopman, D.S., Jagust, W.J., Shaw, L.M., Aisen, P.S., Weiner, M.W., Petersen, R.C., Trojanowski, J.Q.: Hypothetical model of dynamic biomarkers of the alzheimers pathological cascade. The Lancet Neurology 9(1), 119128 (2010)
Kuhn, E., Lavielle, M.: Maximum likelihood estimation in nonlinear mixed effects models. Computational Statistics & Data Analysis 49(4), 10201038 (2005)
Laird, N.M., Ware, J.H.: Random-effects models for longitudinal data. Biometrics pp. 963974 (1982)
Singer, J.D., Willett, J.B.: Applied longitudinal data analysis: Modeling change and event occurrence. Oxford university press (2003)
Singh, N., Hinkle, J., Joshi, S., Fletcher, P.T.: A hierarchical geodesic model for diffeomorphic longitudinal shape analysis. In: Information Processing in Medical Imaging. pp. 560571. Springer (2013)
Su, J., Kurtek, S., Klassen, E., Srivastava, A., et al.: Statistical analysis of trajectories on riemannian manifolds: Bird migration, hurricane tracking and video surveillance. The Annals of Applied Statistics 8(1), 530552 (2014)  9
Karl J. Astrom. Optimal control of Markov decision processes with incomplete state estimation. Journal of Mathematical Analysis and Applications, pages 174205, 1965.
Jan Drugowitsch, Ruben Moreno-Bote, Anne K. Churchland, Michael N. Shadlen, and Alexandre Pouget. The cost of accumulating evidence in perceptual decision making. The Journal of neuroscience, 32(11):36123628, 2012.
Jan Drugowitsch, Ruben Moreno-Bote, and Alexandre Pouget. Relation between belief and performance in perceptual decision making. PLoS ONE, 9(5):e96511, 2014.
Timothy D. Hanks, Mark E. Mazurek, Roozbeh Kiani, Elisabeth Hopp, and Michael N. Shadlen. Elapsed decision time affects the weighting of prior probability in a perceptual decision task. Journal of Neuroscience, 31(17):63396352, 2011.
Yanping Huang, Abram L. Friesen, Timothy D. Hanks, Michael N. Shadlen, and Rajesh P. N. Rao. How prior probability influences decision making: A unifying probabilistic model. In Proceedings of The Twenty-sixth Annual Conference on Neural Information Processing Systems (NIPS), pages 12771285, 2012.
Yanping Huang and Rajesh P. N. Rao. Reward optimization in the primate brain: A probabilistic model of decision making under uncertainty. PLoS ONE, 8(1):e53344, 01 2013.
Peter Juslin, Henrik Olsson, and Mats Bjorkman. Brunswikian and Thurstonian origins of bias in probability assessment: On the interpretation of stochastic components of judgment. Journal of Behavioral Decision Making, 10(3):189209, 1997.
Leslie Pack Kaelbling, Michael L. Littman, and Anthony R. Cassandra. Planning and acting in partially observable stochastic domains. Artificial Intelligence, 101(1?2):99  134, 1998.
Adam Kepecs and Zachary F Mainen. A computational framework for the study of confidence in humans and animals. Philosophical Transactions of the Royal Society B: Biological Sciences, 367(1594):13221337, 2012.
Adam Kepecs, Naoshige Uchida, Hatim A. Zariwala, and Zachary F. Mainen. Neural correlates, computation and behavioural impact of decision confidence. Nature, 455(7210):227 231, 2012.
Koosha Khalvati and Alan K. Mackworth. A fast pairwise heuristic for planning under uncertainty. In Proceedings of The Twenty-Seventh AAAI Conference on Artificial Intelligence, pages 187193, 2013.
Roozbeh Kiani, Leah Corthell, and Michael N. Shadlen. Choice certainty is informed by both evidence and decision time. Neuron, 84(6):1329  1342, 2014.
Roozbeh Kiani and Michael N. Shadlen. Representation of confidence associated with a decision by neurons in the parietal cortex. Science, 324(5928):759764, 2009.
Hanna Kurniawati, David Hsu, and Wee Sun Lee. SARSOP: Efficient point-based POMDP planning by approximating optimally reachable belief spaces. In Proceedings of The Robotics: Science and Systems IV, 2008.
Navindra Persaud, Peter McLeod, and Alan Cowey. Post-decision wagering objectively measures awareness. Nature Neuroscience, 10(2):257261, 2007.
Rajesh P. N. Rao. Decision making under uncertainty: a neural model based on partially observable Markov decision processes. Frontiers in computational neuroscience, 4, 2010.
Stphane Ross, Joelle Pineau, Sebastien Paquet, and Brahim Chaib-draa. Online planning algorithms for POMDPs. Journal of Artificial Intelligence Research, 32(1), 2008.
Michael N. Shadlen and William T. Newsome. Motion perception: seeing and deciding. Proceedings of the National Academy of Sciences of the United States of America, 93(2):628633, 1996.
Richard D. Smallwood and Edward J. Sondik. The optimal control of partially observable markov processes over a finite horizon. Operations Research, 21(5):10711088, 1973.
Edward J. Sondik. The optimal control of partially observable markov processes over the infinite horizon: Discounted costs. Operations Research, 26(2):pp. 282304, 1978.  9
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121  2159, 2011.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, 2010.
Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron C. Courville, and Yoshua Bengio. Maxout networks. In Proceedings of the 30th International Conference on Machine Learning, ICML, pages 13191327, 2013.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. arXiv preprint arXiv:1502.01852, 2015.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In arXiv, 2015.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Computer Science Department, University of Toronto, Tech. Rep, 1(4):7, 2009.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, 1998.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature. In ICML, 2015.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, 2011.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. International Conference on Learning Representations (ICLR) workshop track, 2015.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks. COLT, 2015.
Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In Learning Theory, pages 545560. Springer, 2005.
Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. On the universality of online mirror descent. In Advances in neural information processing systems, pages 26452653, 2011.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):19291958, 2014.
I. Sutskever, J. Martens, George Dahl, and Geoffery Hinton. On the importance of momentum and initialization in deep learning. In ICML, 2013.  9
David L Donoho. Compressed sensing. 52(4):12891306, 2006.  IEEE Transactions on Information Theory,
Richard Baraniuk. Compressive sensing. IEEE Signal Processing Magazine, 24(4), 2007.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Statistical Methodology), 58(1):267288, 1996.
Jianqing Fan and Runze Li. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96(456):13481360, 2001.
Emmanuel Candes and Terence Tao. The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):23132351, 2007.
Peter J Bickel, Yaacov Ritov, and Alexandre B Tsybakov. Simultaneous analysis of lasso and dantzig selector. The Annals of Statistics, 37(4):17051732, 2009.
Cun-Hui Zhang. Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics, 38(2):894942, 2010.
Peng Zhao and Bin Yu. On model selection consistency of lasso. The Journal of Machine Learning Research, 7:25412563, 2006.
Martin J Wainwright. Sharp thresholds for high-dimensional and noisy recovery of sparsity using l1-constrained quadratic programming. IEEE Transactions on Information Theory, 2009.
Jason D Lee, Yuekai Sun, and Jonathan E Taylor. On model selection consistency of mestimators with geometrically decomposable penalties. Advances in Neural Processing Information Systems, 2013.
Jianqing Fan and Jinchi Lv. Sure independence screening for ultrahigh dimensional feature space. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70(5):849 911, 2008.
Gaorong Li, Heng Peng, Jun Zhang, Lixing Zhu, et al. Robust rank correlation based screening. The Annals of Statistics, 40(3):18461877, 2012.
Hansheng Wang. Forward regression for ultra-high dimensional variable screening. Journal of the American Statistical Association, 104(488):15121524, 2009.
Haeran Cho and Piotr Fryzlewicz. High dimensional variable selection via tilting. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 74(3):593622, 2012.
Xiangyu Wang and Chenlei Leng. High-dimensional ordinary least-squares projection for screening variables. https://stat.duke.edu/xw56/holp-paper.pdf, 2015.
Cun-Hui Zhang and Jian Huang. The sparsity and bias of the lasso selection in highdimensional linear regression. The Annals of Statistics, 36(4):15671594, 2008.
Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Restricted eigenvalue properties for correlated gaussian designs. The Journal of Machine Learning Research, 11:22412259, 2010.
Shuheng Zhou. Restricted eigenvalue conditions on subgaussian random matrices. arXiv preprint arXiv:0912.4045, 2009.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027, 2010.
Lingzhou Xue and Hui Zou. Sure independence screening and compressed random sensing. Biometrika, 98(2):371380, 2011.
Jinzhu Jia and Karl Rohe. Preconditioning to comply with the irrepresentable condition. arXiv preprint arXiv:1208.5584, 2012.  9
C. G. Atkeson and S. Schaal. Memory-based neural networks for robot learning. Neurocomputing, 9:243269, 1995.
D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations (ICLR), 2015.
Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. J. Mach. Learn. Res., 3:11371155, Mar. 2003.
J. Chung, C. Gulcehre, K. Cho, and Y. Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint: 1412.3555, 2014.
S. Das, C. L. Giles, and G.-Z. Sun. Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory. In In Proceedings of The Fourteenth Annual Conference of Cognitive Science Society, 1992.
J. Goodman. A bit of progress in language modeling. CoRR, cs.CL/0108005, 2001.
A. Graves. Generating sequences with recurrent neural networks. arXiv preprint: 1308.0850, 2013.
A. Graves, G. Wayne, and I. Danihelka. Neural turing machines. arXiv preprint: 1410.5401, 2014.
K. Gregor, I. Danihelka, A. Graves, and D. Wierstra. DRAW: A recurrent neural network for image generation. CoRR, abs/1502.04623, 2015.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735 1780, 1997.
A. Joulin and T. Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. NIPS, 2015.
J. Koutnk, K. Greff, F. J. Gomez, and J. Schmidhuber. A clockwork RNN. In ICML, 2014.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini. Building a large annotated corpus of english: The Penn Treebank. Comput. Linguist., 19(2):313330, June 1993.
T. Mikolov. Statistical language models based on neural networks. Ph. D. thesis, Brno University of Technology, 2012.
T. Mikolov, A. Joulin, S. Chopra, M. Mathieu, and M. Ranzato. Learning longer memory in recurrent neural networks. arXiv preprint: 1412.7753, 2014.
M. C. Mozer and S. Das. A connectionist symbol manipulator that discovers the structure of context-free languages. NIPS, pages 863863, 1993.
B. Peng, Z. Lu, H. Li, and K. Wong. Towards Neural Network-based Reasoning. ArXiv preprint: 1508.05508, 2015.
J. Pollack. The induction of dynamical recognizers. Machine Learning, 7(2-3):227252, 1991.
K. Steinbuch and U. Piske. Learning matrices and their applications. IEEE Transactions on Electronic Computers, 12:846862, 1963.
M. Sundermeyer, R. Schluter, and H. Ney. LSTM neural networks for language modeling. In Interspeech, pages 194197, 2012.
W. K. Taylor. Pattern recognition by means of automatic analogue apparatus. Proceedings of The Institution of Electrical Engineers, 106:198209, 1959.
J. Weston, A. Bordes, S. Chopra, and T. Mikolov. Towards AI-complete question answering: A set of prerequisite toy tasks. arXiv preprint: 1502.05698, 2015.
J. Weston, S. Chopra, and A. Bordes. Memory networks. In International Conference on Learning Representations (ICLR), 2015.
K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. ArXiv preprint: 1502.03044, 2015.
W. Zaremba, I. Sutskever, and O. Vinyals. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329, 2014.  9
Bengio, Yoshua and LeCun, Yann. Scaling learning algorithms towards AI. In Bottou, Leon, Chapelle, Olivier,
DeCoste, D., and Weston, J. (eds.), Large Scale Kernel Machines. MIT Press, 2007.
Goodfellow, Ian J., Warde-Farley, David, Mirza, Mehdi, Courville, Aaron C., and Bengio, Yoshua. Maxout networks. CoRR, abs/1302.4389, 2013. URL http://dblp.uni-trier.de/db/journals/corr/
corr1302.html#abs-1302-4389.
Hinton, Geoffrey. Whats wrong with convolutional nets?
MIT Brain and Cognitive Sciences - Fall
Colloquium Series, Dec 2014a. URL http://techtv.mit.edu/collections/bcs/videos/
30698-what-s-wrong-with-convolutional-nets.
Hinton, Geoffrey. Ask me anything: Geoffrey hinton. Reddit Machine Learning, 2014b. URL https:
//www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/.
Ioffe, Sergey and Szegedy, Christian. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. CoRR, abs/1502.03167, 2015. URL http://arxiv.org/abs/1502.03167.
Karpathy, Andrej, Toderici, George, Shetty, Sanketh, Leung, Thomas, Sukthankar, Rahul, and Fei-Fei, Li.
Large-scale video classification with convolutional neural networks. In Computer Vision and Pattern Recognition, 2014.
Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2015.
URL http://arxiv.org/abs/1412.6980.
Krizhevsky, Alex. Learning multiple layers of features from tiny images. Technical report, 2009.
Krizhevsky, Alex., Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classification with deep convolutional
neural networks. In Advances in Neural Information Processing Systems, 2012.
LeCun, Yann, Boser, Bernhard, Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel., L. D.
Handwritten digit recognition with a back-propagation network. In Advances in Neural Information Processing Systems, 1989.
Lee, Chen-Yu, Xie, Saining, Gallagher, Patrick, Zhang, Zhengyou, and Tu, Zhuowen. Deeply-supervised nets.
CoRR, abs/1409.5185, 2014. URL http://arxiv.org/abs/1409.5185.
Lin, Min, Chen, Qiang, and Yan, Shuicheng. Network in network. CoRR, abs/1312.4400, 2013. URL http:
//dblp.uni-trier.de/db/journals/corr/corr1312.html#LinCY13.
Mathieu, Michael, Henaff, Mikael, and LeCun, Yann. Fast training of convolutional networks through FFTs.
CoRR, abs/1312.5851, 2013. URL http://arxiv.org/abs/1312.5851.
Rippel, Oren, Gelbart, Michael A., and Adams, Ryan P. Learning ordered representations with nested dropout.
In International Conference on Machine Learning, 2014.
Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma, Sean, Huang, Zhiheng,
Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael, Berg, Alexander C., and Li, Fei-Fei. ImageNet
Large Scale Visual Recognition Challenge. International Journal of Computer Vision, 2015. doi: 10.1007/
s11263-015-0816-y.
Snoek, Jasper, Larochelle, Hugo, and Adams, Ryan Prescott. Practical Bayesian optimization of machine
learning algorithms. In Neural Information Processing Systems, 2012.
Snoek, Jasper, Rippel, Oren, Swersky, Kevin, Kiros, Ryan, Satish, Nadathur, Sundaram, Narayanan, Patwary,
Md. Mostofa Ali, Prabhat, and Adams, Ryan P. Scalable Bayesian optimization using deep neural networks.
In International Conference on Machine Learning, 2015.
Torralba, Antonio and Oliva, Aude. Statistics of natural image categories. Network, 14(3):391412, August
2003. ISSN 0954-898X.
Vasilache, Nicolas, Johnson, Jeff, Mathieu, Michael, Chintala, Soumith, Piantino, Serkan, and LeCun, Yann.
Fast convolutional nets with fbfft: A GPU performance evaluation. CoRR, abs/1412.7580, 2014. URL
http://arxiv.org/abs/1412.7580.
Zeiler, Matthew D. and Fergus, Rob. Stochastic pooling for regularization of deep convolutional neural networks. CoRR, abs/1301.3557, 2013. URL http://dblp.uni-trier.de/db/journals/corr/
corr1301.html#abs-1301-3557.
Peter L. Bartlett and Mikhail Traskin. AdaBoost is consistent. JMLR, 8:23472368, 2007.
Alina Beygelzimer, Satyen Kale, and Haipeng Luo. Optimal and adaptive algorithms for online boosting. In ICML, 2015.
Avrim Blum, Adam Kalai, and John Langford. Beating the hold-out: Bounds for k-fold and progressive cross-validation. In COLT, pages 203208, 1999.
Shang-Tse Chen, Hsuan-Tien Lin, and Chi-Jen Lu. An Online Boosting Algorithm with Theoretical Justifications. In ICML, 2012.
Shang-Tse Chen, Hsuan-Tien Lin, and Chi-Jen Lu. Boosting with Online Binary Learners for the Multiclass Bandit Problem. In ICML, 2014.
Michael Collins, Robert E. Schapire, and Yoram Singer. Logistic regression, AdaBoost and Bregman distances. In COLT, 2000.
Nigel Duy and David Helmbold. Boosting methods for regression. Machine Learning, 47(2/3):153200, 2002.
Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval Res. Logis. Quart., 3:95110, 1956.
Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. JCSS, 55(1):119139, August 1997.
Jerome H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29(5), October 2001.
Helmut Grabner and Horst Bischof. On-line boosting and vision. In CVPR, volume 1, pages 260267, 2006.
Helmut Grabner, Christian Leistner, and Horst Bischof. Semi-supervised on-line boosting for robust tracking. In ECCV, pages 234247, 2008.
Trevor Hastie and R. J Robet Tibshirani. Generalized Additive Models. Chapman and Hall, 1990.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer Verlag, 2001.
Elad Hazan and Satyen Kale. Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization. JMLR, 15(1):24892512, 2014.
Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex optimization. Machine Learning, 69(2-3):169192, 2007.
Xiaoming Liu and Ting Yu. Gradient feature selection for online boosting. In ICCV, pages 18, 2007.
Stephane G. Mallat and Zhifeng Zhang. Matching pursuits with time-frequency dictionaries. IEEE Transactions on Signal Processing, 41(12):33973415, December 1993.
Llew Mason, Jonathan Baxter, Peter Bartlett, and Marcus Frean. Boosting algorithms as gradient descent. In NIPS, 2000.
Nikunj C. Oza and Stuart Russell. Online bagging and boosting. In AISTATS, pages 105112, 2001.
Robert E. Schapire and Yoav Freund. Boosting: Foundations and Algorithms. MIT Press, 2012.
Matus Telgarsky. Boosting with the logistic loss is consistent. In COLT, 2013.
VW. URL https://github.com/JohnLangford/vowpal_wabbit/.
Tong Zhang and Bin Yu. Boosting with early stopping: Convergence and consistency. Annals of Statistics, 33(4):15381579, 2005.
Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In ICML, 2003.  9
L. Rabiner and B. Juang. An introduction to hidden markov models. In ASSP Magazine, IEEE, 1986.
R. Kalman. Mathematical description of linear dynamical systems. In J. the Society for Industrial & Applied Mathematics, Series A: Control, 1963.
M. Hermans and B. Schrauwen. Training and analysing deep recurrent neural networks. In NIPS, 2013.
J. Martens and I. Sutskever. Learning recurrent neural networks with hessian-free optimization. In ICML, 2011.
R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. In ICML, 2013.
A. Graves. Generating sequences with recurrent neural networks. In arXiv:1308.0850, 2013.
G. Taylor, G. Hinton, and S. Roweis. Modeling human motion using binary latent variables. In NIPS, 2006.
I. Sutskever and G. Hinton. Learning multilevel distributed representations for high-dimensional sequences. In AISTATS, 2007.
I. Sutskever, G. Hinton, and G. Taylor. The recurrent temporal restricted boltzmann machine. In NIPS, 2009.
N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent. Modeling temporal dependencies in highdimensional sequences: Application to polyphonic music generation and transcription. In ICML, 2012.
R. Mittelman, B. Kuipers, S. Savarese, and H. Lee. Structured recurrent temporal restricted boltzmann machines. In ICML, 2014.
D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In ICLR, 2014.
A. Mnih and K. Gregor. Neural variational inference and learning in belief networks. In ICML, 2014.
D. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In ICML, 2014.
Z. Gan, R. Henao, D. Carlson, and L. Carin. Learning deep sigmoid belief networks with data augmentation. In AISTATS, 2015.
R. Neal. Connectionist learning of belief networks. In Artificial intelligence, 1992.
G. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep belief nets. In Neural computation, 2006.
G. Hinton. Training products of experts by minimizing contrastive divergence. In Neural computation, 2002.
G. Hinton and R. Salakhutdinov. Replicated softmax: an undirected topic model. In NIPS, 2009.
G. Hinton, P. Dayan, B. Frey, and R. Neal. The wake-sleep algorithm for unsupervised neural networks. In Science, 1995.
T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. In COURSERA: Neural Networks for Machine Learning, 2012.
P. Werbos. Backpropagation through time: what it does and how to do it. In Proc. of the IEEE, 1990.
G. Taylor and G. Hinton. Factored conditional restricted boltzmann machines for modeling motion style. In ICML, 2009.
Z. Gan, C. Chen, R. Henao, D. Carlson, and L. Carin. Scalable deep poisson factor analysis for topic modeling. In ICML, 2015.
J. Henderson and I. Titov. Incremental sigmoid belief networks for grammar learning. In JMLR, 2010.
G. Hinton, P. Dayan, A. To, and R. Neal. The helmholtz machine through time. In Proc. of the ICANN, 1995.
J. Bayer and C. Osendorfer. Learning stochastic recurrent networks. In arXiv:1411.7610, 2014.
O. Fabius, J. R. van Amersfoort, and D. P. Kingma. Variational recurrent auto-encoders. In arXiv:1412.6581, 2014.
J. Chung, K. Kastner, L. Dinh, K. Goel, A. Courville, and Y. Bengio. A recurrent latent variable model for sequential data. In NIPS, 2015.
S. Han, L. Du, E. Salazar, and L. Carin. Dynamic rank factor model for text streams. In NIPS, 2014.
A. Acharya, J. Ghosh, and M. Zhou. Nonparametric Bayesian factor analysis for dynamic count matrices. In AISTATS, 2015.
K. Fan, Z. Wang, J. Kwok, and K. Heller. Fast Second-Order Stochastic Backpropagation for Variational Inference. In NIPS, 2015.  9
D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding. In Society for Industrial and Applied Mathematics, editors, Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, 2007.
M. Beck, A.and Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal of Imaging Sciences, 2(1):183202, 2009.
E. J. Chichilnisky and Rachel S. Kalmar. Functional asymmetries in on and off ganglion cells of primate retina. The Journal of Neuroscience, 22(7):27372747, 2002.
A. Coates and A. Y. Ng. The importance of encoding versus training with sparse coding and vector quantization. In International Conference in Machine Learning (ICML), volume 28, 2011.
A. Coates, A. Y. Ng, and H. Lee. An analysis of single-layer networks in unsupervised feature learning. In International Conference on Artificial Intelligence and Statistics (AISTATS), pages 215223, 2011.
D. M. Dacey. The Cognitive Neurosciences, book section Origins of perception: retinal ganglion cell diversity and the creation of parallel visual pathways, pages 281301. MIT Press, 2004.
D. M. Dacey and O S Packer. Colour coding in the primate retina: diverse cell types and cone-specific circuitry. Curr Opin Neurobiol, 13:421427, 2003.
D. M. Dacey and M. R. Petersen. Dendritic field size and morphology of midget and parasol cells of the human retina. PNAS, 89:96669670, 1992.
Steven H. Devries and Denis A. Baylor. Mosaic arrangement of ganglion cell receptive fields in rabbit retina. Journal of Neurophysiology, 78(4):20482060, 1997.
M. Greschner, J. Shlens, C. Bakolista, G. D. Field, J. L. Gauthier, L. H. Jepson, A. Sher, A. M. Litke, and E. J. Chichilnisky. Correlated firing among major ganglion cell types in primate retina. J Physiol, 589:7586, 2011.
L. H. Jepson, P. Hottowy, G. A. Wiener, W. Dabrowski, A. M. Litke, and E. J. Chichilnisky. High-fidelity reproduction of spatiotemporal visual signals for retinal prosthesis. Neuron, 83:8792, 2014.
R. H. Keshavan, A. Montanari, and S. Oh. Matrix completion from a few entries. IEEE Transactions on Information Theory, 56(6):29802998, 2010.
P. H. Li, J. L. Gauthier, M. Schiff, A. Sher, D. Ahn, G. D. Field, M. Greschner, E. M. Callaway, A. M. Litke, and E. J. Chichilnisky. Anatomical identification of extracellularly recorded cells in large-scale multielectrode recordings. J Neurosci, 35(11):466375, 2015.
A. M. Litke, N. Bezayiff, E. J. Chichilnisky, W. Cunningham, W. Dabrowski, A. A. Grillo, M. I. Grivich, P. Grybos, P. Hottowy, S. Kachiguine, R. S. Kalmar, K. Mathieson, D. Petrusca, M. Rahman, and A. Sher. What does the eye tell the brain? development of a system for the large-scale recording of retinal output activity. IEEE Trans. on Nuclear Science, 51(4):1434 1440, 2004.
J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online dictionary learning for sparse coding. In International Conference on Machine Learning (ICML), pages 689696, 2009.
D. N. Mastronarde. Correlated firing of cat retinal ganglion cells. i. spontaneously active inputs to x- and y-cells. J Neurophysiol, 49(2):303324, 1983.
A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in neural information processing systems (NIPS), pages 11771184, 2007.
L. C. L. Silveira and V.H. Perry. The topography of magnocellular projecting ganglion cells (m-ganglion cells) in the primate retina. Neuroscience, 40(1):217237, 1991.  9
D. R. J. Laming, Information theory of choice-reaction times. London: Academic Press, 1968.
R. Ratcliff, A theory of memory retrieval., Psychological Review, vol. 85, no. 2, pp. 59108, 1978.  8
M. Usher and J. L. McClelland, The time course of perceptual choice: The leaky, competing accumulator model., Psychological Review, vol. 108, no. 3, pp. 550592, 2001.
R. Bogacz, E. Brown, J. Moehlis, P. Holmes, and J. D. Cohen, The physics of optimal decision making: a formal analysis of models of performance in two-alternative forced-choice tasks., Psychological review, vol. 113, pp. 70065, Oct. 2006.
A. J. Yu, P. Dayan, and J. D. Cohen, Dynamics of attentional selection under conflict: toward a rational Bayesian account., Journal of experimental psychology. Human perception and performance, vol. 35, pp. 70017, June 2009.
D. Servan-Schreiber, J. D. Cohen, and S. Steingard, Schizophrenic Deficits in the Processing of Context, Archives of General Psychiatry, vol. 53, p. 1105, Dec. 1996.
A. Wald and J. Wolfowitz, Optimum Character of the Sequential Probability Ratio Test, The Annals of Mathematical Statistics, vol. 19, pp. 326339, Sept. 1948.
S. Kira, T. Yang, and M. N. Shadlen, A Neural Implementation of Walds Sequential Probability Ratio Test, Neuron, vol. 85, pp. 861873, Feb. 2015.
R. Bogacz and K. N. Gurney, The basal ganglia and cortex implement optimal decision making between alternative actions., Neural computation, vol. 19, pp. 44277, Feb. 2007.
M. K. van Vugt, P. Simen, L. E. Nystrom, P. Holmes, and J. D. Cohen, EEG oscillations reveal neural correlates of evidence accumulation., Frontiers in neuroscience, vol. 6, p. 106, Jan. 2012.
B. M. Turner, L. van Maanen, and B. U. Forstmann, Informing cognitive abstractions through neuroimaging: The neural drift diffusion model., Psychological Review, vol. 122, no. 2, pp. 312336, 2015.
D. Norris, The Bayesian reader: explaining word recognition as an optimal Bayesian decision process., Psychological review, vol. 113, pp. 327357, Apr. 2006.
K.-F. Wong and X.-J. Wang, A recurrent network mechanism of time integration in perceptual decisions., The Journal of neuroscience : the official journal of the Society for Neuroscience, vol. 26, no. 4, pp. 13141328, 2006.
P. I. Frazier and A. J. Yu, Sequential hypothesis testing under stochastic deadlines, Advances in Neural Information Processing Systems, pp. 18, 2008.
J. Drugowitsch, R. Moreno-Bote, A. K. Churchland, M. N. Shadlen, and A. Pouget, The cost of accumulating evidence in perceptual decision making., The Journal of Neuroscience, vol. 32, pp. 361228, Mar. 2012.
N. Srivastava and P. Schrater, Rational inference of relative preferences, in Advances in Neural Information Processing Systems 25, pp. 23122320, 2012.
R. C. OReilly and M. J. Frank, Making Working Memory Work: A Computational Model of Learning in the Prefrontal Cortex and Basal Ganglia, Neural Computation, vol. 18, pp. 283328, Feb. 2006.
J. P. Sheppard, D. Raposo, and A. K. Churchland, Dynamic weighting of multisensory stimuli shapes decision-making in rats and humans., Journal of vision, vol. 13, no. 6, pp. 119, 2013.
J. R. Stroop, Studies of interference in serial verbal reactions., Journal of Experimental Psychology, vol. 18, no. 6, pp. 643662, 1935.
G. Gratton, M. G. Coles, E. J. Sirevaag, C. W. Eriksen, and E. Donchin, Pre- and poststimulus activation of response channels: a psychophysiological analysis., Journal of experimental psychology. Human perception and performance, vol. 14, no. 3, pp. 331344, 1988.
J. R. Simon and J. D. Wolf, Choice reaction time as a function of angular stimulus-response correspondence and age, Ergonomics, vol. 6, pp. 99105, Jan. 1963.
Y. S. Liu, A. Yu, and P. Holmes, Dynamical analysis of Bayesian inference models for the Eriksen task., Neural computation, vol. 21, pp. 152053, June 2009.
T. D. Hanks, M. E. Mazurek, R. Kiani, E. Hopp, and M. N. Shadlen, Elapsed decision time affects the weighting of prior probability in a perceptual decision task., The Journal of Neuroscience, vol. 31, pp. 633952, Apr. 2011.
O. Lositsky, R. C. Wilson, M. Shvartsman, and J. D. Cohen, A Drift Diffusion Model of Proactive and Reactive Control in a Context-Dependent Two-Alternative Forced Choice Task, in The Multi-disciplinary Conference on Reinforcement Learning and Decision Making, pp. 103107, 2015.
T. S. Braver, The variable nature of cognitive control: a dual mechanisms framework., Trends in cognitive sciences, vol. 16, pp. 10613, Feb. 2012.
T. D. Hanks, C. D. Kopec, B. W. Brunton, C. A. Duan, J. C. Erlich, and C. D. Brody, Distinct relationships of parietal and prefrontal cortices to evidence accumulation, Nature, 2015.
Y. Liu and S. Blostein, Optimality of the sequential probability ratio test for nonstationary observations, IEEE Transactions on Information Theory, vol. 38, no. 1, pp. 177182, 1992.  9
Shadab Alam, Franco D Albareti, Carlos Allende Prieto, F Anders, Scott F Anderson, Brett H Andrews, Eric Armengaud, ric Aubourg, Stephen Bailey, Julian E Bautista, et al. The 8  eleventh and twelfth data releases of the Sloan digital sky survey: Final data from SDSS-III. arXiv preprint arXiv:1501.00963, 2015.
Jo Bovy, Adam D Myers, Joseph F Hennawi, David W Hogg, Richard G McMahon, David Schiminovich, Erin S Sheldon, Jon Brinkmann, Donald P Schneider, and Benjamin A Weaver. Photometric redshifts and quasar probabilities from a single, data-driven generative model. The Astrophysical Journal, 749(1):41, 2012.
M Brescia, S Cavuoti, R DAbrusco, G Longo, and A Mercurio. Photometric redshifts for quasars in multi-band surveys. The Astrophysical Journal, 772(2):140, 2013.
Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng. Handbook of Markov Chain Monte Carlo. CRC press, 2011.
Kyle S Dawson, David J Schlegel, Christopher P Ahn, Scott F Anderson, ric Aubourg, Stephen Bailey, Robert H Barkhouser, Julian E Bautista, Alessandra Beifiori, Andreas A Berlind, et al. The baryon oscillation spectroscopic survey of SDSS-III. The Astronomical Journal, 145(1):10, 2013.
RO Gray, PW Graham, and SR Hoyt. The physical basis of luminosity classification in the late a-, f-, and early g-type stars. ii. basic parameters of program stars and the role of microturbulence. The Astronomical Journal, 121(4):2159, 2001.
Edward Harrison. The redshift-distance and velocity-distance laws. The Astrophysical Journal, 403:2831, 1993.
David W Hogg. Distance measures in cosmology. arXiv preprint astro-ph/9905116, 1999.
Dougal Maclaurin, David Duvenaud, and Ryan P. Adams. Autograd: Reverse-mode differentiation of native python. ICML workshop on Automatic Machine Learning, 2015.
D Christopher Martin, James Fanson, David Schiminovich, Patrick Morrissey, Peter G Friedman, Tom A Barlow, Tim Conrow, Robert Grange, Patrick N Jelinksy, Bruno Millard, et al. The galaxy evolution explorer: A space ultraviolet survey mission. The Astrophysical Journal Letters, 619(1), 2005.
Radford M Neal. Slice sampling. Annals of statistics, pages 705741, 2003.
Jorge Nocedal. Updating quasi-newton matrices with limited storage. Mathematics of computation, 35(151):773782, 1980.
Isabelle Pris, Patrick Petitjean, ric Aubourg, Nicholas P Ross, Adam D Myers, Alina Streblyanska, Stephen Bailey, Patrick B Hall, Michael A Strauss, Scott F Anderson, et al. The Sloan digital sky survey quasar catalog: tenth data release. Astronomy & Astrophysics, 563:A54, 2014.
Jeffrey Regier, Andrew Miller, Jon McAuliffe, Ryan Adams, Matt Hoffman, Dustin Lang, David Schlegel, and Prabhat. Celeste: Variational inference for a generative model of astronomical images. In Proceedings of The 32nd International Conference on Machine Learning, 2015.
SDSSIII. Measures of flux and magnitude. 2013. https://www.sdss3.org/dr8/ algorithms/magnitudes.php.
Joseph Silk and Martin J Rees. Quasars and galaxy formation. Astronomy and Astrophysics, 1998.
Chris Stoughton, Robert H Lupton, Mariangela Bernardi, Michael R Blanton, Scott Burles, Francisco J Castander, AJ Connolly, Daniel J Eisenstein, Joshua A Frieman, GS Hennessy, et al. Sloan digital sky survey: early data release. The Astronomical Journal, 123(1):485, 2002.
Jakob Walcher, Brent Groves, Tams Budavri, and Daniel Dale. Fitting the integrated spectral energy distributions of galaxies. Astrophysics and Space Science, 331(1):151, 2011.
David H Weinberg, Romeel Dave, Neal Katz, and Juna A Kollmeier. The Lyman-alpha forest as a cosmological tool. Proceedings of the 13th Annual Astrophysica Conference in Maryland, 666, 2003.  9
R. Ananthanarayanan, V. Basker, S. Das, A. Gupta, H. Jiang, T. Qiu, A. Reznichenko, D. Ryabkov, M. Singh, and S. Venkataraman. Photon: Fault-tolerant and scalable joining of continuous data streams. In SIGMOD 13: Proceedings of the 2013 international conference on Management of data, pages 577 588, New York, NY, USA, 2013.
A. Anonymous. Machine learning: The high-interest credit card of technical debt. SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop).
L. Bottou, J. Peters, J. Quinonero Candela, D. X. Charles, D. M. Chickering, E. Portugaly, D. Ray, P. Simard, and E. Snelson. Counterfactual reasoning and learning systems: The example of computational advertising. Journal of Machine Learning Research, 14(Nov), 2013.
W. J. Brown, H. W. McCormick, T. J. Mowbray, and R. C. Malveau. Antipatterns: refactoring software, architectures, and projects in crisis. 1998.
T. M. Chilimbi, Y. Suzue, J. Apacible, and K. Kalyanaraman. Project adam: Building an efficient and scalable deep learning training system. In 11th USENIX Symposium on Operating Systems Design and Implementation, OSDI 14, Broomfield, CO, USA, October 6-8, 2014., pages 571582, 2014.
B. Dalessandro, D. Chen, T. Raeder, C. Perlich, M. Han Williams, and F. Provost. Scalable handsfree transfer learning for online advertising. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 15731582. ACM, 2014.
M. Fowler. Code smells. http://http://martinfowler.com/bliki/CodeSmell.html.
M. Fowler. Refactoring: improving the design of existing code. Pearson Education India, 1999.
J. Langford and T. Zhang. The epoch-greedy algorithm for multi-armed bandits with side information. In Advances in neural information processing systems, pages 817824, 2008.
M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, V. Josifovski, J. Long, E. J. Shekita, and B. Su. Scaling distributed machine learning with the parameter server. In 11th USENIX Symposium on Operating Systems Design and Implementation, OSDI 14, Broomfield, CO, USA, October 6-8, 2014., pages 583598, 2014.
J. Lin and D. Ryaboy. Scaling big data mining infrastructure: the twitter experience. ACM SIGKDD Explorations Newsletter, 14(2):619, 2013.
H. B. McMahan, G. Holt, D. Sculley, M. Young, D. Ebner, J. Grady, L. Nie, T. Phillips, E. Davydov, D. Golovin, S. Chikkerur, D. Liu, M. Wattenberg, A. M. Hrafnkelsson, T. Boulos, and J. Kubica. Ad click prediction: a view from the trenches. In The 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 2013, Chicago, IL, USA, August 11-14, 2013, 2013.
J. D. Morgenthaler, M. Gridnev, R. Sauciuc, and S. Bhansali. Searching for build debt: Experiences managing technical debt at google. In Proceedings of the Third International Workshop on Managing Technical Debt, 2012.
D. Sculley, M. E. Otey, M. Pohl, B. Spitznagel, J. Hainsworth, and Y. Zhou. Detecting adversarial advertisements in the wild. In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Diego, CA, USA, August 21-24, 2011, 2011.
Securities and E. Commission. SEC Charges Knight Capital With Violations of Market Access Rule, 2013.
A. Spector, P. Norvig, and S. Petrov. Googles hybrid approach to research. Communications of the ACM, 55 Issue 7, 2012.
A. Zheng. The challenges of building machine learning tools for the masses. SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop).  9
Constantin Aliferis, Ioannis Tsamardinos, Alexander Statnikov, C. F. Aliferis M. D, Ph. D, I. Tsamardinos Ph. D, and Er Statnikov M. S. Hiton, a novel markov blanket algorithm for optimal variable selection, 2003.
David Maxwell Chickering. Optimal structure identification with greedy search. Journal of Machine Learning Research, 2002.
Gregory F Cooper. A simple constraint-based algorithm for efficiently mining observational databases for causal relationships. Data Mining and Knowledge Discovery, 1(2):203224, 1997.
Isabelle Guyon, Andre Elisseeff, and Constantin Aliferis. Causal feature selection. 2007.
Daphne Koller and Mehran Sahami. Toward optimal feature selection. In ICML 1996, pages 284292. Morgan Kaufmann, 1996.
Subramani Mani, Constantin F Aliferis, Alexander R Statnikov, and MED NYU. Bayesian algorithms for causal data mining. In NIPS Causality: Objectives and Assessment, pages 121136, 2010.
Subramani Mani and Gregory F Cooper. A study in causal discovery from population-based infant birth and death records. In Proceedings of the AMIA Symposium, page 315. American Medical Informatics Association, 1999.
Subramani Mani and Gregory F Cooper. Causal discovery using a bayesian local causal discovery algorithm. Medinfo, 11(Pt 1):731735, 2004.
Dimitris Margaritis and Sebastian Thrun. Bayesian network induction via local neighborhoods. In Advances in Neural Information Processing Systems 12, pages 505511. MIT Press, 1999.
Christopher Meek. Causal inference and causal explanation with background knowledge. In Proceedings of the Eleventh conference on Uncertainty in artificial intelligence, pages 403410. Morgan Kaufmann Publishers Inc., 1995.
Teppo Niinimaki and Pekka Parviainen. Local structure disocvery in bayesian network. In Proceedings of Uncertainy in Artifical Intelligence, Workshop on Causal Structure Learning, pages 634643, 2012.
Judea Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann Publishers, Inc., 2 edition, 1988.
Judea Pearl. Causality: models, reasoning and inference, volume 29. Cambridge Univ Press, 2000.
Jean-Philippe Pellet and Andre Elisseeff. Finding latent causes in causal networks: an efficient approach based on markov blankets. In Advances in Neural Information Processing Systems, pages 12491256, 2009.
Jean-Philippe Pellet and Andre Ellisseeff. Using markov blankets for causal structure learning. Journal of Machine Learning, 2008.
Jose M. Peoa, Roland Nilsson, Johan Bjorkegren, and Jesper Tegner. Towards scalable and data efficient learning of markov boundaries. Int. J. Approx. Reasoning, 45(2):211232, July 2007.
Craig Silverstein, Sergey Brin, Rajeev Motwani, and Jeff Ullman. Scalable techniques for mining causal structures. Data Mining and Knowledge Discovery, 4(2-3):163192, 2000.
P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction, and Search. The MIT Press, 2nd edition, 2000.
Peter Spirtes, Clark Glymour, Richard Scheines, Stuart Kauffman, Valerio Aimale, and Frank Wimberly. Constructing bayesian network models of gene expression networks from microarray data, 2000.
Peter Spirtes, Christopher Meek, and Thomas Richardson. Causal inference in the presence of latent variables and selection bias. In Proceedings of the Eleventh conference on Uncertainty in artificial intelligence, pages 499506. Morgan Kaufmann Publishers Inc., 1995.
Alexander Statnikov, Ioannis Tsamardinos, Laura E. Brown, and Constatin F. Aliferis. Causal explorer: A matlab library for algorithms for causal discovery and variable selection for classification. In Causation and Prediction Challenge at WCCI, 2008.
Ioannis Tsamardinos, Constantin F. Aliferis, and Alexander Statnikov. Time and sample efficient discovery of markov blankets and direct causal relations. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD 03, pages 673678, New York, NY, USA, 2003. ACM.
Ioannis Tsamardinos, LauraE. Brown, and ConstantinF. Aliferis. The max-min hill-climbing bayesian network structure learning algorithm. Machine Learning, 65(1):3178, 2006.
Jiji Zhang. On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias. Artificial Intelligence, 172(16):18731896, 2008.  9
A N A N D K U M A R , A ., G E , R ., H S U , D ., K A K A D E , S . M . and T E L G A R S K Y , M . (2014). Tensor decompositions for learning latent variable models. Journal of Machine Learning Research 15 27732832.
B A L A K R I S H N A N , S ., W A I N W R I G H T , M . J . and Y U , B . (2014). Statistical guarantees for the EM algorithm: From population to sample-based analysis. arXiv preprint arXiv:1408.2156 .
B A R T H O L O M E W , D . J ., K N O T T , M . and M O U S TA K I , I . (2011). Latent variable models and factor analysis: A unified approach, vol. 899. Wiley.
B E L L O N I , A ., C H E N , D ., C H E R N O Z H U K O V , V. and H A N S E N , C . (2012). Sparse models and methods for optimal instruments with an application to eminent domain. Econometrica 80 23692429.
B I C K E L , P. J ., R I T O V , Y. and T S Y B A K O V , A . B . (2009). Simultaneous analysis of Lasso and Dantzig selector. Annals of Statistics 37 17051732.  8
B O U C H E R O N , S ., L U G O S I , G . and M A S S A R T , P. (2013). Concentration inequalities: A nonasymptotic theory of independence. Oxford University Press.
C A I , T., L I U , W. and L U O , X . (2011). A constrained `1 minimization approach to sparse precision matrix estimation. Journal of the American Statistical Association 106 594607.
C A N D E S , E . and T A O , T. (2007). The Dantzig selector: Statistical estimation when p is much larger than n. Annals of Statistics 35 23132351.
C H A G A N T Y , A . T. and L I A N G , P. (2013). Spectral experts for estimating mixtures of linear regressions. arXiv preprint arXiv:1306.3729 .
C H A U D H U R I , K ., D A S G U P TA , S . and V AT TA N I , A . (2009). Learning mixtures of Gaussians using the k-means algorithm. arXiv preprint arXiv:0912.0086 .
D A S G U P TA , S . and S C H U L M A N , L . (2007). A probabilistic analysis of EM for mixtures of separated, spherical Gaussians. Journal of Machine Learning Research 8 203226.
D E M P S T E R , A . P., L A I R D , N . M . and R U B I N , D . B . (1977). Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Statistical Methodology) 39 138.
J AVA N M A R D , A . and M O N TA N A R I , A . (2014). Confidence intervals and hypothesis testing for high-dimensional regression. Journal of Machine Learning Research 15 28692909.
K H A L I L I , A . and C H E N , J . (2007). Variables selection in finite mixture of regression models. Journal of the American Statistical Association 102 10251038.
K N I G H T , K . and F U , W. (2000). Asymptotics for Lasso-type estimators. Annals of Statistics 28 13561378.
L E E , J . D ., S U N , D . L ., S U N , Y. and T AY L O R , J . E . (2013). Exact inference after model selection via the Lasso. arXiv preprint arXiv:1311.6238 .
L O C K H A R T , R ., T AY L O R , J ., T I B S H I R A N I , R . J . and T I B S H I R A N I , R . (2014). A significance test for the Lasso. Annals of Statistics 42 413468.
M C L A C H L A N , G . and K R I S H N A N , T. (2007). The EM algorithm and extensions, vol. 382. Wiley.
M E I N S H A U S E N , N . and B U H L M A N N , P. (2010). Stability selection. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 72 417473.
M E I N S H A U S E N , N ., M E I E R , L . and B U H L M A N N , P. (2009). p-values for high-dimensional regression. Journal of the American Statistical Association 104 16711681.
N E S T E R O V , Y. (2004). Introductory lectures on convex optimization:A basic course, vol. 87. Springer.
N I C K L , R . and 41 28522876.  VA N D E  G E E R , S . (2013). Confidence sets in sparse regression. Annals of Statistics
S T A D L E R , N ., B U H L M A N N , P. and regression models. TEST 19 209256.  VA N D E  G E E R , S . (2010). `1 -penalization for mixture
T AY L O R , J ., L O C K H A R T , R ., T I B S H I R A N I , R . J . and T I B S H I R A N I , R . (2014). Post-selection adaptive inference for least angle regression and the Lasso. arXiv preprint arXiv:1401.3889 .
T S E N G , P. (2004). An analysis of the EM algorithm and entropy-like proximal point methods. Mathematics of Operations Research 29 2744.
VA N D E G E E R , S ., B U H L M A N N , P., R I T O V , Y. and D E Z E U R E , R . (2014). On asymptotically optimal confidence regions and tests for high-dimensional models. Annals of Statistics 42 11661202.
VA N D E R  VA A R T , A . W. (2000). Asymptotic statistics, vol. 3. Cambridge University Press.
V E R S H Y N I N , R . (2010). Introduction to the non-asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027 .
W A S S E R M A N , L . and R O E D E R , K . (2009). High-dimensional variable selection. Annals of Statistics 37 21782201.
W U , C . F. J . (1983). On the convergence properties of the EM algorithm. Annals of Statistics 11 95103.
Y I , X ., C A R A M A N I S , C . and S A N G H AV I , S . (2013). Alternating minimization for mixed linear regression. arXiv preprint arXiv:1310.3745 .
Z H A N G , C . - H . and Z H A N G , S . S . (2014). Confidence intervals for low dimensional parameters in high dimensional linear models. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 76 217242.  9
Abernethy, J., E. Hazan, and A. Rakhlin (2008). Competing in the dark: An efficient algorithm for
bandit linear optimization. In Proceedings of COLT 2008, pp. 263274.
Amin, K., A. Rostamizadeh, and U. Syed (2013). Learning prices for repeated auctions with strategic
buyers. In Proceedings of NIPS, pp. 11691177.
Amin, K., A. Rostamizadeh, and U. Syed (2014). Repeated contextual auctions with strategic buyers. In Proceedings of NIPS 2014, pp. 622630.
Arora, R., O. Dekel, and A. Tewari (2012). Online bandit learning against an adaptive adversary:
from regret to policy regret. In Proceedings of ICML.
Auer, P., N. Cesa-Bianchi, and P. Fischer (2002). Finite-time analysis of the multiarmed bandit
problem. Machine Learning 47(2-3), 235256.
Bikhchandani, S. and K. McCardle (2012). Behaviour-based price discrimination by a patient seller.
The B.E. Journal of Theoretical Economics 12(1), 19351704.
Bubeck, S. and N. Cesa-Bianchi (2012). Regret analysis of stochastic and nonstochastic multi-armed
bandit problems. Foundations and Trends in Machine Learning 5(1), 1122.
Cesa-Bianchi, N., C. Gentile, and Y. Mansour (2015). Regret minimization for reserve prices in
second-price auctions. IEEE Transactions on Information Theory 61(1), 549564.
Cole, R. and T. Roughgarden (2014). The sample complexity of revenue maximization. In Proceedings of STOC 2014, pp. 243252.
Cui, Y., R. Zhang, W. Li, and J. Mao (2011). Bid landscape forecasting in online ad exchange
marketplace. In Proceedings of SIGKDD 2011, pp. 265273.
Dani, V. and T. P. Hayes (2006). Robbing the bandit: less regret in online geometric optimization
against an adaptive adversary. In Proceedings of SODA 2006, pp. 937943.
Edelman, B. and M. Ostrovsky (2007). Strategic bidder behavior in sponsored search auctions.
Decision Support Systems 43(1), 192198.
Kanoria, Y. and H. Nazerzadeh (2014). Dynamic reserve prices for repeated auctions: Learning
from bids. In Proceedings of WINE 2014, pp. 232.
Kleinberg, R. D. and F. T. Leighton (2003). The value of knowing a demand curve: Bounds on
regret for online posted-price auctions. In Proceedings of FOCS 2003, pp. 594605.
Lai, T. and H. Robbins (1985). Asymptotically efficient adaptive allocation rules. Advances in
Applied Mathematics 6(1), 4  22.
Milgrom, P. and R. Weber (1982). A theory of auctions and competitive bidding. Econometrica:
Journal of the Econometric Society 50(5), 10891122.
Milgrom, P. R. (2004). Putting auction theory to work. Cambridge University Press.
Mohri, M. and A. M. Medina (2014a). Learning theory and algorithms for revenue optimization in
second price auctions with reserve. In Proceedings of ICML 2014, pp. 262270.
Mohri, M. and A. M. Medina (2014b). Optimal regret minimization in posted-price auctions with
strategic buyers. In Proceedings of NIPS 2014, pp. 18711879.
Myerson, R. B. (1981). Optimal auction design. Mathematics of Operations Research 6(1), pp.
Nachbar, J. (2001). Bayesian learning in repeated games of incomplete information. Social Choice
and Welfare 18(2), 303326.
Nachbar, J. H. (1997). Prediction, optimization, and learning in repeated games. Econometrica:
Journal of the Econometric Society 65(2), 275309.
Vickrey, W. (1961). Counterspeculation, auctions, and competitive sealed tenders. The Journal of
finance 16(1), 837.
M. Aubry, D. Maturana, A. Efros, B. Russell, and J. Sivic. Seeing 3d chairs: exemplar partbased 2d-3d alignment using a large dataset of cad models. In CVPR, 2014. R in Machine Learning,
Y. Bengio. Learning deep architectures for ai. Foundations and trends 2(1):1127, 2009.
Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(8):17981828, 2013.
T. Cohen and M. Welling. Learning the irreducible representations of commutative lie groups. arXiv preprint arXiv:1402.4437, 2014.
G. Desjardins, A. Courville, and Y. Bengio. Disentangling factors of variation via generative entangling. arXiv preprint arXiv:1210.5474, 2012.
A. Dosovitskiy, J. Springenberg, and T. Brox. Learning to generate chairs with convolutional neural networks. arXiv:1411.5928, 2015.
I. Goodfellow, H. Lee, Q. V. Le, A. Saxe, and A. Y. Ng. Measuring invariances in deep networks. In Advances in neural information processing systems, pages 646654, 2009.
G. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural computation, 18(7):15271554, 2006.
G. E. Hinton, A. Krizhevsky, and S. D. Wang. Transforming auto-encoders. In Artificial Neural Networks and Machine LearningICANN 2011, pages 4451. Springer, 2011.
D. P. Kingma and M. Welling. arXiv:1312.6114, 2013.  Auto-encoding variational bayes.  arXiv preprint
T. D. Kulkarni, P. Kohli, J. B. Tenenbaum, and V. Mansinghka. Picture: A probabilistic programming language for scene perception. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 43904399, 2015.
T. D. Kulkarni, V. K. Mansinghka, P. Kohli, and J. B. Tenenbaum. Inverse graphics with probabilistic cad models. arXiv preprint arXiv:1407.1339, 2014.
Y. LeCun and Y. Bengio. Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks, 3361, 1995.
H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 609616. ACM, 2009.
V. Mansinghka, T. D. Kulkarni, Y. N. Perov, and J. Tenenbaum. Approximate bayesian image interpretation using generative probabilistic graphics programs. In Advances in Neural Information Processing Systems, pages 15201528, 2013.
R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fidler, R. Urtasun, and A. Yuille. The role of context for object detection and semantic segmentation in the wild. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.
P. Paysan, R. Knothe, B. Amberg, S. Romdhani, and T. Vetter. A 3d face model for pose and illumination invariant face recognition. Genova, Italy, 2009. IEEE.
M. Ranzato, F. J. Huang, Y.-L. Boureau, and Y. LeCun. Unsupervised learning of invariant feature hierarchies with applications to object recognition. In Computer Vision and Pattern Recognition, 2007. CVPR07. IEEE Conference on, pages 18. IEEE, 2007.
R. Salakhutdinov and G. E. Hinton. Deep boltzmann machines. In International Conference on Artificial Intelligence and Statistics, pages 448455, 2009.
Y. Tang, R. Salakhutdinov, and G. Hinton. arXiv:1206.6445, 2012.  Deep lambertian networks.  arXiv preprint
T. Tieleman. Optimizing Neural Networks that Generate Images. PhD thesis, University of Toronto, 2014.
T. Tieleman and G. Hinton. Lecture 6.5 - rmsprop, coursera: Neural networks for machine learning. 2012.
P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. The Journal of Machine Learning Research, 11:33713408, 2010.  9
A. A NANDKUMAR , R. G E , D. H SU , AND S. M. K AKADE, A tensor approach to learning mixed membership community models, The Journal of Machine Learning Research, 15 (2014), pp. 22392312.
A. A NANDKUMAR , R. G E , D. H SU , S. M. K AKADE , AND M. T ELGARSKY, Tensor decompositions for learning latent variable models, Tech. Rep. 1, 2014.  8
C. B ECKMANN AND S. S MITH, Tensorial extensions of independent component analysis for multisubject FMRI analysis, NeuroImage, 25 (2005), pp. 294311.
A. B HASKARA , M. C HARIKAR , A. M OITRA , AND A. V IJAYARAGHAVAN, Smoothed analysis of tensor decompositions, in Proceedings of the 46th Annual ACM Symposium on Theory of Computing, ACM, 2014, pp. 594603.
S. B HOJANAPALLI AND S. S ANGHAVI, A new sampling technique for tensors, arXiv preprint arXiv:1502.05023, (2015).
E. J. C AND ES , X. L I , Y. M A , ACM, 58 (2011), pp. 1137.  AND  J. W RIGHT, Robust principal component analysis?, Journal of the
E. J. C AND ES AND B. R ECHT, Exact matrix completion via convex optimization, Foundations of Computational Mathematics, 9 (2009), pp. 717772.
R. B. C ATTELL, Parallel proportional profiles and other principles for determining the choice of factors by rotation, Psychometrika, 9 (1944), pp. 267283.
V. C HANDRASEKARAN , S. S ANGHAVI , P. A. PARRILO , AND A. S. W ILLSKY, Rank-sparsity incoherence for matrix decomposition, SIAM Journal on Optimization, 21 (2011), pp. 572596.
Y. C HEN , H. X U , C. C ARAMANIS , AND S. S ANGHAVI, Robust matrix completion and corrupted columns, in Proceedings of the 28th International Conference on Machine Learning (ICML-11), L. Getoor and T. Scheffer, eds., New York, NY, USA, 2011, ACM, pp. 873880.
N. G OYAL , S. V EMPALA , AND Y. X IAO, Fourier PCA and robust tensor decomposition, in Proceedings of the 46th Annual ACM Symposium on Theory of Computing, ACM, 2014, pp. 584593.
C. J. H ILLAR AND L.-H. L IM, Most tensor problems are NP-hard, Journal of the ACM, 60 (2013), pp. 45:145:39.
D. H SU , S. K AKADE , AND T. Z HANG, Robust matrix decomposition with sparse corruptions, Information Theory, IEEE Transactions on, 57 (2011), pp. 72217234.
B. H UANG , C. M U , D. G OLDFARB , AND J. W RIGHT, Provable models for robust low-rank tensor completion, Pacific Journal of Optimization, 11 (2015), pp. 339364.
A. K RISHNAMURTHY AND A. S INGH, Low-rank matrix and tensor completion via adaptive sampling, in Advances in Neural Information Processing Systems, 2013.
J. B. K RUSKAL, Three-way arrays: Rank and uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics, Linear Algebra Applicat., 18 (1977).
V. K ULESHOV, A. C HAGANTY, AND P. L IANG, Tensor factorization via matrix factorization, arXiv.org, (2015).
S. L EURGANS , R. ROSS , AND R. A BEL, A decomposition for three-way arrays, SIAM Journal on Matrix Analysis and Applications, 14 (1993), pp. 10641083.
Q. L I , A. P RATER , L. S HEN , AND G. TANG, Overcomplete tensor decomposition via convex optimization, in IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP), Cancun, Mexico, Dec. 2015.
N. M ESGARANI , M. S LANEY, AND S. A. S HAMMA, Discrimination of speech from non-speech based on multiscale spectro-temporal modulations, Audio, Speech and Language Processing, IEEE Transactions on, 14 (2006), pp. 920930.
C. M U , B. H UANG , J. W RIGHT, AND D. G OLDFARB, Square deal: Lower bounds and improved relaxations for tensor recovery, preprint arXiv:1307.5870, 2013.
P. N ETRAPALLI , U. N IRANJAN , S. S ANGHAVI , A. A NANDKUMAR , PCA, in Advances in Neural Information Processing Systems, 2014.  AND  P. JAIN, Non-convex robust
N. R AO , P. S HAH , AND S. W RIGHT, Forward-backward greedy algorithms for signal demixing, in Signals, Systems and Computers, 2013 Asilomar Conference on, IEEE, 2014.
P. S HAH , N. R AO , AND G. TANG, Optimal low-rank tensor recovery from separable measurements: Four contractions suffice, arXiv.org, (2015).
G. TANG AND P. S HAH, Guaranteed tensor decomposition: A moment approach, International Conference on Machine Learning (ICML 2015), (2015), pp. 14911500.
R. T OMIOKA , K. H AYASHI , AND H. K ASHIMA, Estimation of low-rank tensors via convex optimization, preprint arXiv:1010.0789, 2011.
M. Y UAN AND C.-H. Z HANG, On tensor completion via nuclear norm minimization, preprint arXiv:1405.1773, 2014.  9
Mark Herbster and Manfred K Warmuth. Tracking the best linear predictor. The Journal of Machine Learning Research, 1:281309, 2001.
Mark Herbster and Manfred K. Warmuth. Tracking the best expert. Machine Learning, 32:151178, 1998.
Claire Monteleoni. Online learning of non-stationary sequences. Masters thesis, MIT, May 2003. Artificial Intelligence Report 2003-11.
Kamalika Chaudhuri, Yoav Freund, and Daniel Hsu. An online learning-based framework for tracking. In Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI), pages 101108, 2010.
Olivier Bousquet and Manfred K Warmuth. Tracking a small set of experts by mixing past posteriors. The Journal of Machine Learning Research, 3:363396, 2003.
Nicolo Cesa-bianchi, Pierre Gaillard, Gabor Lugosi, and Gilles Stoltz. Mirror Descent meets Fixed Share (and feels no regret). In F. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 980988. Curran Associates, Inc., 2012.
Avrim Blum and Carl Burch. On-line learning and the metrical task system problem. Machine Learning, 39(1):3558, 2000.
Eiji Takimoto and Manfred K. Warmuth. The minimax strategy for Gaussian density estimation. In 13th COLT, pages 100106, 2000.
Peter L. Bartlett, Wouter M. Koolen, Alan Malek, Manfred K. Warmuth, and Eiji Takimoto. Minimax fixed-design linear regression. In P. Grunwald, E. Hazan, and S. Kale, editors, Proceedings of The 28th Annual Conference on Learning Theory (COLT), pages 226239, 2015.
Jacob Abernethy, Peter L. Bartlett, Alexander Rakhlin, and Ambuj Tewari. Optimal strategies and minimax lower bounds for online convex games. In Proceedings of the 21st Annual Conference on Learning Theory (COLT 2008), pages 415423, December 2008.
Edward Moroshko and Koby Crammer. Weighted last-step min-max algorithm with improved sub-logarithmic regret. In N. H. Bshouty, G. Stoltz, N. Vayatis, and T. Zeugmann, editors, Algorithmic Learning Theory - 23rd International Conference, ALT 2012, Lyon, France, October 29-31, 2012. Proceedings, volume 7568 of Lecture Notes in Computer Science, pages 245259. Springer, 2012.
Edward Moroshko and Koby Crammer. A last-step regression algorithm for non-stationary online learning. In Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2013, Scottsdale, AZ, USA, April 29 - May 1, 2013, volume 31 of JMLR Proceedings, pages 451462. JMLR.org, 2013.
Wouter M. Koolen, Alan Malek, and Peter L. Bartlett. Efficient minimax strategies for square loss games. In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems (NIPS) 27, pages 32303238, December 2014.
G. Y. Hu and Robert F. OConnell. Analytical inversion of symmetric tridiagonal matrices. Journal of Physics A: Mathematical and General, 29(7):1511, 1996.  9
C. Dwork. The differential privacy frontier (extended abstract). In TCC, pages 496502, 2009.
S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Learning mixtures of structured distributions over discrete domains. In SODA, 2013.
S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Efficient density estimation via piecewise polynomial approximation. In STOC, pages 604613, 2014.
J. Acharya, I. Diakonikolas, J. Li, and L. Schmidt. Sample-Optimal Density Estimation in Nearly-Linear Time. Available at http://arxiv.org/abs/1506.00671, 2015.
M. Kearns, Y. Mansour, D. Ron, R. Rubinfeld, R. Schapire, and L. Sellie. On the learnability of discrete distributions. In Proc. 26th STOC, pages 273282, 1994.
L. Devroye and G. Lugosi. Combinatorial methods in density estimation. Springer Series in Statistics, Springer, 2001.
L. Birg. Estimation of unimodal densities without smoothness assumptions. Annals of Statistics, 25(3):970981, 1997.
S. Chan, I. Diakonikolas, R. Servedio, and X. Sun. Near-optimal density estimation in near-linear time using variable-width histograms. In NIPS, pages 18441852, 2014.
M. Bun, K. Nissim, U. Stemmer, and S. P. Vadhan. Differentially private release and learning of threshold functions. CoRR, abs/1504.07553, 2015.
U. Grenander. On the theory of mortality measurement. Skand. Aktuarietidskr., 39:125153, 1956.
B.L.S. Prakasa Rao. Estimation of a unimodal density. Sankhya Ser. A, 31:2336, 1969.
P. Groeneboom. Estimating a monotone density. In Proc. of the Berkeley Conference in Honor of Jerzy Neyman and Jack Kiefer, pages 539555, 1985.
L. Birg. Estimating a density under order restrictions: Nonasymptotic minimax risk. Ann. of Stat., pages 9951012, 1987.
F. Balabdaoui and J. A. Wellner. Estimation of a k-monotone density: Limit distribution theory and the spline connection. The Annals of Statistics, 35(6):pp. 25362564, 2007.
L. D umbgen and K. Rufibach. Maximum likelihood estimation of a log-concave density and its distribution function: Basic properties and uniform consistency. Bernoulli, 15(1):4068, 2009.
G. Walther. Inference and modeling with log-concave distributions. Stat. Science, 2009.
Y. Freund and Y. Mansour. Estimating a mixture of two product distributions. In COLT, 1999.
J. Feldman, R. ODonnell, and R. Servedio. Learning mixtures of product distributions over discrete domains. In FOCS, pages 501510, 2005.
C. Daskalakis, I. Diakonikolas, and R.A. Servedio. Learning k-modal distributions via testing. In SODA, pages 13711385, 2012.
S. L. Warner. Randomized Response: A Survey Technique for Eliminating Evasive Answer Bias. Journal of the American Statistical Association, 60(309), 1965.
J. C. Duchi, M. I. Jordan, and M. J. Wainwright. Local privacy and statistical minimax rates. In FOCS, pages 429438, 2013.
J. C. Duchi, M. J. Wainwright, and M. I. Jordan. Local privacy and minimax bounds: Sharp rates for probability estimation. In NIPS, pages 15291537, 2013.
M. Hardt, K. Ligett, and F. McSherry. A simple and practical algorithm for differentially-private data release. In NIPS, 2012.
C. Li, M. Hay, G. Miklau, and Y. Wang. A data- and workload-aware query answering algorithm for range queries under differential privacy. PVLDB, 7(5):341352, 2014.
A. Beimel, K. Nissim, and U. Stemmer. Private learning and sanitization: Pure vs. approximate differential privacy. In RANDOM, pages 363378, 2013.
A. Dvoretzky, J. Kiefer, and J. Wolfowitz. Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator. Ann. Mathematical Statistics, 27(3):642669, 1956.
G. Rote. The convergence rate of the sandwich algorithm for approximating convex functions. Computing, 48:337361, 1992.
F. McSherry and K. Talwar. Mechanism design via differential privacy. In FOCS, pages 94103, 2007.
Pierre Baldi, Peter Sadowski, and Daniel Whiteson. Searching for exotic particles in high-energy physics with deep learning. Nature Communications, (5), 2014.
C. Dwork, G. N. Rothblum, and S. Vadhan. Boosting and differential privacy. In FOCS, 2010.  9
K.Q. Weinberger and L.K. Saul. Distance metric learning for large margin nearest neighbor classification. Journal of Machine Learning Research (JMLR), 10:207244, 2009.
J.V. Davis, B. Kulis, P. Jain, S. Sra, and I.S. Dhillon. Information-theoretic metric learning. International Conference on Machine Learning (ICML), pages 209216, 2007.
M. Schultz and T. Joachims. Learning a distance metric from relative comparisons. Neural Information Processing Systems (NIPS), 2004.
E.P. Xing, A.Y. Ng, M.I. Jordan, and S.J. Russell. Distance metric learning with application to clustering with side-information. Neural Information Processing Systems (NIPS), pages 505512, 2002.
B. McFee and G.R.G. Lanckriet. Metric learning to rank. International Conference on Machine Learning (ICML), 2010.
B. Shaw, B. Huang, and T. Jebara. Learning a distance metric from a network. Neural Information Processing Systems (NIPS), 2011.
D.K.H. Lim, B. McFee, and G.R.G. Lanckriet. Robust structural metric learning. International Conference on Machine Learning (ICML), 2013.
M.T. Law, N. Thome, and M. Cord. Fantope regularization in metric learning. Computer Vision and Pattern Recognition (CVPR), 2014.
M. Anthony and P. Bartlett. Neural network learning: Theoretical foundations. Cambridge University Press, 1999.
K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal approximators. Neural Networks, 4:359366, 1989.
K. Bache and M. Lichman. UCI machine learning repository, 2013.
R. Jin, S. Wang, and Y. Zhou. Regularized distance metric learning: Theory and algorithm. Neural Information Processing Systems (NIPS), pages 862870, 2009.
O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning Research (JMLR), 2:499526, 2002.
W. Bian and D. Tao. Learning a distance metric by empirical loss minimization. International Joint Conference on Artificial Intelligence (IJCAI), pages 11861191, 2011.
Q. Cao, Z. Guo, and Y. Ying. abs/1207.5437, 2013.  Generalization bounds for metric and similarity learning.  CoRR,
A. Bellet and A. Habrard. Robustness and generalization for metric learning. CoRR, abs/1209.1086, 2012.
Y. Ying and C. Campbell. Generalization bounds for learning the kernel. Conference on Computational Learning Theory (COLT), 2009.
C. Cortes, M. Mohri, and A. Rostamizadeh. New generalization bounds for learning kernels. International Conference on Machine Learning (ICML), 2010.
M-F. Balcan, A. Blum, and N. Srebro. Improved guarantees for learning via similarity functions. Conference on Computational Learning Theory (COLT), 2008.
A. Bellet, A. Habrard, and M. Sebban. Similarity learning for provably accurate sparse linear classification. International Conference on Machine Learning (ICML), 2012.
Z. Guo and Y. Ying. Generalization classification via regularized similarity learning. Neural Computation, 26(3):497552, 2014.
A. Bellet, A. Habrard, and M. Sebban. A survey on metric learning for feature vectors and structured data. CoRR, abs/1306.6709, 2014.
P. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research (JMLR), 3:463482, 2002.
R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Compressed Sensing, Theory and Applications. 2010.  9
A. Krizhevsky, I. Sutskever, , and G. E. Hinton. ImageNet classification with deep convolutional neural networks. In Neural Information Processing Systems, 2012.
J. Ba, V. Mnih, and K. Kavukcuoglu. Multiple object recognition with visual attention. In International Conference on Learning Representations, 2015.
V. Mnih, N. Heess, A. Graves, and K. Kavukcuoglu. Recurrent models of visual attention. In Neural Information Processing Systems, 2014.
Y. Tang, N. Srivastava, and R. Salakhutdinov. Learning generative models with visual attention. In Neural Information Processing Systems, 2014.
K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. S. Zemel, and Y. Bengio. Show, attend, and tell: neural image caption generation with visual attention. In International Conference on Machine Learning, 2015.
D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. In International Conference on Learning Representations, 2015.
W. Zaremba and I. Sutskever. Reinforcement learning neural Turing machines. arXiv:1505.00521, 2015.
P. Dayan, G. E. Hinton, R. M. Neal, and R. S. Zemel. The Helmholtz machine. Neural Computation, 7:889904, 1995.
J. Bornschein and Y. Bengio. Reweighted wake-sleep. arXiv:1406.2751, 2014.
J. Paisley, D. M. Blei, and M. I. Jordan. Variational Bayesian inference with stochastic search. In International Conference on Machine Learning, 2012.
A. Mnih and K. Gregor. Neural variational inference and learning in belief networks. In International Conference on Machine Learning, 2014.
H. Larochelle and G. E. Hinton. Learning to combine foveal glimpses with a third-order Boltzmann machine. In Neural Information Processing Systems, 2010.
M. Denil, L. Bazzani, H. Larochelle, and N. de Freitas. Learning where to attend with deep architectures for image tracking. Neural Computation, 24(8):215184, April 2012.
A. Graves. Generating sequences with recurrent neural networks. arXiv:1308.0850, 2014.
K. Gregor, I. Danihelka, A. Graves, and D. Wierstra. DRAW: a recurrent neural network for image generation. arXiv:1502.04623, 2015.
Radford M. Neal. Connectionist learning of belief networks. Artificial Intelligence, 1992.
R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229256, 1992.
D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In International Conference on Learning Representations, 2014.
D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning, 2014.
L. Itti, C. Koch, and E. Niebur. A model of saliency-based visual attention for rapid scene analysis. IEEE Transactions of Pattern Analysis and Machine Intelligence, 20(11):125459, November 1998.
T. Judd, K. Ehinger, F. Durand, and A. Torralba. Learning to predict where humans look. In International Conference on Computer Vision, 2009.
Y. Tang and R. Salakhutdinov. Learning stochastic feedforward neural networks. In Neural Information Processing Systems, 2013.
Y. Burda, R. Grosse, and R. Salakhutdinov. Importance weighted autoencoders. arXiv:1509.00519, 2015.
Lex Weaver and Nigel Tao. The optimal reward baseline for gradient-based reinforcement learning. In Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence, pages 538545. Morgan Kaufmann Publishers Inc., 2001.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, 1998.
Micah Hodosh, Peter Young, and Julia Hockenmaier. Framing image description as a ranking task: Data, models and evaluation metrics. Journal of Artificial Intelligence Research, pages 853899, 2013.
D. Kingma and J. L. Ba. Adam: a method for stochastic optimization. arXiv:1412.6980, 2014.
Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. arXiv preprint arXiv:1412.2306, 2014.  9
S.L. Lauritzen. Graphical models. Oxford University Press, USA, 1996.
Jung Hun Oh and Joseph O. Deasy. Inference of radio-responsive gene regulatory networks using the graphical lasso algorithm. BMC Bioinformatics, 15(S-7):S5, 2014.
C. D. Manning and H. Schutze. Foundations of Statistical Natural Language Processing. MIT Press, 1999.
J.W. Woods. Markov image modeling. IEEE Transactions on Automatic Control, 23:846850, October 1978.
M. Hassner and J. Sklansky. Markov random field models of digitized image texture. In ICPR78, pages 538540, 1978.
G. Cross and A. Jain. Markov random field texture models. IEEE Trans. PAMI, 5:2539, 1983.
E. Ising. Beitrag zur theorie der ferromagnetismus. Zeitschrift fur Physik, 31:253258, 1925.
B. D. Ripley. Spatial statistics. Wiley, New York, 1981.
E. Yang, A. C. Lozano, and P. Ravikumar. Elementary estimators for graphical models. In Neur. Info. Proc. Sys. (NIPS), 27, 2014.
M. Yuan and Y. Lin. Model selection and estimation in the Gaussian graphical model. Biometrika, 94(1): 1935, 2007.
J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graphical Lasso. Biostatistics, 2007.
O. Bannerjee, , L. El Ghaoui, and A. dAspremont. Model selection through sparse maximum likelihood estimation for multivariate Gaussian or binary data. Jour. Mach. Lear. Res., 9:485516, March 2008.
P. Ravikumar, M. J. Wainwright, G. Raskutti, and B. Yu. High-dimensional covariance estimation by minimizing `1 -penalized log-determinant divergence. Electronic Journal of Statistics, 5:935980, 2011.
S. Boyd and L. Vandenberghe. Convex optimization. Cambridge University Press, Cambridge, UK, 2004.
N. Meinshausen and P. Buhlmann. High-dimensional graphs and variable selection with the Lasso. Annals of Statistics, 34:14361462, 2006.
E. Yang, P. Ravikumar, G. I. Allen, and Z. Liu. Graphical models via generalized linear models. In Neur. Info. Proc. Sys. (NIPS), 25, 2012.
R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B, 58(1):267288, 1996.
Z.J. Daye, J. Chen, and Li H. High-dimensional heteroscedastic regression with an application to eqtl data analysis. Biometrics, 68:316326, 2012.
Michael Finegold and Mathias Drton. Robust graphical modeling of gene networks using classical and alternative t-distributions. The Annals of Applied Statistics, 5(2A):10571080, 2011.
H. Sun and H. Li. Robust Gaussian graphical modeling via l1 penalization. Biometrics, 68:1197206, 2012.
A. Alfons, C. Croux, and S. Gelper. Sparse least trimmed squares regression for analyzing highdimensional large data sets. Ann. Appl. Stat., 7:226248, 2013.
P-L Loh and M. J. Wainwright. Regularized m-estimators with nonconvexity: Statistical and algorithmic theory for local optima. Arxiv preprint arXiv:1305.2436v2, 2013.
C. J. Hsieh, M. Sustik, I. Dhillon, and P. Ravikumar. Sparse inverse covariance matrix estimation using quadratic approximation. In Neur. Info. Proc. Sys. (NIPS), 24, 2011.
Y. Nesterov. Gradient methods for minimizing composite objective function. Technical Report 76, Center for Operations Research and Econometrics (CORE), Catholic Univ. Louvain (UCL)., 2007.
N. H. Nguyen and T. D. Tran. Robust Lasso with missing and grossly corrupted observations. IEEE Trans. Info. Theory, 59(4):20362058, 2013.
S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A unified framework for high-dimensional analysis of M-estimators with decomposable regularizers. Statistical Science, 27(4):538557, 2012.
E. Yang and A. C. Lozano. Robust gaussian graphical modeling with the trimmed graphical Lasso. arXiv:1510.08512, 2015.
Rachel B Brem and Leonid Kruglyak. The landscape of genetic complexity across 5,700 gene expression traits in yeast. Proceedings of the National Academy of Sciences of the United States of America, 102(5): 15721577, 2005.
M. Kanehisa, S. Goto, Y. Sato, M. Kawashima, M. Furumichi, and M. Tanabe. Data, information, knowledge and principle: back to metabolism in kegg. Nucleic Acids Res., 42:D199D205, 2014.  9
J. Acharya, H. Das, A. Jafarpour, A. Orlitsky, and S. Pan, Competitive closeness testing, COLT, 2011.
J. Acharya, H. Das, A. Jafarpour, A. Orlitsky, and S. Pan, Competitive classification and closeness testing. COLT, 2012.
J. Acharya, A. Jafarpour, A. Orlitsky, and A. T. Suresh, Sublinear algorithms for outlier detection and generalized closeness testing, Proceedings of the International Symposium on Information Theory (ISIT), 32003204, 2014.
Z. Bar-Yossef, R. Kumar, and D. Sivakumar. Sampling algorithms: lower bounds and applications, STOC, 2001.
T. Batu, L. Fortnow, R. Rubinfeld, W. D. Smith, and P. White, Testing that distributions are close, FOCS, 2000.
T. Batu, S. Dasgupta, R. Kumar, and R. Rubinfeld, The complexity of approximating the entropy, SIAM Journal on Computing, 2005.
T. Batu, E. Fischer, L. Fortnow, R. Kumar, R. Rubinfeld, and P. White, Testing random variables for independence and identity, FOCS, 2001.
S.-on Chan, I. Diakonikolas, P. Valiant, G. Valiant, Optimal Algorithms for Testing Closeness of Discrete Distributions, Symposium on Discrete Algorithms (SODA), 11931203, 2014,
M. Charikar, S. Chaudhuri, R. Motwani, and V.R. Narasayya, Towards estimation error guarantees for distinct values, Symposium on Principles of Database Systems (PODS), 2000.
A. Czumaj and C. Sohler, Testing expansion in bounded-degree graphs, FOCS, 2007.
O. Goldreich and D. Ron, On testing expansion in bounded-degree graphs, Technical Report TR00-020, Electronic Colloquium on Computational Complexity, 2000.
S. Guha, A. McGregor, and S. Venkatasubramanian, Streaming and sublinear approximation of entropy and information distances, Symposium on Discrete Algorithms (SODA), 2006.
D. Hsu, A. Kontorovich, and C. Szepesvari, Mixing time estimation in reversible Markov chains from a single sample path, arXiv:1506.02903, 2015 (to appear in NIPS 2015).
S. Kale and C. Seshadhri, An expansion tester for bounded degree graphs, ICALP (1), Lecture Notes in Computer Science, Vol. 5125, 527538, 2008.
A. Keinan and A. G. Clark. Recent explosive human population growth has resulted in an excess of rare genetic variants. Science, 336(6082):740743, 2012.
D. A. Levin, Y. Peres, and E. L. Wilmer, Markov Chains and Mixing Times, Amer. Math. Soc., 2009.
A. Nachmias and A. Shapira, Testing the expansion of a graph, Electronic Colloquium on Computational Complexity (ECCC), Vol. 14 (118), 2007.
M. R. Nelson and D. Wegmann et al., An abundance of rare functional variants in 202 drug target genes sequenced in 14,002 people. Science, 337(6090):100104, 2012.
L. Paninski, Estimation of entropy and mutual information, Neural Comp., Vol. 15 (6), 11911253, 2003.
L. Paninski, Estimating entropy on m bins given fewer than m samples, IEEE Transactions on Information Theory, Vol. 50 (9), 22002203, 2004.
L. Paninski, A coincidence-based test for uniformity given very sparsely-sampled discrete data, IEEE Transactions on Information Theory, Vol. 54, 47504755, 2008.
S. Raskhodnikova, D. Ron, A. Shpilka, and A. Smith, Strong lower bounds for approximating distribution support size and the distinct elements problem, SIAM Journal on Computing, Vol. 39(3), 813842, 2009.
R. Rubinfeld, Taming big probability distributions, XRDS, Vol. 19(1), 2428, 2012.
A. Sinclair and M. Jerrum, Approximate counting, uniform generation and rapidly mixing Markov chains, Information and Computation, Vol. 82(1), 93133, 1989.
J. A. Tennessen, A.W. Bigham, and T.D. OConnor et al. Evolution and functional impact of rare coding variation from deep sequencing of human exomes. Science, 337(6090):6469, 2012
G. Valiant and P. Valiant, Estimating the unseen: an n/ log n-sample estimator for entropy and support size, shown optimal via new CLTs, STOC, 2011.
G. Valiant and P. Valiant, Estimating the unseen: improved estimators for entropy and other properties, NIPS, 2013.
G. Valiant and P. Valiant, An Automatic Inequality Prover and Instance Optimal Identity Testing, FOCS, 5160, 2014.
P. Valiant, Testing symmetric properties of distributions, STOC, 2008.
P. Valiant, Testing Symmetric Properties of Distributions, PhD thesis, M.I.T., 2008.  9
E.G. Birgin and M. Raydan. Robust stopping criteria for Dykstras algorithm. SIAM Journal on Scientific Computing, 26(4):14051414, 2005.
M. Bouchard, A.L. Jousselme, and P.E. Dore. A proof for the positive definiteness of the Jaccard index matrix. International Journal of Approximate Reasoning, 54(5):615626, 2013.
S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, New York, NY, USA, 2004.
S. Boyd and L. Xiao. Least-squares covariance matrix adjustment. SIAM Journal on Matrix Analysis and Applications, 27(2):532546, 2005.
A.Z. Broder, M. Charikar, A.M. Frieze, and M. Mitzenmacher. Min-wise independent permutations. In Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, pages 327336. ACM, 1998.
A.P. Dempster, N.M. Laird, and D.B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39:138, 1977.
F. Deutsch. Best Approximation in Inner Product Spaces. Springer, New York, NY, USA, 2001.
R.O. Duda and P.E. Hart. Pattern Classification. John Wiley and Sons, Hoboken, NJ, USA, 2000.
R.L. Dykstra. An algorithm for restricted least squares regression. Journal of the American Statistical Association, 78(384):837842, 1983.
R. Escalante and M. Raydan. Alternating Projection Methods. SIAM, Philadelphia, PA, USA, 2011.
Z. Ghahramani and M.I. Jordan. Supervised learning from incomplete data via an EM approach. In Advances in Neural Information Processing Systems, volume 6, pages 120127. Morgan Kaufmann, 1994.
G.H. Golub and C.F. Van Loan. Matrix Computations. Johns Hopkins University Press, Baltimore, MD, USA, 1996.
N.J. Higham. Computing the nearest correlation matrix - a problem from finance. IMA Journal of Numerical Analysis, 22:329343, 2002.
P. Jaccard. The distribution of the flora in the alpine zone. New Phytologist, 11(2):3750, 1912.
A.K. Jain, M.N. Murty, and P.J. Flynn. Data clustering: A review. ACM Computing Surveys, 31(3):264 323, 1999.
D.L. Knol and J.M.F. ten Berge. Least-squares approximation of an improper correlation matrix by a proper one. Psychometrika, 54(1):5361, 1989.
J. Leskovec, A. Rajaraman, and J. Ullman. Mining of Massive Datasets. Cambridge University Press, New York, NY, USA, 2014.
P. Li and A.C. Konig. Theory and applications of b-bit minwise hashing. Communications of the ACM, 54(8):101109, 2011.
W. Li, K.H. Lee, and K.S. Leung. Large-scale RLSC learning without agony. In Proceedings of the 24th International Conference on Machine Learning, pages 529536. ACM, 2007.
D.G. Luenberger. Optimization by Vector Space Methods. John Wiley & Sons, New York, NY, USA, 1969.
J. Malick. A dual approach to semidefinite least-squares problems. SIAM Journal on Matrix Analysis and Applications, 26(1):272284, 2004.
H. Qi and D. Sun. A quadratically convergent newton method for computing the nearest correlation matrix. SIAM Journal on Matrix Analysis and Applications, 28(2):360385, 2006.
D.J. Rogers and T.T. Tanimoto. A computer program for classifying plants. Science, 132(3434):1115 1118, 1960.
G. Salton, A. Wong, and C.S. Yang. A vector space model for automatic indexing. Communications of the ACM, 18(11):613620, 1975.
B. Scholkopf and A.J. Smola. Learning With Kernels, Support Vector Machines, Regularization, Optimization, and Beyond. The MIT Press, Cambridge, MA, USA, 2001.  9
N. J. Gordon, D. J. Salmond, and A. F. Smith, Novel approach to nonlinear/non-gaussian bayesian state estimation, in IEE Proceedings F (Radar and Signal Processing), vol. 140, pp. 107113, IET, 1993.
A. Doucet, N. De Freitas, and N. Gordon, Sequential monte carlo methods in practice. Springer-Verlag, 2001.
C. Andrieu, A. Doucet, and R. Holenstein, Particle markov chain monte carlo methods, Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 72, no. 3, pp. 269342, 2010.
G. Poyiadjis, A. Doucet, and S. S. Singh, Particle approximations of the score and observed information matrix in state space models with application to parameter estimation, Biometrika, vol. 98, no. 1, pp. 65 80, 2011.
R. Van Der Merwe, A. Doucet, N. De Freitas, and E. Wan, The unscented particle filter, in Advances in Neural Information Processing Systems, pp. 584590, 2000.
R. Frigola, Y. Chen, and C. Rasmussen, Variational gaussian process state-space models, in Advances in Neural Information Processing Systems, pp. 36803688, 2014.
D. J. MacKay, Information theory, inference, and learning algorithms, vol. 7. Cambridge university press Cambridge, 2003.
T. P. Minka, Expectation propagation for approximate bayesian inference, in Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence, pp. 362369, Morgan Kaufmann Publishers Inc., 2001.
J. Cornebise, Adaptive Sequential Monte Carlo Methods. PhD thesis, Ph. D. thesis, University Pierre and Marie CurieParis 6, 2009.
A. Graves, Supervised sequence labelling with recurrent neural networks, vol. 385. Springer, 2012.
I. Sutskever, O. Vinyals, and Q. V. Le, Sequence to sequence learning with neural networks, in Advances in Neural Information Processing Systems, pp. 31043112, 2014.
A. Graves, Generating sequences with recurrent neural networks, CoRR, vol. abs/1308.0850, 2013.
S. Hochreiter and J. Schmidhuber, Long short-term memory, Neural computation, vol. 9, no. 8, pp. 17351780, 1997.
C. M. Bishop, Mixture density networks, 1994.
K. Gregor, I. Danihelka, A. Graves, D. J. Rezende, and D. Wierstra, DRAW: A recurrent neural network for image generation, in Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pp. 14621471, 2015.
A. McHutchon, Nonlinear modelling and control using Gaussian processes. PhD thesis, University of Cambridge UK, Department of Engineering, 2014.
N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent, Modeling temporal dependencies in highdimensional sequences: Application to polyphonic music generation and transcription, in International Conference on Machine Learning (ICML), 2012.
Y. Bengio, N. Boulanger-Lewandowski, and R. Pascanu, Advances in optimizing recurrent networks, in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pp. 8624 8628, IEEE, 2013.
J. Bayer and C. Osendorfer, Learning stochastic recurrent networks, arXiv preprint arXiv:1411.7610, 2014.
D. P. Kingma and J. Ba, Adam: A method for stochastic optimization, The International Conference on Learning Representations (ICLR), 2015.
D. P. Kingma and M. Welling, Auto-encoding variational bayes, The International Conference on Learning Representations (ICLR), 2014.
D. J. Rezende, S. Mohamed, and D. Wierstra, Stochastic backpropagation and approximate inference in deep generative models, International Conference on Machine Learning (ICML), 2014.
A. Mnih and K. Gregor, Neural variational inference and learning in belief networks, International Conference on Machine Learning (ICML), 2014.
R. E. Turner and M. Sahani, Two problems with variational expectation maximisation for time-series models, in Bayesian Time series models (D. Barber, T. Cemgil, and S. Chiappa, eds.), ch. 5, pp. 109 130, Cambridge University Press, 2011.
G. E. Hinton, P. Dayan, B. J. Frey, and R. M. Neal, The wake-sleep algorithm for unsupervised neural networks, Science, vol. 268, no. 5214, pp. 11581161, 1995.
J. Bornschein and Y. Bengio, Reweighted wake-sleep, The International Conference on Learning Representations (ICLR), 2015.  9
Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.
Jrg Bornschein and Yoshua Bengio. Reweighted wake-sleep. CoRR, pages 11, 2014.
Peter W. Glynn. Likelihood ratio gradient estimation for stochastic systems. Commun. ACM, 33(10):75 84, October 1990.
Geoffrey Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The wake-sleep algorithm for unsupervised neural networks. Science, 268(5214):11581161, 1995.
Matthew D. Hoffman, David M. Blei, and Francis R. Bach. Online learning for latent dirichlet allocation. In NIPS, pages 856864, 2010.
Matthew D. Hoffman, David M. Blei, Chong Wang, and John William Paisley. Stochastic variational inference. Journal of Machine Learning Research, 14(1):13031347, 2013.
Michael I. Jordan, Zoubin Ghahramani, Tommi S. Jaakkola, and Lawrence K. Saul. An introduction to variational methods for graphical models. Mach. Learn., 37(2):183233, November 1999.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.
A. Kucukelbir, R. Ranganath, A. Gelman, and D.M. Blei. Automatic variational inference in stan. In Advances in Neural Information Processing Systems, 28, 2015.
Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. In The 31st International Conference on Machine Learning (ICML 2014), 2014.
Radford M. Neal. Connectionist learning of belief networks. Artif. Intell., 56(1):71113, July 1992.
John William Paisley, David M. Blei, and Michael I. Jordan. Variational bayesian inference with stochastic search. In ICML, 2012.
J. Peters and S. Schaal. Policy gradient methods for robotics. In Proceedings of the IEEE International Conference on Intelligent Robotics Systems (IROS 2006), 2006.
Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics (AISTATS), page 814822, 2014.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In The 31st International Conference on Machine Learning (ICML 2014), 2014.
Herbert Robbins and Sutton Monro. A Stochastic Approximation Method. The Annals of Mathematical Statistics, 22(3):400407, 1951.
Ruslan Salakhutdinov and Iain Murray. On the quantitative analysis of Deep Belief Networks. In Andrew McCallum and Sam Roweis, editors, Proceedings of the 25th Annual International Conference on Machine Learning (ICML 2008), pages 872879. Omnipress, 2008.
Tim Salimans and David A. Knowles. Fixed-form variational posterior approximation through stochastic linear regression. Bayesian Anal., 8(4):837882, 12 2013.
Tim Salimans and David A. Knowles. On Using Control Variates with Stochastic Approximation for Variational Bayes and its Connection to Stochastic Linear Regression, January 2014.
Michalis K. Titsias and Miguel Lazaro-Gredilla. Doubly stochastic variational bayes for non-conjugate inference. In The 31st International Conference on Machine Learning (ICML 2014), 2014.
Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Mach. Learn., 8(3-4):229256, May 1992.  9
Alekh Agarwal and Leon Bottou. A lower bound for the optimization of finite sums. arXiv:1410.0723, 2014.
Alekh Agarwal and John C Duchi. Distributed delayed stochastic optimization. In Advances in Neural Information Processing Systems, pages 873881, 2011.
Dimitri P Bertsekas. Incremental gradient, subgradient, and proximal methods for convex optimization: A survey. Optimization for Machine Learning, 2010:138, 2011.
Aaron Defazio. New Optimization Methods for Machine Learning. PhD thesis, Australian National University, 2014.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In NIPS 27, pages 16461654. 2014.
Aaron J Defazio, Tiberio S Caetano, and Justin Domke. Finito: A faster, permutable incremental gradient method for big data problems. arXiv:1407.2710, 2014.
Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. Optimal distributed online prediction using mini-batches. The Journal of Machine Learning Research, 13(1):165202, 2012.
M. Gurbuzbalaban, A. Ozdaglar, and P. Parrilo. A globally convergent incremental Newton method. Mathematical Programming, 151(1):283313, 2015.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In NIPS 26, pages 315323. 2013.
Jakub Konecny, Jie Liu, Peter Richtarik, and Martin Takac. Mini-Batch Semi-Stochastic Gradient Descent in the Proximal Setting. arXiv:1504.04407, 2015.
Jakub Konecny and Peter Richtarik. Semi-Stochastic Gradient Descent Methods. arXiv:1312.1666, 2013.
Mu Li, David G Andersen, Alex J Smola, and Kai Yu. Communication Efficient Distributed Machine Learning with the Parameter Server. In NIPS 27, pages 1927, 2014.
Ji Liu and Stephen J. Wright. Asynchronous stochastic coordinate descent: Parallelism and convergence properties. SIAM Journal on Optimization, 25(1):351376, 2015.
Ji Liu, Steve Wright, Christopher Re, Victor Bittorf, and Srikrishna Sridhar. An asynchronous parallel stochastic coordinate descent algorithm. In ICML 2014, pages 469477, 2014.
Julien Mairal. Optimization with first-order surrogate functions. arXiv:1305.3120, 2013.
A Nedic, Dimitri P Bertsekas, and Vivek S Borkar. Distributed asynchronous incremental subgradient methods. Studies in Computational Mathematics, 8:381407, 2001.
A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):15741609, 2009.
Yu Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341362, 2012.
Atsushi Nitanda. Stochastic Proximal Gradient Descent with Acceleration Techniques. In NIPS 27, pages 15741582, 2014.
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent. In NIPS 24, pages 693701, 2011.
Sashank Reddi, Ahmed Hefny, Carlton Downey, Avinava Dubey, and Suvrit Sra. Large-scale randomizedcoordinate descent methods with non-separable linear constraints. In UAI 31, 2015.
Peter Richtarik and Martin Takac. Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function. Mathematical Programming, 144(1-2):138, 2014.
H. Robbins and S. Monro. A stochastic approximation method. Annals of Mathematical Statistics, 22:400407, 1951.
Mark W. Schmidt, Nicolas Le Roux, and Francis R. Bach. Minimizing Finite Sums with the Stochastic Average Gradient. arXiv:1309.2388, 2013.
Shai Shalev-Shwartz and Tong Zhang. Accelerated mini-batch stochastic dual coordinate ascent. In NIPS 26, pages 378385, 2013.
Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss. The Journal of Machine Learning Research, 14(1):567599, 2013.
Ohad Shamir and Nathan Srebro. On distributed stochastic optimization and learning. In Proceedings of the 52nd Annual Allerton Conference on Communication, Control, and Computing, 2014.
Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM Journal on Optimization, 24(4):20572075, 2014.
Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J Smola. Parallelized stochastic gradient descent. In NIPS, pages 25952603, 2010.  9
T.M. Gureckis, J. Martin, J. McDonnell, et al. psiTurk: An open-source framework for conducting replicable behavioral experiments online. (in press).
Daniel W Barowy, Charlie Curtsinger, Emery D Berger, and Andrew McGregor. Automan: A platform for integrating human-based and digital computation. ACM SIGPLAN Notices, 47(10):639654, 2012.
Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits problem. Journal of Computer and System Sciences, 78(5), 2012.
Tanguy Urvoy, Fabrice Clerot, Raphael Feraud, and Sami Naamane. Generic exploration and k-armed voting bandits. In Proceedings of the 30th International Conference on Machine Learning, 2013.
Kevin Jamieson, Sumeet Katariya, Atul Deshpande, and Robert Nowak. Sparse dueling bandits. AISTATS, 2015.
Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. The Journal of Machine Learning Research, 7:10791105, 2006.
Kevin Jamieson, Matthew Malloy, Robert Nowak, and Sebastien Bubeck. lilucb: An optimal exploration algorithm for multi-armed bandits. In Proceedings of The 27th Conference on Learning Theory, 2014.
Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In Advances in neural information processing systems, pages 22492257, 2011.
Yisong Yue and Thorsten Joachims. Beat the mean bandit. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 241248, 2011.
Sameer Agarwal, Josh Wills, Lawrence Cayton, et al. Generalized non-metric multidimensional scaling. In International Conference on Artificial Intelligence and Statistics, pages 1118, 2007.
Omer Tamuz, Ce Liu, Ohad Shamir, Adam Kalai, and Serge J Belongie. Adaptively learning the crowd kernel. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), 2011.
Laurens Van Der Maaten and Kilian Weinberger. Stochastic triplet embedding. In Machine Learning for Signal Processing (MLSP), 2012 IEEE International Workshop on, pages 16. IEEE, 2012.
Burr Settles. Active learning literature survey. University of Wisconsin, Madison, 52(55-66):11.
Kevin G Jamieson and Robert D Nowak. Low-dimensional embedding using adaptively selected ordinal data. In Allerton Conference on Communication, Control, and Computing, 2011.
Alekh Agarwal, Leon Bottou, Miroslav Dudik, and John Langford. Para-active learning. arXiv preprint arXiv:1310.8243, 2013.
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems, 2011.
All our ideas. http://allourideas.org/. Accessed: 2015-09-15.
Byron C Wallace, Kevin Small, Carla E Brodley, Joseph Lau, and Thomas A Trikalinos. Deploying an interactive machine learning system in an evidence-based practice center: abstrackr. In Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium, pages 819824. ACM, 2012.
Kyle H Ambert, Aaron M Cohen, Gully APC Burns, Eilis Boudreau, and Kemal Sonmez. Virk: an active learning-based system for bootstrapping knowledge base development in the neurosciences. Frontiers in neuroinformatics, 7, 2013.
Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation. In International conference on World wide web. ACM, 2010.
Deepak Agarwal, Bee-Chung Chen, and Pradheep Elango. Explore/exploit schemes for web content optimization. In Data Mining, 2009. ICDM. Ninth IEEE International Conference on. IEEE, 2009.
Alekh Agarwal, Olivier Chapelle, Miroslav Dudk, and John Langford. A reliable effective terascale linear learning system. The Journal of Machine Learning Research, 15(1):11111133, 2014.
Xiangrui Meng et al. MLlib: Machine learning in apache spark. arXiv:1505.06807, 2015.
Oryx 2. https://github.com/OryxProject/oryx. Accessed: 2015-06-05.
Yucheng Low, Danny Bickson, Joseph Gonzalez, et al. Distributed graphlab: a framework for machine learning and data mining in the cloud. Proceedings of the VLDB Endowment, 5(8):716727, 2012.
CrowdFlower. http://www.crowdflower.com/. Accessed: 2015-06-05.
Seungwhan Moon and Jaime G Carbonell. Proactive learning with multiple class-sensitive labelers. In Data Science and Advanced Analytics (DSAA), 2014 International Conference on. IEEE, 2014.
Daniel Crankshaw, Peter Bailis, Joseph E Gonzalez, et al. The missing piece in complex analytics: Low latency, scalable model management and serving with velox. CIDR, 2015.  9
A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky. Tensor decompositions for learning latent variable models. The Journal of Machine Learning Research, 15(1):27732832, 2014.
A. Anandkumar, D. Hsu, and S. M. Kakade. A method of moments for mixture models and hidden markov models. arXiv preprint arXiv:1203.0683, 2012.
E. J. Candes and C. Fernandez-Granda. Super-resolution from noisy data. Journal of Fourier Analysis and Applications, 19(6):12291254, 2013.
E. J. Candes and C. Fernandez-Granda. Towards a mathematical theory of super-resolution. Communications on Pure and Applied Mathematics, 67(6):906956, 2014.
Y. Chen and Y. Chi. Robust spectral compressed sensing via structured matrix completion. Information Theory, IEEE Transactions on, 60(10):65766601, 2014.
S. Dasgupta. Learning mixtures of gaussians. In Foundations of Computer Science, 1999. 40th Annual Symposium on, pages 634644. IEEE, 1999.
S. Dasgupta and A. Gupta. An elementary proof of a theorem of johnson and lindenstrauss. Random structures and algorithms, 22(1):6065, 2003.
S. Dasgupta and L. J. Schulman. A two-round variant of em for gaussian mixtures. In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence, pages 152159. Morgan Kaufmann Publishers Inc., 2000.
D. L. Donoho. Superresolution via sparsity constraints. SIAM Journal on Mathematical Analysis, 23(5):13091331, 1992.
C. Fernandez-Granda. A Convex-programming Framework for Super-resolution. PhD thesis, Stanford University, 2014.
R. A. Harshman. Foundations of the parafac procedure: Models and conditions for an explanatory multi-modal factor analysis. 1970.
V. Komornik and P. Loreti. Fourier series in control theory. Springer Science & Business Media, 2005.
S. Leurgans, R. Ross, and R. Abel. A decomposition for three-way arrays. SIAM Journal on Matrix Analysis and Applications, 14(4):10641083, 1993.
W. Liao and A. Fannjiang. Music for single-snapshot spectral estimation: Stability and superresolution. Applied and Computational Harmonic Analysis, 2014.
A. Moitra. The threshold for super-resolution via extremal functions. arXiv:1408.1681, 2014.  arXiv preprint
E. Mossel and S. Roch. Learning nonsingular phylogenies and hidden markov models. In Proceedings of the thirty-seventh annual ACM symposium on Theory of computing, pages 366 375. ACM, 2005.
S. Nandi, D. Kundu, and R. K. Srivastava. Noise space decomposition method for twodimensional sinusoidal model. Computational Statistics & Data Analysis, 58:147161, 2013.
K. Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions of the Royal Society of London. A, pages 71110, 1894.
D. Potts and M. Tasche. Parameter estimation for nonincreasing exponential sums by pronylike methods. Linear Algebra and its Applications, 439(4):10241039, 2013.
D. L. Russell. Controllability and stabilizability theory for linear partial differential equations: recent progress and open questions. Siam Review, 20(4):639739, 1978. 8
A. Sanjeev and R. Kannan. Learning mixtures of arbitrary gaussians. In Proceedings of the thirty-third annual ACM symposium on Theory of computing, pages 247257. ACM, 2001.
G. Schiebinger, E. Robeva, and B. Recht. Superresolution without separation. arXiv preprint arXiv:1506.03144, 2015.
G. Tang, B. N. Bhaskar, P. Shah, and B. Recht. Compressed sensing off the grid. Information Theory, IEEE Transactions on, 59(11):74657490, 2013.
S. S. Vempala and Y. F. Xiao. Max vs min: Independent component analysis with nearly linear sample complexity. arXiv preprint arXiv:1412.2954, 2014.  9
Leon Bottou. Large-scale machine learning with stochastic gradient descent. In COMPSTAT2010, pages 177186. Springer, 2010.
Leon Bottou. Stochastic gradient descent tricks. In Neural Networks: Tricks of the Trade, pages 421436. Springer, 2012.
Leon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, NIPS, volume 20, pages 161168. NIPS Foundation, 2008.
Christopher De Sa, Kunle Olukotun, and Christopher Re. Global convergence of stochastic gradient descent for some nonconvex matrix problems. ICML, 2015.
John C Duchi, Peter L Bartlett, and Martin J Wainwright. Randomized smoothing for stochastic optimization. SIAM Journal on Optimization, 22(2):674701, 2012.
Olivier Fercoq and Peter Richtarik. Accelerated, parallel and proximal coordinate descent. arXiv preprint arXiv:1312.5799, 2013.
Thomas R Fleming and David P Harrington. Counting processes and survival analysis. volume 169, pages 5657. John Wiley & Sons, 1991.
Pankaj Gupta, Ashish Goel, Jimmy Lin, Aneesh Sharma, Dong Wang, and Reza Zadeh. WTF: The who to follow service at twitter. WWW 13, pages 505514, 2013.
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. ICML, 2015.
Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternating minimization. In STOC, pages 665674. ACM, 2013.
Bjorn Johansson, Maben Rabi, and Mikael Johansson. A randomized incremental subgradient method for distributed optimization in networked systems. SIAM Journal on Optimization, 20(3):11571170, 2009.
Jakub Konecny, Zheng Qu, and Peter Richtarik. S2cd: Semi-stochastic coordinate descent. In NIPS Optimization in Machine Learning workshop, 2014.
Yann Le Cun, Leon Bottou, Genevieve B. Orr, and Klaus-Robert Muller. Efficient backprop. In Neural Networks, Tricks of the Trade. 1998.
Ji Liu and Stephen J. Wright. Asynchronous stochastic coordinate descent: Parallelism and convergence properties. SIOPT, 25(1):351376, 2015.
Ji Liu, Stephen J Wright, Christopher Re, Victor Bittorf, and Srikrishna Sridhar. An asynchronous parallel stochastic coordinate descent algorithm. JMLR, 16:285322, 2015.
Ioannis Mitliagkas, Michael Borokhovich, Alexandros G. Dimakis, and Constantine Caramanis. Frogwild!: Fast pagerank approximations on graph engines. PVLDB, 2015.
Feng Niu, Benjamin Recht, Christopher Re, and Stephen Wright. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In NIPS, pages 693701, 2011.
Cyprien Noel and Simon Osindero. Dogwild!Distributed Hogwild for CPU & GPU. 2014.
Shameem Ahamed Puthiya Parambath. Matrix factorization methods for recommender systems. 2013.
Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for strongly convex stochastic optimization. ICML, 2012.
Peter Richtarik and Martin Takac. Parallel coordinate descent methods for big data optimization. Mathematical Programming, pages 152, 2012.
Qing Tao, Kang Kong, Dejun Chu, and Gaowei Wu. Stochastic coordinate descent methods for regularized smooth and nonsmooth losses. In Machine Learning and Knowledge Discovery in Databases, pages 537552. Springer, 2012.
Rachael Tappenden, Martin Takac, and Peter Richtarik. On the complexity of parallel coordinate descent. arXiv preprint arXiv:1503.03033, 2015.
Hsiang-Fu Yu, Cho-Jui Hsieh, Si Si, and Inderjit S Dhillon. Scalable coordinate descent approaches to parallel matrix factorization for recommender systems. In ICDM, pages 765774, 2012.
Ce Zhang and Christopher Re. Dimmwitted: A study of main-memory statistical analytics. PVLDB, 2014.  9
Harold Christopher Burger, Christian Schuler, and Stefan Harmeling. Learning how to combine internal and external denoising methods. In Pattern Recognition, pages 121130. Springer, 2013.
Harold Christopher Burger, Christian J Schuler, and Stefan Harmeling. Image denoising with multilayer perceptrons, part 1: comparison with existing algorithms and with bounds. arXiv preprint arXiv:1211.1544, 2012.
Yunjin Chen, Thomas Pock, Rene Ranftl, and Horst Bischof. Revisiting loss-specific training of filterbased mrfs for image restoration. In Pattern Recognition, pages 271281. Springer, 2013.
Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Image denoising by sparse 3-d transform-domain collaborative filtering. Image Processing, IEEE Transactions on, 16(8):20802095, 2007.
Michael Elad and Michal Aharon. Image denoising via sparse and redundant representations over learned dictionaries. Image Processing, IEEE Transactions on, 15(12):37363745, 2006.
Sean Ryan Fanello, Cem Keskin, Pushmeet Kohli, Shahram Izadi, Jamie Shotton, Antonio Criminisi, Ugo Pattacini, and Tim Paek. Filter forests for learning data-dependent convolutional kernels. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 17091716. IEEE, 2014.
Yacov Hel-Or and Doron Shaked. A discriminative approach for wavelet denoising. Image Processing, IEEE Transactions on, 17(4):443457, 2008.
Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):7987, 1991.
Viren Jain and Sebastian Seung. Natural image denoising with convolutional networks. In Advances in Neural Information Processing Systems, pages 769776, 2009.
Yan Karklin and Michael S Lewicki. Emergence of complex cell properties by learning to generalize in natural scenes. Nature, 457(7225):8386, 2009.
Effi Levi. Using natural image priors-maximizing or sampling? PhD thesis, The Hebrew University of Jerusalem, 2009.
Anat Levin and Boaz Nadler. Natural image denoising: Optimality and inherent bounds. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 28332840. IEEE, 2011.
Siwei Lyu and Eero P Simoncelli. Statistical modeling of images with fields of gaussian scale mixtures. In Advances in Neural Information Processing Systems, pages 945952, 2006.
Julien Mairal, Francis Bach, Jean Ponce, Guillermo Sapiro, and Andrew Zisserman. Non-local sparse models for image restoration. In Computer Vision, 2009 IEEE 12th International Conference on, pages 22722279. IEEE, 2009.
Carl E Rassmusen. minimize.m, 2006. http://learning.eng.cam.ac.uk/carl/code/minimize/.
Stefan Roth and Michael J Black. Fields of experts: A framework for learning image priors. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on, volume 2, pages 860867. IEEE, 2005.
Uwe Schmidt, Qi Gao, and Stefan Roth. A generative perspective on mrfs in low-level vision. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 17511758. IEEE, 2010.
Uwe Schmidt and Stefan Roth. Shrinkage fields for effective image restoration. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 27742781. IEEE, 2014.
Libin Sun, Sunghyun Cho, Jue Wang, and James Hays. Edge-based blur kernel estimation using patch priors. In Computational Photography (ICCP), 2013 IEEE International Conference on, pages 18. IEEE, 2013.
Benigno Uria, Iain Murray, and Hugo Larochelle. Rnade: The real-valued neural autoregressive densityestimator. In Advances in Neural Information Processing Systems, pages 21752183, 2013.
Guoshen Yu, Guillermo Sapiro, and Stephane Mallat. Solving inverse problems with piecewise linear estimators: From gaussian mixture models to structured sparsity. Image Processing, IEEE Transactions on, 21(5):24812499, 2012.
Daniel Zoran and Yair Weiss. From learning models of natural image patches to whole image restoration. In Computer Vision (ICCV), 2011 IEEE International Conference on, pages 479486. IEEE, 2011.
Daniel Zoran and Yair Weiss. Natural images, gaussian mixtures and dead leaves. In NIPS, pages 1745 1753, 2012.  9
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 31043112, 2014.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations by error propagation. Technical report, DTIC Document, 1985.
Anthony J Robinson. An application of recurrent nets to phone probability estimation. Neural Networks, IEEE Transactions on, 5(2):298305, 1994.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In ICLR 2015, arXiv preprint arXiv:1409.0473, 2014.
Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In ICLEAR 2015, arXiv preprint arXiv:1410.3916, 2014.
Alex Graves. Generating sequences with recurrent neural networks. arXiv:1308.0850, 2013.  arXiv preprint
Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. Grammar as a foreign language. arXiv preprint arXiv:1412.7449, 2014.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In CVPR 2015, arXiv preprint arXiv:1411.4555, 2014.
Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks for visual recognition and description. In CVPR 2015, arXiv preprint arXiv:1411.4389, 2014.
Wojciech Zaremba and Ilya Sutskever. Learning to execute. arXiv preprint arXiv:1410.4615, 2014.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):17351780, 1997.
Nitish Srivastava, Elman Mansimov, and Ruslan Salakhutdinov. Unsupervised learning of video representations using lstms. In ICML 2015, arXiv preprint arXiv:1502.04681, 2015.
Ray A Jarvis. On the identification of the convex hull of a finite set of points in the plane. Information Processing Letters, 2(1):1821, 1973.
Ronald L. Graham. An efficient algorith for determining the convex hull of a finite planar set. Information processing letters, 1(4):132133, 1972.
Franco P. Preparata and Se June Hong. Convex hulls of finite sets of points in two and three dimensions. Communications of the ACM, 20(2):8793, 1977.
S1 Rebay. Efficient unstructured mesh generation by means of delaunay triangulation and bowyer-watson algorithm. Journal of computational physics, 106(1):125138, 1993.
Richard Bellman. Dynamic programming treatment of the travelling salesman problem. Journal of the ACM (JACM), 9(1):6163, 1962.
Suboptimal travelling salesman https://github.com/dmishin/tsp-solver.  problem  (tsp)  solver.
Traveling salesman problem c++ implementation. https://github.com/samlbest/traveling-salesman.  Available  at  Available  at
C++ implementation of traveling salesman problem using christofides and 2-opt. Available at https://github.com/beckysag/traveling-salesman.  9
A. Agarwal, A. Anandkumar, P. Jain, P. Netrapalli, and R. Tandon. Learning sparsely used overcomplete dictionaries via alternating minimization. CoRR, abs/1310.7991, 2013.
S. Arora, R. Ge, T. Ma, and A. Moitra. Simple, efficient, and neural algorithms for sparse coding. CoRR, abs/1503.00778, 2015.
S. Arora, R. Ge, and A. Moitra. New algorithms for learning incoherent and overcomplete dictionaries. arXiv preprint arXiv:1308.6273, 2013.
A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2(1):183202, 2009.
E. J. Candes. The restricted isometry property and its implications for compressed sensing. Comptes Rendus Mathematique, 346(9):589592, 2008.
E. J. Candes, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information. IEEE Trans. on Inf. Theory, 52(2):489509, 2006.
E. J. Candes and T. Tao. Near-optimal signal recovery from random projections: Universal encoding strategies? IEEE Trans. on Inf. Theory, 52(12):54065425, Dec 2006.
D. L. Donoho. Compressed sensing. IEEE Trans. on Inf. Theory, 52(4):12891306, 2006.
D. L. Donoho, A. Maleki, and A. Montanari. Message-passing algorithms for compressed sensing. Proceedings of the National Academy of Sciences, 106(45):1891418919, 2009.
S. Foucart and H. Rauhut. A Mathematical Introduction to Compressive Sensing. Birkhauser Basel, 2013.
V. Gripon and C. Berrou. Sparse neural networks with large learning diversity. IEEE Transactions on Neural Networks, 22(7):10871096, 2011.
D. J. Gross and M. Mezard. The simplest spin glass. Nuclear Physics B, 240(4):431  452, 1984.
D. O. Hebb. The organization of behavior: A neuropsychological theory. Psychology Press, 2005.
C. Hillar and N. M. Tran. Robust exponential memory in hopfield networks. arXiv preprint arXiv:1411.4625, 2014.
J. J. Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8):25542558, 1982.
T. Hu, A. Genkin, and D. B. Chklovskii. A network of spiking neurons for computing sparse representations in an energy-efficient way. Neural computation, 24(11):28522872, 2012.
S. Jankowski, A. Lozowski, and J. M. Zurada. Complex-valued multistate neural associative memory. IEEE Transactions on Neural Networks, 7(6):14911496, Nov 1996.
A. Karbasi, A. H. Salavati, and A. Shokrollahi. Convolutional neural associative memories: Massive capacity with noise tolerance. CoRR, abs/1407.6513, 2014.
K. R. Kumar, A. H. Salavati, and A. Shokrollahi. Exponential pattern retrieval capacity with non-binary associative memory. In 2011 IEEE Information Theory Workshop (ITW), pages 8084, Oct 2011.
A. Maleki. Coherence analysis of iterative thresholding algorithms. In 47th Annual Allerton Conference on Communication, Control, and Computing, 2009. Allerton 2009, pages 236243, Sept 2009.
R. J. McEliece and E. C. Posner. The number of stable points of an infinite-range spin glass memory. Telecommunications and Data Acquisition Progress Report, 83:209215, 1985.
R. J. McEliece, E. C. Posner, E. R. Rodemich, and S. S. Venkatesh. The capacity of the hopfield associative memory. Information Theory, IEEE Transactions on, 33(4):461482, 1987.
M. K. Muezzinoglu, C. Guzelis, and J. M. Zurada. A new design method for the complex-valued multistate hopfield associative memory. IEEE Transactions on Neural Networks, 14(4):891899, July 2003.
B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision Research, 37(23):3311  3325, 1997.
A. H. Salavati and A. Karbasi. Multi-level error-resilient neural networks. In 2012 IEEE International Symposium on Information Theory Proceedings (ISIT), pages 10641068, July 2012.
F. Tanaka and S. F. Edwards. Analytic theory of the ground state properties of a spin glass. i. ising spin glass. Journal of Physics F: Metal Physics, 10(12):2769, 1980.
R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027, 2010.
M. J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using `1 constrained quadratic programming (lasso). IEEE Trans. Inform. Theory, 55(5):21832202, May 2009.
W. Yin, S. Osher, D. Goldfarb, and J. Darbon. Bregman iterative algorithms for `1 -minimization with applications to compressed sensing. SIAM Journal on Imaging Sciences, 1(1):143168, 2008.  9
Alan Mislove, Bimal Viswanath, Krishna P. Gummadi, and Peter Druschel. You are who you know: Inferring user profiles in Online Social Networks. In Proceedings of the 3rd ACM International Conference of Web Search and Data Mining (WSDM10), New York, NY, February 2010.
Shuo Chen, J. Moore, D. Turnbull, and T. Joachims. Playlist prediction via metric embedding. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), pages 714722, 2012.
Jeffrey Pennington, Richard Socher, and Christopher D. Manning. GloVe: Global vectors for word representation. In EMNLP, 2014.
Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In NIPS, 2014.
S. Arora, R. Ge, and A. Moitra. Learning topic models  going beyond SVD. In FOCS, 2012.
Sanjeev Arora, Rong Ge, Yonatan Halpern, David Mimno, Ankur Moitra, David Sontag, Yichen Wu, and Michael Zhu. A practical algorithm for topic modeling with provable guarantees. In ICML, 2013.
T. Hofmann. Probabilistic latent semantic analysis. In UAI, pages 289296, 1999.
D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research, pages 9931022, 2003. Preliminary version in NIPS 2001.
JamesP. Boyle and RichardL. Dykstra. A method for finding projections onto the intersection of convex sets in Hilbert spaces. In Advances in Order Restricted Statistical Inference, volume 37 of Lecture Notes in Statistics, pages 2847. Springer New York, 1986.
Adrian S. Lewis, D. R. Luke, and Jrme Malick. Local linear convergence for alternating and averaged nonconvex projections. Foundations of Computational Mathematics, 9:485513, 2009.
T. L. Griffiths and M. Steyvers. Finding scientific topics. Proceedings of the National Academy of Sciences, 101:52285235, 2004.
Moontae Lee and David Mimno. Low-dimensional embeddings for interpretable anchor-based topic inference. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 13191328. Association for Computational Linguistics, 2014.
Mary E Broadbent, Martin Brown, Kevin Penner, I Ipsen, and R Rehman. Subset selection algorithms: Randomized vs. deterministic. SIAM Undergraduate Research Online, 3:5071, 2010.
D. Blei and J. Lafferty. A correlated topic model of science. Annals of Applied Statistics, pages 1735, 2007.
David Mimno, Hanna Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. Optimizing semantic coherence in topic models. In EMNLP, 2011.
A. Daniilidis, A. S. Lewis, J. Malick, and H. Sendov. Prox-regularity of spectral functions and spectral sets. Journal of Convex Analysis, 15(3):547560, 2008.
Christian Thurau, Kristian Kersting, and Christian Bauckhage. Yes we can: simplex volume maximization for descriptive web-scale matrix factorization. In CIKM10, pages 17851788, 2010.
Abhishek Kumar, Vikas Sindhwani, and Prabhanjan Kambadur. Fast conical hull algorithms for nearseparable non-negative matrix factorization. CoRR, pages 11, 2012.
Jos M. P. Nascimento, Student Member, and Jos M. Bioucas Dias. Vertex component analysis: A fast algorithm to unmix hyperspectral data. IEEE Transactions on Geoscience and Remote Sensing, pages 898910, 2005.
Cecile Gomez, H. Le Borgne, Pascal Allemand, Christophe Delacourt, and Patrick Ledru. N-FindR method versus independent component analysis for lithological identification in hyperspectral imagery. International Journal of Remote Sensing, 28(23):53155338, 2007.
Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective learning on multi-relational data. In Proceedings of the 28th International Conference on Machine Learning (ICML11), ICML, pages 809816. ACM, 2011.
Da Kuang, Haesun Park, and Chris H. Q. Ding. Symmetric nonnegative matrix factorization for graph clustering. In SDM. SIAM / Omnipress, 2012.
Anima Anandkumar, Dean P. Foster, Daniel Hsu, Sham Kakade, and Yi-Kai Liu. A spectral algorithm for latent Dirichlet allocation. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States., pages 926934, 2012.
Thang Nguyen, Yuening Hu, and Jordan Boyd-Graber. Anchors regularized: Adding robustness and extensibility to scalable topic-modeling algorithms. In Association for Computational Linguistics, 2014.
Tianyi Zhou, Jeff A Bilmes, and Carlos Guestrin. Divide-and-conquer learning by anchoring a conical hull. In Advances in Neural Information Processing Systems 27, pages 12421250. 2014.  9
M. Ayer, H. D. Brunk, G. M. Ewing, W. T. Reid, and E. Silverman. An empirical distribution function for sampling with incomplete information. The Annals of Mathematical Statistics, 26(4):pp. 641647, 1955.
D. J. Barlow, R. E .and Bartholomew, J.M. Bremner, and H. D. Brunk. Statistical inference under order restrictions: the theory and application of Isotonic Regression. Wiley New York, 1972.
F. Gebhardt. An algorithm for monotone Regression with one or more independent variables. Biometrika, 57(2):263271, 1970.
W. L. Maxwell and J. A. Muckstadt. Establishing consistent and realistic reorder intervals in productiondistribution systems. Operations Research, 33(6):13161341, 1985.
R. Roundy. A 98%-effective lot-sizing rule for a multi-product, multi-stage production / inventory system. Mathematics of Operations Research, 11(4):pp. 699727, 1986.
S.T. Acton and A.C. Bovik. Nonlinear image estimation using piecewise and local image models. Image Processing, IEEE Transactions on, 7(7):979991, Jul 1998.  8
C.I.C. Lee. The Min-Max algorithm and Isotonic Regression. The Annals of Statistics, 11(2):pp. 467477, 1983.
R. L. Dykstra and T. Robertson. An algorithm for Isotonic Regression for two or more independent variables. The Annals of Statistics, 10(3):pp. 708716, 1982.
S. Chatterjee, A. Guntuboyina, and B. Sen. On Risk Bounds in Isotonic and Other Shape Restricted Regression Problems. The Annals of Statistics, to appear.
A. T. Kalai and R. Sastry. The isotron algorithm: High-dimensional Isotonic Regression. In COLT, 2009.
T. Moon, A. Smola, Y. Chang, and Z. Zheng. Intervalrank: Isotonic Regression with listwise and pairwise constraints. In WSDM, pages 151160. ACM, 2010.
S. M Kakade, V. Kanade, O. Shamir, and A. Kalai. Efficient learning of generalized linear and single index models with Isotonic Regression. In NIPS. 2011.
B. Zadrozny and C. Elkan. Transforming classifier scores into accurate multiclass probability estimates. KDD, pages 694699, 2002.
H. Narasimhan and S. Agarwal. On the relationship between binary classification, bipartite ranking, and binary class probability estimation. In NIPS. 2013.
S. Angelov, B. Harb, S. Kannan, and L. Wang. Weighted Isotonic Regression under the l1 norm. In SODA, 2006.
K. Punera and J. Ghosh. Enhanced hierarchical classification via Isotonic smoothing. In WWW, 2008.
Z. Zheng, H. Zha, and G. Sun. Query-level learning to rank using Isotonic Regression. In Communication, Control, and Computing, Allerton Conference on, 2008.
D.S. Hochbaum and M. Queyranne. Minimizing a convex cost closure set. SIAM Journal on Discrete Mathematics, 16(2):192207, 2003.
Q. F. Stout. Isotonic Regression via partitioning. Algorithmica, 66(1):93112, 2013.
Q. F. Stout. Weighted l Isotonic Regression. Manuscript, 2011.
Q. F. Stout. Strict l Isotonic Regression. Journal of Optimization Theory and Applications, 152(1):121 135, 2012.
Q. F Stout. Fastest Isotonic Regression algorithms. http://web.eecs.umich.edu/qstout/ IsoRegAlg_140812.pdf.
Q. F. Stout. Isotonic Regression for multiple independent variables. Algorithmica, 71(2):450470, 2015.
Y. Kaufman and A. Tamir. Locating service centers with precedence constraints. Discrete Applied Mathematics, 47(3):251  261, 1993.
Q. F. Stout. L infinity Isotonic Regression for linear, multidimensional, and tree orders. CoRR, 2015.
S. I. Daitch and D. A. Spielman. Faster approximate lossy generalized flow via interior point algorithms. STOC 08, pages 451460. ACM, 2008.
A. Madry. Navigating central path with electrical flows. In FOCS, 2013.
Y.  T. Lee and A. Sidford. Path finding methods for linear programming: Solving linear programs in O( rank) iterations and faster algorithms for maximum flow. In FOCS, 2014.
D. A. Spielman and S. Teng. Nearly-linear time algorithms for graph partitioning, graph sparsification, and solving linear systems. STOC 04, pages 8190. ACM, 2004.
I. Koutis, G. L. Miller, and R. Peng. A nearly-m log n time solver for SDD linear systems. FOCS 11, pages 590598, Washington, DC, USA, 2011. IEEE Computer Society.
M. B. Cohen, R. Kyng, G. L. Miller, J. W. Pachocki, R. Peng, A. B. Rao, and S. C. Xu. Solving SDD linear systems in nearly m log1/2 n time. STOC 14, 2014.
R. Kyng, A. Rao, S. Sachdeva, and D. A. Spielman. Algorithms for Lipschitz learning on graphs. In Proceedings of COLT 2015, pages 11901223, 2015.
S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
D. den Hertog, F. Jarre, C. Roos, and T. Terlaky. A sufficient condition for self-concordance. Math. Program., 69(1):7588, July 1995.
J. Renegar. A mathematical view of interior-point methods in convex optimization. SIAM, 2001.
A. Nemirovski. Lecure notes: Interior point polynomial time methods in convex programming, 2004.
E. J. McShane. Extension of range of functions. Bull. Amer. Math. Soc., 40(12):837842, 12 1934.
H. Whitney. Analytic extensions of differentiable functions defined in closed sets. Transactions of the American Mathematical Society, 36(1):pp. 6389, 1934.  9
Kaiser Asif, Wei Xing, Sima Behpour, and Brian D. Ziebart. Adversarial cost-sensitive classification. In Proceedings of the Conference on Uncertainty in Artificial Intelligence, 2015.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. Learning to rank: from pairwise approach to listwise approach. In Proceedings of the International Conference on Machine Learning, pages 129 136. ACM, 2007.
Corinna Cortes and Mehryar Mohri. AUC optimization vs. error rate minimization. In Advances in Neural Information Processing Systems, pages 313320, 2004.
Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3):273297, 1995.  8
Krzysztof J Dembczynski, Willem Waegeman, Weiwei Cheng, and Eyke Hullermeier. An exact algorithm for F-measure maximization. In Advances in Neural Information Processing Systems, pages 14041412, 2011.
Yoav Freund, Raj Iyer, Robert E Schapire, and Yoram Singer. An efficient boosting algorithm for combining preferences. The Journal of machine learning research, 4:933969, 2003.
Andrew Gilpin, Javier Pena, and Tuomas Sandholm. First-order algorithm with o (ln (1/e)) convergence for e-equilibrium in two-person zero-sum games. In AAAI Conference on Artificial Intelligence, pages 7582, 2008.
Peter D. Grunwald and A. Phillip Dawid. Game theory, maximum entropy, minimum discrepancy, and robust Bayesian decision theory. Annals of Statistics, 32:13671433, 2004.
Tamir Hazan, Joseph Keshet, and David A McAllester. Direct loss minimization for structured prediction. In Advances in Neural Information Processing Systems, pages 15941602, 2010.
Klaus-Uwe Hoffgen, Hans-Ulrich Simon, and Kevin S Vanhorn. Robust trainability of single neurons. Journal of Computer and System Sciences, 50(1):114125, 1995.
Martin Jansche. Maximum expected F-measure training of logistic regression models. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 692699. Association for Computational Linguistics, 2005.
Thorsten Joachims. Optimizing search engines using clickthrough data. In Proceedings of the International Conference on Knowledge Discovery and Data Mining, pages 133142. ACM, 2002.
Thorsten Joachims. A support vector method for multivariate performance measures. In Proceedings of the International Conference on Machine Learning, pages 377384. ACM, 2005.
Richard M. Karp. Reducibility among combinatorial problems. Springer, 1972.
Adrian S Lewis and Michael L Overton. Nonsmooth optimization via BFGS. 2008.
M. Lichman. UCI machine learning repository, 2013.
Richard J Lipton and Neal E Young. Simple strategies for large zero-sum games with applications to complexity theory. In Proc. of the ACM Symposium on Theory of Computing, pages 734740. ACM, 1994.
Dong C Liu and Jorge Nocedal. On the limited memory BFGS method for large scale optimization. Mathematical programming, 45(1-3):503528, 1989.
H Brendan McMahan, Geoffrey J Gordon, and Avrim Blum. Planning in the presence of cost functions controlled by an adversary. In Proceedings of the International Conference on Machine Learning, pages 536543, 2003.
David R Musicant, Vipin Kumar, and Aysel Ozgur. Optimizing F-measure with support vector machines. In FLAIRS Conference, pages 356360, 2003.
Shameem Puthiya Parambath, Nicolas Usunier, and Yves Grandvalet. Optimizing F-measures by costsensitive classification. In Advances in Neural Information Processing Systems, pages 21232131, 2014.
Tao Qin and Tie-Yan Liu. Introducing LETOR 4.0 datasets. arXiv preprint arXiv:1306.2597, 2013.
Mani Ranjbar, Greg Mori, and Yang Wang. Optimizing complex loss functions in structured prediction. In Proceedings of the European Conference on Computer Vision, pages 580593. Springer, 2010.
Ben Taskar, Vassil Chatalbashev, Daphne Koller, and Carlos Guestrin. Learning structured prediction models: A large margin approach. In Proceedings of the International Conference on Machine Learning, pages 896903. ACM, 2005.
Flemming Topse. Information theoretical optimization techniques. Kybernetika, 15(1):827, 1979.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. Support vector machine learning for interdependent and structured output spaces. In Proceedings of the International Conference on Machine Learning, page 104. ACM, 2004.
Vladimir Vapnik. Principles of risk minimization for learning theory. In Advances in Neural Information Processing Systems, pages 831838, 1992.
John von Neumann and Oskar Morgenstern. Theory of Games and Economic Behavior. Princeton University Press, 1947.
Jun Xu and Hang Li. Adarank: a boosting algorithm for information retrieval. In Proc. of the International Conference on Research and Development in Information Retrieval, pages 391398. ACM, 2007.  9
A. Agarwal and J. C. Duchi. Distributed delayed stochastic optimization. NIPS, 2011.
H. Avron, A. Druinsky, and A. Gupta. Revisiting asynchronous linear solvers: Provable convergence rate
through randomization. IPDPS, 2014.
D. P. Bertsekas and J. N. Tsitsiklis. Parallel and distributed computation: numerical methods, volume 23.
Prentice hall Englewood Cliffs, NJ, 1989.
J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, A. Senior, P. Tucker, K. Yang, Q. V. Le, et al.
Large scale distributed deep networks. NIPS, 2012.
O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Optimal distributed online prediction using mini-batches.
Journal of Machine Learning Research, 13(1):165202, 2012.
O. Fercoq and P. Richtarik.
arXiv:1312.5799, 2013.
Accelerated, parallel and proximal coordinate descent.
arXiv preprint
H. R. Feyzmahdavian, A. Aytekin, and M. Johansson. An asynchronous mini-batch algorithm for regularized
stochastic optimization. ArXiv e-prints, May 18 2015.
S. Ghadimi and G. Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming.
SIAM Journal on Optimization, 23(4):23412368, 2013.
M. Hong. A distributed, asynchronous and incremental algorithm for nonconvex optimization: An ADMM
based approach. arXiv preprint arXiv:1412.6058, 2014.
Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe:
Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.
A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Computer Science
Department, University of Toronto, Tech. Rep, 1(4):7, 2009.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks.
NIPS, pages 10971105, 2012.
M. Li, L. Zhou, Z. Yang, A. Li, F. Xia, D. G. Andersen, and A. Smola. Parameter server for distributed machine
learning. Big Learning NIPS Workshop, 2013.
M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, V. Josifovski, J. Long, E. J. Shekita, and B.-Y. Su.
Scaling distributed machine learning with the parameter server. OSDI, 2014a.
M. Li, D. G. Andersen, A. J. Smola, and K. Yu. Communication efficient distributed machine learning with the
parameter server. NIPS, 2014b.
J. Liu and S. J. Wright. Asynchronous stochastic coordinate descent: Parallelism and convergence properties.
arXiv preprint arXiv:1403.3862, 2014.
J. Liu, S. J. Wright, C. Re, V. Bittorf, and S. Sridhar. An asynchronous parallel stochastic coordinate descent
algorithm. ICML, 2014a.
J. Liu, S. J. Wright, and S. Sridhar. An asynchronous parallel randomized kaczmarz algorithm. arXiv preprint
arXiv:1401.4780, 2014b.
H. Mania, X. Pan, D. Papailiopoulos, B. Recht, K. Ramchandran, and M. I. Jordan. Perturbed iterate analysis
for asynchronous stochastic optimization. arXiv preprint arXiv:1507.06970, 2015.
J. Marecek, P. Richtarik, and M. Takac. Distributed block coordinate descent for minimizing partially separable
functions. arXiv preprint arXiv:1406.0238, 2014.
A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic
programming. SIAM Journal on Optimization, 19(4):15741609, 2009.
F. Niu, B. Recht, C. Re, and S. Wright. Hogwild: A lock-free approach to parallelizing stochastic gradient
descent. NIPS, 2011.
T. Paine, H. Jin, J. Yang, Z. Lin, and T. Huang. Gpu asynchronous stochastic gradient descent to speed up
neural network training. NIPS, 2013.
F. Petroni and L. Querzoni. Gasgd: stochastic gradient descent for distributed asynchronous matrix completion
via graph partitioning. ACM Conference on Recommender systems, 2014.
S. Sridhar, S. Wright, C. Re, J. Liu, V. Bittorf, and C. Zhang. An approximate, efficient LP solver for lp
rounding. NIPS, 2013.
R. Tappenden, M. Takac, and P. Richtarik. On the complexity of parallel coordinate descent. arXiv preprint
arXiv:1503.03033, 2015.
K. Tran, S. Hosseini, L. Xiao, T. Finley, and M. Bilenko. Scaling up stochastic dual coordinate ascent. ICML,
H. Yun, H.-F. Yu, C.-J. Hsieh, S. Vishwanathan, and I. Dhillon. Nomad: Non-locking, stochastic multi-machine
algorithm for asynchronous and decentralized matrix completion. arXiv preprint arXiv:1312.0193, 2013.
R. Zhang and J. Kwok. Asynchronous distributed ADMM for consensus optimization. ICML, 2014.
S. Zhang, A. Choromanska, and Y. LeCun. Deep learning with elastic averaging SGD. CoRR, abs/1412.6651,
D. Jacobson and D. Mayne. Differential dynamic programming. American Elsevier, 1970.
E. Todorov and W. Li. A generalized iterative LQG method for locally-optimal feedback control of constrained nonlinear stochastic systems. In ACC. IEEE, 2005.
Y. Tassa, T. Erez, and W. D. Smart. Receding horizon differential dynamic programming. In Proc. of NIPS, 2008.
Y. Pan and E. Theodorou. Probabilistic differential dynamic programming. In Proc. of NIPS, 2014.
S. Levine and V. Koltun. Variational policy search via trajectory optimization. In Proc. of NIPS, 2013.
D. P. Kingma and M. Welling. Auto-encoding variational bayes. In Proc. of ICLR, 2014.
D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proc. of ICML, 2014.
M. D. Zeiler, D. Krishnan, G. W. Taylor, and R. Fergus. Deconvolutional networks. In CVPR, 2010.
A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning to generate chairs with convolutional neural networks. In Proc. of CVPR, 2015.
R. F. Stengel. Optimal Control and Estimation. Dover Publications, 1994.
W. Li and E. Todorov. Iterative Linear Quadratic Regulator Design for Nonlinear Biological Movement Systems. In Proc. of ICINCO, 2004.
M. Toussaint. Robot Trajectory Optimization using Approximate Inference. In Proc. of ICML, 2009.
M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for graphical models. In Machine Learning, 1999.
D. Kingma and J. Ba. Adam: A method for stochastic optimization. In Proc. of ICLR, 2015.
H. Wang, K. Tanaka, and M. Griffin. An approach to fuzzy control of nonlinear systems; stability and design issues. IEEE Trans. on Fuzzy Systems, 4(1), 1996.
R. S. Sutton and A. G. Barto. Introduction to Reinforcement Learning. MIT Press, Cambridge, MA, USA, 1st edition, 1998. ISBN 0262193981.
W. Bohmer, J. T. Springenberg, J. Boedecker, M. Riedmiller, and K. Obermayer. Autonomous learning of state representations for control. KI - Kunstliche Intelligenz, 2015.
S. Lange and M. Riedmiller. Deep auto-encoder neural networks in reinforcement learning. In Proc. of IJCNN, 2010.
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540), 02 2015.
H. van Hoof, J. Peters, and G. Neumann. Learning of non-parametric control policies with highdimensional state features. In Proc. of AISTATS, 2015.
S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. CoRR, abs/1504.00702, 2015. URL http://arxiv.org/abs/1504.00702.
N. Wahlstrom, T. B. Schon, and M. P. Deisenroth. From pixels to torques: Policy learning with deep dynamical models. CoRR, abs/1502.02251, 2015. URL http://arxiv.org/abs/1502.02251.
K. Gregor, I. Danihelka, A. Graves, D. Rezende, and D. Wierstra. DRAW: A recurrent neural network for image generation. In Proc. of ICML, 2015.
J. Bayer and C. Osendorfer. Learning stochastic recurrent networks. In NIPS 2014 Workshop on Advances in Variational Inference, 2014.
G. Hinton, A. Krizhevsky, and S. Wang. Transforming auto-encoders. In Proc. of ICANN, 2011.
L. Dinh, D. Krueger, and Y. Bengio. Nice: Non-linear independent components estimation. CoRR, abs/1410.8516, 2015. URL http://arxiv.org/abs/1410.8516.
T. Cohen and M. Welling. Transformation properties of learned visual representations. In ICLR, 2015.
G. W. Taylor, L. Sigal, D. J. Fleet, and G. E. Hinton. Dynamical binary latent variable models for 3d human pose tracking. In Proc. of CVPR, 2010.
R. Memisevic. Learning to relate images. IEEE Trans. on PAMI, 35(8):18291846, 2013.
J. Langford, R. Salakhutdinov, and T. Zhang. Learning nonlinear dynamic models. In ICML, 2009.
M. West and J. Harrison. Bayesian Forecasting and Dynamic Models (Springer Series in Statistics). Springer-Verlag, February 1997. ISBN 0387947256.
T. Matsubara, V. Gomez, and H. J. Kappen. Latent Kullback Leibler control for continuous-state systems using probabilistic graphical models. UAI, 2014.  9
Maria-Florina Balcan and Phil Long. Active and passive learning of linear separators under log-concave distributions. In Conference on Learning Theory, pages 288316, 2013.
Maria-Florina Balcan, Alina Beygelzimer, and John Langford. Agnostic active learning. In Proceedings of the 23rd international conference on Machine learning, pages 6572. ACM, 2006.
Maria-Florina Balcan, Andrei Broder, and Tong Zhang. Margin based active learning. In Proceedings of the 20th annual conference on Learning theory, pages 3550. Springer-Verlag, 2007.
A. Beygelzimer, S. Dasgupta, and J. Langford. Importance weighted active learning. In ICML, 2009.
A. Beygelzimer, D. Hsu, J. Langford, and T. Zhang. Agnostic active learning without constraints. In NIPS, 2010.
R.M. Castro and R.D. Nowak. Minimax bounds for active learning. Information Theory, IEEE Transactions on, 54(5):2339 2353, 2008.
D. Cohn, L. Atlas, and R. Ladner. Improving generalization with active learning. Machine Learning, 15:201221, 1994.
S. Dasgupta. Coarse sample complexity bounds for active learning. In Advances in Neural Information Processing Systems 18, 2005.
S. Dasgupta, D. Hsu, and C. Monteleoni. A general agnostic active learning algorithm. In NIPS, 2007.
S. Hanneke. Theoretical Foundations of Active Learning. PhD thesis, Carnegie Mellon University, 2009.
Steve Hanneke. Theory of disagreement-based active learning. Foundations and Trends in Machine Learning, 7(2-3):131309, 2014.
D. G. Horvitz and D. J. Thompson. A generalization of sampling without replacement from a finite universe. J. Amer. Statist. Assoc., 47:663685, 1952. ISSN 0162-1459.
Daniel J. Hsu. Algorithms for Active Learning. PhD thesis, University of California at San Diego, 2010.
Tzu-Kuo Huang, Alekh Agarwal, Daniel J Hsu, John Langford, and Robert E Schapire. Efficient and parsimonious agnostic active learning. arXiv preprint arXiv:1506.08669, 2015.
Nikos Karampatziakis and John Langford. Online importance weight aware updates. In UAI 2011, Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, Barcelona, Spain, July 14-17, 2011, pages 392399, 2011.
Vladimir Koltchinskii. Rademacher complexities and bounding the excess risk in active learning. J. Mach. Learn. Res., 11:24572485, December 2010.
A. B. Tsybakov. Optimal aggregation of classifiers in statistical learning. Ann. Statist., 32: 135166, 2004.
Chicheng Zhang and Kamalika Chaudhuri. Beyond disagreement-based agnostic active learning. In Advances in Neural Information Processing Systems, pages 442450, 2014.  9
Peter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings International Conference on Machine Learning, pages 18, 2004.
Monica Babes, Vukosi Marivate, Kaushik Subramanian, and Michael L Littman. Apprenticeship learning about multiple intentions. In International Conference on Machine Learning, 2011.
Chris L. Baker, Joshua B. Tenenbaum, and Rebecca R. Saxe. Goal inference as inverse planning. In Conference of the Cognitive Science Society, 2007.
Leonard E Baum. An equality and associated maximization technique in statistical estimation for probabilistic functions of markov processes. Inequalities, 3:18, 1972.
Richard Bellman. A Markovian decision process. Journal of Mathematics and Mechanics, 6:679684, 1957.
Abdeslam Boularias, Jens Kober, and Jan Peters. Relative entropy inverse reinforcement learning. In Proceedings of the International Conference on Artificial Intelligence and Statistics, pages 182189, 2011.
Arunkumar Byravan, Mathew Monfort, Brian Ziebart, Byron Boots, and Dieter Fox. Graph-based inverse optimal control for robot manipulation. In Proceedings of the International Joint Conference on Artificial Intelligence, 2015.
Rina Dechter and Judea Pearl. Generalized best-first search strategies and the optimality of a*. J. ACM, July 1985.
Edsger W. Dijkstra. A note on two problems in connexion with graphs. Numerische Mathematik, 1959.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, July 2011.
Jacqueline J. Goodnow and Rochelle A. Levine. the grammar of action: Sequence and syntax in childrens copying. Cognitive Psychology, 4(1):82  98, 1973.
Peter J. Green. Reversible jump markov chain monte carlo computation and bayesian model determination. Biometrika, 82:711732, 1995.
Peter E. Hart, Nils J. Nilsson, and Bertram Raphael. A formal basis for the heuristic determination of minimum cost paths. IEEE Transactions on Systems Science and Cybernetics, 4:100107, 1968.
De-An Huang, Amir massoud Farahman, Kris M. Kitani, and J. Andrew Bagnell. Approximate maxent inverse optimal control and its application for mental simulation of human interactions. In AAAI, 2015.
Rudolf E. Kalman. When is a linear control system optimal? Trans. ASME, J. Basic Engrg., 86:5160, 1964.
Brenden M Lake, Ruslan Salakhutdinov, and Josh Tenenbaum. One-shot learning by inverting a compositional causal process. In NIPS, 2013.
Mathew Monfort, Brenden M. Lake, Brian D. Ziebart, and Joshua B. Tenenbaum. Predictive inverse optimal control in large decision processes via heuristic-based search. In ICML Workshop on Robot Learning, 2013.
Mathew Monfort, Anqi Liu, and Brian Ziebart. Intent prediction and trajectory forecasting via predictive inverse linear-quadratic regulation. In AAAI, 2015.
Gergely Neu and Csaba Szepesvari. Apprenticeship learning using inverse reinforcement learning and gradient methods. In Proceedings UAI, pages 295302, 2007.
Andrew Y. Ng and Stuart Russell. Algorithms for inverse reinforcement learning. In Proceedings International Conference on Machine Learning, 2000.
Deepak Ramachandran and Eyal Amir. Bayesian inverse reinforcement learning. In Proceedings International Joint Conferences on Artificial Intelligence, pages 25862591, 2007.
Nathan D. Ratliff, J. Andrew Bagnell, and Martin A. Zinkevich. Maximum margin planning. In Proceedings International Conference on Machine Learning, pages 729736, 2006.
Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. On the importance of initialization and momentum in deep learning. In International Conference on Machine Learning, 2013.
Paul Vernaza and Drew Bagnell. Efficient high dimensional maximum entropy modeling via symmetric partition functions. In Advances in Neural Information Processing Systems, pages 575583, 2012.
Brian D. Ziebart, J. Andrew Bagnell, and Anind K. Dey. Modeling interaction via the principle of maximum causal entropy. In International Conference on Machine Learning, 2010.
Brian D. Ziebart, Andrew Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy inverse reinforcement learning. In Association for the Advancement of Artificial Intelligence, 2008.  9
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 31043112, 2014.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Thang Luong, Ilya Sutskever, Quoc V Le, Oriol Vinyals, and Wojciech Zaremba. Addressing the rare word problem in neural machine translation. arXiv preprint arXiv:1410.8206, 2014.
Sebastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. On using very large target vocabulary for neural machine translation. arXiv preprint arXiv:1412.2007, 2014.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735 1780, 1997.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. Ontonotes: The 90% solution. In NAACL. ACL, June 2006.
Slav Petrov and Ryan McDonald. Overview of the 2012 shared task on parsing the web. Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL), 2012.
John Judge, Aoife Cahill, and Josef van Genabith. Questionbank: Creating a corpus of parse-annotated questions. In Proceedings of ICCL & ACL06, pages 497504. ACL, July 2006.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313330, 1993.
Zhenghua Li, Min Zhang, and Wenliang Chen. Ambiguity-aware ensemble training for semi-supervised dependency parsing. In Proceedings of ACL14, pages 457467. ACL, 2014.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In ACL. ACL, July 2006.
Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In ACL. ACL, August 2013.
Slav Petrov. Products of random latent variable grammars. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 1927. ACL, June 2010.
Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In EMNLP. ACL, August 2009.
David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In NAACL. ACL, June 2006.
David Hall, Taylor Berg-Kirkpatrick, John Canny, and Dan Klein. Sparser, better, faster gpu parsing. In ACL, 2014.
Michael Collins. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the ACL, pages 1623. ACL, July 1997.
Dan Klein and Christopher D. Manning. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the ACL, pages 423430. ACL, July 2003.
James Henderson. Inducing history representations for broad coverage statistical parsing. In NAACL, May 2003.
James Henderson. Discriminative training of a neural network statistical parser. In Proceedings of the 42nd Meeting of the ACL (ACL04), Main Volume, pages 95102, July 2004.
Ivan Titov and James Henderson. Constituent parsing with incremental sigmoid belief networks. In ACL. ACL, June 2007.
Ronan Collobert. Deep learning for efficient discriminative parsing. In International Conference on Artificial Intelligence and Statistics, 2011.
Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. Parsing natural scenes and natural language with recursive neural networks. In ICML, 2011.
Adwait Ratnaparkhi. A linear observed time statistical parser based on maximum entropy models. In Second Conference on Empirical Methods in Natural Language Processing, 1997.
Michael Collins and Brian Roark. Incremental parsing with the perceptron algorithm. In Proceedings of the 42nd Meeting of the ACL (ACL04), Main Volume, pages 111118, July 2004.
Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. End-to-end continuous speech recognition using attention-based recurrent nn: First results. arXiv preprint arXiv:1412.1602, 2014.
Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models. In EMNLP, pages 1700 1709, 2013.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. arXiv preprint arXiv:1411.4555, 2014.
Zoubin Ghahramani. A neural network for learning how to parse tree adjoining grammar. B.S.Eng Thesis, University of Pennsylvania, 1990.  9
CBCL face dataset. http://cbcl.mit.edu/software-datasets/FaceData2.html.
D. Amelunxen, M. Lotz, M. McCoy, and J. Tropp. Living on the edge: phase transitions in convex programs with random data. Information and Inference, 3:224294, 2014.
T. Cai and A. Zhang. ROP: Matrix recovery via rank-one projections. The Annals of Statistics, 43:102 138, 2015.
E. Candes and X. Li. Solving quadratic equations via PhaseLift when there are about as many equations as unknowns. Foundation of Computational Mathematics, 14:10171026, 2014.
E. Candes and Y. Plan. Tight oracle bounds for low-rank matrix recovery from a minimal number of noisy measurements. IEEE Transactions on Information Theory, 57:23422359, 2011.
E. Candes and B. Recht. Exact matrix completion via convex optimization. Foundation of Computational Mathematics, 9:20532080, 2009.
E. Candes, T. Strohmer, and V. Voroninski. PhaseLift: exact and stable signal recovery from magnitude measurements via convex programming. Communications on Pure and Applied Mathematics, 66:1241 1274, 2012.
Y. Chen, Y. Chi, and A. Goldsmith. Exact and Stable Covariance Estimation from Quadratic Sampling via Convex Programming. IEEE Transactions on Information Theory, 61:40344059, 2015.
K. Davidson and S. Szarek. Handbook of the Geometry of Banach Spaces, volume 1, chapter Local operator theory, random matrices and Banach spaces, pages 317366. 2001.
L. Demanet and P. Hand. Stable optimizationless recovery from phaseless measurements. Journal of Fourier Analysis and its Applications, 20:199221, 2014.
D. Gross, Y.-K. Liu, S. Flammia, S. Becker, and J. Eisert. Quantum State Tomography via Compressed Sensing. Physical Review Letters, 105:15040115404, 2010.
R. Horn and C. Johnson. Matrix Analysis. Cambridge University Press, 1985.
M. Kabanva, R. Kueng, and H. Rauhut und U. Terstiege. Stable low rank matrix recovery via null space properties. arXiv:1507.07184, 2015.
M. Klibanov, P. Sacks, and A. Tikhonarov. The phase retrieval problem. Inverse Problems, 11:128, 1995.
V. Koltchinskii, K. Lounici, and A. Tsybakov. Nuclear-norm penalization and optimal rates for noisy low-rank matrix completion. The Annals of Statistics, 39:23022329, 2011.
N. Meinshausen. Sign-constrained least squares estimation for high-dimensional regression. The Electronic Journal of Statistics, 7:16071631, 2013.
S. Negahban and M. Wainwright. Estimation of (near) low-rank matrices with noise and high-dimensional scaling. The Annals of Statistics, 39:10691097, 2011.
B. Recht, M. Fazel, and P. Parillo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM Review, 52:471501, 2010.
A. Rohde and A. Tsybakov. Estimation of high-dimensional low-rank matrices. The Annals of Statistics, 39:887930, 2011.
B. Scholkopf and A. Smola. Learning with kernels. MIT Press, Cambridge, Massachussets, 2002.
M. Slawski and M. Hein. Non-negative least squares for high-dimensional linear models: consistency and sparse recovery without regularization. The Electronic Journal of Statistics, 7:30043056, 2013.
M. Slawski, P. Li, and M. Hein. Regularization-free estimation in trace regression with positive semidefinite matrices. arXiv:1504.06305, 2015.
N. Srebro, J. Rennie, and T. Jaakola. Maximum margin matrix factorization. In Advances in Neural Information Processing Systems 17, pages 13291336, 2005.
R. Tibshirani. Regression shrinkage and variable selection via the lasso. Journal of the Royal Statistical Society Series B, 58:671686, 1996.
J. Tropp. User-friendly tools for random matrices: An introduction. 2014. http://users.cms. caltech.edu/jtropp/.
R. Vershynin. How close is the sample covariance matrix to the actual covariance matrix ? Journal of Theoretical Probability, 153:405419, 2012.
M. Wang, W. Xu, and A. Tang. A unique nonnegative solution to an underdetermined system: from vectors to matrices. IEEE Transactions on Signal Processing, 59:10071016, 2011.
C. Williams and M. Seeger. Using the Nystrom method to speed up kernel machines. In Advances in Neural Information Processing Systems 14, pages 682688, 2001.  9
A. Krizhevsky, I. Sutskever, and G. E. Hinton, Imagenet classification with deep convolutional neural networks., in NIPS, vol. 1, p. 4, 2012.
A. Ng, Sparse autoencoder, CS294A Lecture notes, vol. 72, 2011.
A. Coates, A. Y. Ng, and H. Lee, An analysis of single-layer networks in unsupervised feature learning, in International Conference on Artificial Intelligence and Statistics, 2011.
K. Kavukcuoglu, P. Sermanet, Y.-L. Boureau, K. Gregor, M. Mathieu, and Y. LeCun, Learning convolutional feature hierarchies for visual recognition., in NIPS, vol. 1, p. 5, 2010.
H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng, Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations, in Proceedings of the 26th Annual International Conference on Machine Learning, pp. 609616, ACM, 2009.
A. Krizhevsky, Convolutional deep belief networks on cifar-10, Unpublished, 2010.
M. D. Zeiler, D. Krishnan, G. W. Taylor, and R. Fergus, Deconvolutional networks, in Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pp. 2528 2535, IEEE, 2010.
P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun, Pedestrian detection with unsupervised multi-stage feature learning, in Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on, pp. 36263633, IEEE, 2013.
P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol, Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion, The Journal of Machine Learning Research, vol. 11, pp. 33713408, 2010.
G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, Improving neural networks by preventing co-adaptation of feature detectors, arXiv preprint arXiv:1207.0580, 2012.
Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng, Reading digits in natural images with unsupervised feature learning, in NIPS workshop on deep learning and unsupervised feature learning, vol. 2011, p. 5, Granada, Spain, 2011.
M. D. Zeiler and R. Fergus, Differentiable pooling for hierarchical feature learning, arXiv preprint arXiv:1207.0151, 2012.
R. Salakhutdinov and G. E. Hinton, Deep boltzmann machines, in International Conference on Artificial Intelligence and Statistics, pp. 448455, 2009.
A. Makhzani and B. Frey, k-sparse autoencoders, International Conference on Learning Representations, ICLR, 2014.
J. Bruna and S. Mallat, Invariant scattering convolution networks, Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 35, no. 8, pp. 18721886, 2013.
J. Mairal, P. Koniusz, Z. Harchaoui, and C. Schmid, Convolutional kernel networks, in Advances in Neural Information Processing Systems, pp. 26272635, 2014.
M. Ranzato, F. J. Huang, Y.-L. Boureau, and Y. Lecun, Unsupervised learning of invariant feature hierarchies with applications to object recognition, in Computer Vision and Pattern Recognition, 2007. CVPR07. IEEE Conference on, pp. 18, IEEE, 2007.
D. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling, Semi-supervised learning with deep generative models, in Advances in Neural Information Processing Systems, pp. 35813589, 2014.
I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio, Maxout networks, ICML, 2013.
A. Coates and A. Y. Ng, Selecting receptive fields in deep networks., in NIPS, 2011.
A. Dosovitskiy, J. T. Springenberg, M. Riedmiller, and T. Brox, Discriminative unsupervised feature learning with convolutional neural networks, in Advances in Neural Information Processing Systems, pp. 766774, 2014.
T.-H. Lin and H. Kung, Stable and efficient representation learning with nonnegativity constraints, in Proceedings of the 31st International Conference on Machine Learning (ICML-14), pp. 13231331, 2014.  9
D. M. Blei, D. M. Griffiths, M. I. Jordan, and J. B. Tenenbaum. Hierarchical topic models and the nested Chinese restaurant process. In NIPS, 2004.
D. M. Blei and J. D. Lafferty. A correlated topic model of science. AOAS, 2007.
D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. JMLR, 2003.
T. Chen, E. B. Fox, and C. Guestrin. Stochastic gradient Hamiltonian Monte Carlo. In ICML, 2014.
N. Ding, Y. Fang, R. Babbush, C. Chen, R. D. Skeel, and H. Neven. Bayesian sampling using stochastic gradient thermostats. In NIPS, 2014.
Z. Gan, C. Chen, R. Henao, D. Carlson, and L. Carin. Scalable deep Poisson factor analysis for topic modeling. In ICML, 2015.
Z. Gan, R. Henao, D. Carlson, and L. Carin. Learning deep sigmoid belief networks with data augmentation. In AISTATS, 2015.
Z. Gan, C. Li, R. Henao, D. Carlson, and L. Carin. Deep temporal sigmoid belief networks for sequence modeling. In NIPS, 2015.
R. Guhaniyogi, S. Qamar, and D. B. Dunson. Bayesian conditional density filtering. arXiv:1401.3632, 2014.
G. Hinton. Training products of experts by minimizing contrastive divergence. Neural computation, 2002.
G. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural computation, 2006.
G. E. Hinton and R. R. Salakhutdinov. Replicated softmax: an undirected topic model. In NIPS, 2009.
Q. Ho, J. Cipar, H. Cui, S. Lee, J. K. Kim, P. B. Gibbons, G. A. Gibson, G. Ganger, and E. P. Xing. More effective distributed ML via a stale synchronous parallel parameter server. In NIPS, 2013.
M. Hoffman, F. R. Bach, and D. M. Blei. Online learning for latent Dirichlet allocation. In NIPS, 2010.
M. D. Hoffman, D. M. Blei, C. Wang, and J. Paisley. Stochastic variational inference. JMLR, 2013.
S. Lacoste-Julien, F. Sha, and M. I. Jordan. DiscLDA: Discriminative learning for dimensionality reduction and classification. In NIPS, 2009.
H. Larochelle and S. Lauly. A neural autoregressive topic model. In NIPS, 2012.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 1998.
M. Li, D. G. Andersen, A. J. Smola, and K. Yu. Communication efficient distributed machine learning with the parameter server. In NIPS, 2014.
L. Maaloe, M. Arngren, and O. Winther. Deep belief nets for topic modeling. arXiv:1501.04325, 2015.
J. D. Mcauliffe and D. M. Blei. Supervised topic models. In NIPS, 2008.
R. M. Neal. Connectionist learning of belief networks. Artificial Intelligence, 1992.
J. Paisley, C. Wang, D. M. Blei, and M. I. Jordan. Nested hierarchical Dirichlet processes. PAMI, 2015.
R. Ranganath, L. Tang, L. Charlin, and D. M. Blei. Deep exponential families. In AISTATS, 2014.
R. Salakhutdinov and G. E. Hinton. Deep Boltzmann machines. In AISTATS, 2009.
R. R. S. Srivastava, Nitish and G. E. Hinton. Modeling documents with deep Boltzmann machines. In UAI, 2013.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical Dirichlet processes. JASA, 2006.
M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient Langevin dynamics. In ICML, 2011.
S. Williamson, C. Wang, K. Heller, and D. Blei. The IBP compound Dirichlet process and its application to focused topic modeling. In ICML, 2010.
M. Zhou. Infinite edge partition models for overlapping community detection and link prediction. In AISTATS, 2015.
M. Zhou and L. Carin. Negative binomial process count and mixture modeling. PAMI, 2015.
M. Zhou, L. Hannah, D. Dunson, and L. Carin. Beta-negative binomial process and Poisson factor analysis. In AISTATS, 2012.
J. Zhu, A. Ahmed, and E. P. Xing. MedLDA: maximum margin supervised topic models. JMLR, 2012.  9
N. De Freitas, A. J. Smola, and M. Zoghi. Exponential regret bounds for Gaussian process bandits with deterministic observations. In Proceedings of the 29th International Conference on Machine Learning (ICML), 2012.
Z. Wang, B. Shakibi, L. Jin, and N. de Freitas. Bayesian Multi-Scale Optimistic Optimization. In Proceedings of the 17th International Conference on Artificial Intelligence and Statistics (AISTAT), pages 10051014, 2014.
J. Snoek, H. Larochelle, and R. P. Adams. Practical Bayesian optimization of machine learning algorithms. In Proceedings of Advances in Neural Information Processing Systems (NIPS), pages 29512959, 2012.
R. G. Carter, J. M. Gablonsky, A. Patrick, C. T. Kelley, and O. J. Eslinger. Algorithms for noisy problems in gas transmission pipeline optimization. Optimization and engineering, 2(2):139157, 2001.
J. W. Zwolak, J. J. Tyson, and L. T. Watson. Globally optimised parameters for a model of mitotic control in frog egg extracts. IEEE Proceedings-Systems Biology, 152(2):8192, 2005.
L. C. W. Dixon. Global optima without convexity. Numerical Optimisation Centre, Hatfield Polytechnic, 1977.
B. O. Shubert. A sequential method seeking the global maximum of a function. SIAM Journal on Numerical Analysis, 9(3):379388, 1972.
D. Q. Mayne and E. Polak. Outer approximation algorithm for nondifferentiable optimization problems. Journal of Optimization Theory and Applications, 42(1):1930, 1984.
R. H. Mladineo. An algorithm for finding the global maximum of a multimodal, multivariate function. Mathematical Programming, 34(2):188200, 1986.
R. G. Strongin. Convergence of an algorithm for finding a global extremum. Engineering Cybernetics, 11(4):549555, 1973.
D. E. Kvasov, C. Pizzuti, and Y. D. Sergeyev. Local tuning and partition strategies for diagonal GO methods. Numerische Mathematik, 94(1):93106, 2003.
S. Bubeck, G. Stoltz, and J. Y. Yu. Lipschitz bandits without the Lipschitz constant. In Algorithmic Learning Theory, pages 144158. Springer, 2011.
J. Gardner, M. Kusner, K. Weinberger, and J. Cunningham. Bayesian Optimization with Inequality Constraints. In Proceedings of The 31st International Conference on Machine Learning (ICML), pages 937 945, 2014.
Z. Wang, M. Zoghi, F. Hutter, D. Matheson, and N. De Freitas. Bayesian optimization in high dimensions via random embeddings. In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence, pages 17781784. AAAI Press, 2013.
N. Srinivas, A. Krause, M. Seeger, and S. M. Kakade. Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design. In Proceedings of the 27th International Conference on Machine Learning (ICML), pages 10151022, 2010.
K. P. Murphy. Machine learning: a probabilistic perspective. MIT press, page 521, 2012.
C. E. Rasmussen and C. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.
R. Munos. Optimistic optimization of deterministic functions without the knowledge of its smoothness. In Proceedings of Advances in neural information processing systems (NIPS), 2011.
D. R. Jones, C. D. Perttunen, and B. E. Stuckman. Lipschitzian optimization without the Lipschitz constant. Journal of Optimization Theory and Applications, 79(1):157181, 1993.
K. Kandasamy, J. Schneider, and B. Poczos. High dimensional Bayesian optimisation and bandits via additive models. arXiv preprint arXiv:1503.01673, 2015.
S. Surjanovic and D. Bingham. Virtual library of simulation experiments: Test functions and datasets. Retrieved November 30, 2014, from http://www.sfu.ca/ssurjano, 2014.
D. B. McDonald, W. J. Grantham, W. L. Tabor, and M. J. Murphy. Global and local optimization using radial basis function response surface models. Applied Mathematical Modelling, 31(10):20952110, 2007.
T. J. Walsh, S. Goschin, and M. L. Littman. Integrating Sample-Based Planning and Model-Based Reinforcement Learning. In Proceedings of the 24th AAAI conference on Artificial Intelligence (AAAI), 2010.
A. L. Strehl, L. Li, and M. L. Littman. Reinforcement learning in finite MDPs: PAC analysis. The Journal of Machine Learning Research (JMLR), 10:24132444, 2009.  9
Alexander L. Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael L. Littman. PAC Model-Free Reinforcement Learning. In International Conference on Machine Learning, 2006.
Michael J Kearns and Satinder P Singh. Finite-Sample Convergence Rates for Q-Learning and Indirect Algorithms. In Advances in Neural Information Processing Systems, 1999.
Ronen I Brafman and Moshe Tennenholtz. R-MAX  A General Polynomail Time Algorithm for Near-Optimal Reinforcement Learning. Journal of Machine Learning Research, 3:213 231, 2002.
Sham M. Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis, University College London, 2003.
Peter Auer and Ronald Ortner. Online Regret Bounds for a New Reinforcement Learning Algorithm. In Proceedings 1st Austrian Cognitive Vision Workshop, 2005.
Tor Lattimore and Marcus Hutter. PAC bounds for discounted MDPs. In International Conference on Algorithmic Learning Theory, 2012.
Istvan Szita and Csaba Szepesvari. Model-based reinforcement learning with nearly tight exploration complexity bounds. In International Conference on Machine Learning, 2010.
Mohammad Gheshlaghi Azar, Remi Munos, and Hilbert J. Kappen. On the Sample Complexity of Reinforcement Learning with a Generative Model. In International Conference on Machine Learning, 2012.
J Zico Kolter and Andrew Y Ng. Near-Bayesian exploration in polynomial time. In International Conference on Machine Learning, 2009.
Claude-Nicolas Fiechter. Efficient reinforcement learning. In Conference on Learning Theory, 1994.
Claude-Nicolas Fiechter. Expected Mistake Bound Model for On-Line Reinforcement Learning. In International Conference on Machine Learning, 1997.
Spyros Reveliotis and Theologos Bountourelis. Efficient PAC learning for episodic tasks with acyclic state spaces. Discrete Event Dynamic Systems: Theory and Applications, 17(3):307 327, 2007.
Alexander L Strehl, Lihong Li, and Michael L Littman. Incremental Model-based Learners With Formal Learning-Time Guarantees. In Conference on Uncertainty in Artificial Intelligence, 2006.
Alexander L Strehl, Lihong Li, and Michael L Littman. Reinforcement Learning in Finite MDPs : PAC Analysis. Journal of Machine Learning Research, 10:24132444, 2009.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal Regret Bounds for Reinforcement Learning. In Advances in Neural Information Processing Systems, 2010.
Alexander L. Strehl and Michael L. Littman. An analysis of model-based Interval Estimation for Markov Decision Processes. Journal of Computer and System Sciences, 74(8):13091331, dec 2008.
Matthew J Sobel. The Variance of Markov Decision Processes. Journal of Applied Probability, 19(4):794802, 1982.
Andreas Maurer and Massimiliano Pontil. Empirical Bernstein Bounds and Sample-Variance Penalization. In Conference on Learning Theory, 2009.
Shie Mannor and John N Tsitsiklis. The Sample Complexity of Exploration in the Multi-Armed Bandit Problem. Journal of Machine Learning Research, 5:623648, 2004.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal Regret Bounds for Reinforcement Learning. Journal of Machine Learning Research, 11:15631600, 2010.
Fan Chung and Linyuan Lu. Concentration Inequalities and Martingale Inequalities: A Survey. Internet Mathematics, 3(1):79127, 2006.  9
J. Clarke, D. Goldwasser, M. Chang, and D. Roth. Driving semantic parsing from the worlds response. In Computational Natural Language Learning (CoNLL), pages 1827, 2010.
P. Liang, M. I. Jordan, and D. Klein. Learning dependency-based compositional semantics. In Association for Computational Linguistics (ACL), pages 590599, 2011.
Y. Artzi and L. Zettlemoyer. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics (TACL), 1:4962, 2013.
M. Fisher, D. Ritchie, M. Savva, T. Funkhouser, and P. Hanrahan. Example-based synthesis of 3D object arrangements. ACM SIGGRAPH Asia, 12, 2012.
V. Mansinghka, T. D. Kulkarni, Y. N. Perov, and J. Tenenbaum. Approximate Bayesian image interpretation using generative probabilistic graphics programs. In Advances in Neural Information Processing Systems (NIPS), pages 15201528, 2013.
A. X. Chang, M. Savva, and C. D. Manning. Learning spatial knowledge for text to 3D scene generation. In Empirical Methods in Natural Language Processing (EMNLP), 2014.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. Distant supervision for relation extraction without labeled data. In Association for Computational Linguistics (ACL), pages 10031011, 2009.
S. Riedel, L. Yao, and A. McCallum. Modeling relations and their mentions without labeled text. In Machine Learning and Knowledge Discovery in Databases (ECML PKDD), pages 148163, 2010.
S. Gulwani. Automating string processing in spreadsheets using input-output examples. ACM SIGPLAN Notices, 46(1):317330, 2011.
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529533, 2015.
M. Chang, L. Ratinov, and D. Roth. Guiding semi-supervision with constraint-driven learning. In Association for Computational Linguistics (ACL), pages 280287, 2007.
J. Graca, K. Ganchev, and B. Taskar. Expectation maximization and posterior constraints. In NIPS, 2008.
A. W. van der Vaart. Asymptotic statistics. Cambridge University Press, 1998.
F. Nielsen and V. Garcia. Statistical exponential families: A digest with flash cards. arXiv preprint arXiv:0911.4863, 2009.
P. E. Gill, W. Murray, and M. A. Saunders. SNOPT: An SQP algorithm for large-scale constrained optimization. SIAM Journal on Optimization, 12(4):9791006, 2002.
K. Gimpel and N. A. Smith. Softmax-margin CRFs: Training log-linear models with cost functions. In North American Association for Computational Linguistics (NAACL), pages 733736, 2010.
J. Besag. The analysis of non-lattice data. The Statistician, 24:179195, 1975.
P. Liang and M. I. Jordan. An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators. In International Conference on Machine Learning (ICML), pages 584591, 2008.
A. Y. Ng, D. Harada, and S. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In International Conference on Machine Learning (ICML), 1999.
G. Mann and A. McCallum. Generalized expectation criteria for semi-supervised learning of conditional random fields. In HLT/ACL, pages 870878, 2008.
G. Druck, G. Mann, and A. McCallum. Learning from labeled features using generalized expectation criteria. In ACM Special Interest Group on Information Retreival (SIGIR), pages 595602, 2008.
P. Liang, M. I. Jordan, and D. Klein. Learning from measurements in exponential families. In International Conference on Machine Learning (ICML), 2009.
K. Ganchev, J. Graca, J. Gillenwater, and B. Taskar. Posterior regularization for structured latent variable models. Journal of Machine Learning Research (JMLR), 11:20012049, 2010.
J. Jiang, A. Teichert, J. Eisner, and H. Daume. Learned prioritization for trading off accuracy and speed. In Advances in Neural Information Processing Systems (NIPS), 2012.
T. Shi, J. Steinhardt, and P. Liang. Learning where to sample in structured prediction. In AISTATS, 2015.
J. Steinhardt and P. Liang. Learning fast-mixing models for structured prediction. In ICML, 2015.
H. He, H. Daume, and J. Eisner. Cost-sensitive dynamic feature selection. In ICML Inferning Workshop, 2012.
H. He, H. Daume, and J. Eisner. Dynamic feature selection for dependency parsing. In EMNLP, 2013.
D. J. Weiss and B. Taskar. Learning adaptive value of information for structured prediction. In Advances in Neural Information Processing Systems (NIPS), pages 953961, 2013.  9
E. Abbe, A. S. Bandeira, and G. Hall. Exact recovery in the stochastic block model. arXiv preprint arXiv:1405.3267, 2014.
D. Achlioptas and F. McSherry. Fast computation of low rank matrix approximations. In STOC, pages 611618, 2001.
Q. Berthet and P. Rigollet. Complexity theoretic lower bounds for sparse principal component detection. In COLT, pages 10461066, 2013.
A. Blum. Learning boolean functions in an infinite attribute space. Machine Learning, 9:373386, 1992.
A. Bogdanov and Y. Qiao. On the security of goldreichs one-way function. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages 392405. 2009.
R. B. Boppana. Eigenvalues and graph bisection: An average-case analysis. In FOCS, pages 280285, 1987.
A. Coja-Oghlan. Graph partitioning via adaptive spectral techniques. Combinatorics, Probability & Computing, 19(2):227, 2010.  8
A. Coja-Oghlan, C. Cooper, and A. Frieze. An efficient sparse regularity concept. SIAM Journal on Discrete Mathematics, 23(4):20002034, 2010.
A. Coja-Oghlan, A. Goerdt, A. Lanka, and F. Schadlich. Certifying unsatisfiability of random 2k-sat formulas using approximation techniques. In Fundamentals of Computation Theory, pages 1526. Springer, 2003.
A. Daniely, N. Linial, and S. Shalev-Shwartz. More data speeds up training time in learning halfspaces over sparse vectors. In NIPS, pages 145153, 2013.
A. Daniely and S. Shalev-Shwartz. Complexity theoretic limitations on learning dnfs. CoRR, abs/1404.3378, 2014.
S. Decatur, O. Goldreich, and D. Ron. Computational sample complexity. SIAM Journal on Computing, 29(3):854879, 1999.
U. Feige and E. Ofek. Easily refutable subformulas of large random 3cnf formulas. In Automata, languages and programming, pages 519530. Springer, 2004.
U. Feige and E. Ofek. Spectral techniques applied to sparse random graphs. Random Structures & Algorithms, 27(2):251275, 2005.
V. Feldman. Attribute efficient and non-adaptive learning of parities and DNF expressions. Journal of Machine Learning Research, (8):14311460, 2007.
V. Feldman. Open problem: The statistical query complexity of learning sparse halfspaces. In COLT, pages 12831289, 2014.
V. Feldman, E. Grigorescu, L. Reyzin, S. Vempala, and Y. Xiao. Statistical algorithms and a lower bound for planted clique. In STOC, pages 655664, 2013.
V. Feldman, W. Perkins, and S. Vempala. Subsampled power iteration: a unified algorithm for block models and planted csps. CoRR, abs/1407.2774, 2014.
V. Feldman, W. Perkins, and S. Vempala. On the complexity of random satisfiability problems with planted solutions. In STOC, pages 7786, 2015.
L. Florescu and W. Perkins. Spectral thresholds in the bipartite stochastic block model. arXiv preprint arXiv:1506.06737, 2015.
M. L. Fredman, J. Komlos, and E. Szemeredi. Storing a sparse table with 0 (1) worst case access time. Journal of the ACM (JACM), 31(3):538544, 1984.
J. Friedman, A. Goerdt, and M. Krivelevich. Recognizing more unsatisfiable random k-sat instances efficiently. SIAM Journal on Computing, 35(2):408430, 2005.
A. Goerdt and M. Krivelevich. Efficient recognition of random unsatisfiable k-sat instances by spectral methods. In STACS 2001, pages 294304. Springer, 2001.
M. Kearns. Efficient noise-tolerant learning from statistical queries. JACM, 45(6):9831006, 1998.
S. B. Korada, A. Montanari, and S. Oh. Gossip pca. In SIGMETRICS, pages 209220, 2011.
F. Krzakala, C. Moore, E. Mossel, J. Neeman, A. Sly, L. Zdeborova, and P. Zhang. Spectral redemption in clustering sparse networks. PNAS, 110(52):2093520940, 2013.
L. Massoulie. Community detection thresholds and the weak ramanujan property. In STOC, pages 110, 2014.
F. McSherry. Spectral partitioning of random graphs. In FOCS, pages 529537, 2001.
E. Mossel, J. Neeman, and A. Sly. A proof of the block model threshold conjecture. arXiv preprint arXiv:1311.4115, 2013.
R. ODonnell and D. Witmer. Goldreichs prg: Evidence for near-optimal polynomial stretch. In Conference on Computational Complexity, 2014.
R. Servedio. Computational sample complexity and attribute-efficient learning. Journal of Computer and System Sciences, 60(1):161178, 2000.
S. Shalev-Shwartz, O. Shamir, and E. Tromer. Using more data to speed-up training time. In AISTATS, pages 10191027, 2012.
V. Vu. A simple svd algorithm for finding hidden partitions. arXiv preprint arXiv:1404.3918, 2014.  9
Zeyuan Allen-Zhu and Lorenzo Orecchia. Linear coupling: An ultimate unification of gradient and mirror descent. In ArXiv, 2014.
Arindam Banerjee, Srujana Merugu, Inderjit S. Dhillon, and Joydeep Ghosh. Clustering with Bregman divergences. J. Mach. Learn. Res., 6:17051749, December 2005.
Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Oper. Res. Lett., 31(3):167175, May 2003.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2(1):183202, 2009.
A. Ben-Tal and A. Nemirovski. Lectures on Modern Convex Optimization. SIAM, 2001.
Aharon Ben-Tal, Tamar Margalit, and Arkadi Nemirovski. The ordered subsets mirror descent optimization method with applications to tomography. SIAM J. on Optimization, 12(1):79108, January 2001.
Anthony Bloch, editor. Hamiltonian and gradient flows, algorithms, and control. American Mathematical Society, 1994.
A. A. Brown and M. C. Bartholomew-Biggs. Some effective methods for unconstrained optimization based on the solution of systems of ordinary differential equations. Journal of Optimization Theory and Applications, 62(2):211224, 1989.
Sebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1122, 2012.
J. C. Butcher. Numerical Methods for Ordinary Differential Equations. John Wiley & Sons, Ltd, 2008.
Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge, 2006.
Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao. Optimal distributed online prediction. In Proceedings of the 28th International Conference on Machine Learning (ICML), June 2011.
U. Helmke and J.B. Moore. Optimization and dynamical systems. Communications and control engineering series. Springer-Verlag, 1994.
Anatoli Juditsky. Convex Optimization II: Algorithms, Lecture Notes. 2013.
Anatoli Juditsky, Arkadi Nemirovski, and Claire Tauvel. Solving variational inequalities with stochastic mirror-prox algorithm. Stoch. Syst., 1(1):1758, 2011.
H.K. Khalil. Nonlinear systems. Macmillan Pub. Co., 1992.
Walid Krichene, Syrine Krichene, and Alexandre Bayen. Efficient Bregman projections onto the simplex. In 54th IEEE Conference on Decision and Control, 2015.
A.M. Lyapunov. General Problem of the Stability Of Motion. Control Theory and Applications Series. Taylor & Francis, 1992.
A. S. Nemirovsky and D. B. Yudin. Problem Complexity and Method Efficiency in Optimization. WileyInterscience series in discrete mathematics. Wiley, 1983.
Yu. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming, 103(1):127 152, 2005.
Yu. Nesterov. Gradient methods for minimizing composite functions. Mathematical Programming, 140(1):125161, 2013.
Yurii Nesterov. A method of solving a convex programming problem with convergence rate o(1/k2). Soviet Mathematics Doklady, 27(2):372376, 1983.
Yurii Nesterov. Introductory Lectures on Convex Optimization, volume 87. Springer Science & Business Media, 2004.
Brendan ODonoghue and Emmanuel Candes. Adaptive restart for accelerated gradient schemes. Foundations of Computational Mathematics, 15(3):715732, 2015.
M. Raginsky and J. Bouvrie. Continuous-time stochastic mirror descent on a network: Variance reduction, consensus, convergence. In CDC 2012, pages 67936800, 2012.
R.T. Rockafellar. Convex Analysis. Princeton University Press, 1970.
J. Schropp and I. Singer. A dynamical systems approach to constrained minimization. Numerical Functional Analysis and Optimization, 21(3-4):537551, 2000.
Weijie Su, Stephen Boyd, and Emmanuel Candes. A differential equation for modeling Nesterovs accelerated gradient method: Theory and insights. In NIPS, 2014.
Gerald Teschl. Ordinary differential equations and dynamical systems, volume 140. American Mathematical Soc., 2012.  9
W.S. McCulloch and W. Pitts. A logical calculus of the ideas immanent in nervous activity. Bulletin of mathematical biology, 5(4):115133, 1943.
Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.
K. Doya, S. Ishii, A. Pouget, and R.P.N. Rao. Bayesian brain: probabilistic approaches to neural coding. MIT Press, 2007.
Zoubin Ghahramani. Probabilistic machine learning and artificial intelligence. Nature, 521(7553):452 459, 2015.
Daniel M Wolpert, Zoubin Ghahramani, and Michael I Jordan. An internal model for sensorimotor integration. Science, 269(5232):18801882, 1995.
David C Knill and Whitman Richards. Perception as Bayesian inference. Cambridge University Press, 1996.
Sophie Deneve. Bayesian spiking neurons i: inference. Neural computation, 20(1):91117, 2008.
Thomas L Griffiths and Joshua B Tenenbaum. Optimal predictions in everyday cognition. Psychological Science, 17(9):767773, 2006.
J.B. Tenenbaum, C. Kemp, T.L. Griffiths, and N.D. Goodman. How to grow a mind: Statistics, structure, and abstraction. Science, 331(6022):12791285, 2011.
R.M. Neal. Bayesian Learning for Neural Networks. Springer Verlag, 1996. ISBN 0387947248.
C. E. Rasmussen and C. K. I. Williams. Gaussian processes for Machine Learning. MIT Press, 2006.
Andrew Gordon Wilson. Covariance kernels for fast automatic pattern discovery and extrapolation with Gaussian processes. PhD thesis, University of Cambridge, 2014. http://www.cs.cmu.edu/andrewgw/andrewgwthesis.pdf.
Tommi Jaakkola, David Haussler, et al. Exploiting generative models in discriminative classifiers. Advances in neural information processing systems, pages 487493, 1998.
Andrew Gordon Wilson and Ryan Prescott Adams. Gaussian process kernels for pattern discovery and extrapolation. International Conference on Machine Learning (ICML), 2013.
J Douglas Carroll. Functional learning: The learning of continuous functional mappings relating stimulus and response continua. ETS Research Bulletin Series, 1963(2), 1963.
Kyunghee Koh and David E Meyer. Function learning: Induction of continuous stimulus-response relations. Journal of Experimental Psychology: Learning, Memory, and Cognition, 17(5):811, 1991.
Edward L DeLosh, Jerome R Busemeyer, and Mark A McDaniel. Extrapolation: The sine qua non for abstraction in function learning. Journal of Experimental Psychology: Learning, Memory, and Cognition, 23(4):968, 1997.
Jerome R Busemeyer, Eunhee Byun, Edward L Delosh, and Mark A McDaniel. Learning functional relations based on experience with input-output pairs by humans and artificial neural networks. Concepts and Categories, 1997.
Thomas L Griffiths, Chris Lucas, Joseph Williams, and Michael L Kalish. Modeling human function learning with Gaussian processes. In Neural Information Processing Systems, 2009.
Christopher G Lucas, Thomas L Griffiths, Joseph J Williams, and Michael L Kalish. A rational model of function learning. Psychonomic bulletin & review, pages 123, 2015.
Christopher G Lucas, Douglas Sterling, and Charles Kemp. Superspace extrapolation reveals inductive biases in function learning. In Cognitive Science Society, 2012.
Mark A Mcdaniel and Jerome R Busemeyer. The conceptual basis of function learning and extrapolation: Comparison of rule-based and associative-based models. Psychonomic bulletin & review, 12(1):2442, 2005.
Michael L Kalish, Thomas L Griffiths, and Stephan Lewandowsky. Iterated learning: Intergenerational knowledge transmission reveals inductive biases. Psychonomic Bulletin & Review, 14(2):288294, 2007.
Daniel R Little and Richard M Shiffrin. Simplicity bias in the estimation of causal functions. In Proceedings of the 31st Annual Conference of the Cognitive Science Society, pages 11571162, 2009.
Samuel GB Johnson, Andy Jin, and Frank C Keil. Simplicity and goodness-of-fit in explanation: The case of intuitive curve-fitting. In Proceedings of the 36th Annual Conference of the Cognitive Science Society, pages 701706, 2014.
Samuel J Gershman, Edward Vul, and Joshua B Tenenbaum. Multistability and perceptual inference. Neural computation, 24(1):124, 2012.
Thomas L Griffiths, Edward Vul, and Adam N Sanborn. Bridging levels of analysis for probabilistic models of cognition. Current Directions in Psychological Science, 21(4):263268, 2012.
Edward Vul, Noah Goodman, Thomas L Griffiths, and Joshua B Tenenbaum. One and done? optimal decisions from very few samples. Cognitive science, 38(4):599637, 2014.
Andrew Gordon Wilson, Elad Gilboa, Arye Nehorai, and John P. Cunningham. Fast kernel learning for multidimensional pattern extrapolation. In Advances in Neural Information Processing Systems, 2014.
David JC MacKay. Information theory, inference, and learning algorithms. Cambridge U. Press, 2003.
Carl Edward Rasmussen and Zoubin Ghahramani. Occams razor. In Neural Information Processing Systems (NIPS), 2001.
Andrew Gordon Wilson. A process over all stationary kernels. Technical report, University of Cambridge, 2012.  9
M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The Arcade Learning Environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253279, 2013.
M. G. Bellemare, J. Veness, and M. Bowling. Investigating contingency awareness using Atari 2600 games. In AAAI, 2012.
M. G. Bellemare, J. Veness, and M. Bowling. Bayesian learning of recursively factored environments. In ICML, 2013.
M. G. Bellemare, J. Veness, and E. Talvitie. Skip context tree switching. In ICML, 2014.
Y. Bengio. Learning deep architectures for AI. Foundations and Trends in Machine Learning, 2(1):1127, 2009.
Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In ICML, 2009.
D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classification. In CVPR, 2012.  8
A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning to generate chairs with convolutional neural networks. In CVPR, 2015.
R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.
A. Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
X. Guo, S. Singh, H. Lee, R. L. Lewis, and X. Wang. Deep learning for real-time Atari game play using offline Monte-Carlo tree search planning. In NIPS, 2014.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):17351780, 1997.
Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. In ACM Multimedia, 2014.
A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei. Large-scale video classification with convolutional neural networks. In CVPR, 2014.
L. Kocsis and C. Szepesvari. Bandit based Monte-Carlo planning. In ECML. 2006.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.
I. Lenz, R. Knepper, and A. Saxena. DeepMPC: Learning deep latent features for model predictive control. In RSS, 2015.
R. Memisevic. Learning to relate images. IEEE TPAMI, 35(8):18291846, 2013.
V. Michalski, R. Memisevic, and K. Konda. Modeling deep temporal dependencies with recurrent grammar cells. In NIPS, 2014.
R. Mittelman, B. Kuipers, S. Savarese, and H. Lee. Structured recurrent temporal restricted Boltzmann machines. In ICML, 2014.
V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. Playing Atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529533, 2015.
V. Nair and G. E. Hinton. Rectified linear units improve restricted Boltzmann machines. In ICML, 2010.
S. Reed, K. Sohn, Y. Zhang, and H. Lee. Learning to disentangle factors of variation with manifold interaction. In ICML, 2014.
S. Rifai, Y. Bengio, A. Courville, P. Vincent, and M. Mirza. Disentangling factors of variation for facial expression recognition. In ECCV. 2012.
J. Schmidhuber. Deep learning in neural networks: An overview. Neural Networks, 61:85117, 2015.
J. Schmidhuber and R. Huber. Learning to generate artificial fovea trajectories for target detection. International Journal of Neural Systems, 2:125134, 1991.
N. Srivastava, E. Mansimov, and R. Salakhutdinov. Unsupervised learning of video representations using LSTMs. In ICML, 2015.
I. Sutskever, G. E. Hinton, and G. W. Taylor. The recurrent temporal restricted Boltzmann machine. In NIPS, 2009.
I. Sutskever, J. Martens, and G. E. Hinton. Generating text with recurrent neural networks. In ICML, 2011.
I. Sutskever, O. Vinyals, and Q. Le. Sequence to sequence learning with neural networks. In NIPS, 2014.
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. arXiv preprint arXiv:1409.4842, 2014.
G. W. Taylor and G. E. Hinton. Factored conditional restricted Boltzmann machines for modeling motion style. In ICML, 2009.
T. Tieleman and G. Hinton. Lecture 6.5 - RMSProp: Divde the gradient by a running average of its recent magnitude. Coursera, 2012.
D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri. Learning spatiotemporal features with 3D convolutional networks. In ICCV, 2015.
C. J. Watkins and P. Dayan. Q-learning. Machine learning, 8(3-4):279292, 1992.
J. Yang, S. Reed, M.-H. Yang, and H. Lee. Weakly-supervised disentangling with recurrent transformations for 3D view synthesis. In NIPS, 2015.  9
L. Albera, A. Ferreol, P. Comon, and P. Chevalier. Blind identification of overcomplete mixtures of sources (BIOME). Linear algebra and its applications, 391:330, 2004.
S. Arora, R. Ge, A. Moitra, and S. Sachdeva. Provable ICA with unknown Gaussian noise, with implications for Gaussian mixtures and autoencoders. In NIPS, pages 23842392, 2012.
J. Cardoso and A. Souloumiac. Blind beamforming for non-Gaussian signals. In Radar and Signal Processing, IEE Proceedings F, volume 140(6), pages 362370. IET, 1993.
J.-F. Cardoso. Super-symmetric decomposition of the fourth-order cumulant tensor. Blind identification of more sources than sensors. In ICASSP, pages 31093112. IEEE, 1991.
J.-F. Cardoso and A. Souloumiac. Matlab JADE for real-valued data v 1.8. http://perso. telecom-paristech.fr/cardoso/Algo/Jade/jadeR.m, 2005.
P. Chevalier. Optimal separation of independent narrow-band sources: Concept and performance 1. Signal Processing, 73(12):27  47, 1999. ISSN 0165-1684.
P. Comon and C. Jutten, editors. Handbook of Blind Source Separation. Academic Press, 2010.
L. De Lathauwer, B. De Moor, and J. Vandewalle. Independent component analysis based on higher-order statistics only. In Statistical Signal and Array Processing, 1996. Proceedings., 8th IEEE Signal Processing Workshop on, pages 356359. IEEE, 1996.
L. De Lathauwer, J. Castaing, and J. Cardoso. Fourth-order cumulant-based blind identification of underdetermined mixtures. Signal Processing, IEEE Transactions on, 55(6):29652973, June 2007. ISSN 1053-587X. doi: 10.1109/TSP.2007.893943.
H. Gavert, J. Hurri, J. Sarela, and A. Hyvarinen. Matlab FastICA v 2.5. http://research.ics. aalto.fi/ica/fastica/code/dlcode.shtml, 2005.
N. Goyal, S. Vempala, and Y. Xiao. Fourier PCA and robust tensor decomposition. In STOC, pages 584593, 2014.
A. Hyvarinen and E. Oja. Independent component analysis: Algorithms and applications. Neural Networks, 13(4-5):411430, 2000.
A. Hyvarinen, J. Karhunen, and E. Oja. Independent component analysis. John Wiley & Sons, 2001.
M. Joho, H. Mathis, and R. H. Lambert. Overdetermined blind source separation: Using more sensors than source signals in a noisy mixture. In Proc. International Conference on Independent Component Analysis and Blind Signal Separation. Helsinki, Finland, pages 8186, 2000.
Z. Koldovsky and P. Tichavsky. Methods of fair comparison of performance of linear ICA techniques in presence of additive noise. In ICASSP, pages 873876, 2006.
Z. Koldovsky and P. Tichavsky. Asymptotic analysis of bias of fastica-based algorithms in presence of additive noise. Technical report, Technical report, 2007.
Z. Koldovsky and P. Tichavsky. Blind instantaneous noisy mixture separation with best interference-plusnoise rejection. In Independent Component Analysis and Signal Separation, pages 730737. Springer, 2007.
S. Makino, T.-W. Lee, and H. Sawada. Blind speech separation. Springer, 2007.
B. D. Van Veen and K. M. Buckley. Beamforming: A versatile approach to spatial filtering. IEEE assp magazine, 5(2):424, 1988.
R. Vigario, J. Sarela, V. Jousmiki, M. Hamalainen, and E. Oja. Independent component approach to the analysis of EEG and MEG recordings. Biomedical Engineering, IEEE Transactions on, 47(5):589593, 2000.
J. R. Voss, L. Rademacher, and M. Belkin. Fast algorithms for Gaussian noise invariant independent component analysis. In Advances in Neural Information Processing Systems 26, pages 25442552. 2013.
A. Yeredor. Blind source separation via the second characteristic function. Signal Processing, 80(5): 897902, 2000.
A. Yeredor. Non-orthogonal joint diagonalization in the least-squares sense with application in blind source separation. Signal Processing, IEEE Transactions on, 50(7):15451553, 2002.  9
Ryan Gomes and Andreas Krause. Budgeted nonparametric learning from data streams. In ICML, 2010.
Sebastian Tschiatschek, Rishabh Iyer, Haochen Wei, and Jeff Bilmes. Learning Mixtures of Submodular Functions for Image Collection Summarization. In NIPS, 2014.
Khalid El-Arini, Gaurav Veda, Dafna Shahaf, and Carlos Guestrin. Turning down the noise in the blogosphere. In KDD, 2009.
Khalid El-Arini and Carlos Guestrin. Beyond keyword search: Discovering relevant scientific literature. In KDD, 2011.
Andreas Krause and Daniel Golovin. Submodular function maximization. In Tractability: Practical Approaches to Hard Problems. Cambridge University Press, 2013.
Laurence A. Wolsey. An analysis of the greedy algorithm for the submodular set covering problem. Combinatorica, 1982.
Uriel Feige. A threshold of ln n for approximating set cover. Journal of the ACM, 1998.
J. Dean and S. Ghemawat. Mapreduce: Simplified data processing on large clusters. In OSDI, 2004.
Matei Zaharia, Mosharaf Chowdhury, Michael J Franklin, Scott Shenker, and Ion Stoica. In Spark: cluster computing with working sets, pages 181213. Springer, 2010.
David Kempe, Jon Kleinberg, and Eva Tardos. Maximizing the spread of influence through a social network. In Proceedings of the ninth ACM SIGKDD, 2003.
Andreas Krause and Carlos Guestrin. Intelligent information gathering and submodular function optimization. Tutorial at the International Joint Conference in Artificial Intelligence, 2009.
Andrew Guillory and Jeff Bilmes. In ICML, Haifa, Israel, 2010.
Daniel Golovin and Andreas Krause. Adaptive submodularity: Theory and applications in active learning and stochastic optimization. Journal of Artificial Intelligence Research, 2011.
Bonnie Berger, John Rompel, and Peter W Shor. Efficient nc algorithms for set cover with applications to learning and geometry. Journal of Computer and System Sciences, 1994.
Guy E. Blelloch, Richard Peng, and Kanat Tangwongsan. Linear-work greedy parallel approximate set cover and variants. In SPAA, 2011.
Stergios Stergiou and Kostas Tsioutsiouliklis. Set cover at web scale. In SIGKDD. ACM, 2015.
Flavio Chierichetti, Ravi Kumar, and Andrew Tomkins. Max-cover in map-reduce. In WWW, 2010.
Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, and Andreas Krause. Distributed submodular maximization: Identifying representative elements in massive data. In NIPS, 2013.
Ravi Kumar, Benjamin Moseley, Sergei Vassilvitskii, and Andrea Vattani. Fast greedy algorithms in mapreduce and streaming. In SPAA, 2013.
Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan Vondrak, and Andreas Krause. Lazier than lazy greedy. In AAAI, 2015.
Ashwinkumar Badanidiyuru, Baharan Mirzasoleiman, Amin Karbasi, and Andreas Krause. Streaming submodular maximization: Massive data summarization on the fly. In SIGKDD. ACM, 2014.
Silvio Lattanzi, Benjamin Moseley, Siddharth Suri, and Sergei Vassilvitskii. Filtering: a method for solving graph problems in mapreduce. In SPAA, 2011.
Roberto Battiti. Using mutual information for selecting features in supervised neural net learning. Neural Networks, IEEE Transactions on, 5(4):537550, 1994.
Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning). 2006.
Alex Kulesza and Ben Taskar. Determinantal point processes for machine learning. Mach. Learn, 2012.
Rafael Barbosa, Alina Ene, Huy L. Nguyen, and Justin Ward. The power of randomization: Distributed submodular maximization on massive datasets. In arXiv, 2015.
Vahab Mirrokni and Morteza Zadimoghaddam. Randomized composable core-sets for distributed submodular maximization. In STOC, 2015.
Antonio Torralba, Rob Fergus, and William T Freeman. 80 million tiny images: A large data set for nonparametric object and scene recognition. TPAMI, 2008.
Athanasios Tsanas, Max Little, Patrick McSharry, and Lorraine Ramig. Enhanced classical dysphonia measures and sparse regression for telemonitoring of parkinsons disease progression. In ICASSP, 2010.
Jaewon Yang and Jure Leskovec. Defining and evaluating network communities based on ground-truth. Knowledge and Information Systems, 42(1):181213, 2015.  9
are two among the many examples. Perhaps the simplest generative model for non-overlapping communities is the stochastic block model, see
which we now define: Let P = P1 , . . . , Pk be a partition of V into k subsets. p, q-SBM is a distribution over the graphs on vertex set V , such that all edges are independent and for i, j  V , the edge (i, j) exists with probability p if i, j belong to the same Ps , and it exists with probability q otherwise. If q << p, the components Pi will be well separated in this model. We denote the number of nodes by N = |V | throughout the paper. 2  Graphs generated from SBMs can serve as a benchmark for community detection algorithms. However, such graphs lack certain desirable properties, such as power-law degree and community size distributions. Some of these issues were fixed in the benchmark models in
, and these models are referred to as LFR models in the literature. More details on these models are given in Section 4. We now turn to the discussion of the theoretical guarantees. Typically results in this direction provide algorithms that can reconstruct,with high probability, the ground partition of a graph drawn from a variant of a p, q-SBM model, with some, possibly large, number of components k. Recent results include the works
. In this paper, however, we only analytically analyse the k = 2 case, and such that, in addition, |P1 | = |P2 |. For this case, the best known reconstruction result was obtained already in
and was only improved in terms of runtime since then. Namely, Bopannas result states that if p  c1 logNN and p  q  c2 logNN , then with high probability the partition is reconstructible. Similar bound can be obtained, for instance, from the approaches in
, to name a few. The methods in this group are generally based on the spectral properties of adjacency (or related) matrices. The run time of these algorithms is non-linear in the size of the graph and it is not known how these algorithms behave on graphs not generated by the probabilistic models that they assume. It is generally known that when the graphs are dense (p of order of constant), simple linear time reconstruction algorithms exist (see
). The first, and to the best of our knowledge, the only previous linear time algorithm for non dense graphs was proposed in
. This algorithm works 1 for p  c3 ()N  2 + , for any fixed  > 0. The approach of
was further extended in
, to handle more general cluster sizes. These approaches approaches differ significantly from the spectrum based methods, and provide equally important theoretical insight. However, their empirical behaviour was never studied, and it is likely that even for graphs generated from the SBM, extremely high values of N would be required for the algorithms to work, due to large constants in the concentration inequalities (see the concluding remarks in
).  3  Algorithm  Let G be a finite undirected graph with a vertex set V = {1, . . . , n}. Denote by A = {aij } the symmetric P adjacency matrix of G, where aij  0 are edge weights, and for a vertex i  V , set di = j aij to be the degree of i. Let D be an n  n diagonal matrix such that Dii = di , and set T = D1 A to be the transition matrix of the random walk on G. Set also pij = Tij . Finally, denote by , (i) = Pdidj the stationary measure of the random walk. j  A number of community detection algorithms are based on the intuition that distinct communities should be relatively closed under the random walk (see
), and employ different notions of closedness. Our approach also takes this point of view. For a fixed L  N , consider the following sampling process on the graph: Choose vertex v0 randomly from , and perform L steps of a random walk on G, starting from v0 . This results in a length L + 1 sequence of vertices, x1 . Repeat the process N times independently, to obtain also x1 , . . . , x N . Suppose now that we would like to model the sequences xs as a multinomial mixture model with a single component. Since each coordinate xst is distributed according to , the single component of the mixture should be  itself, when N grows. Now suppose that we would like to model the same sequences with a mixture of two components. Because the sequences are sampled from a random walk rather then independently from each other, the components need no longer be  itself, as in any mixture where some elements appear more often together then others. The mixture as above can be found using the EM algorithm, and this in principle summarizes our approach. The only additional step, as discussed above, is to replace the sampled random walks with their true distributions, which simplifies the analysis and also leads to somewhat improved empirical performance. We now present the DER algorithm for detecting the non-overlapping communities. Its input is the number of components to detect, k, the length of the walks L, an initialization partition P = 3  Algorithm 1 DER 1: Input: Graph G, walk length L,  number of components k. 2: Compute the measures wi . 3: Initialize P1 , . . . , Pk to be a random partition such that  |Pi | = |V |/k for all i. 4: repeat 5: (1) For all s  k, construct s = Ps . 6: (2) For all s  k, set   Ps =   i  V | s = argmax D(wi , l ) . l  7: until the sets Ps do not change  {P1 , . . . , Pk } of V into disjoint subsets. P would be usually taken to be a random partition of V into equally sized subsets. For t = 0, 1, . . . and a vertex i  V , denote by wit the i-th row of the matrix T t . Then wit is the distribution of the random walk on G, started at i, after t steps. Set wi = L1 (wi1 + . . . + wiL ), which is the distribution corresponding to the average of the empirical measures of sequences x that start at i. For two probability measures ,  on V , set D(, ) =  X  (i) log (i).  iV  Although D is not a metric, will act as a distance function in our algorithm. Note that if  was an empirical measure, then, up to a constant, D would be just the log-likelihood of observing  from independent samples of . P For a subset S  V , set S to be the restriction of the measure  to S, and also set dS = iS di to be the full degree of S. Let 1 X di wi (1) S = dS iS  denote the distribution of the random walk started from S . The complete DER algorithm is described in Algorithm 1. The algorithm is essentially a k-means algorithm in a non-Euclidean space, where the points are the measures wi , each occurring with multiplicity di . Step (1) is the means step, and (2) is the maximization step. Let C=  L X X  di  D(wi , l )  (2)  l=1 iPl  be the associated cost. As with the usual k-means, we have the following Lemma 3.1. Either P is unchanged by steps (1) and (2) or both steps (1) and (2) strictly increase the value of C. The proof is by direct computation and is deferred to the supplementary material. Since the number of configurations P is finite, it follows that DER always terminates and provides a local maximum of the cost C. The cost C can be rewritten in a somewhat more informative form. To do so, we introduce some notation first. Let X be a random variable on V , distributed according to measure . Let Y a step of a random walk started at X, so that the distribution of Y given X = i is wi . Finally, for a partition P , let Z be the indicator variable of a partition, Z = s iff X  Ps . With this notation, one can write C = dV  H(Y |Z) = dV (H(Y ) + H(Z)  H(Z|Y )) , 4  (3)  26 14  20  29  22  18 15  23  33  32 30  9 27 25  28  8 13  2  31  19 1  24 3  17  0  7  21  12  10 11  5  4  6  16  (b) Political Blogs  (a) Karate Club  where H are the full and conditional Shannon entropies. Therefore, DER algorithm can be interpreted as seeking a partition that maximizes the information between current known state (Z), and the next step from it (Y ). This interpretation gives rise to the name of the algorithm, DER, since every iteration reduces the entropy H(Y |Z) of the random walk, or diffusion, with respect to the partition. The second equality in (3) has another interesting interpretation. Suppose, for simplicity, that k = 2, with partition P1 , P2 . In general, a clustering algorithm aims to minimize the cut, the number of edges between P1 and P2 . However, minimizing the number of edges directly will lead to situations where P1 is a single node, connected with one edge to the rest of the graph in P2 . To avoid such situation, a relative, normalized version of a cut needs to be introduced, which takes into account the sizes of P1 , P2 . Every clustering algorithms has a way to resolve this issue, implicitly or explicitly. For DER, this is shown in second equality of (3). H(Z) is maximized when the components are of equal sizes (with respect to ), while H(Z|Y ) is minimized when the measures Ps are as disjointly supported as possible. As any k-means algorithm, DERs results depend somewhat on its random initialization. All kmeans-like schemes are usually restarted several times and the solution with the best cost is chosen. In all cases which we evaluated we observed empirically that the dependence of DER on the initial parameters is rather weak. After two or three restarts it usually found a partition nearly as good as after 100 restarts. For clustering problems, however, there is another simple way to aggregate the results of multiple runs into a single partition, which slightly improves the quality of the final results. We use this technique in all our experiments and we provide the details in the Supplementary Material, Section A. We conclude by mentioning two algorithms that use some of the concepts that we use. The Walktrap,
, similarly to DER constructs the random walks (the measures wi , possibly for L > 1) as part of its computation. However, Walktrap uses wi s in a completely different way. Both the optimization procedure and the cost function are different from ours. The Infomap ,
, has a cost that is related to the notion of information. It aims to minimize to the information required to transmit a random walk on G through a channel, the source coding is constructed using the clusters, and best clusters are those that yield the best compression. This does not seem to be directly connected to the maximum likelyhood motivated approach that we use. As with Walktrap, the optimization procedure of Infomap also completely differs from ours.  4  Evaluation  In this section results of the evaluation of DER algorithm are presented. In Section 4.1 we illustrate DER on two classical graphs. Sections 4.2 and 4.3 contain the evaluation on the LFR benchmarks. 4.1  Basic examples  When a new clustering algorithm is introduced, it is useful to get a general feel of it with some simple examples. Figure 1a shows the classical Zacharys Karate Club,
. This graph has a 5  ground partition into two subsets. The partition shown in Figure 1a is a partition obtained from a typical run of DER algorithm, with k = 2, and wide range of Ls. (L 
were tested). As is the case with many other clustering algorithms, the shown partition differs from the ground partition in one element, node 8 (see
). Figure 1b shows the political blogs graph,
. The nodes are political blogs, and the graph has an (undirected) edge if one of the blogs had a link to the other. There are 1222 nodes in the graph. The ground truth partition of this graph has two components - the right wing and left wing blogs. The labeling of the ground truth was partially automatic and partially manual, and both processes could introduce some errors. The run of DER reconstructs the ground truth partition with only 57 nodes missclassifed. The NMI (see the next section, Eq. (4)) to the ground truth partition is .74. The political blogs graphs is particularly interesting since it is an example of a graph for which fitting an SBM model to reconstruct the clusters produces results very different from the ground truth. To overcome the problem with SBM fitting on this graph, a degree sensitive version of SBM, DCBM, was introduced in
. That algorithm produces partition with NMI .75. Another approach to DCBM can be found in
. 4.2  LFR benchmarks  The LFR benchmark model,
, is a widely used extension of the stochastic block model, where node degrees and community sizes have power law distribution, as often observed in real graphs. An important parameter of this model is the mixing parameter  
that controls the fraction of the edges of a node that go outside the nodes community (or outside all of nodes communities, in the overlapping case). For small , there will be a small number of edges going outside the communities, leading to disjoint, easily separable graphs, and the boundaries between communities will become less pronounced as  grows. Given a set of communities P on a graph, and the ground truth set of communities Q, there are several ways to measure how close P is to Q. One standard measure is the normalized mutual information (NMI), given by: N M I(P, Q) = 2  I(P, Q) , H(P ) + H(Q)  (4)  where H is the Shannon entropy of a partition and I is the mutual information (see
for details). NMI is equal 1 if and only if the partitions P and Q coincide, and it takes values between 0 and 1 otherwise. When computed with NMI, the sets inside P, Q can not overlap. To deal with overlapping communities, an extension of NMI was proposed in
. We refer to the original paper for the definition, as the definition is somewhat lengthy. This extension, which we denote here as ENMI, was subsequently used in the literature as a measure of closeness of two sets of communities, event in the cases of disjoint communities. Note that most papers use the notation NMI while the metric that they really use is ENMI. Figure 2a shows the results of evaluation of DER for four cases: the size of a graph was either N = 1000 or N = 5000 nodes, and the size of the communities was restricted to be either between 10 to 50 (denoted S in the figures) or between 20 to 100 (denoted B). For each combination of these parameters,  varied between 0.1 and 0.8. For each combination of graph size, community size restrictions as above and  value, we generated 20 graphs from that model and run DER. To provide some basic intuition about these graphs, we note that the number of communities in the 1000S graphs is strongly concentrated around 40, and in 1000B, 5000S, and 5000B graphs it is around 25, 200 and 100 respectively. Each point in Figure 2a is a the average ENMI on the 20 corresponding graphs, with standard deviation as the error bar. These experiments correspond precisely to the ones performed in
(see Supplementary Material, Section Cfor more details). In all runs on DER we have set L = 5 and set k to be the true number of communities for each graph, as was done in
for the methods that required it. Therefore our Figure 2a can be compared directly with Figure 2 in
. From this comparison we see that DER and the two of the best algorithms identified in
, reconstruct the partition perfectly for   0.5, for  = 0.6 DERs reconstruction scores are between Infomaps and RNs, with values for all of the algorithms above 0.95, and for 6  1.0  0.8  0.8  0.6  0.6  ENMI  ENMI  1.0  0.4  0.4  n1000S n1000B n5000S n5000B  0.2  0.0 0.1  0.2  0.3  n1000S n1000B n5000S n5000B  0.2  0.4  0.5  0.6  0.7  0.0 0.1  0.8  0.2  0.3  0.4  0.5  0.6  0.7  0.8      (a) DER, LFR benchmarks  (b) Spectral Alg., LFR benchmarks   = 0.7 DER has the best performance in two of the four cases. For  = 0.8 all algorithms have score 0. We have also performed the same experiments with the standard version of spectral clustering,
, because this version was not evaluated in
. The results are shown in Fig. 2b. Although the performance is generally good, the scores are mostly lower than those of DER, Infomap and RN. 4.3  Overlapping LFR benchmarks  We now describe how DER can be applied to overlapping community detection. Observe that DER internally operates on measures Ps rather then subsets of the vertex set. Recall that Ps (i) is the probability that a random walk started from Ps will hit node i. We can therefore consider each i to be a member of those communities from which the probability to hit it is high enough. To define this formally, we first note that for any partition P , the following decomposition holds: =  k X  (Ps )Ps .  (5)  s=1  This follows from the invariance of  under the random walk. Now, given the out put of DER - the sets Ps and measures Ps set Ps (i)(Ps ) P (i)(Ps ) , = mi (s) = Pk s (i) t=1 Pt (i)(Pt )  (6)  where we used (5) in the second equality. Then mi (s) is the probability that the walks started at Ps , given that it finished in i. For each i  V , set si = argmaxl mi (l) to be the most likely community given i. Then define the overlapping communities C1 , . . . , Ck via   1 Ct = i  V | mi (t)   mi (si ) . (7) 2 The paper
introduces a new algorithm for overlapping communities detection and contains also an evaluation of that algorithm as well as of several other algorithms on a set of overlapping LFR benchmarks. The overlapping communities LFR model was defined in
. In Table 1 we present the ENMI results of DER runs on the N = 10000 graphs with same parameters as in
, and also show the values obtained on these benchmarks in
(Figure S4 in
), for four other algorithms. The DER algorithm was run with L = 2, and k was set to the true number of communities. Each number is an average over ENMIs on 10 instances of graphs with a given set of parameters (as in
). The standard deviation around this average for DER was less then 0.02 in all cases. Variances for other algorithms are provided in
. For   0.6 all algorithms yield ENMI of less then 0.3. As we see in Table 1, DER performs better than all other algorithms in all the cases. We believe this indicates that DER together with equation (7) is a good choice for overlapping community detection in situations where community overlap between each two communities is sparse, as is the case in the LFR models considered above. Further discussion is provided in the Supplementary Material, Section D. 7  Table 1: Evaluation for Overlapping LFR. All values except DER are from
Alg. DER SVI (
)  =0 0.94 0.89 0.86 0.42 0.65   = 0.2 0.9 0.73 0.68 0.38 0.43   = 0.4 0.83 0.6 0.55 0.4 0.0  We conclude this section by noting that while in the non-overlapping case the models generated with  = 0 result in trivial community detection problems, because in these cases communities are simply the connected components of the graph, this is no longer true in the overlapping case. As a point of reference, the well known Clique Percolation method was also evaluated in
, in the  = 0 case. The average ENMI for this algorithm was 0.2 (Table S3 in
).  5  Analytic bounds  In this section we restrict our attention to the case L = 1 of the DER algorithm. Recall that the p, q-SBM model was defined in Section 2. We shall consider the model with k = 2 and such that |P1 | = |P2 |. We assume that the initial partition for the DER, denoted C1 , C2 in what follows, is chosen as in step 3 of DER (Algorithm 1) - a random partition of V into two equal sized subsets. In this setting we have the following: Theorem 5.1. For every  > 0 there exists C > 0 and c > 0 such that if 1  and  p  C  N  2 +  (8)  q 1 p  q  c pN  2 + log N  (9)  then DER recovers the partition P1 , P2 after one iteration, with probability (N ) such that (N )  1 when N  . Note that the probability in the conclusion of the theorem refers to a joint probability of a draw from the SBM and of an independent draw from the random initialization. The proof of the theorem has essentially three steps. First, we observe that the random initialization C1 , C2 is necessarily somewhat biased, in the sense that C1 and C2 never divide P1 exactly into two 1 halves. Specifically, ||C1  P1 |  |C2  P1 ||  N  2  with high probability. Assume that C1 has the bigger half, |C1  P1 | > |C2  P1 |. In the second step, by an appropriate linearization argument we show that for a node i  P1 , deciding whether D(wi , C1 ) > D(wi , C2 ) or vice versa amounts to counting paths of length two between i and |C1  P1 |. In the third step we estimate the number of 1 these length two paths in the model. The fact that |C1  P1 | > |C2  P1 | + N  2  will imply more paths to C1  P1 from i  P1 and we will conclude that D(wi , C1 ) > D(wi , C2 ) for all i  P1 and D(wi , C2 ) > D(wi , C1 ) for all i  P2 . The full proof is provided in the supplementary material.
R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.
Cs. Szepesvari. Algorithms for Reinforcement Learning. Morgan & Claypool, 2010.
D. P. Bertsekas and D. A. Castanon. Adaptive Aggregation Methods for Infinite Horizon Dynamic
Programming. IEEE Transactions on Automatic Control, 34, 1989.
R. Munos and A. Moore. Variable Resolution Discretization in Optimal Control. Machine Learning,
49(2-3):291323, 2002.
S. Mahadevan. Proto-Value Functions: Developmental Reinforcement Learning. In ICML, pages
553560, 2005.
P. W. Keller, S. Mannor, and D. Precup. Automatic Basis Function Construction for Approximate
Dynamic Programming and Reinforcement Learning. In ICML, pages 449456, 2006.
R. Parr, C. Painter-Wakefiled, L. Li, and M. L. Littman. Analyzing Feature Generation for Value
Function Approximation. In ICML, pages 737744, 2008a.
G. D. Konidaris, S. Osentoski, and P. S. Thomas. Value Function Approximation in Reinforcement
Learning using the Fourier Basis. In AAAI, pages 380385, 2011.
A. Geramifard, F. Doshi, J. Redding, N. Roy, and J. How. Online Discovery of Feature Dependencies. In ICML, pages 881888, 2011.
B. Ravindran and A. G. Barto. Model Minimization in Hierarchical Reinforcement Learning. In
Symposium on Abstraction, Reformulation and Approximation (SARA), pages 196211, 2002.
N. Ferns, P. Panangaden, and D. Precup. Metrics for finite Markov Decision Processes. In UAI,
pages 162169, 2004.
S. Ruan, G. Comanici, P. Panangaden, and D. Precup. Representation Discovery for MDPs using
Bisimulation Metrics. In AAAI, pages 35783584, 2015.
R. Givan, T. Dean, and M. Greig. Equivalence Notions and Model Minimization in Markov Decision
Processes. Artificial Intelligence, 147(1-2):163223, 2003.
D. Ormoneit and S. Sen. Kernel-Based Reinforcement Learning. Machine Learning, 49(2-3):161
N. Jong and P. Stone. Kernel-Based Models for Reinforcement Learning. In ICML Workshop on
Kernel Machines and Reinforcement Learning, 2006.
A. S. Barreto, D. Precup, and J. Pineau. Reinforcement Learning using Kernel-Based Stochastic
Factorization. In NIPS, pages 720728, 2011.
R. S. Sutton. Learning to Predict by the Methods of Temporal Differences. Machine Learning, 3
(1):944, 1988.
S. J. Bradtke and A. G. Barto. Linear Least-Squares Algorithms for Temporal Difference Learning.
Machine Learning, 22(1-3):3357, 1996.
H. Yu and D. Bertsekas. Convergence Results for Some Temporal Difference Methods Based on
Least Squares. Technical report, LIDS MIT, 2006.
R. Parr, L. Li, G. Taylor, C. Painter-Wakefield, and M. L. Littman. An Analysis of Linear Models,
Linear Value-Function Approximation, and Feature Selection for Reinforcement Learning. In
ICML, pages 752759, 2008b.
K. G. Larsen and A. Skou. Bisimulation through Probabilistic Testing. Information and Computation, 94:128, 1991.
J. Desharnais, V. Gupta, R. Jagadeesan, and P. Panangaden. Metrics for Labeled Markov Systems.
In CONCUR, 1999.
J. Desharnais, V. Gupta, R. Jagadeesan, and P. Panangaden. A metric for labelled Markov processes.
Theoretical Computer Science, 318(3):323354, 2004.
C. Villani. Topics in optimal transportation. American Mathematical Society, 2003.
G. Comanici and D. Precup. Basis Function Discovery Using Spectral Clustering and Bisimulation
Metrics. In AAAI, 2011.
D. Blackwell. Discounted Dynamic Programming. Annals of Mathematical Statistics, 36:226235,
D. Amelunxen, M. Lotz, M. B. McCoy, and J. A. Tropp. Living on the edge: Phase transitions in convex programs with random data. Inform. Inference, 3(3):224294, 2014.
A. Argyriou, R. Foygel, and N. Srebro. Sparse prediction with the k-support norm. In Advances in Neural Information Processing Systems (NIPS), 2012.
A. Banerjee, S. Chen, F. Fazayeli, and V. Sivakumar. Estimation with norm regularization. In Advances in Neural Information Processing Systems (NIPS), 2014.
P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. The Annals of Statistics, 37(4):17051732, 2009.
M. Bogdan, E. van den Berg, W. Su, and E. Candes. Statistical estimation and testing via the sorted L1 norm. arXiv:1310.1969, 2013.
T. T. Cai, T. Liang, and A. Rakhlin. Geometrizing Local Rates of Convergence for High-Dimensional Linear Inverse Problems. arXiv:1404.4408, 2014.
E. Candes and T Tao. The Dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics, 35(6):23132351, 2007.
E. J. Candes, J. K. Romberg, and T. Tao. Stable signal recovery from incomplete and inaccurate measurements. Communications on Pure and Applied Mathematics, 59(8):12071223, 2006.
E. J. Cands and B. Recht. Simple bounds for recovering low-complexity models. Math. Program., 141(12):577589, 2013.
V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky. The convex geometry of linear inverse problems. Foundations of Computational Mathematics, 12(6):805849, 2012.
S. Chatterjee, S. Chen, and A. Banerjee. Generalized dantzig selector: Application to the k-support norm. In Advances in Neural Information Processing Systems (NIPS), 2014.
S. Chen and A. Banerjee. One-bit compressed sensing with the k-support norm. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2015.
M. A. T. Figueiredo and R. D. Nowak. Sparse estimation with strongly correlated variables using ordered weighted l1 regularization. arXiv:1409.4005, 2014.
Y. Gordon. Some inequalities for gaussian processes and applications. Israel Journal of Mathematics, 50(4):265289, 1985.
L. Jacob, G. Obozinski, and J.-P. Vert. Group lasso with overlap and graph lasso. In International Conference on Machine Learning (ICML), 2009.
A. Maurer, M. Pontil, and B. Romera-Paredes. An Inequality with Applications to Structured Sparsity and Multitask Dictionary Learning. In Conference on Learning Theory (COLT), 2014.
A. M. McDonald, M. Pontil, and D. Stamos. Spectral k-support norm regularization. In Advances in Neural Information Processing Systems (NIPS), 2014.
S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A unified framework for the analysis of regularized M -estimators. Statistical Science, 27(4):538557, 2012.
S. Oymak, C. Thrampoulidis, and B. Hassibi. The Squared-Error of Generalized Lasso: A Precise Analysis. arXiv:1311.0830, 2013.
Y. Plan and R. Vershynin. Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach. IEEE Transactions on Information Theory, 59(1):482494, 2013.
N. Rao, B. Recht, and R. Nowak. Universal Measurement Bounds for Structured Sparse Signal Recovery. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2012.
R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society, Series B, 58(1):267288, 1996.
J. A. Tropp. Convex recovery of a structured signal from independent random linear measurements. In Sampling Theory, a Renaissance. 2015.
M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society, Series B, 68:4967, 2006.
X. Zeng and M. A. T. Figueiredo. The Ordered Weighted `1 Norm: Atomic Formulation, Projections, and Algorithms. arXiv:1409.4271, 2014.
X. Zhang, Y. Yu, and D. Schuurmans. Polar operators for structured sparse estimation. In Advances in Neural Information Processing Systems (NIPS), 2013.  9
S. Ahn, A. Korattikara, and M. Welling. Bayesian posterior sampling via stochastic gradient Fisher scoring. In Proceedings of the 29th International Conference on Machine Learning (ICML12), 2012.
S. Ahn, B. Shahbaba, and M. Welling. Distributed stochastic gradient MCMC. In Proceeding of 31st International Conference on Machine Learning (ICML14), 2014.
R. Bardenet, A. Doucet, and C. Holmes. Towards scaling up Markov chain Monte Carlo: An adaptive subsampling approach. In Proceedings of the 30th International Conference on Machine Learning (ICML14), 2014.
M. Betancourt. The fundamental incompatibility of scalable Hamiltonian Monte Carlo and naive data subsampling. In Proceedings of the 31th International Conference on Machine Learning (ICML15), 2015.
D.M. Blei, A.Y. Ng, and M.I. Jordan. Latent dirichlet allocation. Journal of Machine Learning Research, 3:9931022, March 2003.
T. Chen, E.B. Fox, and C. Guestrin. Stochastic gradient Hamiltonian Monte Carlo. In Proceeding of 31st International Conference on Machine Learning (ICML14), 2014.
N. Ding, Y. Fang, R. Babbush, C. Chen, R.D. Skeel, and H. Neven. Bayesian sampling using stochastic gradient thermostats. In Advances in Neural Information Processing Systems 27 (NIPS14). 2014.
S. Duane, A.D. Kennedy, B.J. Pendleton, and D. Roweth. Hybrid Monte Carlo. Physics Letters B, 195(2):216  222, 1987.
W. Feller. Introduction to Probability Theory and its Applications. John Wiley & Sons, 1950.
M. Girolami and B. Calderhead. Riemann manifold Langevin and Hamiltonian Monte Carlo methods. Journal of the Royal Statistical Society Series B, 73(2):123214, 03 2011.
A. Korattikara, Y. Chen, and M. Welling. Austerity in MCMC land: Cutting the MetropolisHastings budget. In Proceedings of the 30th International Conference on Machine Learning (ICML14), 2014.
R.M. Neal. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, 54:113162, 2010.
S. Patterson and Y.W. Teh. Stochastic gradient Riemannian Langevin dynamics on the probability simplex. In Advances in Neural Information Processing Systems 26 (NIPS13). 2013.
H. Risken and T. Frank. The Fokker-Planck Equation: Methods of Solutions and Applications. Springer, 1996.
H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics, 22(3):400407, 09 1951.
J. Shi, T. Chen, R. Yuan, B. Yuan, and P. Ao. Relation of a new interpretation of stochastic differential equations to Ito process. Journal of Statistical Physics, 148(3):579590, 2012.
M. Welling and Y.W. Teh. Bayesian learning via stochastic gradient Langevin dynamics. In Proceedings of the 28th International Conference on Machine Learning (ICML11), pages 681688, June 2011.
T. Xifara, C. Sherlock, S. Livingstone, S. Byrne, and M. Girolami. Langevin diffusions and the Metropolis-adjusted Langevin algorithm. Statistics & Probability Letters, 91:1419, 2014.
L. Yin and P. Ao. Existence and construction of dynamical potential in nonequilibrium processes without detailed balance. Journal of Physics A: Mathematical and General, 39(27):8593, 2006.
R. Zwanzig. Nonequilibrium Statistical Mechanics. Oxford University Press, 2001.  9
J. Abernethy and A. Rakhlin. Beating the adaptive bandit with high probability. In Information Theory and Applications Workshop, 2009, pages 280289. IEEE, 2009.
J. Abernethy, E. Hazan, and A. Rakhlin. Competing in the dark: An efficient algorithm for bandit linear optimization. In Proceedings of the 21st Annual Conference on Learning Theory (COLT), 2008.
A. Agarwal, O. Dekel, and L. Xiao. Optimal algorithms for online convex optimization with multi-point bandit feedback. In Proceedings of the 23rd Annual Conference on Learning Theory (COLT), 2010.
A. Agarwal, D. P. Foster, D. Hsu, S. M. Kakade, and A. Rakhlin. Stochastic convex optimization with bandit feedback. In Advances in Neural Information Processing Systems (NIPS), 2011.
S. Bubeck and N. Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1122, 2012.
S. Bubeck and R. Eldan. The entropic barrier: a simple and optimal universal self-concordant barrier. arXiv preprint arXiv:1412.1587, 2015.
S. Bubeck and R. Eldan. Multi-scale exploration of convex functions and bandit convex optimization. arXiv preprint arXiv:1507.06580, 2015. p
S. Bubeck, O. Dekel, T. Koren, and Y. Peres. Bandit convex optimization: T regret in one dimension. In In Proceedings of the 28st Annual Conference on Learning Theory (COLT), 2015.
V. Dani, T. Hayes, and S. M. Kakade. The price of bandit information for online optimization. In Advances in Neural Information Processing Systems (NIPS), 2008.
O. Dekel, J. Ding, T. Koren, and Y. Peres. Bandits with switching costs: T 2/3 regret. In Proceedings of the 46th Annual Symposium on the Theory of Computing, 2014.
A. D. Flaxman, A. Kalai, and H. B. McMahan. Online convex optimization in the bandit setting: gradient descent without a gradient. In Proceedings of the sixteenth annual ACMSIAM symposium on Discrete algorithms, pages 385394. Society for Industrial and Applied Mathematics, 2005.
E. Hazan and K. Levy. Bandit convex optimization: Towards tight bounds. In Advances in Neural Information Processing Systems (NIPS), 2014.
Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical programming, 120(1):221259, 2009.
Y. Nesterov and A. Nemirovskii. Interior-point polynomial algorithms in convex programming, volume 13. SIAM, 1994.
A. Saha and A. Tewari. Improved regret guarantees for online smooth convex optimization with bandit feedback. In International Conference on Artificial Intelligence and Statistics (AISTAT), pages 636642, 2011.
S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in Machine Learning, 4(2):107194, 2011.
M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In Proceedings of the 20th International Conference on Machine Learning (ICML03), pages 928936, 2003.  9
Nicolo Cesa-Bianchi, Claudio Gentile, and Fabio Vitale. Fast and optimal prediction on a labeled tree. In Proceedings of the 22nd Annual Conference on Learning. Omnipress, 2009.
Avrim Blum and Shuchi Chawla. Learning from labeled and unlabeled data using graph mincuts. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML 01, pages 1926, San Francisco, CA, USA, 2001. Morgan Kaufmann Publishers Inc.
Olivier Chapelle, Jason Weston, and Bernhard Scholkopf. Cluster kernels for semi-supervised learning. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 601608. MIT Press, 2003.
Mikhail Belkin and Partha Niyogi. Semi-supervised learning on riemannian manifolds. Mach. Learn., 56(1-3):209239, 2004.
Xiaojin Zhu, Zoubin Ghahramani, and John D. Lafferty. Semi-supervised learning using gaussian fields and harmonic functions. In ICML, pages 912919, 2003.
Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Scholkopf. Learning with local and global consistency. In NIPS, 2003.
Martin Szummer and Tommi Jaakkola. Partially labeled classification with markov random walks. In NIPS, pages 945952, 2001.
Leslie Ann Goldberg and Mark Jerrum. The complexity of ferromagnetic ising with local fields. Combinatorics, Probability & Computing, 16(1):4361, 2007.
Jean-Claude Picard and Maurice Queyranne. On the structure of all minimum cuts in a network and applications. In V.J. Rayward-Smith, editor, Combinatorial Optimization II, volume 13 of Mathematical Programming Studies, pages 816. Springer Berlin Heidelberg, 1980.
J. Scott Provan and Michael O. Ball. The complexity of counting cuts and of computing the probability that a graph is connected. SIAM Journal on Computing, 12(4):777788, 1983.
Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine Learning, 2:285318, April 1988.
Mark Herbster, Massimiliano Pontil, and Lisa Wainer. Online learning over graphs. In ICML 05: Proceedings of the 22nd international conference on Machine learning, pages 305312, New York, NY, USA, 2005. ACM.
Mark Herbster. Exploiting cluster-structure to predict the labeling of a graph. In Proceedings of the 19th International Conference on Algorithmic Learning Theory, pages 5469, 2008.
Mark Herbster and Guy Lever. Predicting the labelling of a graph via minimum p-seminorm interpolation. In Proceedings of the 22nd Annual Conference on Learning Theory (COLT09), 2009.
Mark Herbster, Guy Lever, and Massimiliano Pontil. Online prediction on large diameter graphs. In Advances in Neural Information Processing Systems (NIPS 22), pages 649656. MIT Press, 2009.
Nicolo Cesa-Bianchi, Claudio Gentile, Fabio Vitale, and Giovanni Zappella. Random spanning trees and the prediction of weighted graphs. In Proceedings of the 27th International Conference on Machine Learning (27th ICML), pages 175182, 2010.
Fabio Vitale, Nicolo Cesa-Bianchi, Claudio Gentile, and Giovanni Zappella. See the tree through the lines: The shazoo algorithm. In John Shawe-Taylor, Richard S. Zemel, Peter L. Bartlett, Fernando C. N. Pereira, and Kilian Q. Weinberger, editors, NIPS, pages 15841592, 2011.
L. R. Ford and D. R. Fulkerson. Maximal Flow through a Network. Canadian Journal of Mathematics, 8:399404, 1956.
Michael O. Ball and J. Scott Provan. Calculating bounds on reachability and connectedness in stochastic networks. Networks, 13(2):253278, 1983.
Thomas Gartner and Gemma C. Garriga. The cost of learning directed cuts. In Proceedings of the 18th European Conference on Machine Learning, 2007.
J. M. Barzdin and R. V. Frievald. On the prediction of general recursive functions. Soviet Math. Doklady, 13:12241228, 1972.
Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. Inf. Comput., 108(2):212 261, 1994.  9
P. Abbeel, M. Quigley, and A. Y. Ng. Using inaccurate models in reinforcement learning. In ICML, 2006. C. G. Atkeson. Efficient robust policy optimization. In ACC, 2012. L. Baird. Residual algorithms: Reinforcement learning with function approximation. In ICML, 1995. D. Balduzzi and M. Ghifary. Compatible value gradients for reinforcement learning of continuous deep policies. arXiv preprint arXiv:1509.03005, 2015. R. Coulom. Reinforcement learning using neural networks, with applications to motor control. PhD thesis, Institut National Polytechnique de Grenoble-INPG, 2002. M. Deisenroth and C. E. Rasmussen. Pilco: A model-based and data-efficient approach to policy search. In (ICML), 2011. M. Fairbank. Value-gradient learning. PhD thesis, City University London, 2014. M. Fairbank and E. Alonso. Value-gradient learning. In IJCNN, 2012. I. Grondman. Online Model Learning Algorithms for Actor-Critic Control. PhD thesis, TU Delft, Delft University of Technology, 2015. D. H. Jacobson and D. Q. Mayne. Differential dynamic programming. 1970. M. I. Jordan and D. E. Rumelhart. Forward models: Supervised learning with a distal teacher. Cognitive science, 16(3):307354, 1992. D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. S. Levine and P. Abbeel. Learning neural network policies with guided policy search under unknown dynamics. In NIPS, 2014. T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015. L.-J. Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning, 8(3-4):293321, 1992. R. Munos. Policy gradient in continuous time. Journal of Machine Learning Research, 7:771791, 2006. K. S. Narendra and K. Parthasarathy. Identification and control of dynamical systems using neural networks. IEEE Transactions on Neural Networks, 1(1):427, 1990. D. H. Nguyen and B. Widrow. Neural networks for self-learning control systems. IEEE Control Systems Magazine, 10(3):1823, 1990. R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. In ICML, 2013. D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In ICML, 2014. Martin Riedmiller. Neural fitted q iterationfirst experiences with a data efficient neural reinforcement learning method. In Machine Learning: ECML 2005, pages 317328. Springer, 2005. J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel. Trust region policy optimization. CoRR, abs/1502.05477, 2015. D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller. Deterministic policy gradient algorithms. In ICML, 2014. S. P. Singh. Learning without state-estimation in partially observable Markovian decision processes. In ICML, 1994. Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3(1):9 44, 1988. R.S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In NIPS, 1999. Y. Tassa, T. Erez, and W.D. Smart. Receding horizon differential dynamic programming. In NIPS, 2008. E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In IROS, 2012. P. Wawrzynski. A cat-like robot real-time learning to run. In Adaptive and Natural Computing Algorithms, pages 380390. Springer, 2009. P. Wawrzynski. Real-time reinforcement learning by sequential actorcritics and experience replay. Neural Networks, 22(10):14841497, 2009. P. J Werbos. A menu of designs for reinforcement learning over time. Neural networks for control, pages 6795, 1990. R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229256, 1992.  9
O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, Show and tell: A neural image caption generator, in CVPR, 2015.
R. Kiros, R. Salakhutdinov, and R. S. Zemel, Unifying visual-semantic embeddings with multimodal neural language models, TACL, 2015.  8
A. Karpathy, A. Joulin, and L. Fei-Fei, Deep fragment embeddings for bidirectional image sentence mapping, in NIPS, 2013.
J. Mao, W. Xu, Y. Yang, J. Wang, and A. L. Yuille, Explain images with multimodal recurrent neural networks, NIPS Deep Learning Workshop, 2014.
J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell, Long-term recurrent convolutional networks for visual recognition and description, in CVPR, 2014.
X. Chen and C. L. Zitnick, Learning a recurrent visual representation for image caption generation, CoRR, vol. abs/1411.5654, 2014.
H. Fang, S. Gupta, F. N. Iandola, R. Srivastava, L. Deng, P. Dollar, J. Gao, X. He, M. Mitchell, J. C. Platt, C. L. Zitnick, and G. Zweig, From captions to visual concepts and back, in CVPR, 2015.
K. Xu, J. Ba, R. Kiros, K. Cho, A. C. Courville, R. Salakhutdinov, R. S. Zemel, and Y. Bengio, Show, attend and tell: Neural image caption generation with visual attention, in ICML, 2015.
R. Lebret, P. O. Pinheiro, and R. Collobert, Phrase-based image captioning, in ICML, 2015.
B. Klein, G. Lev, G. Lev, and L. Wolf, Fisher vectors derived from hybrid Gaussian-Laplacian mixture models for image annotations, in CVPR, 2015.
M. Malinowski and M. Fritz, Towards a visual Turing challenge, in NIPS Workshop on Learning Semantics, 2014.
N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, Indoor segmentation and support inference from RGBD images, in ECCV, 2012.
S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh, VQA: Visual Question Answering, CoRR, vol. abs/1505.00468, 2015.
M. Malinowski, M. Rohrbach, and M. Fritz, Ask Your Neurons: A Neural-based Approach to Answering Questions about Images, CoRR, vol. abs/1505.01121, 2015.
H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and W. Xu, Are you talking to a machine? dataset and methods for multilingual image question answering, CoRR, vol. abs/1505.05612, 2015.
L. Ma, Z. Lu, and H. Li, Learning to answer questions from image using convolutional neural network, CoRR, vol. abs/1506.00333, 2015.
T. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick, Microsoft COCO: Common Objects in Context, in ECCV, 2014.
X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Dollar, and C. L. Zitnick, Microsoft COCO captions: Data collection and evaluation server, CoRR, vol. abs/1504.00325, 2015.
S. Hochreiter and J. Schmidhuber, Long short-term memory, Neural Computation, vol. 9, no. 8, pp. 17351780, 1997.
K. Simonyan and A. Zisserman, Very deep convolutional networks for large-scale image recognition, in ICLR, 2015.
O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein, A. C. Berg, and L. Fei-Fei, Imagenet large scale visual recognition challenge, IJCV, 2015.
T. Mikolov, K. Chen, G. Corrado, and J. Dean, Efficient estimation of word representations in vector space, in ICLR, 2013.
A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, M. Ranzato, and T. Mikolov, DeViSE: A deep visual-semantic embedding model, in NIPS, 2013.
M. Hodosh, P. Young, and J. Hockenmaier, Framing image description as a ranking task: Data, models and evaluation metrics, J. Artif. Intell. Res. (JAIR), vol. 47, pp. 853899, 2013.
V. Ordonez, G. Kulkarni, and T. L. Berg, Im2text: Describing images using 1 million captioned photographs, in NIPS, 2011.
D. Klein and C. D. Manning, Accurate unlexicalized parsing, in ACL, 2003.
N. Chomsky, Conditions on Transformations. New York: Academic Press, 1973.
C. Fellbaum, Ed., WordNet An Electronic Lexical Database. Cambridge, MA; London: The MIT Press, May 1998.
S. Bird, NLTK: the natural language toolkit, in ACL, 2006.
J. Devlin, S. Gupta, R. Girshick, M. Mitchell, and C. L. Zitnick, Exploring nearest neighbor approaches for image captioning, CoRR, vol. abs/1505.04467, 2015.
Z. Wu and M. Palmer, Verb semantics and lexical selection, in ACL, 1994.
M. Malinowski and M. Fritz, A multi-world approach to question answering about real-world scenes based on uncertain input, in NIPS, 2014.  9
I. Guyon, K. Bennett, G. Cawley, H. Escalante, S. Escalera, T. Ho, N.Macia, B. Ray, M. Saeed, A. Statnikov, and E. Viegas. Design of the 2015 ChaLearn AutoML Challenge. In Proc. of IJCNN15, 2015.
C. Thornton, F. Hutter, H. Hoos, and K. Leyton-Brown. Auto-WEKA: combined selection and hyperparameter optimization of classification algorithms. In Proc. of KDD13, pages 847855, 2013.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. Witten. The WEKA data mining software: An update. SIGKDD, 11(1):1018, 2009.
E. Brochu, V. Cora, and N. de Freitas. A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. CoRR, abs/1012.2599, 2010.
M. Feurer, J. Springenberg, and F. Hutter. Initializing Bayesian hyperparameter optimization via metalearning. In Proc. of AAAI15, pages 11281135, 2015.
Reif M, F. Shafait, and A. Dengel. Meta-learning for evolutionary parameter optimization of classifiers. Machine Learning, 87:357380, 2012.
T. Gomes, R. Prudencio, C. Soares, A. Rossi, and A. Carvalho. Combining meta-learning and search techniques to select parameters for support vector machines. Neurocomputing, 75(1):313, 2012.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. JMLR, 12:28252830, 2011.
F. Hutter, H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for general algorithm configuration. In Proc. of LION11, pages 507523, 2011.
J. Bergstra, R. Bardenet, Y. Bengio, and B. Kegl. Algorithms for hyper-parameter optimization. In Proc. of NIPS11, pages 25462554, 2011.
J. Snoek, H. Larochelle, and R. P. Adams. Practical Bayesian optimization of machine learning algorithms. In Proc. of NIPS12, pages 29602968, 2012.
K. Eggensperger, M. Feurer, F. Hutter, J. Bergstra, J. Snoek, H. Hoos, and K. Leyton-Brown. Towards an empirical foundation for assessing Bayesian optimization of hyperparameters. In Proc. of NIPS BayesOpt Workshop, 2013.
B. Komer, J. Bergstra, and C. Eliasmith. Hyperopt-sklearn: Automatic hyperparameter configuration for scikit-learn. In ICML workshop on AutoML, 2014.
L. Breiman. Random forests. MLJ, 45:532, 2001.
P. Brazdil, C. Giraud-Carrier, C. Soares, and R. Vilalta. Metalearning: Applications to Data Mining. Springer, 2009.
R. Bardenet, M. Brendel, B. Kegl, and M. Sebag. Collaborative hyperparameter tuning. In Proc. of ICML13, pages 199207, 2014.
D. Yogatama and G. Mann. Efficient transfer learning method for automatic hyperparameter tuning. In Proc. of AISTATS14, pages 10771085, 2014.
J. Vanschoren, J. van Rijn, B. Bischl, and L. Torgo. OpenML: Networked science in machine learning. SIGKDD Explorations, 15(2):4960, 2013.
D. Michie, D. Spiegelhalter, C. Taylor, and J. Campbell. Machine Learning, Neural and Statistical Classification. Ellis Horwood, 1994.
A. Kalousis. Algorithm Selection via Meta-Learning. PhD thesis, University of Geneve, 2002.
B. Pfahringer, H. Bensusan, and C. Giraud-Carrier. Meta-learning by landmarking various learning algorithms. In Proc. of (ICML00), pages 743750, 2000.
I. Guyon, A. Saffari, G. Dror, and G. Cawley. Model selection: Beyond the Bayesian/Frequentist divide. JMLR, 11:6187, 2010.
A. Lacoste, M. Marchand, F. Laviolette, and H. Larochelle. Agnostic Bayesian learning of ensembles. In Proc. of ICML14, pages 611619, 2014.
R. Caruana, A. Niculescu-Mizil, G. Crew, and A. Ksikes. Ensemble selection from libraries of models. In Proc. of ICML04, page 18, 2004.
R. Caruana, A. Munson, and A. Niculescu-Mizil. Getting the most out of ensemble selection. In Proc. of ICDM06, pages 828833, 2006.
D. Wolpert. Stacked generalization. Neural Networks, 5:241259, 1992.
G. Hamerly and C. Elkan. Learning the k in k-means. In Proc. of NIPS04, pages 281288, 2004.  9
D. Carlson, V. Cevher, and L. Carin. Stochastic Spectral Descent for Restricted Boltzmann Machines. AISTATS, 2015.
D. Carlson, Y.-P. Hsieh, E. Collins, L. Carin, and V. Cevher. Stochastic Spectral Descent for Discrete Graphical Models. IEEE J. Special Topics in Signal Processing, 2016.
K. Cho, T. Raiko, and A. Ilin. Enhanced Gradient for Training Restricted Boltzmann Machines. Neural Computation, 2013.
A. Choromanska, M. Henaff, M. Mathieu, G. B. Arous, and Y. LeCun. The Loss Surfaces of Multilayer Networks. AISTATS 2015.
Y. N. Dauphin, H. de Vries, J. Chung, and Y. Bengio. RMSProp and equilibrated adaptive learning rates for non-convex optimization. arXiv:1502.04390 2015.
Y. N. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In NIPS, 2014.
J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. JMLR, 2010.
D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent, and S. Bengio. Why Does Unsupervised Pre-training Help Deep Learning? JMLR 2010.
N. Halko, P. G. Martinsson, and J. A. Tropp. Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions. SIAM Review 2011.
G. Hinton. A Practical Guide to Training Restricted Boltzmann Machines. U. Toronto Technical Report, 2010.
G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural Computation, 2006.
G. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation, 2002.
J. A. Kelner, Y. T. Lee, L. Orecchia, and A. Sidford. An Almost-Linear-Time Algorithm for Approximate Max Flow in Undirected Graphs, and its Multicommodity Generalizations 2013.
A. Krizhevsky and G. E. Hinton. ImageNet Classification with Deep Convolutional Neural Networks. NIPS, 2012.
A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. University of Toronto, Tech. Rep, 2009.
Q. V. Le, A. Coates, B. Prochnow, and A. Y. Ng. On Optimization Methods for Deep Learning. ICML, 2011.
B. Marlin and K. Swersky. Inductive principles for restricted Boltzmann machine learning. ICML, 2010.
J. Martens and R. Grosse. Optimizing Neural Networks with Kronecker-factored Approximate Curvature. arXiv:1503.05671 2015.
J. Martens and I. Sutskever. Parallelizable Sampling of Markov Random Fields. AISTATS, 2010.
V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In ICML, 2010.
R. M. Neal. Annealed Importance Sampling. U. Toronto Technical Report, 1998.
V. Rokhlin, A. Szlam, and M. Tygert. A Randomized Algorithm for Principal Component Analysis. SIAM Journal on Matrix Analysis and Applications 2010.
R. Salakhutdinov and G. Hinton. Deep Boltzmann Machines. AISTATS, 2009.
R. Salakhutdinov and I. Murray. On the Quantitative Analysis of Deep Belief Networks. ICML, 2008.
T. Schaul, S. Zhang, and Y. LeCun. No More Pesky Learning Rates. arXiv 1206.1106 2012.
P. Smolensky. Information Processing in Dynamical Systems: Foundations of Harmony Theory, 1986.
J. Snoek, H. Larochelle, and R. P. Adams. Practical Bayesian Optimization of Machine Learning Algorithms. In NIPS, 2012.
T. Tieleman and G. Hinton. Using fast weights to improve persistent contrastive divergence. ICML, 2009.
L. Wan, M. Zeiler, S. Zhang, Y. L. Cun, and R. Fergus. Regularization of neural networks using dropconnect. In ICML, 2013.
M. D. Zeiler. ADADELTA: An Adaptive Learning Rate Method. arXiv 1212.5701 2012.  9
F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J. Goodfellow, A. Bergeron, N. Bouchard, and Y. Bengio. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.
J. Bayer and C. Osendorfer. Learning stochastic recurrent networks. arXiv preprint arXiv:1411.7610, 2014.
A. Bertrand, K. Demuynck, V. Stouten, and H. V. Hamme. Unsupervised learning of auditory filter banks using non-negative matrix factorisation. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 47134716. IEEE, 2008.
N. Boulanger-lewandowski, Y. Bengio, and P. Vincent. Modeling temporal dependencies in highdimensional sequences: Application to polyphonic music generation and transcription. In Proceedings of the 29th International Conference on Machine Learning (ICML), pages 11591166, 2012.
K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using rnn encoderdecoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724 1734, 2014.
O. Fabius, J. R. van Amersfoort, and D. P. Kingma. Variational recurrent auto-encoders. arXiv preprint arXiv:1412.6581, 2014.
A. Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
K. Gregor, I. Danihelka, A. Graves, and D. Wierstra. Draw: A recurrent neural network for image generation. In Proceedings of The 32nd International Conference on Machine Learning (ICML), 2015.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):17351780, 1997.
S. King and V. Karaiskos. The blizzard challenge 2013. In The Ninth annual Blizzard Challenge, 2013.
D. P. Kingma and M. Welling. Auto-encoding variational bayes. In Proceedings of the International Conference on Learning Representations (ICLR), 2014.
D. P. Kingma and M. Welling. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Representations (ICLR), 2015.
H. Lee, P. Pham, Y. Largman, and A. Y. Ng. Unsupervised feature learning for audio classification using convolutional deep belief networks. In Advances in Neural Information Processing Systems (NIPS), pages 10961104, 2009.
M. Liwicki and H. Bunke. Iam-ondb-an on-line english sentence database acquired from handwritten text on a whiteboard. In Proceedings of Eighth International Conference on Document Analysis and Recognition, pages 956961. IEEE, 2005.
V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML), pages 807814, 2010.
M. Pachitariu and M. Sahani. Learning visual motion in recurrent neural networks. In Advances in Neural Information Processing Systems (NIPS), pages 13221330, 2012.
D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of The 31st International Conference on Machine Learning (ICML), pages 12781286, 2014.
K. Tokuda, Y. Nankaku, T. Toda, H. Zen, J. Yamagishi, and K. Oura. Speech synthesis based on hidden markov models. Proceedings of the IEEE, 101(5):12341252, 2013.
S. Weinberger. The speech accent archieve. http://accent.gmu.edu/, 2015.  9
A. Blum and Y. Mansour. Learning, regret minimization, and equilibria. In Noam Nisan, Tim Roughgarden, Eva Tardos, and Vijay Vazirani, editors, Algorithmic Game Theory, chapter 4, pages 430. Cambridge University Press, 2007.
Avrim Blum, MohammadTaghi Hajiaghayi, Katrina Ligett, and Aaron Roth. Regret minimization and the price of total anarchy. In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing, STOC 08, pages 373382, New York, NY, USA, 2008. ACM.
Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge University Press, New York, NY, USA, 2006.
Constantinos Daskalakis, Alan Deckelbaum, and Anthony Kim. Near-optimal no-regret algorithms for zero-sum games. Games and Economic Behavior, 92:327348, 2014.
Benjamin Edelman, Michael Ostrovsky, and Michael Schwarz. Internet advertising and the generalized second price auction: Selling billions of dollars worth of keywords. Working Paper 11765, National Bureau of Economic Research, November 2005.
Dean P. Foster and Rakesh V. Vohra. Calibrated learning and correlated equilibrium. Games and Economic Behavior, 21(12):40  55, 1997.
Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119  139, 1997.
Yoav Freund and Robert E Schapire. Adaptive game playing using multiplicative weights. Games and Economic Behavior, 29(1):79103, 1999.
Drew Fudenberg and Alexander Peysakhovich. Recency, records and recaps: Learning and nonequilibrium behavior in a simple decision problem. In Proceedings of the Fifteenth ACM Conference on Economics and Computation, EC 14, pages 971986, New York, NY, USA, 2014. ACM.
Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilibrium. Econometrica, 68(5):11271150, 2000.
Wassily Hoeffding and J. Wolfowitz. Distinguishability of sets of distributions. Ann. Math. Statist., 29(3):700718, 1958.
Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of Computer and System Sciences, 71(3):291  307, 2005. Learning Theory 2003 Learning Theory 2003.
Nick Littlestone and Manfred K Warmuth. The weighted majority algorithm. Information and computation, 108(2):212261, 1994.
AS Nemirovsky and DB Yudin. Problem complexity and method efficiency in optimization. 1983.
Yu. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming, 103(1):127 152, 2005.
Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In COLT 2013, pages 9931019, 2013.
Alexander Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable sequences. In Advances in Neural Information Processing Systems, pages 30663074, 2013.
T. Roughgarden. Intrinsic robustness of the price of anarchy. In Proceedings of the 41st annual ACM symposium on Theory of computing, pages 513522, New York, NY, USA, 2009. ACM.
Shai Shalev-Shwartz. Online learning and online convex optimization. Found. Trends Mach. Learn., 4(2):107194, February 2012.
Vasilis Syrgkanis and Eva Tardos. Composable and efficient mechanisms. In Proceedings of the Fortyfifth Annual ACM Symposium on Theory of Computing, STOC 13, pages 211220, New York, NY, USA, 2013. ACM.  9
S. Hochreiter and J. Schmidhuber. Long Short-Term Memory. In: Neural Computation 9.8 (1997). Based on TR FKI-207-95, TUM (1995), pp. 17351780.  8
F. A. Gers, J. Schmidhuber, and F. Cummins. Learning to Forget: Continual Prediction with LSTM. In: ICANN. 1999. A. Graves, M. Liwicki, S. Fernandez, R. Bertolami, H. Bunke, and J. Schmidhuber. A Novel Connectionist System for Improved Unconstrained Handwriting Recognition. In: PAMI 31.5 (2009). H. Sak, A. Senior, and F. Beaufays. Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling. In: Proc. Interspeech. 2014. I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to Sequence Learning with Neural Networks. Tech. rep. NIPS. 2014. A. Graves, S. Fernandez, and J. Schmidhuber. Multi-dimensional Recurrent Neural Networks. In: ICANN. 2007. A. Graves and J. Schmidhuber. Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks. In: NIPS. 2009. W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki. Scene Labeling With LSTM Recurrent Neural Networks. In: CVPR. 2015. M. Kass, A. Witkin, and D. Terzopoulos. Snakes: Active contour models. In: International Journal of Computer Vision (1988). L. Wang, Y. Gao, F. Shi, G. Li, J. H. Gilmore, W. Lin, and D. Shen. LINKS: Learning-based multi-source IntegratioN frameworK for Segmentation of infant brain images. In: NeuroImage (2015). D. C. Ciresan, A. Giusti, L. M. Gambardella, and J. Schmidhuber. Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images. In: NIPS. 2012. A. Cardona, S. Saalfeld, S. Preibisch, B. Schmid, A. Cheng, J. Pulokas, P. Tomancak, and V. Hartenstein. An integrated micro-and macroarchitectural analysis of the Drosophila brain by computer-assisted serial section electron microscopy. In: PLoS biology 8.10 (2010), e1000502. A. M. Mendrik, K. L. Vincken, H. J. Kuijf, G. J. Biessels, and M. A. Viergever (organizers). MRBrainS Challenge: Online Evaluation Framework for Brain Image Segmentation in 3T MRI Scans, http://mrbrains13.isi.uu.nl. 2015. O. Weber, Y. S. Devir, A. M. Bronstein, M. M. Bronstein, and R. Kimmel. Parallel algorithms for approximation of distance maps on parametric surfaces. In: ACM Transactions on Graphics (2008). Segmentation of Neuronal Structures in EM Stacks Challenge. IEEE International Symposium on Biomedical Imaging (ISBI), http://tinyurl.com/d2fgh7g. 2012. S. M. Pizer, E. P. Amburn, J. D. Austin, R. Cromartie, A. Geselowitz, T. Greer, B. T. H. Romeny, and J. B. Zimmerman. Adaptive Histogram Equalization and Its Variations. In: Comput. Vision Graph. Image Process. (1987). T. Tieleman and G. Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. In: COURSERA: Neural Networks for Machine Learning 4 (2012). S. Hochreiter. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut fur Informatik, Lehrstuhl Prof. Brauer, Technische Universitat Munchen. Advisor: J. Schmidhuber. 1991. S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catanzaro, and E. Shelhamer. cuDNN: Efficient Primitives for Deep Learning. In: CoRR abs/1410.0759 (2014). T. Liu, C. Jones, M. Seyedhosseini, and T. Tasdizen. A modular hierarchical approach to 3D electron microscopy image segmentation. In: Journal of Neuroscience Methods (2014). N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. In: Journal of Machine Learning Research (2014). V. Pham, T. Bluche, C. Kermorvant, and J. Louradour. Dropout improves recurrent neural networks for handwriting recognition. In: ICFHR. 2014. D. C. Ciresan, U. Meier, J. Masci, and J. Schmidhuber. A Committee of Neural Networks for Traffic Sign Classification. In: IJCNN. 2011. A. Krizhevsky, I Sutskever, and G. E Hinton. ImageNet Classification with Deep Convolutional Neural Networks. In: NIPS. 2012. M. D. Zeiler and R. Fergus. Visualizing and Understanding Convolutional Networks. Tech. rep. arXiv:1311.2901
. NYU, 2013.  9
Hadi Mohasel Afshar, Scott Sanner, and Ehsan Abbasnejad. Linear-time gibbs sampling in piecewise graphical models. In Association for the Advancement of Artificial Intelligence, pages 665673, 2015.
Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng. Handbook of Markov Chain Monte Carlo. CRC press, 2011.
Hans Adolph Buchdahl. An introduction to Hamiltonian optics. Courier Corporation, 1993.
Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. Hybrid monte carlo. Physics letters B, 195(2):216222, 1987.
Andrew G Glen, Lawrence M Leemis, and John H Drew. Computing the distribution of the product of two continuous random variables. Computational statistics & data analysis, 44(3):451464, 2004.
Donald T Greenwood. Principles of dynamics. Prentice-Hall Englewood Cliffs, NJ, 1988.
Matthew D Homan and Andrew Gelman. The no-u-turn sampler: Adaptively setting path lengths in hamiltonian monte carlo. The Journal of Machine Learning Research, 15(1):1593 1623, 2014.
David Lunn, David Spiegelhalter, Andrew Thomas, and Nicky Best. The bugs project: evolution, critique and future directions. Statistics in medicine, 28(25):30493067, 2009.
Radford M Neal. Mcmc using hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, 2, 2011.
Ari Pakman and Liam Paninski. Auxiliary-variable exact hamiltonian monte carlo samplers for binary distributions. In Advances in Neural Information Processing Systems, pages 2490 2498, 2013.
Ari Pakman and Liam Paninski. Exact hamiltonian monte carlo for truncated multivariate gaussians. Journal of Computational and Graphical Statistics, 23(2):518542, 2014.
Gareth O Roberts, Andrew Gelman, Walter R Gilks, et al. Weak convergence and optimal scaling of random walk metropolis algorithms. The annals of applied probability, 7(1):110 120, 1997.
Stan Development Team. Stan Modeling Language Users Guide and Reference Manual, Version 2.5.0, 2014.  9
L. Adamic and E. Adar. Friends and neighbors on the web. Social Networks, 25:211230, 2003.
L. Backstrom and J. Leskovec. Supervised random walks: Predicting and recommending links in social networks. In Proceedings of the Fourth ACM International Conference on Web Search and Data Mining, pages 635644, New York, NY, USA, 2011. ACM.
P. J. Bickel and A. Chen. A nonparametric view of network models and newman girvan and other modularities. Proceedings of the National Academy of Sciences of the Unites States of America, 106(50):2106821073, 2009.
K. Chaudhuri, F. C. Graham, and A. Tsiatas. Spectral clustering of graphs with general degrees in the extended planted partition model. Journal of Machine Learning Research - Proceedings Track, 23:35.135.23, 2012.
M. S. Handcock, A. E. Raftery, and J. M. Tantrum. Model-based clustering for social networks. Journal of the Royal Statistical Society: Series A (Statistics in Society), 170(2):301354, 2007.
P. W. Holland, K. Laskey, and S. Leinhardt. Stochastic blockmodels: First steps. Social Networks, 5(2):109137, 1983.
L. Katz. A new status index derived from sociometric analysis. In Psychometrika, volume 18, pages 3943, 1953.
D. Liben-Nowell and J. Kleinberg. The link prediction problem for social networks. In Conference on Information and Knowledge Management. ACM, 2003.
L. L and T. Zhou. Link prediction in complex networks: A survey. Physica A, 390(6):11501170, 2011.
F. McSherry. Spectral partitioning of random graphs. In FOCS, pages 529537, 2001.
S. C. Olhede and P. J. Wolfe. Network histograms and universality of blockmodel approximation. Proceedings of the National Academy of Sciences of the Unites States of America, 111(41):1472214727, 2014.
A. E. Raftery, M. S. Handcock, and P. D. Hoff. Latent space approaches to social network analysis. Journal of the American Statistical Association, 15:460, 2002.
K. Rohe, S. Chatterjee, and B. Yu. Spectral clustering and the high-dimensional stochastic blockmodel. Annals of Statistics, 39:18781915, 2011.
P. Sarkar and P. J. Bickel. Role of normalization in spectral clustering for stochastic blockmodels. To appear in the Annals of Statistics., 2014.
P. Sarkar, D. Chakrabarti, and A. Moore. Theoretical justification of popular link prediction heuristics. In Conference on Learning Theory. ACM, 2010.
P. Sarkar and A. Moore. A tractable approach to finding closest truncated-commutetime neighbors in large graphs. In Proc. UAI, 2007.  9
P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. The Journal of Machine Learning Research, 3:463482, 2003.
R. Bassily, A. Smith, and A. Thakurta. Private empirical risk minimization, revisited. In FOCS, 2014.
R. Bhaskar, S. Laxman, A. Smith, and A. Thakurta. Discovering frequent patterns in sensitive data. In KDD, New York, NY, USA, 2010.
M. Bun, J. Ullman, and S. Vadhan. Fingerprinting codes and the price of approximate differential privacy. In STOC, 2014.
V. Chandrasekaran, B. Recht, P. A. Parrilo, and A. S. Willsky. The convex geometry of linear inverse problems. Foundations of Computational Mathematics, 12(6):805849, 2012.
K. Chaudhuri and C. Monteleoni. Privacy-preserving logistic regression. In NIPS, 2008.
K. Chaudhuri, C. Monteleoni, and A. D. Sarwate. Differentially private empirical risk minimization. JMLR, 12:10691109, 2011.
K. L. Clarkson. Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm. ACM Transations on Algorithms, 2010.
J. C. Duchi, M. I. Jordan, and M. J. Wainwright. Local privacy and statistical minimax rates. In FOCS, 2013.
C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography Conference, pages 265284. Springer, 2006.
C. Dwork, A. Nikolov, and K. Talwar. Efficient algorithms for privately releasing marginals via convex relaxations. arXiv preprint arXiv:1308.1385, 2013.
C. Dwork and A. Roth. The Algorithmic Foundations of Differential Privacy. Foundations and Trends in Theoretical Computer Science. NOW Publishers, 2014.
C. Dwork, G. N. Rothblum, and S. P. Vadhan. Boosting and differential privacy. In FOCS, 2010.
C. Dwork, K. Talwar, A. Thakurta, and L. Zhang. Analyze gauss: optimal bounds for privacy-preserving principal component analysis. In STOC, 2014.
M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval research logistics quarterly, 3(1-2):95110, 1956.
E. Hazan and S. Kale. Projection-free online learning. In ICML, 2012.
M. Jaggi. Revisiting {Frank-Wolfe}: Projection-free sparse convex optimization. In ICML, 2013.
P. Jain, P. Kothari, and A. Thakurta. Differentially private online learning. In COLT, pages 24.124.34, 2012.
P. Jain and A. Thakurta. (near) dimension independent risk bounds for differentially private learning. In International Conference on Machine Learning (ICML), 2014.
D. Kifer, A. Smith, and A. Thakurta. Private convex empirical risk minimization and high-dimensional regression. In COLT, pages 25.125.40, 2012.
F. McSherry and K. Talwar. Mechanism design via differential privacy. In FOCS, pages 94103. IEEE, 2007.
A. Nikolov, K. Talwar, and L. Zhang. The geometry of differential privacy: The sparse and approximate cases. In STOC, 2013.
S. Shalev-Shwartz, N. Srebro, and T. Zhang. Trading accuracy for sparsity in optimization problems with sparsity constraints. SIAM Journal on Optimization, 2010.
A. Smith and A. Thakurta. Differentially private feature selection via stability arguments, and the robustness of the Lasso. In COLT, 2013.
A. Smith and A. Thakurta. Follow the perturbed leader is differentially private with optimal regret guarantees. Manuscript in preparation, 2013.
A. Smith and A. Thakurta. Nearly optimal algorithms for private online learning in full-information and bandit settings. In NIPS, 2013.
R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), 1996.
R. Tibshirani et al. The Lasso method for variable selection in the cox model. Statistics in medicine, 16(4):385395, 1997.
J. Ullman. Private multiplicative weights beyond linear queries. CoRR, abs/1407.1571, 2014.  9
Jacob Abernethy, Yiling Chen, and Jennifer Wortman Vaughan. Efficient market making via convex optimization, and a connection to online learning. ACM Transactions on Economics and Computation, 1(2):12, 2013.
Jacob Abernethy, Sindhu Kutty, Sebastien Lahaie, and Rahul Sami. Information aggregation in exponential family markets. In Proceedings of the fifteenth ACM conference on Economics and computation, pages 395412. ACM, 2014.
Jacob D Abernethy and Rafael M Frongillo. A collaborative mechanism for crowdsourcing prediction problems. In Advances in Neural Information Processing Systems, pages 26002608, 2011.
Aharon Ben-Tal and Marc Teboulle. An old-new concept of convex risk measures: The optimized certainty equivalent. Mathematical Finance, 17(3):449476, 2007.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Christian Burgert and Ludger Ruschendorf. On the optimal risk allocation problem. Statistics & decisions, 24(1/2006):153171, 2006.
Yiling Chen and Jennifer Wortman Vaughan. A new understanding of prediction markets via no-regret learning. In Proceedings of the 11th ACM conference on Electronic commerce, pages 189198. ACM, 2010.
Nair Maria Maia de Abreu. Old and new results on algebraic connectivity of graphs. Linear algebra and its applications, 423(1):5373, 2007.
Hans Follmer and Alexander Schied. Stochastic Finance: An Introduction in Discrete Time, volume 27 of de Gruyter Studies in Mathematics. Walter de Gruyter & Co., Berlin, 2nd edition, 2004.
Rafael M Frongillo, Nicolas Della Penna, and Mark D Reid. Interpreting prediction markets: a stochastic approach. In Proceedings of Neural Information Processing Systems, 2012.
P.D. Grunwald and A.P. Dawid. Game theory, maximum entropy, minimum discrepancy and robust Bayesian decision theory. The Annals of Statistics, 32(4):13671433, 2004.
JB Hiriart-Urruty and C Lemarechal. Grundlehren der mathematischen wissenschaften. Convex Analysis and Minimization Algorithms II, 306, 1993.
Jinli Hu and Amos Storkey. Multi-period trading prediction markets with connections to machine learning. In Proceedings of the 31st International Conference on Machine Learning (ICML), 2014.
Jono Millin, Krzysztof Geras, and Amos J Storkey. Isoelastic agents and wealth updates in machine learning markets. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 18151822, 2012.
Bojan Mohar. The Laplacian spectrum of graphs. In Graph Theory, Combinatorics, and Applications, 1991.
I Necoara, Y Nesterov, and F Glineur. A random coordinate descent method on large-scale optimization problems with linear constraints. Technical Report, 2014.
Ion Necoara. Random coordinate descent algorithms for multi-agent convex optimization over networks. Automatic Control, IEEE Transactions on, 58(8):20012012, 2013.
Yurii Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341362, 2012.
Mindika Premachandra and Mark Reid. Aggregating predictions via sequential mini-trading. In Asian Conference on Machine Learning, pages 373387, 2013.
Sashank Reddi, Ahmed Hefny, Carlton Downey, Avinava Dubey, and Suvrit Sra. Large-scale randomizedcoordinate descent methods with non-separable linear constraints. arXiv preprint arXiv:1409.2617, 2014.
Mark D Reid, Rafael M Frongillo, Robert C Williamson, and Nishant Mehta. Generalized mixability via entropic duality. In Proc. of Conference on Learning Theory (COLT), 2015.
Peter Richtarik and Martin Takac. Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function. Mathematical Programming, 144(1-2):138, 2014.
R.T. Rockafellar. Convex analysis. Princeton University Press, 1997.
Amos J Storkey. Machine learning markets. In International Conference on Artificial Intelligence and Statistics, pages 716724, 2011.  9
Y. Bengio and Y. LeCun. Scaling learning algorithms towards AI. In Leon Bottou, Olivier Chapelle, D. DeCoste, and J. Weston, editors, Large Scale Kernel Machines. MIT Press, 2007.
M Ranzato, F. J. Huang, Y.-L. Boureau, and Y. LeCun. Unsupervised learning of invariant feature hierarchies with applications to object recognition. In CVPR, 2007.
Y. Bengio, I. J. Goodfellow, and A. Courville. Deep Learning. Book in preparation for MIT Press, 2015.
R. M. Neal. Connectionist learning of belief networks. Artificial Intelligence, pages 71113, 1992.
L. K. Saul, T. Jaakkola, and M. I. Jordan. Mean field theory for sigmoid belief networks. Journal of Artificial Intelligence research, pages 6176, 1996.
G. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural Computation, pages 15271554, 2006.
G. Hinton. Training products of experts by minimizing contrastive divergence. Neural computation, pages 17711800, 2002.
R. Salakhutdinov and G. E. Hinton. Deep Boltzmann machines. In AISTATS, 2009.
H. Larochelle and S. Lauly. A neural autoregressive topic model. In NIPS, 2012.
R. Salakhutdinov, J. B. Tenenbaum, and A. Torralba. Learning with hierarchical-deep models. IEEE Trans. Pattern Anal. Mach. Intell., pages 19581971, 2013.
M. Welling, M. Rosen-Zvi, and G. E. Hinton. Exponential family harmoniums with an application to information retrieval. In NIPS, pages 14811488, 2004.
E. P. Xing, R. Yan, and A. G. Hauptmann. Mining associated text and images with dual-wing harmoniums. In UAI, 2005.
M. Zhou and L. Carin. Negative binomial process count and mixture modeling. IEEE Trans. Pattern Anal. Mach. Intell., 2015.
M. Zhou, O. H. M. Padilla, and J. G. Scott. Priors for random count matrices derived from a family of negative binomial processes. to appear in J. Amer. Statist. Assoc., 2015.
V. Nair and G. E. Hinton. Rectified linear units improve restricted Boltzmann machines. In ICML, 2010.
D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. J. Mach. Learn. Res., 2003.
A. Acharya, J. Ghosh, and M. Zhou. Nonparametric Bayesian factor analysis for dynamic count matrices. In AISTATS, 2015.
R. Ranganath, L. Tang, L. Charlin, and D. M. Blei. Deep exponential families. In AISTATS, 2015.
Z. Gan, C. Chen, R. Henao, D. Carlson, and L. Carin. Scalable deep poisson factor analysis for topic modeling. In ICML, 2015.
M. Zhou, L. Hannah, D. Dunson, and L. Carin. Beta-negative binomial process and Poisson factor analysis. In AISTATS, 2012.
T. L. Griffiths and M. Steyvers. Finding scientific topics. PNAS, 2004.
M. Zhou. Beta-negative binomial process and exchangeable random partitions for mixedmembership modeling. In NIPS, 2014.
Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical Dirichlet processes. J. Amer. Statist. Assoc., 2006.
Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep networks. In NIPS, 2007.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLINEAR: A library for large linear classification. JMLR, pages 18711874, 2008.
N. Srivastava, R. Salakhutdinov, and G. Hinton. Modeling documents with a deep Boltzmann machine. In UAI, 2013.  9
Shun-Ichi Amari, Natural gradient works efficiently in learning, Neural computation 10 (1998).
Richard H Byrd, Gillian M Chin, Will Neveitt, and Jorge Nocedal, On the use of stochastic hessian information in optimization methods for machine learning, SIAM Journal on Optimization (2011).
Jock A Blackard and Denis J Dean, Comparative accuracies of artificial neural networks and discriminant analysis in predicting forest cover types from cartographic variables, Compag (1999).
Richard H Byrd, SL Hansen, Jorge Nocedal, and Yoram Singer, A stochastic quasi-newton method for large-scale optimization, arXiv preprint arXiv:1401.7020 (2014).
Christopher M. Bishop, Neural networks for pattern recognition, Oxford University Press, 1995.
Leon Bottou, Large-scale machine learning with stochastic gradient descent, COMPSTAT, 2010.
Stephen Boyd and Lieven Vandenberghe, Convex optimization, Cambridge University Press, 2004.
Jian-Feng Cai, Emmanuel J Candes, and Zuowei Shen, A singular value thresholding algorithm for matrix completion, SIAM Journal on Optimization 20 (2010), no. 4, 19561982.
Olivier Chapelle, Training a support vector machine in the primal, Neural Computation (2007).
Lee H Dicker and Murat A Erdogdu, Flexible results for quadratic forms with applications to variance components estimation, arXiv preprint arXiv:1509.04388 (2015).
David L Donoho, Matan Gavish, and Iain M Johnstone, Optimal shrinkage of eigenvalues in the spiked covariance model, arXiv preprint arXiv:1311.0851 (2013).
John Duchi, Elad Hazan, and Yoram Singer, Adaptive subgradient methods for online learning and stochastic optimization, J. Mach. Learn. Res. 12 (2011), 21212159.
John E Dennis, Jr and Jorge J More, Quasi-newton methods, motivation and theory, SIAM review 19 (1977), 4689.
Murat A Erdogdu and Andrea Montanari, Convergence rates of sub-sampled Newton methods, arXiv preprint arXiv:1508.02810 (2015).
Murat A. Erdogdu, Newton-Stein Method: A second order method for GLMs via Steins lemma, NIPS, 2015.
Michael P Friedlander and Mark Schmidt, Hybrid deterministic-stochastic methods for data fitting, SIAM Journal on Scientific Computing 34 (2012), no. 3, A1380A1405.
Franz Graf, Hans-Peter Kriegel, Matthias Schubert, Sebastian Polsterl, and Alexander Cavallaro, 2d image registration in ct images using radial image descriptors, MICCAI 2011, Springer, 2011.
David Gross and Vincent Nesme, Note on sampling without replacing from a finite collection of matrices, arXiv preprint arXiv:1001.2738 (2010).
Igor Griva, Stephen G Nash, and Ariela Sofer, Linear and nonlinear optimization, Siam, 2009.
Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp, Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions, no. 2, 217288.
M. Lichman, UCI machine learning repository, 2013.
Nicolas Le Roux and Andrew W Fitzgibbon, A fast natural newton method, ICML, 2010.
Nicolas Le Roux, Pierre-A Manzagol, and Yoshua Bengio, Topmoumoute online natural gradient algorithm, NIPS, 2008.
James Martens, Deep learning via hessian-free optimization, ICML, 2010, pp. 735742.
Thierry B. Mahieux, Daniel P.W. Ellis, Brian Whitman, and Paul Lamere, The million song dataset, ISMIR-11.
Yurii Nesterov, A method for unconstrained convex minimization problem with the rate of convergence o (1/k2), Doklady AN SSSR, vol. 269, 1983, pp. 543547.
, Introductory lectures on convex optimization: A basic course, vol. 87, Springer, 2004.
Mark Schmidt, Nicolas Le Roux, and Francis Bach, Minimizing finite sums with the stochastic average gradient, arXiv preprint arXiv:1309.2388 (2013).
Bernhard Scholkopf and Alexander J Smola, Learning with kernels: support vector machines, regularization, optimization, and beyond, MIT press, 2002.
Joel A Tropp, User-friendly tail bounds for sums of random matrices, Foundations of Computational Mathematics (2012).
Roman Vershynin, Introduction to the non-asymptotic analysis of random matrices, arXiv:1011.3027 (2010).
Oriol Vinyals and Daniel Povey, Krylov Subspace Descent for Deep Learning, AISTATS, 2012.  9
Dirk Bergemann and Stephen Morris. Correlated Equilibrium in Games with Incomplete Information. Cowles Foundation Discussion Papers 1822, Cowles Foundation for Research in Economics, Yale University, October 2011.
Avrim Blum, MohammadTaghi Hajiaghayi, Katrina Ligett, and Aaron Roth. Regret minimization and the price of total anarchy. In Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing, STOC 08, pages 373382, New York, NY, USA, 2008. ACM.
Yang Cai and Christos Papadimitriou. Simultaneous bayesian auctions and computational complexity. In Proceedings of the fifteenth ACM conference on Economics and Computation, EC 14, pages 895910, New York, NY, USA, 2014. ACM.
Ioannis Caragiannis, Christos Kaklamanis, Panagiotis Kanellopoulos, Maria Kyropoulou, Brendan Lucier, Renato Paes Leme, and Eva Tardos. Bounding the inefficiency of outcomes in generalized second price auctions. Journal of Economic Theory, (0):, 2014.
Bart de Keijzer, Evangelos Markakis, Guido Schfer, and Orestis Telelis. Inefficiency of standard multi-unit auctions. In HansL. Bodlaender and GiuseppeF. Italiano, editors, Algorithms ESA 2013, volume 8125 of Lecture Notes in Computer Science, pages 385396. Springer Berlin Heidelberg, 2013.
Franoise Forges. Five legitimate definitions of correlated equilibrium in games with incomplete information. Theory and Decision, 35(3):277310, 1993.
Dean P Foster and Rakesh V Vohra. Asymptotic calibration. Biometrika, 85(2):379390, 1998.
ToddR. Kaplan and Shmuel Zamir. Asymmetric first-price auctions with uniform distributions: analytic solutions to the general case. Economic Theory, 50(2):269302, 2012.
Elias Koutsoupias and Christos Papadimitriou. Worst-case equilibria. In Proceedings of the 16th annual conference on Theoretical aspects of computer science, STACS99, pages 404 413, Berlin, Heidelberg, 1999. Springer-Verlag.
B. Lucier and A. Borodin. Price of anarchy for greedy auctions. In Proceedings of the TwentyFirst Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 10, pages 537553, Philadelphia, PA, USA, 2010. Society for Industrial and Applied Mathematics.
T. Roughgarden. Intrinsic robustness of the price of anarchy. In Proceedings of the 41st annual ACM symposium on Theory of computing, STOC 09, pages 513522, New York, NY, USA, 2009. ACM.
Vasilis Syrgkanis and Eva Tardos. Composable and efficient mechanisms. In Proceedings of the Forty-fifth Annual ACM Symposium on Theory of Computing, STOC 13, pages 211220, New York, NY, USA, 2013. ACM.
A. Vetta. Nash equilibria in competitive societies, with applications to facility location, traffic routing and auctions. In Foundations of Computer Science, 2002. Proceedings. The 43rd Annual IEEE Symposium on, pages 416425, 2002.  9
A. Adcock, E. Carlsson, and G. Carlsson. The ring of algebraic functions on persistence bar codes. arXiv, available at http://arxiv.org/abs/1304.0530, 2013.
P. Bendich, J.S. Marron, E. Miller, A. Pieloch, and S. Skwerer. Persistent homology analysis of brain artery trees. arXiv, available at http://arxiv.org/abs/1411.6652, 2014.
L. Bompard, S. Xu, M. Styner, B. Paniagua, M. Ahn, Y. Yuan, V. Jewells, W. Gao, D. Shen, H. Zhu, and W. Lin. Multivariate longitudinal shape analysis of human lateral ventricles during the first twenty-four months of life. PLoS One, 2014.
P. Bubenik. Statistical topological data analysis using persistence landscapes. JMLR, 16:77102, 2015.
G. Carlsson. Topology and data. Bull. Amer. Math. Soc., 46:255308, 2009.
F. Chazal, B.T. Fasy, F. Lecci A. Rinaldo, and L. Wasserman. Stochastic convergence of persistence landscapes and silhouettes. In SoCG, 2014.
A. Christmann and I. Steinwart. Universal kernels on non-standard input spaces. In NIPS, 2010.
M.K. Chung, P. Bubenik, and P.T. Kim. Persistence diagrams of cortical surface data. In IPMI, 2009.
I.L. Dryden and K.V. Mardia. Statistical shape analysis. Wiley series in probability and statistics. Wiley, 1998.
H. Edelsbrunner and J. Harer. Computational Topology. An Introduction. AMS, 2010.
B. Fasy, F. Lecci, A. Rinaldo, L. Wasserman, S. Balakrishnan, and A. Singh. Confidence sets for persistence diagrams. Ann. Statist., 42(6):23012339, 2014.
K. Fukumizu, L. Song, and A. Gretton. Kernel Bayes rule: Bayesian inference with positive definite kernels. JMLR, 14:37533783, 2013.
A. Gretton, K.M. Borgwardt, M.J. Rasch, B. Schlkopf, and A. Smola. A kernel two-sample test. JMLR, 13:723773, 2012.
M. Ledoux and M. Talagrand. Probability in Banach spaces. Classics in Mathematics. Springer, 1991.
H. Lee, M.K. Chung, H. Kang, and D.S. Lee. Hole detection in metabolic connectivity of Alzheimers disease using k-Laplacian. In MICCAI, 2014.
C. Li, M. Ovsjanikov, and F. Chazal. Persistence-based structural recognition. In CVPR, 2014.
D.S. Marcus, A.F. Fotenos, J.G. Csernansky, J.C. Morris, and R.L. Buckner. Open access series of imaging studies: longitudinal MRI data in nondemented and demented older adults. J. Cognitive Neurosci., 22(12):26772684, 2010.
Y. Mileyko, S. Mukherjee, and J. Harer. Probability measures on the space of persistence diagrams. Inverse Probl., 27(12), 2011.
E. Munch, P. Bendich, S. Mukherjee, J. Mattingly, and J. Harer. Probabilistic Frchet means and statistics on vineyards. CoRR, 2013. http://arxiv.org/abs/1307.6530.
R. Reininghaus, U. Bauer, S. Huber, and R. Kwitt. A stable multi-scale kernel for topological machine learning. In CVPR, 2015.
B. Schlkopf and A.J. Smola. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press, Cambridge, MA, USA, 2001.
N. Singh, H. D. Couture, J. S. Marron, C. Perou, and M. Niethammer. Topological descriptors of histology images. In MLMI, 2014.
A. Smola, A. Gretton, L. Song, and B. Schlkopf. Hilbert space embedding for distributions. In ALT, 2007.
B. Sriperumbudur, A. Gretton, K. Fukumizu, B. Schlkopf, and G. Lanckriet. Hilbert space embeddings and metrics on probability measures. JMLR, 11:15171561, 2010.
I. Steinwart. On the influence of the kernel on the consistency of support vector machines. JMLR, 2:67 93, 2001.
I. Steinwart and A. Christmann. Support Vector Machines. Springer, 2008.
J. Sun, M. Ovsjanikov, and L. Guibas. A concise and probably informative multi-scale signature based on heat diffusion. In SGP, 2009.
K. Turner, Y. Mileyko, S. Mukherjee, and J. Harer. Frchet means for distributions of persistence diagrams. Discrete Comput. Geom., 52(1):4470, 2014.  9
R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unlabeled data. J. Mach. Learn. Res., 6:18171853, December 2005.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural probabilistic language model. In JMLR, 2003.
A. Cardoso-Cachopo. Datasets for single-label text categorization. http://web.ist. utl.pt/acardoso/datasets/, 2015.
William Chan, Navdeep Jaitly, Quoc V Le, and Oriol Vinyals. Listen, attend and spell. arXiv preprint arXiv:1508.01211, 2015.
Y. Dauphin and Y. Bengio. Stochastic ratio matching of RBMs for sparse high-dimensional inputs. In NIPS, 2013.
F. A. Gers, J. Schmidhuber, and F. Cummins. Learning to forget: Continual prediction with LSTM. Neural Computation, 2000.
A. Graves. Generating sequences with recurrent neural networks. In Arxiv, 2013.
K. Greff, R. K. Srivastava, J. Koutnk, B. R. Steunebrink, and J. Schmidhuber. LSTM: A search space odyssey. In ICML, 2015.
S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies. A Field Guide to Dynamical Recurrent Neural Networks, 2001.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 1997.
S. Jean, K. Cho, R. Memisevic, and Y. Bengio. On using very large target vocabulary for neural machine translation. In ICML, 2014.
R. Johnson and T. Zhang. Effective use of word order for text categorization with convolutional neural networks. In NAACL, 2014.
Y. Kim. Convolutional neural networks for sentence classification, 2014. 8
R. Kiros, Y. Zhu, R. Salakhutdinov, R. S. Zemel, A. Torralba, R. Urtasun, and S. Fidler. Skipthought vectors. In NIPS, 2015.
A. Krizhevsky. Convolutional deep belief networks on CIFAR-10. Technical report, University of Toronto, 2010.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.
K. Lang. Newsweeder: Learning to filter netnews. In ICML, 1995.
H. Larochelle, M. Mandel, R. Pascanu, and Y. Bengio. Learning algorithms for the classification restricted boltzmann machine. JMLR, 2012.
Q. V. Le and T. Mikolov. Distributed representations of sentences and documents. In ICML, 2014.
J. Lehmann, R. Isele, M. Jakob, A. Jentzsch, D. Kontokostas, P. N. Mendes, S. Hellmann, M. Morsey, P. van Kleef, S. Auer, et al. DBpedia  a large-scale, multilingual knowledge base extracted from wikipedia. Semantic Web, 2014.
T. Luong, I. Sutskever, Q. V. Le, O. Vinyals, and W. Zaremba. Addressing the rare word problem in neural machine translation. arXiv preprint arXiv:1410.8206, 2014.
A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. Learning word vectors for sentiment analysis. In ACL, 2011.
J. McAuley and J. Leskovec. Hidden factors and hidden topics: understanding rating dimensions with review text. In RecSys, pages 165172. ACM, 2013.
T. Mikolov, M. Karafiat, L. Burget, J. Cernocky, and S. Khudanpur. Recurrent neural network based language model. In INTERSPEECH, 2010.
J. Y. H. Ng, M. J. Hausknecht, S. Vijayanarasimhan, O. Vinyals, R. Monga, and G. Toderici. Beyond short snippets: Deep networks for video classification. In CVPR, 2015.
B. Pang and L. Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In ACL, 2005.
D. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors. Nature, 1986.
L. Shang, Z. Lu, and H. Li. Neural responding machine for short-text conversation. In EMNLP, 2015.
R. Socher, B. Huval, C. D. Manning, and A. Y. Ng. Semantic compositionality through recursive matrix-vector spaces. In EMNLP, 2012.
R. Socher, A. Perelygin, J. Y. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, 2013.
N. Srivastava, E. Mansimov, and R. Salakhutdinov. Unsupervised learning of video representations using LSTMs. In ICML, 2015.
I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In NIPS, 2014.
O. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever, and G. Hinton. Grammar as a foreign language. In NIPS, 2015.
O. Vinyals and Q. V. Le. A neural conversational model. In ICML Deep Learning Workshop, 2015.
O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. In CVPR, 2014.
S. I. Wang and C. D. Manning. Baselines and bigrams: Simple, good sentiment and topic classification. In ACL, 2012.
P. J. Werbos. Beyond regression: New tools for prediction and analysis in the behavioral sciences. PhD thesis, Harvard, 1974.
W. Zaremba, I. Sutskever, and O. Vinyals. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329, 2014.
X. Zhang and Y. LeCun. Character-level convolutional networks for text classification. In NIPS, 2015. 9
Supplementary material: Structured transforms for small footprint deep learning. 2015. http://vikas.sindhwani.org/st_supplementary.pdf.
G. Chen, C. Parada, and G. Heigold. Small-footprint keyword spotting using deep neural networks. In ICASSP, 2014.
W. Chen, J. T. Wilson, S. Tyree, K. Q. Weinberger, and Y. Chen. Compressing neural networks with the hashing trick. In ICML, 2015.
Y. Cheng, F. X. Xu, R. S. Feris, S. Kumar, A. Choudhary, and S.-F. Chang. Fast neural networks with circulant projections. In arXiv:1502.03436, 2015.
D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, and Schmidhuber. High-performance neural networks for visual object classification. In arXiv:1102.0183, 2011.
M. D. Collins and P. Kohli. Memory-bounded deep convolutional neural networks. In ICASSP, 2013.
M. Courbariaux, J.-P. David, and Y. Bengio. Low-precision storage for deep learning. In ICLR, 2015.
J. Dean, G. S. Corrado, R. Monga, K. Chen, M. Devin, Q. V. Le, M. Z. Mao, M. Ranzato, A. Senior, P. Tucker, K. Yang, , and A. Y. Ng. Large-scale distributed deep networks. In NIPS, 2012.
M. Denil, B. Shakibi, L. Dinh, and N. de Freitas. Predicting parameters in deep learning. In NIPS, 2013.
R. M. Gray. Toeplitz and circulant matrices: A review. Foundations and Trends in Communications and Information Theory, 2005.
G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. In NIPS workshop, 2014.
T. Kailath and J. Chun. Generalized displacement structure for block toeplitz, toeplitz block and toeplitz-derived matrices. SIAM J. Matrix Anal. Appl., 15, 1994.
T. Kailath, S. Y. Kung, and M. Morf. Displacement ranks of matrices and linear equations. Journal of Mathematical Analysis and Applications, pages 395407, 1979.
T. Kailath and A. H. Sayed. Displacement structure: Theory and applications. SIAM Review, 37, 1995.
H. Larochelle, D. Erhan, A. C. Courville, J. Bergstra, and Y. Bengio. An empirical evaluation of deep architectures on problems with many factors of variation. In ICML, 2007.
Q. Le, T. Sarlos, and A. Smola. Fastfood  approximating kernel expansions in loglinear time. In ICML, 2013.
E. Liberty and S. W. Zucker. The mailman algorithm: a note on matrix vector multiplication. In Information Processing Letters, 2009.
V. Pan. Structured Matrices and Polynomials: Unified Superfast Algorithms. Springer, 2001.
V. Pan. Inversion of displacement operators. SIAM Journal of Matrix Analysis and Applications, pages 660677, 2003.
A. Rahimi and B. Recht. Random features for large-scale kernel machines. In NIPS, 2007.
M. V. Rakhuba and I. V. Oseledets. Fast multidimensional convolution in low-rank tensor formats via cross approximation. SIAM J. Sci. Comput., 37, 2015.
T. Sainath, B. Kingsbury, V. Sindhwani, E. Arisoy, and B. Ramabhadran. Low-rank matrix factorization for deep neural network training with high-dimensional output targets. In ICASSP, 2013.
T. Sainath and C. Parada. Convolutional neural networks for small-footprint keyword spotting. In Proc. Interspeech, 2015.
V. Vanhoucke, A. Senior, and M. Z. Mao. Improving the speed of neural networks on cpus. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011.
Z. Yang, M. Moczulski, M. Denil, N. de Freitas, A. Smola, L. Song, and Z. Wang. Deep fried convnets. In arXiv:1412.7149, 2015.  9
Venkat Chandrasekaran, Nathan Srebro, and Prahladh Harsha. Complexity of inference in graphical models. arXiv preprint arXiv:1206.3240, 2012.
Persi Diaconis, Kshitij Khare, and Laurent Saloff-Coste. Gibbs sampling, exponential families and orthogonal polynomials. Statist. Sci., 23(2):151178, May 2008.
Persi Diaconis, Kshitij Khare, and Laurent Saloff-Coste. Gibbs sampling, conjugate priors and coupling. Sankhya A, (1):136169, 2010.
Pedro Domingos and William Austin Webb. A tractable first-order probabilistic logic. In AAAI, 2012.
Joseph Gonzalez, Yucheng Low, Arthur Gretton, and Carlos Guestrin. Parallel gibbs sampling: From colored fields to thin junction trees. In AISTATS, pages 324332, 2011.
Georg Gottlob, Gianluigi Greco, and Francesco Scarcello. Treewidth and hypertree width. Tractability: Practical Approaches to Hard Problems, page 1, 2014.
Alexander T Ihler, John Iii, and Alan S Willsky. Loopy belief propagation: Convergence and effects of message errors. In Journal of Machine Learning Research, pages 905936, 2005.
Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. MIT press, 2009.
Johan Kwisthout, Hans L Bodlaender, and Linda C van der Gaag. The necessity of bounded treewidth for efficient inference in bayesian networks. In ECAI, pages 237242, 2010.
David Asher Levin, Yuval Peres, and Elizabeth Lee Wilmer. Markov chains and mixing times. American Mathematical Soc., 2009.
Xianghang Liu and Justin Domke. Projecting markov random field parameters for fast mixing. In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 13771385. Curran Associates, Inc., 2014.
David Lunn, David Spiegelhalter, Andrew Thomas, and Nicky Best. The BUGS project: evolution, critique and future directions. Statistics in medicine, (25):30493067, 2009.
Daniel Marx. Tractable hypergraph properties for constraint satisfaction and conjunctive queries. Journal of the ACM (JACM), (6):42, 2013.
Andrew McCallum, Karl Schultz, and Sameer Singh. Factorie: Probabilistic programming via imperatively defined factor graphs. In NIPS, pages 12491257, 2009.
David Newman, Padhraic Smyth, Max Welling, and Arthur U Asuncion. Distributed inference for latent dirichlet allocation. In NIPS, pages 10811088, 2007.
Kee Siong Ng, John W Lloyd, and William TB Uther. Probabilistic modelling, inference and learning using logical theories. Annals of Mathematics and Artificial Intelligence, (1-3):159205, 2008.
Shanan E Peters, Ce Zhang, Miron Livny, and Christopher Re. A machine reading system for assembling synthetic Paleontological databases. PloS ONE, 2014.
David Poole. First-order probabilistic inference. In IJCAI, pages 985991. Citeseer, 2003.
Neil Robertson and Paul D. Seymour. Graph minors. ii. algorithmic aspects of tree-width. Journal of algorithms, (3):309322, 1986.
Jaeho Shin, Sen Wu, Feiran Wang, Christopher De Sa, Ce Zhang, Feiran Wang, and Christopher Re. Incremental knowledge base construction using deepdive. PVLDB, 2015.
Parag Singla and Pedro Domingos. Lifted first-order belief propagation. In AAAI, pages 10941099, 2008.
Alexander Smola and Shravan Narayanamurthy. An architecture for parallel topic models. PVLDB, 2010.
Dan Suciu, Dan Olteanu, Christopher Re, and Christoph Koch. Probabilistic databases. Synthesis Lectures on Data Management, (2):1180, 2011.
Mihai Surdeanu and Heng Ji. Overview of the english slot filling track at the TAC2014 knowledge base population evaluation.
Lucas Theis, Jascha Sohl-dickstein, and Matthias Bethge. Training sparse natural image models with a fast gibbs sampler of an extended state space. In NIPS, pages 11241132. 2012.
Deepak Venugopal and Vibhav Gogate. On lifting the gibbs sampling algorithm. In F. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors, NIPS, pages 16551663. Curran Associates, Inc., 2012.
Deepak Venugopal, Somdeb Sarkhel, and Vibhav Gogate. Just count the satisfied groundings: Scalable local-search and sampling based inference in mlns. In AAAI Conference on Artificial Intelligence, 2015.
Ce Zhang and Christopher Re. DimmWitted: A study of main-memory statistical analytics. PVLDB, 2014.  9
Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In NIPS, pages 10571063, 1999.
Csaba Szepesvari. Algorithms for Reinforcement Learning. Morgan and Claypool Publishers, 2010.
M. P. Deisenroth, G. Neumann, and J. Peters. A survey on policy search for robotics. pages 388403, 2013.  7  simulated quadrotor motion  AscTec Pelican  A* waypoint path  iteration #17  iteration #5  iteration #1 Expected Cost  Probability of Collision  1  350  empirical P robust P PAC bound P +  empirical Jb robust Jb  300  PAC bound J  0.8  +  250  0.6  200 150  0.4  100  0.2 50  0  0 0  5  10 15 iterations  a)  20  0  25  5  10 15 iterations  b)  20  25  c)  Figure 2: Aerial vehicle navigation using a simulated nonlinear quadrotor model (top). Iterative stochastic policy optimization iterations (a,b,c) analogous to those given in Figure 1. Note that the initial policy results in over 50% collisions which is reduced to less than 10% after a few policy iterations. random Goals  Start campus map  iteration #1  iteration #4  iteration #10  Expected Cost  Probability of Collision  1  400  empirical P robust P PAC bound P +  empirical Jb robust Jb  350  PAC bound J  300 250  +  0.8  0.6  200  0.4  150 100  0.2 50  0  0 0  5  10  15  0  5  10  iterations  iterations  b)  c)  a)  15  Figure 3: Analogous plot to Figure 2 but for a typical campus environment using uniformly at random sampled goal states along the northern boundary. The vehicle must fly below 100 feet and is not allowed to fly above buildings. This is a larger less constrained environment resulting in less collisions.  8
S. Schaal and C. Atkeson. Learning control in robotics. Robotics Automation Magazine, IEEE, 17(2):20 29, june 2010.
Alberto Bemporad and Manfred Morari. Robust model predictive control: A survey. In A. Garulli and A. Tesi, editors, Robustness in identification and control, volume 245 of Lecture Notes in Control and Information Sciences, pages 207226. Springer London, 1999.
Vladimir N. Vapnik. The nature of statistical learning theory. Springer-Verlag New York, Inc., New York, NY, USA, 1995.
David A. McAllester. Pac-bayesian stochastic model selection. Mach. Learn., 51:521, April 2003.
J Langford. Tutorial on practical prediction theory for classification. Journal of Machine Learning Research, 6(1):273306, 2005.
Stphane Boucheron, Gbor Lugosi, Pascal Massart, and Michel Ledoux. Concentration inequalities : a nonasymptotic theory of independence. Oxford university press, Oxford, 2013.
M. Vidyasagar. Randomized algorithms for robust controller synthesis using statistical learning theory. Automatica, 37(10):15151528, October 2001.
Laura Ryan Ray and Robert F. Stengel. A monte carlo approach to the analysis of control system robustness. Automatica, 29(1):229236, January 1993.
Qian Wang and RobertF. Stengel. Probabilistic control of nonlinear uncertain systems. In Giuseppe Calafiore and Fabrizio Dabbene, editors, Probabilistic and Randomized Methods for Design under Uncertainty, pages 381414. Springer London, 2006.
R. Tempo, G. Calafiore, and F. Dabbene. Randomized algorithms for analysis and control of uncertain systems. Springer, 2004.
V. Koltchinskii, C.T. Abdallah, M. Ariola, and P. Dorato. Statistical learning control of uncertain systems: theory and algorithms. Applied Mathematics and Computation, 120(13):31  43, 2001. ce:titleThe Bellman Continuum/ce:title.
M. Vidyasagar and Rajeeva L. Karandikar. A learning theory approach to system identification and stochastic adaptive control. Journal of Process Control, 18(34):421  430, 2008. Festschrift honouring Professor Dale Seborg.
Reuven Y. Rubinstein and Dirk P. Kroese. The cross-entropy method: a unified approach to combinatorial optimization. Springer, 2004.
Anatoly Zhigljavsky and Antanasz Zilinskas. Stochastic Global Optimization. Spri, 2008.
Philipp Hennig and Christian J. Schuler. Entropy search for information-efficient global optimization. J. Mach. Learn. Res., 98888:18091837, June 2012.
Christian Igel, Nikolaus Hansen, and Stefan Roth. Covariance matrix adaptation for multi-objective optimization. Evol. Comput., 15(1):128, March 2007.
Pedro Larraaga and Jose A. Lozano, editors. Estimation of distribution algorithms: A new tool for evolutionary computation. Kluwer Academic Publishers, 2002.
Martin Pelikan, David E. Goldberg, and Fernando G. Lobo. A survey of optimization by building and using probabilistic models. Comput. Optim. Appl., 21:520, January 2002.
Howie Choset, Kevin M. Lynch, Seth Hutchinson, George A Kantor, Wolfram Burgard, Lydia E. Kavraki, and Sebastian Thrun. Principles of Robot Motion: Theory, Algorithms, and Implementations. MIT Press, June 2005.
Corinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning Bounds for Importance Weighting. In Advances in Neural Information Processing Systems 23, 2010.
Olivier Catoni. Challenging the empirical mean and empirical variance: A deviation study. Ann. Inst. H. Poincar Probab. Statist., 48(4):11481185, 11 2012.
E. Theodorou, J. Buchli, and S. Schaal. A generalized path integral control approach to reinforcement learning. Journal of Machine Learning Research, (11):31373181, 2010.
Sergey Levine and Pieter Abbeel. Learning neural network policies with guided policy search under unknown dynamics. In Neural Information Processing Systems (NIPS), 2014.
M. Kobilarov. Cross-entropy motion planning. International Journal of Robotics Research, 31(7):855 871, 2012.
Robert Mahony and Tarek Hamel. Robust trajectory tracking for a scale model autonomous helicopter. International Journal of Robust and Nonlinear Control, 14(12):10351059, 2004.
Marin Kobilarov. Trajectory tracking of a class of underactuated systems with external disturbances. In American Control Conference, pages 10441049, 2013.  9
Geoffrey Hinton, Li Deng, George E. Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal Processing Magazine, 29(6):8297, Nov. 2012.
Tara Sainath, Abdel rahman Mohamed, Brian Kingsbury, and Bhuvana Ramabhadran. Deep convolutional neural networks for LVCSR. In ICASSP 2013, 2013.
A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS2012. 2012.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. Technical report, arXiv:1409.4842, 2014.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. Fast and robust neural network joint models for statistical machine translation. In Proc. ACL2014, 2014.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In NIPS2014, 2014.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In ICLR2015, arXiv:1409.0473, 2015.
Rajat Raina, Anand Madhavan, and Andrew Y. Ng. Large-scale deep unsupervised learning using graphics processors. In ICML2009, 2009.
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model. Journal of Machine Learning Research, 3:11371155, 2003.
J. Dean, G.S Corrado, R. Monga, K. Chen, M. Devin, Q.V. Le, M.Z. Mao, M.A. Ranzato, A. Senior, P. Tucker, K. Yang, and A. Y. Ng. Large scale distributed deep networks. In NIPS2012, 2012.
Sang Kyun Kim, Lawrence C McAfee, Peter Leonard McMahon, and Kunle Olukotun. A highly scalable restricted Boltzmann machine FPGA implementation. In Field Programmable Logic and Applications, 2009. FPL 2009. International Conference on, pages 367372. IEEE, 2009.
Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengyong Wu, Yunji Chen, and Olivier Temam. Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning. In Proceedings of the 19th international conference on Architectural support for programming languages and operating systems, pages 269284. ACM, 2014.
Yunji Chen, Tao Luo, Shaoli Liu, Shijin Zhang, Liqiang He, Jia Wang, Ling Li, Tianshi Chen, Zhiwei Xu, Ninghui Sun, et al. Dadiannao: A machine-learning supercomputer. In Microarchitecture (MICRO), 2014 47th Annual IEEE/ACM International Symposium on, pages 609622. IEEE, 2014.
Lorenz K Muller and Giacomo Indiveri. Rounding methods for neural networks with low resolution synaptic weights. arXiv preprint arXiv:1504.05767, 2015.
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In ICML2015, 2015.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Low precision arithmetic for deep learning. In Arxiv:1412.7024, ICLR2015 Workshop, 2015.
Thomas M Bartol, Cailey Bromer, Justin P Kinney, Michael A Chirillo, Jennifer N Bourne, Kristen M Harris, and Terrence J Sejnowski. Hippocampal spine head sizes are highly precise. bioRxiv, 2015.
Alex Graves. Practical variational inference for neural networks. In J. Shawe-Taylor, R.S. Zemel, P.L. Bartlett, F. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 23482356. Curran Associates, Inc., 2011.
Nitish Srivastava. Improving neural networks with dropout. Masters thesis, U. Toronto, 2013.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15:19291958, 2014.  8
Li Wan, Matthew Zeiler, Sixin Zhang, Yann LeCun, and Rob Fergus. Regularization of neural networks using dropconnect. In ICML2013, 2013.
J.P. David, K. Kalach, and N. Tittley. Hardware complexity of modular multiplication and exponentiation. Computers, IEEE Transactions on, 56(10):13081319, Oct 2007.
R. Collobert. Large Scale Machine Learning. PhD thesis, Universite de Paris VI, LIP6, 2004.
X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier neural networks. In AISTATS2011, 2011.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS2010, 2010.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. 2015.
Diederik Kingma and Jimmy Ba. arXiv:1412.6980, 2014.  Adam: A method for stochastic optimization.  arXiv preprint
Yu Nesterov. A method for unconstrained convex minimization problem with the rate of convergence o(1/k2 ). Doklady AN SSSR (translated as Soviet. Math. Docl.), 269:543547, 1983.
Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout networks. Technical Report Arxiv report 1302.4389, Universite de Montreal, February 2013.
Yichuan Tang. Deep learning using linear support vector machines. Workshop on Challenges in Representation Learning, ICML, 2013.
Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013.
Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeply-supervised nets. arXiv preprint arXiv:1409.5185, 2014.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, November 1998.
V. Nair and G.E. Hinton. Rectified linear units improve restricted Boltzmann machines. In ICML2010, 2010.
Benjamin Graham. Spatially-sparse convolutional neural networks. arXiv preprint arXiv:1409.6070, 2014.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.
Daniel Soudry, Itay Hubara, and Ron Meir. Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights. In NIPS2014, 2014.
Zhiyong Cheng, Daniel Soudry, Zexi Mao, and Zhenzhong Lan. Training binary multilayer neural networks for image classification using expectation backpropgation. arXiv preprint arXiv:1503.03562, 2015.
Kyuyeon Hwang and Wonyong Sung. Fixed-point feedforward deep neural network design using weights+ 1, 0, and- 1. In Signal Processing Systems (SiPS), 2014 IEEE Workshop on, pages 16. IEEE, 2014.
Jonghong Kim, Kyuyeon Hwang, and Wonyong Sung. X1000 real-time phoneme recognition vlsi using feed-forward deep neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on, pages 75107514. IEEE, 2014.
Thomas P Minka. Expectation propagation for approximate bayesian inference. In UAI2001, 2001.
James Bergstra, Olivier Breuleux, Frederic Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), June 2010. Oral Presentation.
Frederic Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud Bergeron, Nicolas Bouchard, and Yoshua Bengio. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.
Ian J. Goodfellow, David Warde-Farley, Pascal Lamblin, Vincent Dumoulin, Mehdi Mirza, Razvan Pascanu, James Bergstra, Frederic Bastien, and Yoshua Bengio. Pylearn2: a machine learning research library. arXiv preprint arXiv:1308.4214, 2013.  9
P. Chen. Hessian matrix vs. gauss-newton hessian matrix. SIAM J. Numerical Analysis, 49(4):14171435, 2011.
H. Geyer and H. Herr. A muscle-reflex model that encodes principles of legged mechanics produces human walking dynamics and muscle activities. Neural Systems and Rehabilitation Engineering, IEEE Transactions on, 18(3):263273, 2010.
R. Grzeszczuk, D. Terzopoulos, and G. Hinton. Neuroanimator: Fast neural network emulation and control of physics-based models. In Proceedings of the 25th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 98, pages 920, New York, NY, USA, 1998. ACM.
G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580, 2012.
G. M. Hoerzer, R. Legenstein, and W. Maass. Emergence of complex computational structures from chaotic neural networks through reward-modulated hebbian learning. Cerebral Cortex, 2012.
D. Huh and E. Todorov. Real-time motor control using recurrent neural networks. In Adaptive Dynamic Programming and Reinforcement Learning, 2009. ADPRL 09. IEEE Symposium on, pages 4249, March 2009.
A. J. Ijspeert. Central pattern generators for locomotion control in animals and robots: a review, 2008.
E. Ju, J. Won, J. Lee, B. Choi, J. Noh, and M. G. Choi. Data-driven control of flapping flight. ACM Trans. Graph., 32(5):151:1151:12, Oct. 2013.
S. Levine and V. Koltun. Learning complex neural network policies with trajectory optimization. In ICML 14: Proceedings of the 31st International Conference on Machine Learning, 2014.
V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. A. Riedmiller. Playing atari with deep reinforcement learning. CoRR, abs/1312.5602, 2013.
I. Mordatch and E. Todorov. Combining the benefits of function approximation and trajectory optimization. In Robotics: Science and Systems (RSS), 2014.
I. Mordatch, E. Todorov, and Z. Popovic. Discovery of complex behaviors through contactinvariant optimization. ACM Transactions on Graphics (TOG), 31(4):43, 2012.
J. R. Rebula, P. D. Neuhaus, B. V. Bonnlander, M. J. Johnson, and J. E. Pratt. A controller for the littledog quadruped walking on rough terrain. In Robotics and Automation, 2007 IEEE International Conference on, pages 14671473. IEEE, 2007.
J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel. Trust region policy optimization. CoRR, abs/1502.05477, 2015.
I. Sutskever, J. Martens, G. E. Dahl, and G. E. Hinton. On the importance of initialization and momentum in deep learning. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), volume 28, pages 11391147, May 2013.
E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In IROS12, pages 50265033, 2012.
P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with denoising autoencoders. pages 10961103, 2008.
M. Vukobratovic and B. Borovac. Zero-moment point - thirty five years of its life. I. J. Humanoid Robotics, 1(1):157173, 2004.
S. Wager, S. Wang, and P. Liang. Dropout training as adaptive regularization. In Advances in Neural Information Processing Systems (NIPS), 2013.
J. M. Wang, D. J. Fleet, and A. Hertzmann. Optimizing walking controllers for uncertain inputs and environments. ACM Trans. Graph., 29(4):73:173:8, July 2010.
K. Yin, K. Loken, and M. van de Panne. Simbicon: Simple biped locomotion control. ACM Trans. Graph., 26(3):Article 105, 2007.  9
S. Lloyd. Least Squares Quantization in PCM. IEEE Transactions on Information Theory, 28(2):129137, 1982.
T. Hazan, S. Maji, J. Keshet, and T. Jaakkola. Learning Efficient Random Maximum A-Posteriori Predictors with Non-Decomposable Loss Functions. In NIPS, 2013.
M. Szummer, P. Kohli, and D. Hoiem. Learning CRFs Using Graph Cuts. In ECCV, 2008.
A. Osokin and P. Kohli. Perceptually Inspired Layout-Aware Losses for Image Segmentation. In ECCV, 2014.
J. Yu and M. Blaschko. Learning Submodular Losses with the Lovasz Hinge. In ICML, 2015.
D. Batra, P. Yadollahpour, A. Guzman, and G. Shakhnarovich. Diverse M-Best Solutions in Markov Random Fields. In ECCV, 2012.
H. Lin and J. Bilmes. A Class of Submodular Functions for Document Summarization. In ACL.
S. Tschiatschek, R. Iyer, H. Wei, and J. Bilmes. Learning Mixtures of Submodular Functions for Image Collection Summarization. In NIPS, 2014.
P. Halmos. Measure Theory. Springer, 1974.
S. Jegelka and J. Bilmes. Approximation Bounds for Inference using Cooperative Cuts. In ICML, 2011.
M. Bateni, M. Hajiaghayi, and M. Zadimoghaddam. Submodular Secretary Problem and Extensions. Technical report, MIT, 2010.
W. H. Cunningham. On Submodular Function Minimization. Combinatorica, 3:185  192, 1985.
G. Nemhauser, L. Wolsey, and M. Fisher. An Analysis of Approximations for Maximizing Submodular Set Functions I. 14(1), 1978.
Satoru Fujishige. Submodular Functions and Optimization. Elsevier, 2 edition, 2005.
D. Gusfield. Algorithms on Strings, Trees and Sequences: Computer Science and Computational Biology. Cambridge University Press, 1997.
R. Iyer, S. Jegelka, and J. Bilmes. Curvature and Efficient Approximation Algorithms for Approximation and Minimization of Submodular Functions. In NIPS, 2013.
G. Goel, C. Karande, P. Tripathi, and L. Wang. Approximability of combinatorial problems with multi-agent submodular cost functions. In FOCS, 2009.
J. Vondrak. Submodularity and Curvature: The Optimal Algorithm. RIMS Kokyuroku Bessatsu, 23, 2010.
Z. Svitkina and L. Fleischer. Submodular Approximation: Sampling-Based Algorithms and Lower Bounds. In FOCS, 2008.
S. Jegelka and J. Bilmes. Submodularity Beyond Submodular Energies: Coupling Edges in Graph Cuts. In CVPR, 2011.
R. Iyer and J. Bilmes. The Submodular Bregman and Lovasz-Bregman Divergences with Applications. In NIPS, 2012.
R. Iyer, S. Jegelka, and J. Bilmes. Fast Semidifferential-Based Submodular Function Optimization. In ICML, 2013.
N. Buchbinder, M. Feldman, J. Naor, and R. Schwartz. A Tight Linear Time (1/2)-Approximation for Unconstrained Submodular Maximization. In FOCS, 2012.
N. Buchbinder, M. Feldman, J. Naor, and R. Schwartz. Submodular maximization with cardinality constraints. In SODA, 2014.
D. Arthur and S. Vassilvitskii. k-means++: The Advantages of Careful Seeding. In SODA, 2007.
T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed Representations of Words and Phrases and their Compositionality. In NIPS, 2013.  9
M. Jaggi, Revisiting Frank-Wolfe: Projection-free sparse convex optimization. J. Mach. Learn. Res. Workshop & Conf. Proc., vol. 28, pp. 427435, 2013.
V. Cevher, S. Becker, and M. Schmidt. Convex optimization for big data: Scalable, randomized, and parallel algorithms for big data analytics. IEEE Signal Process. Mag., vol. 31, pp. 3243, Sept. 2014.
M. J. Wainwright, Structured regularizers for high-dimensional problems: Statistical and computational issues. Annu. Review Stat. and Applicat., vol. 1, pp. 233253, Jan. 2014.
G. Lan and R. D. C. Monteiro, Iteration-complexity of first-order augmented Lagrangian methods for convex programming. Math. Program., pp. 137, Jan. 2015, doi:10.1007/s10107015-0861-x.
S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, Distributed optimization and statistical learning via the alternating direction method of multipliers. Found. and Trends in Machine Learning, vol. 3, pp. 1122, Jan. 2011.
P. L. Combettes and J.-C. Pesquet, A proximal decomposition method for solving convex variational inverse problems. Inverse Problems, vol. 24, Nov. 2008, doi:10.1088/02665611/24/6/065014.
T. Goldstein, E. Esser, and R. Baraniuk, Adaptive primal-dual hybrid gradient methods for saddle point problems. 2013, http://arxiv.org/pdf/1305.0546.
R. Shefi and M. Teboulle, Rate of convergence analysis of decomposition methods based on the proximal method of multipliers for convex minimization. SIAM J. Optim., vol. 24, pp. 269 297, Feb. 2014.
Q. Tran-Dinh and V. Cevher, Constrained convex minimization via model-based excessive gap. In Advances Neural Inform. Process. Syst. 27 (NIPS2014), Montreal, Canada, 2014.
A. Beck and M. Teboulle, A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM J. Imaging Sci., vol. 2, pp. 183202, Mar. 2009.
Y. Nesterov, Smooth minimization of non-smooth functions. Math. Program., vol. 103, pp. 127 152, May 2005.
A. Juditsky and A. Nemirovski, Solving variational inequalities with monotone operators on domains given by Linear Minimization Oracles. Math. Program., pp. 136, Mar. 2015, doi:10.1007/s10107-015-0876-3.
Y. Yu, Fast gradient algorithms for structured sparsity. PhD dissertation, Univ. Alberta, Edmonton, Canada, 2014.
Y. Nesterov, Complexity bounds for primal-dual methods minimizing the model of objective function. CORE, Univ. Catholique Louvain, Belgium, Tech. Rep., 2015.
Y. Nesterov, Universal gradient methods for convex optimization problems. Math. Program., vol. 152, pp. 381404, Aug. 2015.
A. Nemirovskii and D. Yudin, Problem complexity and method efficiency in optimization. Hoboken, NJ: Wiley Interscience, 1983.
R. T. Rockafellar, Convex analysis (Princeton Math. Series), Princeton, NJ: Princeton Univ. Press, 1970.
D. Gross, Y.-K. Liu, S. T. Flammia, S. Becker, and J. Eisert, Quantum state tomography via compressed sensing. Phys. Rev. Lett., vol. 105, pp. Oct. 2010, doi:10.1103/PhysRevLett.105.150401.
E. Candes and B. Recht, Exact matrix completion via convex optimization. Commun. ACM, vol. 55, pp. 111119, June 2012.
M. Jaggi and M. Sulovsky, A simple algorithm for nuclear norm regularized problems. In Proc. 27th Int. Conf. Machine Learning (ICML2010), Haifa, Israel, 2010, pp. 471478.
R. M. Larsen, PROPACK - Software for large and sparse SVD calculations. Available: http: //sun.stanford.edu/rmunk/PROPACK/.  9
C. Rose. Charlie Rose: Interview with Jeffrey Skiles, February 10, 2009.
G. Gigerenzer, P. M. Todd, and the ABC Research Group. Simple heuristics that make us smart. Oxford University Press, New York, 1999.
G. Gigerenzer, R. Hertwig, and T. Pachur, editors. Heuristics: The foundations of adaptive behavior. Oxford University Press, New York, 2011.
J. Czerlinski, G. Gigerenzer, and D. G. Goldstein. How good are simple heuristics?, pages 97118. In
L. Martignon and K. B. Laskey. Bayesian benchmarks for fast and frugal heuristics., pages 169188. In
J. K. Woike L. Martignon, K. V. Katsikopoulos. Categorization with limited resources: A family of simple heuristics. Journal of Mathematical Psychology, 52(6):352361, 2008.
S. Luan, L. Schooler, and G. Gigerenzer. From perception to preference and on to inference: An approachavoidance analysis of thresholds. Psychological Review, 121(3):501, 2014.
K. V. Katsikopoulos. Psychological heuristics for making inferences: Definition, performance, and the emerging theory and practice. Decision Analysis, 8(1):1029, 2011.
H. Brighton. Robust inference with simple cognitive models. In C. Lebiere and B. Wray, editors, AAAI spring symposium: Cognitive science principles meet AI-hard problems, pages 1722. American Association for Artificial Intelligence, Menlo Park, CA, 2006.
K. V. Katsikopoulos, L. J. Schooler, and R. Hertwig. The robust beauty of ordinary information. Psychological Review, 117(4):12591266, 2010.
G. Gigerenzer and D. G Goldstein. Reasoning the fast and frugal way: Models of bounded rationality. Psychological Review, 103(4):650669, 1996.
O. Simsek. Linear decision rule as aspiration for simple decision heuristics. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 29042912. Curran Associates, Inc., Red Hook, NY, 2013.
L. Breiman, J. Friedman, C. J. Stone, and R. A. Olshen. Classification and regression trees. CRC Press, Boca Raton, FL, 1984.
T. Therneau, B. Atkinson, and B. Ripley. rpart: Recursive partitioning and regression trees, 2014. R package version 4.1-5.
M. Schmitt and L. Martignon. On the accuracy of bounded rationality: How far from optimal is fast and frugal? In Y. Weiss, B. Scholkopf, and J. C. Platt, editors, Advances in Neural Information Processing Systems 18, pages 11771184. MIT Press, Cambridge, MA, 2006.
M. Schmitt and L. Martignon. On the complexity of learning lexicographic strategies. Journal of Machine Learning Research, 7:5583, 2006.
L. Martignon and U. Hoffrage. Fast, frugal, and fit: Simple heuristics for paired comparison. Theory and Decision, 52(1):2971, 2002.
R. M. Hogarth and N. Karelaia. Ignoring information in binary choice with continuous variables: When is less more? Journal of Mathematical Psychology, 49(2):115124, 2005.
N. Chater, M. Oaksford, R. Nakisa, and M. Redington. Fast, frugal, and rational: How rational norms explain behavior. Organizational Behavior and Human Decision Processes, 90(1):6386, 2003.
H. Brighton and G. Gigerenzer. Bayesian brains and cognitive mechanisms: Harmony or dissonance? In N. Chater and M. Oaksford, editors, The probabilistic mind: Prospects for Bayesian cognitive science, pages 189208. Oxford University Press, New York, 2008.
H. Brighton and G. Gigerenzer. Are rational actor models rational outside small worlds? In S. Okasha and K. Binmore, editors, Evolution and Rationality: Decisions, Co-operation, and Strategic Behaviour, pages 84109. Cambridge University Press, Cambridge, 2012.
P. M. Todd and A. Dieckmann. Heuristics for ordering cue search in decision making. In L. K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17, pages 1393 1400. MIT Press, Cambridge, MA, 2005.
B. R. Newell. Re-visions of rationality? Trends in Cognitive Sciences, 9(1):1115, 2005.
M. R. Dougherty, A. M. Franco-Watkins, and R. Thomas. Psychological plausibility of the theory of probabilistic mental models and the fast and frugal heuristics. Psychological Review, 115(1):199213, 2008.
E. Vul, N. Goodman, T. L. Griffiths, and J. B. Tenenbaum. One and done? Optimal decisions from very few samples. Cognitive science, 38(4):599637, 2014.  9
N. Alon, N. Cesa-Bianchi, C. Gentile, and Y. Mansour. From Bandits to Experts: A Tale of Domination and Independence. In NIPS-25, pages 16101618, 2012.
N. Alon, N. Cesa-Bianchi, C. Gentile, S. Mannor, Y. Mansour, and O. Shamir. Nonstochastic multi-armed bandits with graph-structured feedback. arXiv preprint arXiv:1409.8428, 2014.
J.-Y. Audibert and S. Bubeck. Minimax policies for adversarial and stochastic bandits. In Proceedings of the 22nd Annual Conference on Learning Theory (COLT), 2009.
J.-Y. Audibert and S. Bubeck. Regret bounds and minimax policies under partial monitoring. Journal of Machine Learning Research, 11:27852836, 2010.
P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. SIAM J. Comput., 32(1):4877, 2002. ISSN 0097-5397.
P. L. Bartlett, V. Dani, T. P. Hayes, S. Kakade, A. Rakhlin, and A. Tewari. High-probability regret bounds for bandit online linear optimization. In COLT, pages 335342, 2008.
A. Beygelzimer, J. Langford, L. Li, L. Reyzin, and R. E. Schapire. Contextual bandit algorithms with supervised learning guarantees. In AISTATS 2011, pages 1926, 2011.
S. Bubeck, N. Cesa-Bianchi, and S. M. Kakade. Towards minimax policies for online linear optimization with bandit feedback. 2012.
S. Bubeck and N. Cesa-Bianchi. Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems. Now Publishers Inc, 2012.
N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, New York, NY, USA, 2006.
N. Cesa-Bianchi, P. Gaillard, G. Lugosi, and G. Stoltz. Mirror descent meets fixed share (and feels no regret). In NIPS-25, pages 989997. 2012.
D. A. Freedman. On tail probabilities for martingales. The Annals of Probability, 3:100118, 1975.
Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55:119139, 1997.
J. Hannan. Approximation to Bayes risk in repeated play. Contributions to the theory of games, 3:97139, 1957.
E. Hazan and S. Kale. Better algorithms for benign bandits. The Journal of Machine Learning Research, 12:12871311, 2011.
E. Hazan, Z. Karnin, and R. Meka. Volumetric spanners: an efficient exploration basis for learning. In COLT, pages 408422, 2014.
M. Herbster and M. Warmuth. Tracking the best expert. Machine Learning, 32:151178, 1998.
A. Kalai and S. Vempala. Efficient algorithms for online decision problems. Journal of Computer and System Sciences, 71:291307, 2005.
T. Kocak, G. Neu, M. Valko, and R. Munos. Efficient learning by implicit exploration in bandit problems with side observations. In NIPS-27, pages 613621, 2014.
N. Littlestone and M. Warmuth. The weighted majority algorithm. Information and Computation, 108: 212261, 1994.
S. Mannor and O. Shamir. From Bandits to Experts: On the Value of Side-Observations. In Neural Information Processing Systems, 2011.
H. B. McMahan and M. Streeter. Tighter bounds for multi-armed bandits with expert advice. In COLT, 2009.
G. Neu. First-order regret bounds for combinatorial semi-bandits. In COLT, pages 13601375, 2015.
A. Rakhlin and K. Sridharan. Online learning with predictable sequences. In COLT, pages 9931019, 2013.
Y. Seldin, N. Cesa-Bianchi, P. Auer, F. Laviolette, and J. Shawe-Taylor. PAC-Bayes-Bernstein inequality for martingales and its application to multiarmed bandits. In Proceedings of the Workshop on On-line Trading of Exploration and Exploitation 2, 2012.
V. Vovk. Aggregating strategies. In Proceedings of the third annual workshop on Computational learning theory (COLT), pages 371386, 1990.  9
Dimitris Achlioptas and Frank Mcsherry. Fast computation of low-rank matrix approximations. Journal of the ACM (JACM), 54(2):9, 2007.
Srinadh Bhojanapalli, Prateek Jain, and Sujay Sanghavi. Tighter low-rank approximation via sampling the leveraged element. In Proceedings of the Twenty-Sixth Annual ACMSIAM Symposium on Discrete Algorithms, pages 902920. SIAM, 2015.
Kenneth L Clarkson and David P Woodruff. Numerical linear algebra in the streaming model. In Proceedings of the forty-first annual ACM symposium on Theory of computing, pages 205214. ACM, 2009.
Kenneth L Clarkson and David P Woodruff. Low rank approximation and regression in input sparsity time. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 8190. ACM, 2013.
Mina Ghashami and Jeff M Phillips. Relative errors for deterministic low-rank matrix approximations. In SODA, pages 707717. SIAM, 2014.
Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM review, 53(2):217288, 2011.
Edo Liberty. Simple and deterministic matrix sketching. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 581 588. ACM, 2013.
Ioannis Mitliagkas, Constantine Caramanis, and Prateek Jain. Memory limited, streaming PCA. In Advances in Neural Information Processing Systems, 2013.
Joel A Tropp. Improved analysis of the subsampled randomized hadamard transform. Advances in Adaptive Data Analysis, 3(01n02):115126, 2011.
Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics, 12(4):389434, 2012.
David Woodruff. Low rank approximation lower bounds in row-update streams. In Advances in Neural Information Processing Systems, pages 17811789, 2014.  9
Pedro Domingos and Matthew Richardson. Mining the network value of customers. In KDD, 2001.
David Kempe, Jon M. Kleinberg, and Eva Tardos. Maximizing the spread of influence through a social network. In KDD, 2003.
Amit Goyal, Francesco Bonchi, and Laks VS Lakshmanan. Learning influence probabilities in social networks. In KDD, 2010.
Manuel Gomez-Rodriguez, David Balduzzi, and Bernhard Scholkopf. Uncovering the temporal dynamics of diffusion networks. In ICML, 2011.
Nan Du, Le Song, Alexander J. Smola, and Ming Yuan. Learning networks of heterogeneous influence. In NIPS, 2012.
Manuel Gomez-Rodriguez, Jure Leskovec, and Andreas Krause. Inferring networks of diffusion and influence. ACM Transactions on Knowledge Discovery from Data, 5(4):21, 2012.
Nan Du, Le Song, Manuel Gomez-Rodriguez, and Hongyuan Zha. Scalable influence estimation in continuous-time diffusion networks. In NIPS, 2013.
Abir De, Sourangshu Bhattacharya, Parantapa Bhattacharya, Niloy Ganguly, and Soumen Chakrabarti. Learning a linear influence model from transient opinion dynamics. In CIKM, 2014.
Praneeth Netrapalli and Sujay Sanghavi. Learning the graph of epidemic cascades. In SIGMETRICS, 2012.
Hadi Daneshmand, Manuel Gomez-Rodriguez, Le Song, and Bernhard Scholkopf. Estimating diffusion network structures: Recovery conditions, sample complexity & soft-thresholding algorithm. In ICML, 2014.
Jean Pouget-Abadie and Thibaut Horel. Inferring graphs from cascades: A sparse recovery framework. ICML, 2015.
Bruno D. Abrahao, Flavio Chierichetti, Robert Kleinberg, and Alessandro Panconesi. Trace complexity of network inference. In KDD, 2013.
Nan Du, Yingyu Liang, Maria-Florina Balcan, and Le Song. Influence function learning in information diffusion networks. In ICML, 2014.
Leslie G. Valiant. A theory of the learnable. Commununications of the ACM, 27(11):1134 1142, 1984.
Elchanan Mossel and Sebastien Roch. On the submodularity of influence in social networks. In STOC, 2007.
Eyal Even-Dar and Asaf Shapira. A note on maximizing the spread of influence in social networks. Information Processing Letters, 111(4):184187, 2011.
Maria-Florina Balcan and Nicholas J.A. Harvey. Learning submodular functions. In STOC, 2011.
Vitaly Feldman and Pravesh Kothari. Learning coverage functions and private release of marginals. In COLT, 2014.
Jean Honorio and Luis Ortiz. Learning the structure and parameters of large-population graphical games from behavioral data. Journal of Machine Learning Research, 16:11571210, 2015.
Martin Anthony and Peter L. Bartlett. Neural network learning: Theoretical foundations. Cambridge University Press, 1999.
Peter L. Bartlett and Wolfgang Maass. Vapnik Chervonenkis dimension of neural nets. Handbook of Brain Theory and Neural Networks, pages 11881192, 1995.
Tong Zhang. Statistical behaviour and consistency of classification methods based on convex risk minimization. Annals of Mathematical Statistics, 32:56134, 2004.  9
J. Pearl, Causality: Models, Reasoning and Inference.  Cambridge University Press, 2009.
A. Hauser and P. Buhlmann, Two optimal strategies for active learning of causal models from interventional data, International Journal of Approximate Reasoning, vol. 55, no. 4, pp. 926 939, 2014.
F. Eberhardt, C. Glymour, and R. Scheines, On the number of experiments sufficient and in the worst case necessary to identify all causal relations among n variables, in Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence (UAI), pp. 178184.
A. Hyttinen, F. Eberhardt, and P. Hoyer, Experiment selection for causal discovery, Journal of Machine Learning Research, vol. 14, pp. 30413071, 2013.
H. Hu, Z. Li, and A. Vetta, Randomized experimental design for causal graph discovery, in Proceedings of NIPS 2014, Montreal, CA, December 2014.
S. Shimizu, P. O. Hoyer, A. Hyvarinen, and A. J. Kerminen, A linear non-gaussian acyclic model for causal discovery, Journal of Machine Learning Research, vol. 7, pp. 20032030, 2006.
P. O. Hoyer, D. Janzing, J. Mooij, J. Peters, and B. Scholkopf, Nonlinear causal discovery with additive noise models, in Proceedings of NIPS 2008, 2008.
F. Eberhardt, Causation and Intervention (Ph.D. Thesis), 2007.
P. Spirtes, C. Glymour, and R. Scheines, Causation, Prediction, and Search. Book, 2001.  A Bradford
C. Meek, Strong completeness and faithfulness in bayesian networks, in Proceedings of the eleventh international conference on uncertainty in artificial intelligence, 1995.
S. A. Andersson, D. Madigan, and M. D. Perlman, A characterization of markov equivalence classes for acyclic digraphs, The Annals of Statistics, vol. 25, no. 2, pp. 505541, 1997.
T. Verma and J. Pearl, An algorithm for deciding if a set of observed independencies has a causal explanation, in Proceedings of the Eighth international conference on uncertainty in artificial intelligence, 1992.
C. Meek, Causal inference and causal explanation with background knowledge, in Proceedings of the eleventh international conference on uncertainty in artificial intelligence, 1995.
A. Hauser and P. Buhlmann, Characterization and greedy learning of interventional markov equivalence classes of directed acyclic graphs, Journal of Machine Learning Research, vol. 13, no. 1, pp. 24092464, 2012.
, Two optimal strategies for active learning of causal networks from interventional data, in Proceedings of Sixth European Workshop on Probabilistic Graphical Models, 2012.
G. Katona, On separating systems of a finite set, Journal of Combinatorial Theory, vol. 1(2), pp. 174194, 1966.
I. Wegener, On separating systems whose elements are sets of at most k elements, Discrete Mathematics, vol. 28(2), pp. 219222, 1979.
R. J. Lipton and R. E. Tarjan, A separator theorem for planar graphs, SIAM Journal on Applied Mathematics, vol. 36, no. 2, pp. 177189, 1979.  9
Alekh Agarwal, Ofer Dekel, and Lin Xiao. Optimal algorithms for online convex optimization with multi-point bandit feedback. In COLT 2010 - The 23rd Conference on Learning Theory, Haifa, Israel, June 27-29, 2010, pages 2840, 2010.
Alekh Agarwal, Dean P. Foster, Daniel Hsu, Sham M. Kakade, and Alexander Rakhlin. Stochastic convex optimization with bandit feedback. SIAM Journal on Optimization, 23(1):213240, 2013.
Alexandre Belloni, Tengyuan Liang, Hariharan Narayanan, and Alexander Rakhlin. Escaping the local minima via simulated annealing: Optimization of approximately convex functions. COLT 2015.
Sebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends in Machine Learning, 5(1):1122, 2012.
Devdatt Dubhashi, Volker Priebe, and Desh Ranjan. Negative dependence through the FKG inequality. In Research report MPI-I-96-1-020, Max-Planck Institut fur Informatik, Saarbrucken, 1996.
John C. Duchi, Michael I. Jordan, Martin J. Wainwright, and Andre Wibisono. Optimal rates for zeroorder convex optimization: The power of two function evaluations. IEEE Transactions on Information Theory, 61(5):27882806, 2015.
Uriel Feige, Vahab S. Mirrokni, and Jan Vondrak. Maximizing non-monotone submodular functions. SIAM J. Comput., 40(4):11331153, 2011.
Abraham Flaxman, Adam Tauman Kalai, and H. Brendan McMahan. Online convex optimization in the bandit setting: gradient descent without a gradient. In Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2005, Vancouver, British Columbia, Canada, January 23-25, 2005, pages 385394, 2005.
Kevin G. Jamieson, Robert D. Nowak, and Benjamin Recht. Query complexity of derivative-free optimization. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States., pages 26812689, 2012.
A.S. Nemirovsky and D.B. Yudin. Problem Complexity and Method Efficiency in Optimization. J. Wiley & Sons, New York, 1983.
Yurii Nesterov. Random gradient-free minimization of convex functions. CORE Discussion Papers 2011001, Universite catholique de Louvain, Center for Operations Research and Econometrics (CORE), 2011.
Aaditya Ramdas, Barnabas Poczos, Aarti Singh, and Larry A. Wasserman. An analysis of active learning with uniform feature noise. In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics, AISTATS 2014, Reykjavik, Iceland, April 22-25, 2014, pages 805813, 2014.
Aaditya Ramdas and Aarti Singh. Optimal rates for stochastic convex optimization under tsybakov noise condition. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pages 365373, 2013.
Ohad Shamir. On the complexity of bandit and derivative-free stochastic convex optimization. In COLT 2013 - The 26th Annual Conference on Learning Theory, June 12-14, 2013, Princeton University, NJ, USA, pages 324, 2013.
Sebastian U. Stich, Christian L. Muller, and Bernd Gartner. Optimization of convex functions with random pursuit. CoRR, abs/1111.0194, 2011.
Jan Vondrak. Symmetry and approximability of submodular maximization problems. SIAM J. Comput., 42(1):265304, 2013.  9
E. Yang, P. Ravikumar, G. I. Allen, and Z. Liu, Graphical models via generalized linear models, in NIPS, pp. 13671375, 2012.
D. I. Inouye, P. Ravikumar, and I. S. Dhillon, Admixture of Poisson MRFs: A Topic Model with Word Dependencies, in International Conference on Machine Learning (ICML), 2014.
T. Hofmann, Probabilistic latent semantic analysis, in Uncertainty in Artificial Intelligence (UAI), pp. 289296, Morgan Kaufmann Publishers Inc., 1999.
D. Blei, A. Ng, and M. Jordan, Latent dirichlet allocation, JMLR, vol. 3, pp. 9931022, 2003.
D. Blei, Probabilistic topic models, Communications of the ACM, vol. 55, pp. 7784, Nov. 2012.
E. Yang, P. Ravikumar, G. Allen, and Z. Liu., On poisson graphical models, in NIPS, pp. 17181726, 2013.
J. Chang, J. Boyd-Graber, S. Gerrish, C. Wang, and D. Blei, Reading tea leaves: How humans interpret topic models, NIPS, pp. 19, 2009.
D. Mimno, H. M. Wallach, E. Talley, M. Leenders, and A. McCallum, Optimizing semantic coherence in topic models, in EMNLP, pp. 262272, 2011.
D. Newman, Y. Noh, E. Talley, S. Karimi, and T. Baldwin, Evaluating topic models for digital libraries, in ACM/IEEE Joint Conference on Digital Libraries (JCDL), pp. 215224, 2010.
N. Aletras and R. Court, Evaluating Topic Coherence Using Distributional Semantics, in International Conference on Computational Semantics (IWCS 2013) - Long Papers, pp. 1322, 2013.
D. Mimno and D. Blei, Bayesian Checking for Topic Models, in EMNLP, pp. 227237, 2011.
R. Nallapati, A. Ahmed, W. Cohen, and E. Xing, Sparse word graphs: A scalable algorithm for capturing word correlations in topic models, ICDM, pp. 343348, 2007.
M. Steyvers and T. Griffiths, Probabilistic topic models, in Latent Semantic Analysis: A Road to Meaning, pp. 424440, 2007.
D. I. Inouye, P. K. Ravikumar, and I. S. Dhillon, Capturing Semantically Meaningful Word Dependencies with an Admixture of Poisson MRFs, in NIPS, pp. 113, 2014.
P. M. E. Altham, Two Generalizations of the Binomial Distribution, Journal of the Royal Statistical Society. Series C (Applied Statistics), vol. 27, no. 2, pp. 162167, 1978.
R. M. Neal, Annealed importance sampling, Statistics and Computing, vol. 11, no. 2, pp. 125139, 2001.  9
Rahul Agrawal, Archit Gupta, Yashoteja Prabhu, and Manik Varma. Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages. In WWW, 2013.
Dimitri P Bertsekas. Nonlinear programming. Athena scientific Belmont, 1999.
David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. JMLR, 2003.
Jianfei Chen, Jun Zhu, Zi Wang, Xun Zheng, and Bo Zhang. Scalable inference for logistic-normal topic models. In NIPS, 2013.
Yao-Nan Chen and Hsuan-Tien Lin. Feature-aware label space dimension reduction for multi-label classification. In NIPS, 2012.
Eva Gibaja and Sebastian Ventura. Multilabel learning: A review of the state of the art and ongoing research. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 2014.
Eva Gibaja and Sebastian Ventura. A tutorial on multilabel learning. ACM Comput. Surv., 2015.
Daniel Hsu, Sham Kakade, John Langford, and Tong Zhang. Multi-label prediction via compressed sensing. In NIPS, 2009.
Changwei Hu, Piyush Rai, and Lawrence Carin. Zero-truncated poisson tensor factorization for massive binary tensors. In UAI, 2015.
Ashish Kapoor, Raajay Viswanathan, and Prateek Jain. Multilabel classification using bayesian compressed sensing. In NIPS, 2012.
Nikos Karampatziakis and Paul Mineiro. Scalable multilabel prediction via randomized methods. arXiv preprint arXiv:1502.02710, 2015.
Dae I Kim and Erik B Sudderth. The doubly correlated nonparametric topic model. In NIPS, 2011.
Xiangnan Kong, Zhaoming Wu, Li-Jia Li, Ruofei Zhang, Philip S Yu, Hang Wu, and Wei Fan. Large-scale multi-label learning with incomplete label assignments. In SDM, 2014.
Xin Li, Feipeng Zhao, and Yuhong Guo. Conditional restricted boltzmann machines for multi-label learning with incomplete labels. In AISTATS, 2015.
David Mimno and Andrew McCallum. Topic models conditioned on arbitrary features with dirichletmultinomial regression. In UAI, 2008.
Paul Mineiro and Nikos Karampatziakis. Fast label embeddings for extremely large output spaces. In ICLR Workshop, 2015.
Nicholas G Polson, James G Scott, and Jesse Windle. Bayesian inference for logistic models using polya gamma latent variables. Journal of the American Statistical Association, 108(504):13391349, 2013.
Yashoteja Prabhu and Manik Varma. FastXML: a fast, accurate and stable tree-classifier for extreme multi-label learning. In KDD, 2014.
Maxim Rabinovich and David Blei. The inverse regression topic model. In ICML, 2014.
James G Scott and Liang Sun. arXiv:1306.0040, 2013.  Expectation-maximization for logistic regression.  arXiv preprint
Farbound Tai and Hsuan-Tien Lin. Multilabel classification with principal label space transformation. Neural Computation, 2012.
Michael E Tipping. Bayesian inference: An introduction to principles and practice in machine learning. In Advanced lectures on machine Learning, pages 4162. Springer, 2004.
Jason Weston, Samy Bengio, and Nicolas Usunier. WSABIE: Scaling up to large vocabulary image annotation. In IJCAI, 2011.
Yan Yan, Glenn Fung, Jennifer G Dy, and Romer Rosales. Medical coding classification by leveraging inter-code relationships. In KDD, 2010.
Hsiang-Fu Yu, Prateek Jain, Purushottam Kar, and Inderjit S Dhillon. Large-scale multi-label learning with missing labels. In ICML, 2014.
Yi Zhang and Jeff G Schneider. Multi-label output codes using canonical correlation analysis. In AISTATS, 2011.
M. Zhou, L. A. Hannah, D. Dunson, and L. Carin. Beta-negative binomial process and poisson factor analysis. In AISTATS, 2012.
Mingyuan Zhou. Infinite edge partition models for overlapping community detection and link prediction. In AISTATS, 2015.
Jun Zhu, Ni Lao, Ning Chen, and Eric P Xing. Conditional topical coding: an efficient topic model conditioned on rich features. In KDD, 2011.  9
Adith Swaminathan and Thorsten Joachims. Counterfactual risk minimization: Learning from logged bandit feedback. In ICML, 2015.
Alina Beygelzimer and John Langford. The offset tree for learning with partial labels. In KDD, pages 129138, 2009.
Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge University Press, New York, NY, USA, 2006.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, 1998.
Leon Bottou, Jonas Peters, Joaquin Q. Candela, Denis X. Charles, Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Y. Simard, and Ed Snelson. Counterfactual reasoning and learning systems: the example of computational advertising. Journal of Machine Learning Research, 14(1):32073260, 2013.
Miroslav Dudk, John Langford, and Lihong Li. Doubly robust policy evaluation and learning. In ICML, pages 10971104, 2011.
P. Rosenbaum and D. Rubin. The central role of the propensity score in observational studies for causal effects. Biometrika, 70(1):4155, 1983.
C. Cortes, Y. Mansour, and M. Mohri. Learning bounds for importance weighting. In NIPS, pages 442 450, 2010.
John Langford, Alexander Strehl, and Jennifer Wortman. Exploration scavenging. In ICML, pages 528 535, 2008.
Bianca Zadrozny, John Langford, and Naoki Abe. Cost-sensitive learning by cost-proportionate example weighting. In ICDM, pages 435, 2003.
Alexander L. Strehl, John Langford, Lihong Li, and Sham Kakade. Learning from logged implicit exploration data. In NIPS, pages 22172225, 2010.
H. F. Trotter and J. W. Tukey. Conditional monte carlo for normal samples. In Symposium on Monte Carlo Methods, pages 6479, 1956.
Andreas Maurer and Massimiliano Pontil. Empirical bernstein bounds and sample-variance penalization. In COLT, 2009.
Philip S. Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High-confidence off-policy evaluation. In AAAI, pages 30003006, 2015.
Edward L. Ionides. Truncated importance sampling. Journal of Computational and Graphical Statistics, 17(2):295311, 2008.
Lihong Li, R. Munos, and C. Szepesvari. Toward minimax off-policy value estimation. In AISTATS, 2015.
Phelim Boyle, Mark Broadie, and Paul Glasserman. Monte carlo methods for security pricing. Journal of Economic Dynamics and Control, 21(89):12671321, 1997.
John Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust region policy optimization. In ICML, pages 18891897, 2015.
Tim Hesterberg. Weighted average importance sampling and defensive mixture distributions. Technometrics, 37:185194, 1995.
V. Vapnik. Statistical Learning Theory. Wiley, Chichester, GB, 1998.
Art B. Owen. Monte Carlo theory, methods and examples. 2013.
Augustine Kong. A note on importance sampling using standardized weights. Technical Report 348, Department of Statistics, University of Chicago, 1992.
R. Rubinstein and D. Kroese. Simulation and the Monte Carlo Method. Wiley, 2 edition, 2008.
John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML, pages 282289, 2001.
Adrian S. Lewis and Michael L. Overton. Nonsmooth optimization via quasi-newton methods. Mathematical Programming, 141(1-2):135163, 2013.
Jin Yu, S. V. N. Vishwanathan, S. Gunter, and N. Schraudolph. A quasi-Newton approach to nonsmooth convex optimization problems in machine learning. JMLR, 11:11451200, 2010.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:28252830, 2011.  9
U. Apsel and R. Braman. Exploiting uniform assignments in first-order MPE. In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, pages 7483, 2012.
U. Apsel, K. Kersting, and M. Mladenov. Lifting Relational MAP-LPs Using Cluster Signatures. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, 2014.
H. Bui, T. Huynh, and S. Riedel. Automorphism groups of graphical models and lifted variational inference. In Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, 2013.
R. de Salvo Braz. Lifted First-Order Probabilistic Inference. PhD thesis, University of Illinois, UrbanaChampaign, IL, 2007.
P. Domingos and D. Lowd. Markov Logic: An Interface Layer for Artificial Intelligence. Morgan & Claypool, 2009.
V. Gogate and P. Domingos. Probabilistic Theorem Proving. In Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, pages 256265. AUAI Press, 2011.
F. Hadiji and K. Kersting. Reduce and Re-Lift: Bootstrapped Lifted Likelihood Maximization for MAP. In Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence, 2013.
Gurobi Optimization Inc. Gurobi Optimizer Reference Manual, 2014.
A. Jha, V. Gogate, A. Meliou, and D. Suciu. Lifted Inference from the Other Side: The tractable Features. In Proceedings of the 24th Annual Conference on Neural Information Processing Systems, 2010.
J. Kisynski and D. Poole. Constraint Processing in Lifted Probabilistic Inference. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pages 293302, 2009.
S. Kok, M. Sumner, M. Richardson, P. Singla, H. Poon, D. Lowd, J. Wang, and P. Domingos. The Alchemy System for Statistical Relational AI. Technical report, Department of Computer Science and Engineering, University of Washington, Seattle, WA, 2008. http://alchemy.cs.washington.edu.
R. Marinescu and R. Dechter. AND/OR Branch-and-Bound Search for Combinatorial Optimization in Graphical Models. Artificial Intelligence, 173(16-17):14571491, 2009.
H. Mittal, P. Goyal, V. Gogate, and P. Singla. New Rules for Domain Independent Lifted MAP Inference. In Advances in Neural Information Processing Systems, 2014.
M. Mladenov, A. Globerson, and K. Kersting. Efficient Lifting of MAP LP Relaxations Using k-Locality. Proceedings of the 17th International Conference on Artificial Intelligence and Statistics, 2014.
F. Niu, C. Re, A. Doan, and J. Shavlik. Tuffy: Scaling up Statistical Inference in Markov Logic Networks Using an RDBMS. Proceedings of the VLDB Endowment, 2011.
J. Noessner, M. Niepert, and H. Stuckenschmidt. RockIt: Exploiting Parallelism and Symmetry for MAP Inference in Statistical Relational Models. In Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence, 2013.
D. Poole. First-Order Probabilistic Inference. In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence, pages 985991, Acapulco, Mexico, 2003. Morgan Kaufmann.
S. Sarkhel, D. Venugopal, P. Singla, and V. Gogate. An Integer Polynomial Programming Based Framework for Lifted MAP Inference. In Advances in Neural Information Processing Systems, 2014.
S. Sarkhel, D. Venugopal, P. Singla, and V. Gogate. Lifted MAP inference for Markov Logic Networks. Proceedings of the 17th International Conference on Artificial Intelligence and Statistics, 2014.
B. Selman, H. Kautz, and B. Cohen. Local Search Strategies for Satisfiability Testing. In Cliques, Coloring, and Satisfiability: Second DIMACS Implementation Challenge. 1996.
G. Van den Broeck and A. Darwiche. On the Complexity and Approximation of Binary Evidence in Lifted Inference. In Advances in Neural Information Processing Systems, 2013.
G. Van den Broeck, N. Taghipour, W. Meert, J. Davis, and L. De Raedt. Lifted Probabilistic Inference by First-Order Knowledge Compilation. In Proceedings of the Twenty Second International Joint Conference on Artificial Intelligence, pages 21782185, 2011.
D. Venugopal and V. Gogate. Evidence-based Clustering for Scalable Inference in Markov Logic. In Machine Learning and Knowledge Discovery in Databases. 2014.  9
Emily L Denton, Soumith Chintala, Arthur Szlam, and Robert Fergus. Deep generative models using a laplacian pyramid of adversarial networks. arXiv:1506.05751
Alex Graves. Generating sequences with recurrent neural networks. arXiv:1308.0850
Karol Gregor, Ivo Danihelka, Alex Graves, and Daan Wierstra. Draw: A recurrent neural network for image generation. In International Conference on Machine Learning (ICML), 2015.
Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, and Daan Wierstra. Deep autoregressive networks. In International Conference on Machine Learning (ICML), 2014.
Diederik P Kingma, Danilo J Rezende, Shakir Mohamed, and Max Welling. Semi-supervised learning with deep generative models. In Advances in Neural Information Processing Systems (NIPS), 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference on Learning Representations (ICLR), 2014.
Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. In International Conference on Machine Learning (ICML), 2011.
Sergey Levine and Pieter Abbeel. Learning neural network policies with guided policy search under unknown dynamics. In Advances in Neural Information Processing Systems (NIPS), 2014.
Sergey Levine and Vladlen Koltun. Guided policy search. In International Conference on Machine Learning (ICML), 2013.
Sergey Levine and Vladlen Koltun. Variational policy search via trajectory optimization. In Advances in Neural Information Processing Systems (NIPS), 2013.
Sergey Levine and Vladlen Koltun. Learning complex neural network policies with trajectory optimization. In International Conference on Machine Learning (ICML), 2014.
Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. In International Conference on Machine Learning (ICML), 2014.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011.
Danilo Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning (ICML), 2014.
Danilo J Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International Conference on Machine Learning (ICML), 2015.
Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning (ICML), 2015.
Joshua Susskind, Adam Anderson, and Geoffrey E Hinton. The toronto face database. 2010.  9
Ingo Steinwart, Chlo Pasin, Robert Williamson, and Siyu Zhang. Elicitation and Identification of Properties. In Proceedings of The 27th Conference on Learning Theory, pages 482526, 2014.
A. Agarwal and S. Agrawal. On Consistent Surrogate Risk Minimization and Property Elicitation. In COLT, 2015.
Rafael Frongillo and Ian Kash. Vector-Valued Property Elicitation. In Proceedings of the 28th Conference on Learning Theory, pages 118, 2015.
L.J. Savage. Elicitation of personal probabilities and expectations. Journal of the American Statistical Association, pages 783801, 1971.
Kent Harold Osband. Providing Incentives for Better Cost Forecasting. University of California, Berkeley, 1985.
T. Gneiting and A.E. Raftery. Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Association, 102(477):359378, 2007.
T. Gneiting. Making and Evaluating Point Forecasts. Journal of the American Statistical Association, 106(494):746762, 2011.
J. Abernethy and R. Frongillo. A characterization of scoring rules for linear properties. In Proceedings of the 25th Conference on Learning Theory, pages 127, 2012.
N.S. Lambert. Elicitation and Evaluation of Statistical Forecasts. Preprint, 2011.
N.S. Lambert and Y. Shoham. Eliciting truthful answers to multiple-choice questions. In Proceedings of the 10th ACM conference on Electronic commerce, pages 109118, 2009.
Susanne Emmer, Marie Kratz, and Dirk Tasche. What is the best risk measure in practice? A comparison of standard measures. arXiv:1312.1645
, December 2013. arXiv: 1312.1645.
Fabio Bellini and Valeria Bignozzi. Elicitable risk measures. This is a preprint of an article accepted for publication in Quantitative Finance (doi 10.1080/14697688.2014. 946955), 2013.
Johanna F. Ziegel. Coherence and elicitability. Mathematical Finance, 2014. arXiv: 1303.1690.
Ruodu Wang and Johanna F. Ziegel. Elicitable distortion risk measures: A concise proof. Statistics & Probability Letters, 100:172175, May 2015.
Tobias Fissler and Johanna F. Ziegel. Higher order elicitability and Osbands principle. arXiv:1503.08123
, March 2015. arXiv: 1503.08123.
A. Banerjee, X. Guo, and H. Wang. On the optimality of conditional expectation as a Bregman predictor. IEEE Transactions on Information Theory, 51(7):26642669, July 2005.
N.S. Lambert, D.M. Pennock, and Y. Shoham. Eliciting properties of probability distributions. In Proceedings of the 9th ACM Conference on Electronic Commerce, pages 129138, 2008.
Rafael Frongillo and Ian Kash. General truthfulness characterizations via convex analysis. In Web and Internet Economics, pages 354370. Springer, 2014.
C. Heinrich. The mode functional is not elicitable. Biometrika, page ast048, 2013.
Hans Fllmer and Stefan Weber. The Axiomatic Approach to Risk Measures for Capital Determination. Annual Review of Financial Economics, 7(1), 2015.
R. Tyrrell Rockafellar and Stan Uryasev. The fundamental risk quadrangle in risk management, optimization and statistical estimation. Surveys in Operations Research and Management Science, 18(1):3353, 2013.
Tobias Fissler, Johanna F. Ziegel, and Tilmann Gneiting. Expected Shortfall is jointly elicitable with Value at Risk - Implications for backtesting. arXiv:1507.00244
, July 2015. arXiv: 1507.00244.
Whitney K. Newey and James L. Powell. Asymmetric least squares estimation and testing. Econometrica: Journal of the Econometric Society, pages 819847, 1987.
Aharon Ben-Tal and Marc Teboulle. AN OLD-NEW CONCEPT OF CONVEX RISK MEASURES: THE OPTIMIZED CERTAINTY EQUIVALENT. Mathematical Finance, 17(3):449476, 2007.
R. T. Rockafellar, J. O. Royset, and S. I. Miranda. Superquantile regression with applications to buffered reliability, uncertainty quantification, and conditional value-at-risk. European Journal of Operational Research, 234:140154, 2014.  9
R. Dechter. Reasoning with probabilistic and deterministic graphical models: Exact algorithms. Synthesis Lectures on Artificial Intelligence and Machine Learning, 2013.
R. Dechter and I. Rish. Mini-buckets: A general scheme for bounded inference. JACM, 2003.
J. Domke. Dual decomposition for marginal inference. In AAAI, 2011.
A. Doucet, S. Godsill, and C. Robert. Marginal maximum a posteriori estimation using Markov chain Monte Carlo. Statistics and Computing, 2002.
A. Globerson and T. Jaakkola. Approximate inference using conditional entropy decompositions. In AISTATS, 2007.
A. Globerson and T. Jaakkola. Fixing max-product: Convergent message passing algorithms for MAP LP-relaxations. In NIPS, 2008.
G. H. Hardy, J. E. Littlewood, and G. Polya. Inequalities. Cambridge University Press, 1952.
T. Hazan, J. Peng, and A. Shashua. Tightening fractional covering upper bounds on the partition function for high-order region graphs. In UAI, 2012.
T. Hazan and A. Shashua. Convergent message-passing algorithms for inference over general graphs with convex free energies. In UAI, 2008.
T. Hazan and A. Shashua. Norm-product belief propagation: Primal-dual message-passing for approximate inference. IEEE Transactions on Information Theory, 2010.
A. Ihler, N. Flerova, R. Dechter, and L. Otten. Join-graph based cost-shifting schemes. In UAI, 2012.
J. Jancsary and G. Matz. Convergent decomposition solvers for TRW free energies. In AISTATS, 2011.
I. Kiselev and P. Poupart. Policy optimization by marginal MAP probabilistic inference in generative models. In AAMAS, 2014.
N. Komodakis, N. Paragios, and G. Tziritas. MRF energy minimization and beyond via dual decomposition. TPAMI, 2011.
Q. Liu. Reasoning and Decisions in Probabilistic Graphical ModelsA Unified Framework. PhD thesis, University of California, Irvine, 2014.
Q. Liu and A. Ihler. Bounding the partition function using Holders inequality. In ICML, 2011.
Q. Liu and A. Ihler. Variational algorithms for marginal MAP. JMLR, 2013.
R. Marinescu, R. Dechter, and A. Ihler. AND/OR search for marginal MAP. In UAI, 2014.
D. Maua and C. de Campos. Anytime marginal maximum a posteriori inference. In ICML, 2012.
C. Meek and Y. Wexler. Approximating max-sum-product problems using multiplicative error bounds. Bayesian Statistics, 2011.
T. Meltzer, A. Globerson, and Y. Weiss. Convergent message passing algorithms: a unifying view. In UAI, 2009.
O. Meshi and A. Globerson. An alternating direction method for dual MAP LP relaxation. In ECML/PKDD, 2011.
O. Meshi, D. Sontag, T. Jaakkola, and A. Globerson. Learning efficiently with approximate inference via dual losses. In ICML, 2010.
J. Mooij. libDAI: A free and open source C++ library for discrete approximate inference in graphical models. JMLR, 2010.
J. Naradowsky, S. Riedel, and D. Smith. Improving NLP through marginalization of hidden syntactic structure. In EMNLP, 2012.
S. Nowozin and C. Lampert. Structured learning and prediction in computer vision. Foundations and Trends in Computer Graphics and Vision, 6, 2011.
J. Park and A. Darwiche. Solving MAP exactly using systematic search. In UAI, 2003.
J. Park and A. Darwiche. Complexity results and approximation strategies for MAP explanations. JAIR, 2004.
W. Ping, Q. Liu, and A. Ihler. Marginal structured SVM with hidden variables. In ICML, 2014.
N. Ruozzi and S. Tatikonda. Message-passing algorithms: Reparameterizations and splittings. IEEE Transactions on Information Theory, 2013.
D. Sontag, A. Globerson, and T. Jaakkola. Introduction to dual decomposition for inference. Optimization for Machine Learning, 2011.
D. Sontag and T. Jaakkola. Tree block coordinate descent for MAP in graphical models. AISTATS, 2009.
D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and Y. Weiss. Tightening LP relaxations for MAP using message passing. In UAI, 2008.
M. Wainwright, T. Jaakkola, and A. Willsky. A new class of upper bounds on the log partition function. IEEE Transactions on Information Theory, 2005.
Y. Weiss, C. Yanover, and T. Meltzer. MAP estimation, linear programming and belief propagation with convex free energies. In UAI, 2007.
T. Werner. A linear programming approach to max-sum problem: A review. TPAMI, 2007.
J. Yarkony, C. Fowlkes, and A. Ihler. Covering trees and lower-bounds on quadratic assignment. In CVPR, 2010.
C. Yuan and E. Hansen. Efficient computation of jointree bounds for systematic map search. IJCAI, 2009.
C. Yuan, T. Lu, and M. Druzdzel. Annealed MAP. In UAI, 2004.  9
F. Farnia, M. Razaviyayn, S. Kannan, and D. Tse. Minimum HGR correlation principle: From marginals to joint distribution. arXiv preprint arXiv:1504.06010, 2015.
E. Eban, E. Mezuman, and A. Globerson. Discrete chebyshev classifiers. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 12331241, 2014.
N. Friedman, D. Geiger, and M. Goldszmidt. Bayesian network classifiers. Machine learning, 29(2-3):131163, 1997.
G. R. G. Lanckriet andE. L. Ghaoui, C. Bhattacharyya, and M. I. Jordan. A robust minimax approach to classification. The Journal of Machine Learning Research, 3:555582, 2003.
M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for graphical models. Machine learning, 37(2):183233, 1999.
T. Roughgarden and M. Kearns. Marginals-to-models reducibility. In Advances in Neural Information Processing Systems, pages 10431051, 2013.
E. T. Jaynes. Information theory and statistical mechanics. Physical review, 106(4):620, 1957.
A. Globerson and N. Tishby. The minimum information principle for discriminative learning. In Proceedings of the 20th conference on Uncertainty in artificial intelligence, pages 193200. AUAI Press, 2004.
M. Sion. On general minimax theorems. Pacific J. Math, 8(1):171176, 1958.
J. De Loera and S. Onn. The complexity of three-way statistical tables. SIAM Journal on Computing, 33(4):819836, 2004.
D. Bertsimas and J. Sethuraman. Moment problems and semidefinite optimization. In Handbook of semidefinite programming, pages 469509. Springer, 2000.
H. O. Hirschfeld. A connection between correlation and contingency. In Mathematical Proceedings of the Cambridge Philosophical Society, volume 31, pages 520524. Cambridge Univ. Press, 1935.
H. Gebelein. Das statistische problem der korrelation als variations-und eigenwertproblem und sein zusammenhang mit der ausgleichsrechnung. ZAMM-Journal of Applied Mathematics and Mechanics/Zeitschrift fur Angewandte Mathematik und Mechanik, 21(6):364379, 1941.
A. Renyi. On measures of dependence. Acta mathematica hungarica, 10(3):441451, 1959.
V. Anantharam, A. Gohari, S. Kamath, and C. Nair. On maximal correlation, hypercontractivity, and the data processing inequality studied by Erkip and Cover. arXiv preprint arXiv:1304.6133, 2013.
A. Shapiro, D. Dentcheva, and A. Ruszczynski. Lectures on stochastic programming: modeling and theory, volume 16. SIAM, 2014.
A. Shapiro. Monte carlo sampling methods. Handbooks in operations research and management science, 10:353425, 2003.
S. M. Kakade, K. Sridharan, and A. Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In Advances in neural information processing systems, pages 793800, 2009.
H. Peng, F. Long, and C. Ding. Feature selection based on mutual information criteria of maxdependency, max-relevance, and min-redundancy. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(8):12261238, 2005.
R. Battiti. Using mutual information for selecting features in supervised neural net learning. IEEE Transactions on Neural Networks, 5(4):537550, 1994.
S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical R in learning via the alternating direction method of multipliers. Foundations and Trends Machine Learning, 3(1):1122, 2011.  9
Sanjeev Arora, Rong Ge, Sushant Sachdeva, and Grant Schoenebeck. Finding overlapping communities in social networks: toward a rigorous approach. In Proceedings of the 13th ACM Conference on Electronic Commerce, pages 3754. ACM, 2012.
Sivaraman Balakrishnan, Min Xu, Akshay Krishnamurthy, and Aarti Singh. Noise thresholds for spectral clustering. In Advances in Neural Information Processing Systems, pages 954962, 2011.
Maria-Florina Balcan, Christian Borgs, Mark Braverman, Jennifer Chayes, and Shang-Hua Teng. Finding endogenously formed communities. arxiv preprint arXiv:1201.4899v2, 2012.
Bela Bollobas. Random Graphs. Cambridge University Press, second edition, 2001.
K. Chaudhuri, F. Chung, and A. Tsiatas. Spectral clustering of graphs with general degrees in extended planted partition model. Journal of Machine Learning Research, pages 123, 2012.
Yudong Chen and Jiaming Xu. Statistical-computational tradeoffs in planted problems and submatrix localization with a growing number of clusters and submatrices. arXiv preprint arXiv:1402.1267, 2014.
Amin Coja-Oghlan and Andre Lanka. Finding planted partitions in random graphs with general degree distributions. SIAM Journal on Discrete Mathematics, 23:16821714, 2009.
M. O. Jackson. Social and Economic Networks. Princeton University Press, 2008.
Can M. Le and Roman Vershynin. Concentration and regularization of random graphs. 2015.
Brendan McKay. Asymptotics for symmetric 0-1 matrices with prescribed row sums. Ars Combinatoria, 19A:1526, 1985.
Brendan McKay and Nicholas Wormald. Uniform generation of random regular graphs of moderate degree. Journal of Algorithms, 11:5267, 1990.
Brendan McKay and Nicholas Wormald. Asymptotic enumeration by degree sequence of graphs with degrees o(n1/2 . Combinatorica, 11(4):369382, 1991.
Marina Meila and Jianbo Shi. Learning segmentation by random walks. In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems, volume 13, pages 873879, Cambridge, MA, 2001. MIT Press.
Marina Meila and Jianbo Shi. A random walks view of spectral segmentation. In T. Jaakkola and T. Richardson, editors, Artificial Intelligence and Statistics AISTATS, 2001.
M.E.J. Newman and Travis Martin. Equitable random graphs. 2014.
Andrew Y Ng, Michael I Jordan, Yair Weiss, et al. On spectral clustering: Analysis and an algorithm. Advances in neural information processing systems, 2:849856, 2002.
J.R. Norris. Markov Chains. Cambridge University Press, 1997.
Tai Qin and Karl Rohe. Regularized spectral clustering under the degree-corrected stochastic blockmodel. In Advances in Neural Information Processing Systems, pages 31203128, 2013.
Karl Rohe, Sourav Chatterjee, Bin Yu, et al. Spectral clustering and the high-dimensional stochastic blockmodel. The Annals of Statistics, 39(4):18781915, 2011.
Gilbert W Stewart, Ji-guang Sun, and Harcourt Brace Jovanovich. Matrix perturbation theory, volume 175. Academic press New York, 1990.
Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395 416, 2007.  9
Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, 2013.
Sepp Hochreiter and Jrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735 1780, 1997.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network for modelling sentences. ACL, 2014.
Yoon Kim. Convolutional neural networks for sentence classification. EMNLP, 2014.
Kyunghyun Cho, Bart van Merrinboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches. SSST-8, 2014.
Han Zhao, Zhengdong Lu, and Pascal Poupart. Self-adaptive hierarchical sentence model. IJCAI, 2015.
Quoc V Le and Tomas Mikolov. Distributed representations of sentences and documents. ICML, 2014.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. ICLR, 2013.
Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In ICCV, 2015.  8
Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models. In EMNLP, pages 1700 1709, 2013.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. EMNLP, 2014.
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In NIPS, 2014.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. ICLR, 2015.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. NIPS Deep Learning Workshop, 2014.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever. Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168, 2013.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. ICLR, 2014.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015.
Alice Lai and Julia Hockenmaier. Illinois-lh: A denotational and distributional approach to semantics. SemEval 2014, 2014.
Sergio Jimenez, George Duenas, Julia Baquero, Alexander Gelbukh, Av Juan Dios Btiz, and Av Mendizbal. Unal-nlp: Combining soft cardinality features for semantic textual similarity, relatedness and entailment. SemEval 2014, 2014.
Johannes Bjerva, Johan Bos, Rob van der Goot, and Malvina Nissim. The meaning factory: Formal semantics for recognizing textual entailment and determining semantic similarity. SemEval 2014, page 642, 2014.
Jiang Zhao, Tian Tian Zhu, and Man Lan. Ecnu: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment. SemEval 2014, 2014.
Kai Sheng Tai, Richard Socher, and Christopher D Manning. Improved semantic representations from tree-structured long short-term memory networks. ACL, 2015.
Richard Socher, Andrej Karpathy, Quoc V Le, Christopher D Manning, and Andrew Y Ng. Grounded compositional semantics for finding and describing images with sentences. TACL, 2014.
Richard Socher, Eric H Huang, Jeffrey Pennin, Christopher D Manning, and Andrew Y Ng. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In NIPS, 2011.
Andrew Finch, Young-Sook Hwang, and Eiichiro Sumita. Using machine translation evaluation techniques to determine sentence-level semantic equivalence. In IWP, 2005.
Dipanjan Das and Noah A Smith. Paraphrase identification as probabilistic quasi-synchronous recognition. In ACL, 2009.
Stephen Wan, Mark Dras, Robert Dale, and Ccile Paris. Using dependency-based features to take the "para-farce" out of paraphrase. In Proceedings of the Australasian Language Technology Workshop, 2006.
Nitin Madnani, Joel Tetreault, and Martin Chodorow. Re-examining machine translation metrics for paraphrase identification. In NAACL, 2012.
Yangfeng Ji and Jacob Eisenstein. Discriminative improvements to distributional sentence similarity. In EMNLP, pages 891896, 2013.
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto Zamparelli. Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. SemEval-2014, 2014.
Bill Dolan, Chris Quirk, and Chris Brockett. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of the 20th international conference on Computational Linguistics, 2004.
A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In CVPR, 2015.
Benjamin Klein, Guy Lev, Gil Sadeh, and Lior Wolf. Associating neural word embeddings with deep image representations using fisher vectors. In CVPR, 2015.
Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, and Alan Yuille. Deep captioning with multimodal recurrent neural networks (m-rnn). ICLR, 2015.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollr, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pages 740755. 2014.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. ICLR, 2015.
Sida Wang and Christopher D Manning. Baselines and bigrams: Simple, good sentiment and topic classification. In ACL, 2012.
Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. JMLR, 2008.  9
A. Moneta, N. Chla, D. Entner, and P. Hoyer. Causal search in structural vector autoregressive models. In Journal of Machine Learning Research: Workshop and Conference Proceedings, Causality in Time Series (Proc. NIPS2009 Mini-Symposium on Causality in Time Series), volume 12, pages 95114, 2011.
C.W.J. Granger. Investigating causal relations by econometric models and cross-spectral methods. Econometrica: Journal of the Econometric Society, pages 424438, 1969.
B. Thiesson, D. Chickering, D. Heckerman, and C. Meek. Arma time-series modeling with graphical models. In Proceedings of the Twentieth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-04), pages 552560, Arlington, Virginia, 2004. AUAI Press.
Mark Voortman, Denver Dash, and Marek Druzdzel. Learning why things change: The difference-based causality learner. In Proceedings of the Twenty-Sixth Annual Conference on Uncertainty in Artificial Intelligence (UAI), pages 641650, Corvallis, Oregon, 2010. AUAI Press.
Nir Friedman, Kevin Murphy, and Stuart Russell. Learning the structure of dynamic probabilistic networks. In 15th Annual Conference on Uncertainty in Artificial Intelligence, pages 139147, San Francisco, 1999. Morgan Kaufmann.
Sergey Plis, David Danks, and Jianyu Yang. Mesochronal structure learning. In Proceedings of the Thirty-First Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-15), Corvallis, Oregon, 2015. AUAI Press.
Mingming Gong, Kun Zhang, Bernhard Schoelkopf, Dacheng Tao, and Philipp Geiger. Discovering temporal causal relations from subsampled data. In Proc. ICML, pages 18981906, 2015.
T. Richardson and P. Spirtes. Ancestral graph markov models. The Annals of Statistics, 30(4):9621030, 2002.
David Danks and Sergey Plis. Learning causal structure from undersampled time series. In JMLR: Workshop and Conference Proceedings, volume 1, pages 110, 2013.
Donald B Johnson. Finding all the elementary circuits of a directed graph. SIAM Journal on Computing, 4(1):7784, 1975.
Helmut Lutkepohl. New introduction to multiple time series analysis. Springer Science & Business Media, 2007.
K. Murphy. Dynamic Bayesian Networks: Representation, Inference and Learning. PhD thesis, UC Berkeley, 2002.
Clark Glymour, Peter Spirtes, and Richard Scheines. Causal inference. In Erkenntnis Orientated: A Centennial Volume for Rudolf Carnap and Hans Reichenbach, pages 151189. Springer, 1991.
David Maxwell Chickering. Optimal structure identification with greedy search. The Journal of Machine Learning Research, 3:507554, 2003.
Anil K Seth, Paul Chorley, and Lionel C Barnett. Granger causality analysis of fmri bold signals is invariant to hemodynamic convolution but not downsampling. Neuroimage, 65:540555, 2013.  9
Martial Agueh and Guillaume Carlier. Barycenters in the Wasserstein space. SIAM Journal on Mathematical
Analysis, 43(2):904924, 2011.
Luigi Ambrosio, Nicola Gigli, and Giuseppe Savare. Gradient flows: in metric spaces and in the space of
probability measures. Springer, 2006.
Jean-David Benamou, Guillaume Carlier, Marco Cuturi, Luca Nenna, and Gabriel Peyre. Iterative Bregman
projections for regularized transportation problems. SIAM Journal on Scientific Computing, 37(2):A1111
A1138, 2015.
Jeremie Bigot, Raul Gouet, Thierry Klein, and Alfredo Lopez. Geodesic PCA in the Wasserstein space by
convex PCA. Annales de lInstitut Henri Poincare B: Probability and Statistics, 2015.
Emmanuel Boissard, Thibaut Le Gouic, Jean-Michel Loubes, et al. Distributions template estimate with
Wasserstein metrics. Bernoulli, 21(2):740759, 2015.
Nicolas Bonneel, Julien Rabin, Gabriel Peyre, and Hanspeter Pfister. Sliced and radon Wasserstein barycenters
of measures. Journal of Mathematical Imaging and Vision, 51(1):2245, 2015.
Guillaume Carlier, Adam Oberman, and Edouard Oudet. Numerical methods for matching for teams and
Wasserstein barycenters. ESAIM: Mathematical Modelling and Numerical Analysis, 2015. to appear.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural
Information Processing Systems, pages 22922300, 2013.
Marco Cuturi and Arnaud Doucet. Fast computation of Wasserstein barycenters. In Proceedings of the 31st
International Conference on Machine Learning (ICML-14), pages 685693, 2014.
P. Thomas Fletcher, Conglin Lu, Stephen M. Pizer, and Sarang Joshi. Principal geodesic analysis for the study
of nonlinear statistics of shape. Medical Imaging, IEEE Transactions on, 23(8):9951005, 2004.
Maurice Frechet. Les elements aleatoires de nature quelconque dans un espace distancie. In Annales de linstitut
Henri Poincare, volume 10, pages 215310. Presses universitaires de France, 1948.
Alexandre Gramfort, Gabriel Peyre, and Marco Cuturi. Fast optimal transport averaging of neuroimaging data.
In Information Processing in Medical Imaging (IPMI). Springer, 2015.
Trevor Hastie and Werner Stuetzle. Principal curves. Journal of the American Statistical Association, 84(406):
502516, 1989.
David R Hunter and Kenneth Lange. Quantile regression via an MM algorithm. Journal of Computational and
Graphical Statistics, 9(1):6077, 2000.
Robert J McCann. A convexity principle for interacting gases. Advances in mathematics, 128(1):153179,
Francois Pitie, Anil C Kokaram, and Rozenn Dahyot. Automated colour grading using colour distribution
transfer. Computer Vision and Image Understanding, 107(1):123137, 2007.
Sebastian Reich. A nonparametric ensemble transform method for bayesian inference. SIAM Journal on
Scientific Computing, 35(4):A2013A2024, 2013.
Bernhard Scholkopf, Alexander Smola, and Klaus-Robert Muller. Kernel principal component analysis. In
Artificial Neural Networks, ICANN97, pages 583588. Springer, 1997.
Justin Solomon, Fernando de Goes, Gabriel Peyre, Marco Cuturi, Adrian Butscher, Andy Nguyen, Tao Du,
and Leonidas Guibas. Convolutional Wasserstein distances: Efficient optimal transportation on geometric
domains. ACM Transactions on Graphics (Proc. SIGGRAPH 2015), 34(4), 2015.
Sanvesh Srivastava, Volkan Cevher, Quoc Tran-Dinh, and David B Dunson. Wasp: Scalable bayes via barycenters of subset posteriors. In Proceedings of the Eighteenth International Conference on Artificial Intelligence
and Statistics, pages 912920, 2015.
Jakob J Verbeek, Nikos Vlassis, and B Krose. A k-segments algorithm for finding principal curves. Pattern
Recognition Letters, 23(8):10091017, 2002.
Cedric Villani. Optimal transport: old and new, volume 338. Springer, 2008.
Wei Wang, Dejan Slepcev, Saurav Basu, John A Ozolek, and Gustavo K Rohde. A linear optimal transportation
framework for quantifying and visualizing variations in sets of images. International journal of computer
vision, 101(2):254269, 2013.
Michael Westdickenberg. Projections onto the cone of optimal transport maps and compressible fluid flows.
Journal of Hyperbolic Differential Equations, 7(04):605649, 2010.
Weiwei Cheng, Eyke Hullermeier, and Krzysztof J Dembczynski. Bayes optimal multilabel classification via probabilistic classifier chains. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 279286, 2010.
Krzysztof Dembczynski, Wojciech Kotlowski, and Eyke Hullermeier. Consistent multilabel ranking through univariate losses. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 13191326, 2012.
Krzysztof Dembczynski, Willem Waegeman, Weiwei Cheng, and Eyke Hullermeier. On label dependence and loss minimization in multi-label classification. Machine Learning, 88(1-2): 545, 2012.
Krzysztof Dembczynski, Arkadiusz Jachnik, Wojciech Kotlowski, Willem Waegeman, and Eyke Hullermeier. Optimizing the F-measure in multi-label classification: Plug-in rule approach versus structured loss minimization. In Proceedings of the 30th International Conference on Machine Learning, pages 11301138, 2013.
Krzysztof J Dembczynski, Willem Waegeman, Weiwei Cheng, and Eyke Hullermeier. An exact algorithm for F-measure maximization. In Advances in Neural Information Processing Systems, pages 14041412, 2011.
Luc Devroye. A probabilistic theory of pattern recognition, volume 31. Springer, 1996.
Wei Gao and Zhi-Hua Zhou. On the consistency of multi-label learning. Artificial Intelligence, 199:2244, 2013.
Ashish Kapoor, Raajay Viswanathan, and Prateek Jain. Multilabel classification using bayesian compressed sensing. In Advances in Neural Information Processing Systems, pages 2645 2653, 2012.
Oluwasanmi O Koyejo, Nagarajan Natarajan, Pradeep K Ravikumar, and Inderjit S Dhillon. Consistent binary classification with generalized performance metrics. In Advances in Neural Information Processing Systems, pages 27442752, 2014.
Harikrishna Narasimhan, Rohit Vaish, and Shivani Agarwal. On the statistical consistency of plug-in classifiers for non-decomposable performance measures. In Advances in Neural Information Processing Systems, pages 14931501, 2014.
Harikrishna Narasimhan, Harish Ramaswamy, Aadirupa Saha, and Shivani Agarwal. Consistent multiclass algorithms for complex performance measures. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 23982407, 2015.
James Petterson and Tiberio S Caetano. Submodular multi-label learning. In Advances in Neural Information Processing Systems, pages 15121520, 2011.
Jesse Read, Bernhard Pfahringer, Geoff Holmes, and Eibe Frank. Classifier chains for multilabel classification. Machine learning, 85(3):333359, 2011.
Mark D Reid and Robert C Williamson. Composite binary losses. The Journal of Machine Learning Research, 9999:23872422, 2010.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vlahavas. Mining multi-label data. In Data mining and knowledge discovery handbook, pages 667685. Springer, 2010.
Willem Waegeman, Krzysztof Dembczynski, Arkadiusz Jachnik, Weiwei Cheng, and Eyke Hullermeier. On the bayes-optimality of f-measure maximizers. Journal of Machine Learning Research, 15:33333388, 2014.
Nan Ye, Kian Ming A Chai, Wee Sun Lee, and Hai Leong Chieu. Optimizing F-measures: a tale of two approaches. In Proceedings of the International Conference on Machine Learning, 2012.
Hsiang-Fu Yu, Prateek Jain, Purushottam Kar, and Inderjit Dhillon. Large-scale multi-label learning with missing labels. In Proceedings of the 31st International Conference on Machine Learning, pages 593601, 2014.  9
G. Wang and S. Shan. Review of Metamodeling Techniques in Support of Engineering Design Optimization. Journal of Mechanical Design, 129(4):370380, 2007.
W. Ziemba & R. Vickson. Stochastic Optimization Models in Finance. World Scientific Singapore, 2006.  8
J. Snoek, H. Larochelle, and R. P. Adams. Practical Bayesian Optimization of Machine Learning Algorithms. NIPS, 2012.
J. Mockus. Bayesian Approach to Global Optimization: Theory and Applications. Kluwer, 1989.
D. Lizotte, T. Wang, M. Bowling, and D. Schuurmans. Automatic Gait Optimization with Gaussian Process Regression. IJCAI, pages 944949, 2007.
D. M. Negoescu, P. I. Frazier, and W. B. Powell. The Knowledge-Gradient Algorithm for Sequencing Experiments in Drug Discovery. INFORMS Journal on Computing, 23(3):346363, 2011.
Carl Rasmussen and Chris Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.
A. Shah, A. G. Wilson, and Z. Ghahramani. Student-t Processes as Alternatives to Gaussian Processes. AISTATS, 2014.
J. Snoek, O. Rippel, K. Swersky, R. Kiros, N. Satish, N. Sundaram, M. Patwary, Mr Prabat, and R. P. Adams. Scalable Bayesian Optimization Using Deep Neural Networks. ICML, 2015.
E. Brochu, M. Cora, and N. de Freitas. A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Applications to Active User Modeling and Hierarchical Reinforcement Learning. Technical Report TR-2009-23, University of British Columbia, 2009.
G. Gutin, A. Yeo, and A. Zverovich. Traveling salesman should not be greedy:domination analysis of greedy-type heuristics for the TSP. Discrete Applied Mathematics, 117:8186, 2002.
P. Hennig and C. J. Schuler. Entropy Search for Information-Efficient Global Optimization. JMLR, 2012.
J. M. Hernandez-Lobato, M. W. Hoffman, and Z. Ghahramani. Predictive Entropy Search for Efficient Global Optimization of Black-box Functions. NIPS, 2014.
D. Ginsbourger, J. Janusevskis, and R. Le Riche. Dealing with Asynchronicity in Parallel Gaussian Process Based Optimization. 2011.
J. Azimi, A. Fern, and X. Z. Fern. Batch Bayesian Optimization via Simulation Matching. NIPS, 2010.
N. Srinivas, A. Krause, S. Kakade, and M. Seeger. Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design. ICML, 2010.
T. Desautels, A. Krause, and J. Burdick. Parallelizing Exploration-Exploitation Tradeoffs with Gaussian Process Bandit Optimization. ICML, 2012.
E. Contal, D. Buffoni, D. Robicquet, and N. Vayatis. Parallel Gaussian Process Optimization with Upper Confidence Bound and Pure Exploration. In Machine Learning and Knowledge Discovery in Databases, pages 225240. Springer Berlin Heidelberg, 2013.
D. J. MacKay. Information-Based Objective Functions for Active Data Selection. Neural Computation, 4(4):590604, 1992.
N. Houlsby, J. M. Hernandez-Lobato, F. Huszar, and Z. Ghahramani. Collaborative Gaussian Processes for Preference Learning. NIPS, 2012.
T. P. Minka. A Family of Algorithms for Approximate Bayesian Inference. PhD thesis, Masachusetts Institute of Technology, 2001.
S. Bochner. Lectures on Fourier Integrals. Princeton University Press, 1959.
A. Rahimi and B. Recht. Random Features for Large-Scale Kernel Machines. NIPS, 2007.
R. M. Neal. Bayesian Learning for Neural Networks. PhD thesis, University of Toronto, 1995.
M. Seeger. Expectation Propagation for Exponential Families. Technical Report, U.C. Berkeley, 2008.
J. P. Cunningham, P. Hennig, and S. Lacoste-Julien. Gaussian Probabilities and Expectation Propagation. arXiv, 2013. http://arxiv.org/abs/1111.6832.
I. Ahmad and P. E. Lin. A Nonparametric Estimation of the Entropy for Absolutely Continuous Distributions. IEEE Trans. on Information Theory, 22(3):372375, 1976.
D. Lizotte. Practical Bayesian Optimization. PhD thesis, University of Alberta, 2008.
B. S. Anderson, A. W. Moore, and D. Cohn. A Nonparametric Approach to Noisy and Costly Optimization. ICML, 2000.
J. Shekel. Test Functions for Multimodal Search Techniques. Information Science and Systems, 1971.
K. Bache and M. Lichman. UCI Machine Learning Repository, 2013.
E. H. Burrows, W. K. Wong, X. Fern, F.W.R. Chaplen, and R.L. Ely. Optimization of ph and nitrogen for enhanced hydrogen production by synechocystis sp. pcc 6803 via statistical and machine learning methods. Biotechnology Progress, 25(4):10091017, 2009.
J. E. Hasbun. In Classical Mechanics with MATLAB Applications. Jones & Bartlett Learning, 2008.
E. Westervelt and J. Grizzle. Feedback Control of Dynamic Bipedal Robot Locomotion. Control and Automation Series. CRC PressINC, 2007.  9
Audibert JY, Bubeck S (2009) Minimax policies for adversarial and stochastic bandits. In: Annual Conference on Learning Theory
Auer P, Ortner R (2010) Ucb revisited: Improved regret bounds for the stochastic multi-armed bandit problem. Periodica Mathematica Hungarica 61:5565
Auer P, Cesa-Bianchi N, Fischer P (2002) Finite-time analysis of the multi- armed bandit problem. Machine Learning Journal 47(23):235256
Auer P, Cesa-Bianchi N, Freund Y, Schapire RE (2002) The nonstochastic multiarmed bandit problem. SIAM Journal on Computing 32(1):4877
Bernstein S (1927) Sur lextension du theoreme limite du calcul des probabilites aux sommes de quantites dependantes. Mathematische Annalen 97(1):159
Bubeck S, Cesa-Bianchi N (2012) Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems, Foundation and Trends in Machine Learning, vol 5. NOW
Hoeffding W (1948) A Class of Statistics with Asymptotically Normal Distribution. Annals of Mathematical Statistics 19(3):293325
Hoeffding W (1963) Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association 58(301):1330, DOI 10.2307/2282952, URL http://dx.doi.org/10. 2307/2282952
Karandikar RL, Vidyasagar M (2002) Rates of uniform convergence of empirical means with mixing processes. Statistics & probability letters 58(3):297307
Kontorovich L, Ramanan K (2008) Concentration inequalities for dependent random variables via the martingale method. The Annals of Probability 36(6):21262158
Kulkarni S, Lozano A, Schapire RE (2005) Convergence and consistency of regularized boosting algorithms with stationary -mixing observations. In: Advances in neural information processing systems, pp 819826
Lai TL, Robbins H (1985) Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics 6:422
McDonald D, Shalizi C, Schervish M (2011) Estimating beta-mixing coefficients. arXiv preprint arXiv:11030941
Mohri M, Rostamizadeh A (2009) Rademacher complexity bounds for non-i.i.d. processes. In: Koller D, Schuurmans D, Bengio Y, Bottou L (eds) Advances in Neural Information Processing Systems 21, pp 10971104
Mohri M, Rostamizadeh A (2010) Stability bounds for stationary -mixing and -mixing processes. Journal of Machine Learning Research 11:789814
Ortner R, Ryabko D, Auer P, Munos R (2012) Regret bounds for restless markov bandits. In: Proceeding of the Int. Conf. Algorithmic Learning Theory, pp 214228
Pandey S, Chakrabarti D, Agarwal D (2007) Multi-armed bandit problems with dependent arms. In: Proceedings of the 24th international conference on Machine learning, ACM, pp 721728
Ralaivola L, Szafranski M, Stempfel G (2010) Chromatic pac-bayes bounds for non-iid data: Applications to ranking and stationary -mixing processes. The Journal of Machine Learning Research 11:19271956
Seldin Y, Slivkins A (2014) One practical algorithm for both stochastic and adversarial bandits. In: Proceedings of the 31st International Conference on Machine Learning (ICML-14), pp 12871295
Steinwart I, Christmann A (2009) Fast learning from non-iid observations. In: Advances in Neural Information Processing Systems, pp 17681776
Steinwart I, Hush D, Scovel C (2009) Learning from dependent observations. Journal of Multivariate Analysis 100(1):175194
Tekin C, Liu M (2012) Online learning of rested and restless bandits. IEEE Transactions on Information Theory 58(8):55885611, URL http://dblp.uni-trier.de/db/journals/tit/ tit58.html#TekinL12
Yu B (1994) Rates of convergence for empirical processes of stationary mixing sequences. Annals of Probability 22(1):94116  9
Abraham, A., Pedregosa, F., Eickenberg, M., Gervais, P., Mueller, A., Kossaifi, J., Gramfort, A., Thirion, B., Varoquaux, G.: Machine learning for neuroimaging with scikit-learn. Front Neuroinform 8, 14 (2014)
Amunts, K., Lepage, C., Borgeat, L., Mohlberg, H., Dickscheid, T., Rousseau, M.E., Bludau, S., Bazin, P.L., Lewis, L.B., Oros-Peusquens, A.M., et al.: Bigbrain: an ultrahigh-resolution 3d human brain model. Science 340(6139), 14721475 (2013)
Baldi, P., Hornik, K.: Neural networks and principal component analysis: Learning from examples without local minima. Neural networks 2(1), 5358 (1989)
Barch, D.M., Burgess, G.C., Harms, M.P., Petersen, S.E., Schlaggar, B.L., Corbetta, M., Glasser, M.F., Curtiss, S., Dixit, S., Feldt, C.: Function in the human connectome: task-fmri and individual differences in behavior. Neuroimage 80, 169189 (2013)  8
Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I., Bergeron, A., Bouchard, N., WardeFarley, D., Bengio, Y.: Theano: new features and speed improvements. arXiv preprint arXiv:1211.5590 (2012)
Beckmann, C.F., DeLuca, M., Devlin, J.T., Smith, S.M.: Investigations into resting-state connectivity using independent component analysis. Philos Trans R Soc Lond B Biol Sci 360(1457), 100113 (2005)
Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-Farley, D., Bengio, Y.: Theano: a cpu and gpu math expression compiler. Proceedings of the Python for scientific computing conference (SciPy) 4, 3 (2010)
Biswal, B.B., Mennes, M., Zuo, X.N., Gohel, S., Kelly, C., et al.: Toward discovery science of human brain function. Proc Natl Acad Sci U S A 107(10), 47349 (2010)
Cole, M.W., Bassettf, D.S., Power, J.D., Braver, T.S., Petersen, S.E.: Intrinsic and task-evoked network architectures of the human brain. Neuron 83c, 238251 (2014)
Fox, D.F., Raichle, M.E.: Spontaneous fluctuations in brain activity observed with functional magnetic resonance imaging. Nat Rev Neurosci 8, 700711 (2007)
Frackowiak, R., Markram, H.: The future of human cerebral cartography: a novel approach. Philosophical Transactions of the Royal Society of London B: Biological Sciences 370(1668), 20140171 (2015)
Friston, K.J., Buechel, C., Fink, G.R., Morris, J., Rolls, E., Dolan, R.J.: Psychophysiological and modulatory interactions in neuroimaging. Neuroimage 6(3), 21829 (1997)
Friston, K.J., Holmes, A.P., Worsley, K.J., Poline, J.P., Frith, C.D., Frackowiak, R.S.: Statistical parametric maps in functional imaging: a general linear approach. Hum Brain Mapp 2(4), 189210 (1994)
Gorgolewski, K., Burns, C.D., Madison, C., Clark, D., Halchenko, Y.O., Waskom, M.L., Ghosh, S.S.: Nipype: a flexible, lightweight and extensible neuroimaging data processing framework in python. Front Neuroinform 5, 13 (2011)
Hertz, J., Krogh, A., Palmer, R.G.: Introduction to the theory of neural computation, vol. 1. Basic Books (1991)
Hinton, G.E., Salakhutdinov, R.R.: Reducing the dimensionality of data with neural networks. Science 313(5786), 504507 (2006)
Hipp, J.F., Siegel, M.: Bold fmri correlation reflects frequency-specific neuronal correlation. Curr Biol (2015)
Le, Q.V., Karpenko, A., Ngiam, J., Ng, A.: Ica with reconstruction cost for efficient overcomplete feature learning pp. 10171025 (2011)
Need, A.C., Goldstein, D.B.: Whole genome association studies in complex diseases: where do we stand? Dialogues in clinical neuroscience 12(1), 37 (2010)
Olshausen, B., et al.: Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature 381(6583), 607609 (1996)
Pinel, P., Thirion, B., Meriaux, S., Jobert, A., Serres, J., Le Bihan, D., Poline, J.B., Dehaene, S.: Fast reproducible identification and large-scale databasing of individual functional cognitive networks. BMC Neurosci 8, 91 (2007)
Poldrack, R.A., Gorgolewski, K.J.: Making big data open: data sharing in neuroimaging. Nature Neuroscience 17(11), 15101517 (2014)
Poldrack, R.A., Halchenko, Y.O., Hanson, S.J.: Decoding the large-scale structure of brain function by classifying mental states across individuals. Psychol Sci 20(11), 136472 (2009)
Schwartz, Y., Thirion, B., Varoquaux, G.: Mapping cognitive ontologies to and from the brain. Advances in Neural Information Processing Systems (2013)
Smith, S.M., Beckmann, C.F., Andersson, J., Auerbach, E.J., Bijsterbosch, J., Douaud, G., Duff, E., Feinberg, D.A., Griffanti, L., Harms, M.P., et al.: Resting-state fmri in the human connectome project. NeuroImage 80, 144168 (2013)
Smith, S.M., Fox, P.T., Miller, K.L., Glahn, D.C., Fox, P.M., Mackay, C.E., Filippini, N., Watkins, K.E., Toro, R., Laird, A.R., Beckmann, C.F.: Correspondence of the brains functional architecture during activation and rest. Proc Natl Acad Sci U S A 106(31), 130405 (2009)
Tieleman, T., Hinton, G.: Lecture 6.5rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning (2012)
Varoquaux, G., Gramfort, A., Pedregosa, F., Michel, V., Thirion, B.: Multi-subject dictionary learning to segment an atlas of brain spontaneous activity. Information Processing in Medical Imaging pp. 562573 (2011)  9
Neil D Lawrence. Gaussian process latent variable models for visualisation of high dimensional data. Advances in Neural Information Processing Systems (NIPS), 2004.
Volker Tresp. A Bayesian committee machine. Neural Computation, 12(11):27192741, 2000.
Carl Rasmussen and Chris Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.
Michalis K Titsias and Neil D Lawrence. Bayesian Gaussian process latent variable model. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2010.
Andreas C. Damianou, Michalis K. Titsias, and Neil D. Lawrence. Variational Inference for Latent Variables and Uncertain Inputs in Gaussian Processes. Journal of Machine Learning Research (JMLR), 2015.
Joaquin Quinonero-Candela and Carl Edward Rasmussen. A unifying view of sparse approximate Gaussian process regression. Journal of Machine Learning Research (JMLR), 6:19391959, 2005.
Neil D Lawrence. Learning for larger datasets with the Gaussian process latent variable model. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2007.
Michalis K Titsias. Variational learning of inducing variables in sparse Gaussian processes. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2009.
Duy Nguyen-Tuong, Matthias Seeger, and Jan Peters. Model learning with local Gaussian process regression. Advanced Robotics, 23(15):20152034, 2009.
Chiwoo Park, Jianhua Z Huang, and Yu Ding. Domain decomposition approach for fast Gaussian process regression of large spatial data sets. Journal of Machine Learning Research (JMLR), 12:16971728, 2011.
Krzysztof Chalupka, Christopher KI Williams, and Iain Murray. A framework for evaluating approximation methods for Gaussian process regression. Journal of Machine Learning Research (JMLR), 14:333350, 2013.
Marc Peter Deisenroth and Jun Wei Ng. Distributed Gaussian Processes. In International Conference on Machine Learning (ICML), 2015.
Carl Edward Rasmussen and Zoubin Ghahramani. Infinite mixtures of Gaussian process experts. Advances in Neural Information Processing Systems (NIPS), pages 881888, 2002.
Trung Nguyen and Edwin Bonilla. Fast allocation of Gaussian process experts. In International Conference on Machine Learning (ICML), pages 145153, 2014.
Edward Snelson and Zoubin Ghahramani. Local and global sparse Gaussian process approximations. In Artificial Intelligence and Statistics (AISTATS), 2007.
Jarno Vanhatalo and Aki Vehtari. Modelling local and global phenomena with sparse Gaussian processes. In Uncertainty in Artificial Intelligence (UAI), 2008.
Guoqiang Zhong, Wu-Jun Li, Dit-Yan Yeung, Xinwen Hou, and Cheng-Lin Liu. Gaussian process latent random field. In AAAI Conference on Artificial Intelligence, 2010.
Daphne Koller and Nir Friedman. Probabilistic graphical models: Principles and techniques. MIT Press, 2009.
Jonathan S Yedidia, William T Freeman, and Yair Weiss. Bethe free energy, Kikuchi approximations, and belief propagation algorithms. Advances in Neural Information Processing Systems (NIPS), 13, 2001.
Kevin P Murphy, Yair Weiss, and Michael I Jordan. Loopy belief propagation for approximate inference: An empirical study. In Uncertainty in Artificial Intelligence (UAI), pages 467475, 1999.
Julian Besag. Statistical analysis of non-lattice data. The Statistician, pages 179195, 1975.
Brian Ferris, Dieter Fox, and Neil D Lawrence. WiFi-SLAM using Gaussian process latent variable models. In International Joint Conference on Artificial Intelligence (IJCAI), pages 24802485, 2007.
The GPy authors. GPy: A Gaussian process framework in Python. SheffieldML/GPy, 20122015.  http://github.com/
James Hensman, Nicolo Fusi, and Neil D Lawrence. Gaussian processes for big data. In Uncertainty in Artificial Intelligence (UAI), page 282, 2013.
Yarin Gal, Mark van der Wilk, and Carl Rasmussen. Distributed variational inference in sparse Gaussian process regression and latent variable models. In Advances in Neural Information Processing Systems (NIPS), 2014.
International Seismological Centre. On-line Bulletin. Int. Seis. Cent., Thatcham, United Kingdom, 2015. http://www.isc.ac.uk.
James McNames. A fast nearest-neighbor algorithm based on a principal axis search tree. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 23(9):964976, 2001.  9
F. Desobry, M. Davy, and C. Doncarli. An online kernel change detection algorithm. IEEE Trans. Sig. Proc., 2005.
F. Enikeeva and Z. Harchaoui. High-dimensional change-point detection with sparse alternatives. arXiv:1312.1900, 2014.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola. A kernel two-sample test. The Journal of Machine Learning Research, 13(1):723773, 2012.
Z. Harchaoui, F. Bach, O. Cappe, and E. Moulines. Kernel-based methods for hypothesis testing. IEEE Sig. Proc. Magazine, pages 8797, 2013.
Z. Harchaoui, F. Bach, and E. Moulines. Kernel change-point analysis. In Adv. in Neural Information Processing Systems 21 (NIPS 2008), 2008.
D. Kifer, S. Ben-David, and J. Gehrke. Detecting change in data streams. In Proc. of the 30th VLDB Conf., 2004.
Song Liu, Makoto Yamada, Nigel Collier, and Masashi Sugiyama. Change-point detection in time-series data by direct density-ratio estimation. Neural Networks, 43:7283, 2013.
Aaditya Ramdas, Sashank Jakkam Reddi, Barnabas Poczos, Aarti Singh, and Larry Wasserman. On the decreasing power of kernel and distance based nonparametric hypothesis tests in high dimensions. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.
Z. E. Ross and Y. Ben-Zion. Automatic picking of direct P , S seismic phases and fault zone head waves. Geophys. J. Int., 2014.
Bernhard Scholkopf and Alexander J Smola. Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press, 2001.
R. J. Serfling. U-Statistics. Approximation theorems of mathematical statistics. John Wiley & Sons, 1980.
D. Siegmund. Sequential analysis: tests and confidence intervals. Springer, 1985.
D. Siegmund and E. S. Venkatraman. Using the generalized likelihood ratio statistic for sequential detection of a change-point. Ann. Statist., (23):255271, 1995.
D. Siegmund and B. Yakir. Detecting the emergence of a signal in a noisy image. Stat. Interface, (1):312, 2008.
Y. Xie and D. Siegmund. Sequential multi-sensor change-point detection. Annals of Statistics, 41(2):670692, 2013.
B. Yakir. Extremes in random fields: A theory and its applications. Wiley, 2013.
W. Zaremba, A. Gretton, and M. Blaschko. B-test: low variance kernel two-sample test. In Adv. Neural Info. Proc. Sys. (NIPS), 2013.
S. Zou, Y. Liang, H. V. Poor, and X. Shi. Nonparametric detection of anomalous data via kernel mean embedding. arXiv:1405.2294, 2014.  9
Lucien Birge, Pascal Massart, et al. Minimum contrast estimators on sieves: exponential bounds and rates of convergence. Bernoulli, 4(3):329375, 1998.
Gabor Lugosi and Andrew B Nobel. Adaptive model selection using empirical complexities. Annals of Statistics, pages 18301864, 1999.
Peter L. Bartlett, Stephane Boucheron, and Gabor Lugosi. Model selection and error estimation. Machine Learning, 48(1-3):85113, 2002.
Pascal Massart. Concentration inequalities and model selection, volume 10. Springer, 2007.
Shahar Mendelson. Learning without Concentration. In Conference on Learning Theory, 2014.
Tengyuan Liang, Alexander Rakhlin, and Karthik Sridharan. Learning with square loss: Localization through offset rademacher complexity. Proceedings of The 28th Conference on Learning Theory, 2015.
Alexander Rakhlin and Karthik Sridharan. Online nonparametric regression. Proceedings of The 27th Conference on Learning Theory, 2014.
Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Random averages, combinatorial parameters, and learnability. In Advances in Neural Information Processing Systems 23. 2010.
Elad Hazan and Satyen Kale. Extracting certainty from uncertainty: Regret bounded by variation in costs. Machine learning, 80(2):165188, 2010.
Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and Shenghuo Zhu. Online optimization with gradual variations. In COLT, 2012.
Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In Proceedings of the 26th Annual Conference on Learning Theory (COLT), 2013.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:21212159, 2011.
Nicolo Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz. Improved second-order bounds for prediction with expert advice. Machine Learning, 66(2-3):321352, 2007.
Kamalika Chaudhuri, Yoav Freund, and Daniel J Hsu. A parameter-free hedging algorithm. In Advances in neural information processing systems, pages 297305, 2009.
H. Brendan McMahan and Francesco Orabona. Unconstrained online linear learning in hilbert spaces: Minimax algorithms and normal approximations. Proceedings of The 27th Conference on Learning Theory, 2014.
Nicolo Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006.
Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Smoothness, low noise and fast rates. In Advances in neural information processing systems, pages 21992207, 2010.
Haipeng Luo and Robert E. Schapire. Achieving all with no parameters: Adaptive normalhedge. CoRR, abs/1502.05934, 2015.
Wouter M. Koolen and Tim van Erven. Second-order quantile methods for experts and combinatorial games. In Proceedings of the 28th Annual Conference on Learning Theory (COLT), pages 11551175, 2015.
Thomas M. Cover. Behavior of sequential predictors of binary sequences. In in Trans. 4th Prague Conference on Information Theory, Statistical Decision Functions, Random Processes, pages 263272. Publishing House of the Czechoslovak Academy of Sciences, 1967.
Alexander Rakhlin and Karthik Sridharan. Statistical learning theory and sequential prediction, 2012. Available at http://stat.wharton.upenn.edu/rakhlin/book_draft.pdf.
Iosif Pinelis. Optimum bounds for the distributions of martingales in banach spaces. The Annals of Probability, 22(4):16791706, 10 1994.
Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Beyond regret. arXiv preprint arXiv:1011.3168, 2010.
Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Sequential complexities and uniform martingale laws of large numbers. Probability Theory and Related Fields, 2014.
Eyal Even-Dar, Michael Kearns, Yishay Mansour, and Jennifer Wortman. Regret to the best vs. regret to the average. Machine Learning, 72(1-2):2137, 2008.
Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Relax and randomize: From value to algorithms. Advances in Neural Information Processing Systems 25, pages 21502158, 2012.  9
A. Agarwal and L. Bottou. A lower bound for the optimization of finite sums. In Proc. International Conference on Machine Learning (ICML), 2015.
F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. Optimization with sparsity-inducing penalties. Foundations and Trends in Machine Learning, 4(1):1106, 2012.
H. H. Bauschke and P. L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces. Springer, 2011.
A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2(1):183202, 2009.
D. P. Bertsekas. Convex Optimization Algorithms. Athena Scientific, 2015.
A. J. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Adv. Neural Information Processing Systems (NIPS), 2014.
A. J. Defazio, T. S. Caetano, and J. Domke. Finito: A faster, permutable incremental gradient method for big data problems. In Proc. International Conference on Machine Learning (ICML), 2014.
R. Frostig, R. Ge, S. M. Kakade, and A. Sidford. Un-regularizing: approximate proximal point algorithms for empirical risk minimization. In Proc. International Conference on Machine Learning (ICML), 2015.
O. Guler. New proximal point algorithms for convex minimization. SIAM Journal on Optimization, 2(4):649664, 1992.
B. He and X. Yuan. An accelerated inexact proximal point algorithm for convex minimization. Journal of Optimization Theory and Applications, 154(2):536548, 2012.
J.-B. Hiriart-Urruty and C. Lemarechal. Convex Analysis and Minimization Algorithms I. Springer, 1996.
A. Juditsky and A. Nemirovski. First order methods for nonsmooth convex large-scale optimization. Optimization for Machine Learning, MIT Press, 2012.
G. Lan. An optimal randomized incremental gradient method. arXiv:1507.02000, 2015.
J. Mairal. Incremental majorization-minimization optimization with application to large-scale machine learning. SIAM Journal on Optimization, 25(2):829855, 2015.
A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):15741609, 2009.
Y. Nesterov. A method of solving a convex programming problem with convergence rate O(1/k2 ). Soviet Mathematics Doklady, 27(2):372376, 1983.
Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer, 2004.
Y. Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341362, 2012.
Y. Nesterov. Gradient methods for minimizing composite functions. 140(1):125161, 2013.  Mathematical Programming,
N. Parikh and S.P. Boyd. Proximal algorithms. Foundations and Trends in Optimization, 1(3):123231, 2014.
P. Richtarik and M. Takac. Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function. Mathematical Programming, 144(1-2):138, 2014.
S. Salzo and S. Villa. Inexact and accelerated proximal point algorithms. Journal of Convex Analysis, 19(4):11671192, 2012.
M. Schmidt, N. Le Roux, and F. Bach. Convergence rates of inexact proximal-gradient methods for convex optimization. In Adv. Neural Information Processing Systems (NIPS), 2011.
M. Schmidt, N. Le Roux, and F. Bach. Minimizing finite sums with the stochastic average gradient. arXiv:1309.2388, 2013.
S. Shalev-Shwartz and T. Zhang. Proximal stochastic dual coordinate ascent. arXiv:1211.2717, 2012.
S. Shalev-Shwartz and T. Zhang. Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization. Mathematical Programming, 2015.
L. Xiao and T. Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM Journal on Optimization, 24(4):20572075, 2014.
Y. Zhang and L. Xiao. Stochastic primal-dual coordinate method for regularized empirical risk minimization. In Proc. International Conference on Machine Learning (ICML), 2015.  9
D. J. Daley and D. Vere-Jones. An Introduction to the Theory of Point Processes. Springer, 2nd edition, 2003.
O. Macchi. The coincidence approach to stochastic point processes. Advances in Applied Probability, 7:83122, 1975.
A. Kulesza and B. Taskar. Determinantal point processes for machine learning. Foundations and Trends in Machine Learning, 2012.
F. Lavancier, J. Mller, and E. Rubak. Determinantal point process models and statistical inference. Journal of the Royal Statistical Society, 2014.
J. B. Hough, M. Krishnapur, Y. Peres, and B. Virag. Determinantal processes and independence. Probability surveys, 2006.
J. Y. Zou and R. P. Adams. Priors for diversity in generative latent variable models. In Advances in Neural Information Processing Systems (NIPS), 2012.
R. H. Affandi, E. B. Fox, R. P. Adams, and B. Taskar. Learning the parameters of determinantal point processes. In Proceedings of the International Conference on Machine Learning (ICML), 2014.
J. Gillenwater, A. Kulesza, E. B. Fox, and B. Taskar. Expectation-maximization for learning determinantal point processes. In Advances in Neural Information Proccessing Systems (NIPS), 2014.
Z. Mariet and S. Sra. Fixed-point algorithms for learning determinantal point processes. In Advances in Neural Information systems (NIPS), 2015.
C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.
Michalis K. Titsias. Variational Learning of Inducing Variables in Sparse Gaussian Processes. In AISTATS, volume 5, 2009.
N. Cristianini and J. Shawe-Taylor. Kernel methods for pattern recognition. Cambridge University Press, 2004.
R. H. Affandi, A. Kulesza, E. B. Fox, and B. Taskar. Nystrom approximation for largescale determinantal processes. In Proceedings of the conference on Artificial Intelligence and Statistics (AISTATS), 2013.
E. Seiler and B. Simon. An inequality among determinants. Proceedings of the National Academy of Sciences, 1975.
N. Hansen. The CMA evolution strategy: a comparing review. In Towards a new evolutionary computation. Advances on estimation of distribution algorithms. Springer, 2006.
C. P. Robert and G. Casella. Monte Carlo Statistical Methods. Springer, 2004.
L. Devroye. Non-uniform random variate generation. Springer-Verlag, 1986.
I. Gohberg, S. Goldberg, and M. A. Kaashoek. Classes of linear operators, Volume I. Springer, 1990.
B. Simon. Trace ideals and their applications. American Mathematical Society, 2nd edition, 2005.
G. E. Fasshauer and M. J. McCourt. Stable evaluation of gaussian radial basis function interpolants. SIAM Journal on Scientific Computing, 34(2), 2012.
H. Haario, E. Saksman, and J. Tamminen. An adaptive Metropolis algorithm. Bernoulli, 7:223242, 2001.
L. A. Waller, A. Sarkka, V. Olsbo, M. Myllymaki, I. G. Panoutsopoulou, W. R. Kennedy, and G. Wendelschafer-Crabb. Second-order spatial analysis of epidermal nerve fibers. Statistics in Medicine, 30(23):28272841, 2011.
F. Bornemann. On the numerical evaluation of Fredholm determinants. Mathematics of Computation, 79(270):871915, 2010.  9
Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. The Journal of Machine Learning Research, 14(1):13031347, 2013.
Tim Salimans, David A Knowles, et al. Fixed-form variational posterior approximation through stochastic linear regression. Bayesian Analysis, 8(4):837882, 2013.
Rajesh Ranganath, Sean Gerrish, and David M Blei. Black box variational inference. arXiv preprint arXiv:1401.0118, 2013.
Michalis Titsias and Miguel Lazaro-Gredilla. Doubly Stochastic Variational Bayes for Non-Conjugate Inference. In International Conference on Machine Learning, 2014.
Masa-Aki Sato. Online model selection based on the variational Bayes. Neural Computation, 13(7):1649 1681, 2001.
A. Honkela, T. Raiko, M. Kuusela, M. Tornio, and J. Karhunen. Approximate Riemannian conjugate gradient learning for fixed-form variational Bayes. The Journal of Machine Learning Research, 11:3235 3268, 2011.
Stephane Chretien and Alfred OIII Hero. Kullback proximal algorithms for maximum-likelihood estimation. Information Theory, IEEE Transactions on, 46(5):18001810, 2000.
Paul Tseng. An analysis of the EM algorithm and entropy-like proximal point methods. Mathematics of Operations Research, 29(1):2744, 2004.
M. Teboulle. Convergence of proximal-like algorithms. SIAM Jon Optimization, 7(4):10691083, 1997.
Pradeep Ravikumar, Alekh Agarwal, and Martin J Wainwright. Message-passing for graph-structured linear programs: Proximal projections, convergence and rounding schemes. In International Conference on Machine Learning, 2008.
Behnam Babagholami-Mohamadabadi, Sejong Yoon, and Vladimir Pavlovic. D-MFVI: Distributed mean field variational inference using Bregman ADMM. arXiv preprint arXiv:1507.00824, 2015.
Bo Dai, Niao He, Hanjun Dai, and Le Song. Scalable Bayesian inference via particle mirror descent. Computing Research Repository, abs/1506.03101, 2015.
Lucas Theis and Matthew D Hoffman. A trust-region method for stochastic variational inference with applications to streaming data. International Conference on Machine Learning, 2015.
Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. arXiv preprint arXiv:1301.3584, 2013.
Ulrich Paquet. On the convergence of stochastic variational inference in bayesian networks. NIPS Workshop on variational inference, 2014.
Nicholas G Polson, James G Scott, and Brandon T Willard. Proximal algorithms in statistics and machine learning. arXiv preprint arXiv:1502.03175, 2015.
Harri Lappalainen and Antti Honkela. Bayesian non-linear independent component analysis by multilayer perceptrons. In Advances in independent component analysis, pages 93121. Springer, 2000.
Chong Wang and David M. Blei. Variational inference in nonconjugate models. J. Mach. Learn. Res., 14(1):10051031, April 2013.
M. Seeger and H. Nickisch. Large scale Bayesian inference and experimental design for sparse linear models. SIAM Journal of Imaging Sciences, 4(1):166199, 2011.
Antti Honkela and Harri Valpola. Unsupervised variational Bayesian learning of nonlinear models. In Advances in neural information processing systems, pages 593600, 2004.
Mohammad Emtiyaz Khan, Reza Babanezhad, Wu Lin, Mark Schmidt, and Masashi Sugiyama. Convergence of Proximal-Gradient Stochastic Variational Inference under Non-Decreasing Step-Size Sequence. arXiv preprint, 2015.
Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.
B. Marlin, M. Khan, and K. Murphy. Piecewise bounds for estimating Bernoulli-logistic latent Gaussian models. In International Conference on Machine Learning, 2011.
Mohammad Emtiyaz Khan. Decoupled Variational Inference. In Advances in Neural Information Processing Systems, 2014.
E. Challis and D. Barber. Concave Gaussian variational approximations for inference in large-scale Bayesian linear models. In International conference on Artificial Intelligence and Statistics, 2011.
Huahua Wang and Arindam Banerjee. Bregman alternating direction method of multipliers. In Advances in Neural Information Processing Systems, 2014.  9
Francis Bach. Duality between subgradient and conditional gradient methods. SIAM Journal on Optimization, 2015.
Francis Bach, Rodolphe Jenatton, Julien Mairal, and Guillaume Obozinski. Optimization with sparsityinducing penalties. Found. Trends Mach. Learn., 4(1):1106, 2012.
Heinz H. Bauschke and Patrick L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces. Springer, 2011.
D. P. Bertsekas. Convex Optimization Algorithms. Athena Scientific, 2015.
Xi Chen, Qihang Lin, Seyoung Kim, Jaime G Carbonell, and Eric P Xing. Smoothing proximal gradient method for general structured sparse regression. The Annals of Applied Statistics, 6(2):719752, 2012.
Bruce Cox, Anatoli Juditsky, and Arkadi Nemirovski. Dual subgradient algorithms for large-scale nonsmooth learning problems. Mathematical Programming, pages 138, 2013.
M. Dudik, Z. Harchaoui, and J. Malick. Lifted coordinate descent for learning with trace-norm regularization. Proceedings of the 15th International Conference on Artificial Intelligence and Statistics (AISTATS), 2012.
Dan Garber and Elad Hazan. A linearly convergent conditional gradient algorithm with applications to online and stochastic optimization. arXiv preprint arXiv:1301.4666, 2013.
Zaid Harchaoui, Anatoli Juditsky, and Arkadi Nemirovski. Conditional gradient algorithms for normregularized smooth convex optimization. Mathematical Programming, pages 138, 2013.
E. Hazan and S. Kale. Projection-free online learning. In ICML, 2012.
Niao He, Anatoli Juditsky, and Arkadi Nemirovski. Mirror prox algorithm for multi-term composite minimization and semi-separable problems. arXiv preprint arXiv:1311.1098, 2013.
Martin Jaggi. Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In ICML, pages 427 435, 2013.
Anatoli Juditsky and Arkadi Nemirovski. Solving variational inequalities with monotone operators on domains given by linear minimization oracles. arXiv preprint arXiv:1312.107, 2013.
Guanghui Lan. The complexity of large-scale convex programming under a linear optimization oracle. arXiv, 2013.
Guanghui Lan and Yi Zhou. Conditional gradient sliding for convex optimization. arXiv, 2014.
Cun Mu, Yuqian Zhang, John Wright, and Donald Goldfarb. Scalable robust matrix recovery: Frankwolfe meets proximal methods. arXiv preprint arXiv:1403.7588, 2014.
Arkadi Nemirovski. Prox-method with rate of convergence o(1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on Optimization, 15(1):229251, 2004.
Arkadi Nemirovski, Shmuel Onn, and Uriel G Rothblum. Accuracy certificates for computational problems with convex structure. Mathematics of Operations Research, 35(1):5278, 2010.
Yurii Nesterov. Smooth minimization of non-smooth functions. Mathematical programming, 103(1):127 152, 2005.
Yurii Nesterov. Smoothing technique and its applications in semidefinite optimization. Math. Program., 110(2):245259, 2007.
Yurii Nesterov. Complexity bounds for primal-dual methods minimizing the model of objective function. Technical report, Universite catholique de Louvain, Center for Operations Research and Econometrics (CORE), 2015.
Yuyuan Ouyang, Yunmei Chen, Guanghui Lan, and Eduardo Pasiliao Jr. An accelerated linearized alternating direction method of multipliers, 2014. http://arxiv.org/abs/1401.6607.
Neal Parikh and Stephen Boyd. Proximal algorithms. Foundations and Trends in Optimization, pages 196, 2013.
Federico Pierucci, Zaid Harchaoui, and Jerome Malick. A smoothing approach for composite conditional gradient with nonsmooth loss. In Conference dApprentissage AutomatiqueActes CAP14, 2014.
Mark Schmidt, Nicolas L. Roux, and Francis R. Bach. Convergence rates of inexact proximal-gradient methods for convex optimization. In Adv. NIPS. 2011.
X. Zhang, Y. Yu, and D. Schuurmans. Accelerated training for matrix-norm regularization: A boosting approach. In NIPS, 2012.  9
Francis R Bach. Structured sparsity-inducing norms through submodular functions. In Advances in Neural Information Processing Systems, pages 118126, 2010.
Mohsen Bayati and Andrea Montanari. The lasso risk for gaussian matrices. Information Theory, IEEE Transactions on, 58(4):19972017, 2012.
Alexandre Belloni, Victor Chernozhukov, and Lie Wang. Square-root lasso: pivotal recovery of sparse signals via conic programming. Biometrika, 98(4):791806, 2011.
David R. Brillinger. The identification of a particular nonlinear time series system. Biometrika, 64(3):509 515, 1977.
David R Brillinger. A generalized linear model with gaussian regressor variables. A Festschrift For Erich L. Lehmann, page 97, 1982.
Venkat Chandrasekaran, Benjamin Recht, Pablo A Parrilo, and Alan S Willsky. The convex geometry of linear inverse problems. Foundations of Computational Mathematics, 12(6):805849, 2012.
David L Donoho and Iain M Johnstone. Minimax risk overl p-balls forl p-error. Probability Theory and Related Fields, 99(2):277303, 1994.
David L Donoho, Lain Johnstone, and Andrea Montanari. Accurate prediction of phase transitions in compressed sensing via a connection to minimax denoising. IEEE transactions on information theory, 59(6):33963433, 2013.
David L Donoho, Arian Maleki, and Andrea Montanari. Message-passing algorithms for compressed sensing. Proceedings of the National Academy of Sciences, 106(45):1891418919, 2009.
David L Donoho, Arian Maleki, and Andrea Montanari. The noise-sensitivity phase transition in compressed sensing. Information Theory, IEEE Transactions on, 57(10):69206941, 2011.
Alexandra L Garnham and Luke A Prendergast. A note on least squares sensitivity in single-index model estimation and the benefits of response transformations. Electronic J. of Statistics, 7:19832004, 2013.
Yehoram Gordon. On Milmans inequality and random subspaces which escape through a mesh in Rn . Springer, 1988.
Marwa El Halabi and Volkan Cevher. A totally unimodular view of structured sparsity. arXiv preprint arXiv:1411.1990, 2014.
Hidehiko Ichimura. Semiparametric least squares (sls) and weighted sls estimation of single-index models. Journal of Econometrics, 58(1):71120, 1993.
Ker-Chau Li and Naihua Duan. Regression analysis under link violation. The Annals of Statistics, pages 10091052, 1989.
Samet Oymak, Christos Thrampoulidis, and Babak Hassibi. The squared-error of generalized lasso: A precise analysis. arXiv preprint arXiv:1311.0830, 2013.
Yaniv Plan and Roman Vershynin. The generalized lasso with non-linear observations. arXiv preprint arXiv:1502.04071, 2015.
Mihailo Stojnic. A framework to characterize performance of lasso algorithms. arXiv preprint arXiv:1303.7291, 2013.
Christos Thrampoulidis, Samet Oymak, and Babak Hassibi. Regularized linear regression: A precise analysis of the estimation error. In Proceedings of The 28th Conference on Learning Theory, pages 1683 1709, 2015.
Christos Thrampoulidis, Ashkan Panahi, Daniel Guo, and Babak Hassibi. Precise error analysis of the lasso. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on, pages 34673471.
Christos Thrampoulidis, Ashkan Panahi, and Babak Hassibi. Asymptotically exact error analysis for the generalized `22 -lasso. In Information Theory (ISIT), 2015 IEEE International Symposium on, pages 20212025. IEEE, 2015.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), pages 267288, 1996.
Robert Tibshirani, Michael Saunders, Saharon Rosset, Ji Zhu, and Keith Knight. Sparsity and smoothness via the fused lasso. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(1):91 108, 2005.
Xinyang Yi, Zhaoran Wang, Constantine Caramanis, and Han Liu. Optimal linear estimation under unknown nonlinear transform. arXiv preprint arXiv:1505.03257, 2015.
Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):4967, 2006.  9
M. Alamgir and U. von Luxburg. Phase transition in the family of p-resistances. In Advances in Neural Information Processing Systems, pages 379387, 2011.
M. Alamgir and U. von Luxburg. Shortest path distance in random k-nearest neighbor graphs. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 10311038, 2012.
P. Chebotarev. A class of graph-geodetic distances generalizing the shortest-path and the resistance distances. Discrete Applied Mathematics, 159(5):295302, 2011.
D. A. Croydon and B. M. Hambly. Local limit theorems for sequences of simple random walks on graphs. Potential Analysis, 29(4):351389, 2008.
J. Gehrke, P. Ginsparg, and J. Kleinberg. Overview of the 2003 KDD Cup. ACM SIGKDD Explorations Newsletter, 5(2):149151, 2003.
T. B. Hashimoto, Y. Sun, and T. S. Jaakkola. Metric recovery from directed unweighted graphs. In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, pages 342350, 2015.
G. R. Kiss, C. Armstrong, R. Milroy, and J. Piper. An associative thesaurus of English and its computer analysis. The Computer and Literary Studies, pages 153165, 1973.
I. Kivimki, M. Shimbo, and M. Saerens. Developments in the theory of randomized shortest paths with a comparison of graph node distances. Physica A: Statistical Mechanics and its Applications, 393:600616, 2014.
L. L and T. Zhou. Link prediction in complex networks: A survey. Physica A: Statistical Mechanics and its Applications, 390(6):11501170, 2011.
B. ksendal. Stochastic differential equations: An introduction with applications. Universitext. SpringerVerlag, Berlin, sixth edition, 2003.
P. Sarkar, D. Chakrabarti, and A. W. Moore. Theoretical justification of popular link prediction heuristics. In IJCAI Proceedings-International Joint Conference on Artificial Intelligence, volume 22, page 2722, 2011.
P. Sarkar and A. W. Moore. A tractable approach to finding closest truncated-commute-time neighbors in large graphs. In In Proc. UAI, 2007.
B. Shaw and T. Jebara. Structure preserving embedding. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 937944. ACM, 2009.
S. T. Smith, E. K. Kao, K. D. Senne, G. Bernstein, and S. Philips. Bayesian discovery of threat networks. IEEE Transactions on Signal Processing, 62:53245338, 2014.
D. W. Stroock and S. S. Varadhan. Multidimensional diffussion processes, volume 233. Springer Science & Business Media, 1979.
A. Tahbaz-Salehi and A. Jadbabaie. A one-parameter family of distributed consensus algorithms with boundary: From shortest paths to mean hitting times. In Decision and Control, 2006 45th IEEE Conference on, pages 46644669. IEEE, 2006.
D. Ting, L. Huang, and M. I. Jordan. An analysis of the convergence of graph Laplacians. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 10791086, 2010.
U. von Luxburg, M. Belkin, and O. Bousquet. Consistency of spectral clustering. The Annals of Statistics, pages 555586, 2008.
U. von Luxburg, A. Radl, and M. Hein. Hitting and commute times in large random neighborhood graphs. Journal of Machine Learning Research, 15:17511798, 2014.
M. Yazdani. Similarity Learning Over Large Collaborative Networks. PhD thesis, cole Polytechnique Fdrale de Lausanne, 2013.
L. Yen, M. Saerens, A. Mantrach, and M. Shimbo. A family of dissimilarity measures between nodes generalizing both the shortest-path and the commute-time distances. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 785793. ACM, 2008.  9
S. Ahn, A. Korattikara, and M. Welling. Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring. In ICML, 2012.
Sungjin Ahn, Babak Shahbaba, and Max Welling. Distributed stochastic gradient MCMC. In ICML, 2014.
C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight uncertainty in neural networks. In ICML, 2015.
Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In KDD, 2006.
J Eric Bickel. Some comparisons among quadratic, spherical, and logarithmic scoring rules. Decision Analysis, 4(2):4965, 2007.
Tianqi Chen, Emily B Fox, and Carlos Guestrin. Stochastic Gradient Hamiltonian Monte Carlo. In ICML, 2014.
N Ding, Y Fang, R Babbush, C Chen, R Skeel, and H Neven. Bayesian sampling using stochastic gradient thermostats. In NIPS, 2014.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. 6 June 2015.
Alex Graves. Practical variational inference for neural networks. In NIPS, 2011.
J. Hernandez-Lobato and R. Adams. Probabilistic backpropagation for scalable learning of bayesian neural networks. In ICML, 2015.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In NIPS Deep Learning Workshop, 2014.
Diederik P Kingma and Max Welling. Stochastic gradient VB and the variational auto-encoder. In ICLR, 2014.
Radford Neal. MCMC using hamiltonian dynamics. In Handbook of Markov chain Monte Carlo. Chapman and Hall, 2011.
Sam Patterson and Yee Whye Teh. Stochastic gradient riemannian langevin dynamics on the probability simplex. In NIPS, 2013.
Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. FitNets: Hints for thin deep nets. Arxiv, 19 2014.
D. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In ICML, 2014.
Edward Snelson and Zoubin Ghahramani. Compact approximations to bayesian predictive distributions. In ICML, 2005.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In ICLR, 2014.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient Langevin dynamics. In ICML, 2011.  9
J. Abernethy, F. Bach, T. Evgeniou, and J.-P. Vert. A new approach to collaborative filtering: Operator estimation with spectral regularization. JMLR, 10:803826, 2009.
P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. JMLR, 3:463482, 2003.
D. P. Bertsekas. Nonlinear Programming. Athena Scientific, Belmont, MA 02178-9998, 1999.
E. Candes and Y. Plan. Matrix completion with noise. Proceedings of the IEEE, 98(6):925936, 2010.
E. Candes and B. Recht. Exact matrix completion via convex optimization. Commun. ACM, 55(6):111 119, 2012.
E. J. Candes, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? J. ACM, 58(3):11:1 11:37, 2011.
E. J. Candes and T. Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE Trans. Inf. Theor., 56(5):20532080, 2010.
V. Chandrasekaran, P. A. Parrilo, and A. S. Willsky. Latent variable graphical model selection via convex optimization. The Annals of Statistics, 2012.
T. Chen, W. Zhang, Q. Lu, K. Chen, Z. Zheng, and Y. Yu. SVDFeature: A toolkit for feature-based collaborative filtering. JMLR, 13:36193622, 2012.
Y. Chen, S. Bhojanapalli, S. Sanghavi, and R. Ward. Coherent matrix completion. In ICML, 2014.
Y. Chen, A. Jalali, S. Sanghavi, and H. Xu. Clustering partially observed graphs via convex optimization. JMLR, 15(1):22132238, 2014.
K.-Y. Chiang, C.-J. Hsieh, N. Natarajan, I. S. Dhillon, and A. Tewari. Prediction and clustering in signed networks: A local to global perspective. JMLR, 15:11771213, 2014.
J. V. Davis, B. Kulis, P. Jain, S. Sra, and I. S. Dhillon. Information-theoretic metric learning. In ICML, pages 209216, 2007.
U. Feige and G. Schechtman. On the optimality of the random hyperplane rounding technique for max cut. Random Struct. Algorithms, 20(3):403440, 2002.
L. Grippo and M. Sciandrone. Globally convergent block-coordinate techniques for unconstrained optimization. Optimization Methods and Software, 10:587637, 1999.
C.-J. Hsieh, K.-Y. Chiang, and I. S. Dhillon. Low rank modeling of signed networks. In KDD, 2012.
C.-J. Hsieh and P. A. Olsan. Nuclear norm minimization via active subspace selection. In ICML, 2014.
P. Jain and I. S. Dhillon. Provable inductive matrix completion. CoRR, abs/1306.0626, 2013.
A. Jalali, P. Ravikumar, S. Sanghavi, and C. Ruan. A dirty model for multi-task learning. In NIPS, 2010.
S. M. Kakade, K. Sridharan, and A. Tewari. On the complexity of linear prediction: Risk bounds, margin bounds, and regularization. In NIPS, pages 793  800, 2008.
R. Keshavan, A. Montanari, and S. Oh. Matrix completion from noisy entries. JMLR, 2010.
Y. Koren, R. M. Bell, and C. Volinsky. Matrix factorization techniques for recommender systems. IEEE Computer, 42:3037, 2009.
B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. The Annals of Statistics, 28(5):13021338, 2000.
J. Leskovec, D. Huttenlocher, and J. Kleinberg. Predicting positive and negative links in online social networks. In WWW, 2010.
Z. Li and J. Liu. Constrained clustering by spectral kernel learning. In ICCV, 2009.
P. Massa and P. Avesani. Trust-aware bootstrapping of recommender systems. In Proceedings of ECAI 2006 Workshop on Recommender Systems, pages 2933, 2006.
R. Meir and T. Zhang. Generalization error bounds for bayesian mixture algorithms. JMLR, 2003.
A. K. Menon, K.-P. Chitrapura, S. Garg, D. Agarwal, and N. Kota. Response prediction using collaborative filtering with hierarchies and side-information. In KDD, pages 141149, 2011.
N. Natarajan and I. S. Dhillon. Inductive matrix completion for predicting gene-disease associations. Bioinformatics, 30(12):6068, 2014.
S. Negahban and M. J. Wainwright. Restricted strong convexity and weighted matrix completion: Optimal bounds with noise. JMLR, 13(1):16651697, 2012.
M. Rudelson and R. Vershynin. Smallest singular value of a random rectangular matrix. Comm. Pure Appl. Math, pages 17071739, 2009.
O. Shamir and S. Shalev-Shwartz. Matrix completion with the trace norm: Learning, bounding, and transducing. JMLR, 15(1):34013423, 2014.
D. Shin, S. Cetintas, K.-C. Lee, and I. S. Dhillon. Tumblr blog recommendation with boosted inductive matrix completion. In CIKM, pages 203212, 2015.
N. Srebro and A. Shraibman. Rank, trace-norm and max-norm. In COLT, pages 545560, 2005.
M. Xu, R. Jin, and Z.-H. Zhou. Speedup matrix completion with side information: Application to multilabel learning. In NIPS, 2013.
E. Yang and P. Ravikumar. Dirty statistical models. In NIPS, 2013.
J. Yi, L. Zhang, R. Jin, Q. Qian, and A. Jain. Semi-supervised clustering by input pattern assisted pairwise similarity matrix completion. In ICML, 2013.
K. Zhong, P. Jain, and I. S. Dhillon. Efficient matrix sensing using rank-1 gaussian measurements. In International Conference on Algorithmic Learning Theory(ALT), 2015.  9
Christophe Andrieu, Arnaud Doucet, and Roman Holenstein. Particle Markov chain Monte Carlo methods. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72 (3):269342, 2010.
Iain Murray, Ryan P. Adams, and David J.C. MacKay. Elliptical slice sampling. Journal of Machine Learning Research: Workshop and Conference Proceedings (AISTATS), 9:541548, 05/2010 2010.
Nicholas G Polson, James G Scott, and Jesse Windle. Bayesian inference for logistic models using Polyagamma latent variables. Journal of the American Statistical Association, 108 (504):13391349, 2013.
Mingyuan Zhou, Lingbo Li, David Dunson, and Lawrence Carin. Lognormal and gamma mixed negative binomial regression. In Proceedings of the International Conference on Machine Learning, volume 2012, page 1343, 2012.
Jianfei Chen, Jun Zhu, Zi Wang, Xun Zheng, and Bo Zhang. Scalable inference for logisticnormal topic models. In Advances in Neural Information Processing Systems, pages 2445 2453, 2013.
Chris C Holmes, Leonhard Held, et al. Bayesian auxiliary variable models for binary and multinomial regression. Bayesian Analysis, 1(1):145168, 2006.
David Blei and John Lafferty. Correlated topic models. Advances in Neural Information Processing Systems, 18:147, 2006.
David M Blei, Andrew Y Ng, and Michael I Jordan. Latent Dirichlet allocation. the Journal of machine Learning research, 3:9931022, 2003.
David M Blei and John D Lafferty. Dynamic topic models. In Proceedings of the International Conference on Machine Learning, pages 113120. ACM, 2006.
Xiaogang Wang and Eric Grimson. Spatial latent Dirichlet allocation. In Advances in Neural Information Processing Systems, pages 15771584, 2008.
David Belanger and Sham Kakade. A linear dynamical system model for text. In Proceedings of the International Conference on Machine Learning, 2015.
Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the International Conference on Machine Learning, pages 160167. ACM, 2008.
David Belanger and Sham Kakade. Embedding word tokens using a linear dynamical system. In NIPS 2014 Modern ML+NLP Workshop, 2014.
Eric A Wan and Rudolph Van Der Merwe. The unscented Kalman filter for nonlinear estimation. In Adaptive Systems for Signal Processing, Communications, and Control Symposium 2000. AS-SPCC. The IEEE 2000, pages 153158. IEEE, 2000.
Sebastian Thrun, Wolfram Burgard, and Dieter Fox. Probabilistic robotics. MIT press, 2005.
Fredrik Lindsten, Thomas Schon, and Michael I Jordan. Ancestor sampling for particle Gibbs. In Advances in Neural Information Processing Systems, pages 25912599, 2012.
Mohammad E Khan, Shakir Mohamed, Benjamin M Marlin, and Kevin P Murphy. A stickbreaking likelihood for categorical data analysis with latent Gaussian models. In International Conference on Artificial Intelligence and Statistics, pages 610618, 2012.  9
J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei. ImageNet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition (CVPR), pages 248255, 2009.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems (NIPS), pages 10971105, 2012.  8
M. S. Bernstein, G. Little, R. C. Miller, B. Hartmann, M. S. Ackerman, D. R. Karger, D. Crowell, and K. Panovich. Soylent: a word processor with a crowd inside. In Symposium on User Interface Software and Technology, pages 313322, 2010.
N. Kokkalis, T. Kohn, C. Pfeiffer, D. Chornyi, M. S. Bernstein, and S. R. Klemmer. Emailvalet: Managing email overload through private, accountable crowdsourcing. In Conference on Computer Supported Cooperative Work, pages 12911300, 2013.
C. Li, J. Weng, Q. He, Y. Yao, A. Datta, A. Sun, and B. Lee. Twiner: named entity recognition in targeted twitter stream. In ACM Special Interest Group on Information Retreival (SIGIR), pages 721730, 2012.
Walter S Lasecki, Young Chol Song, Henry Kautz, and Jeffrey P Bigham. Real-time crowd labeling for deployable activity recognition. In Proceedings of the 2013 conference on Computer supported cooperative work, 2013.
N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press, 2006.
D. Helmbold and S. Panizza. Some label efficient learning results. In Conference on Learning Theory (COLT), pages 218230, 1997.
D. Sculley. Online active learning methods for fast label-efficient spam filtering. In Conference on Email and Anti-spam (CEAS), 2007.
W. Chu, M. Zinkevich, L. Li, A. Thomas, and B. Tseng. Unbiased online active learning in data streams. In International Conference on Knowledge Discovery and Data Mining (KDD), pages 195203, 2011.
T. Gao and D. Koller. Active classification based on value of classifier. In Advances in Neural Information Processing Systems (NIPS), pages 10621070, 2011.
L. Kocsis and C. Szepesvari. Bandit based Monte-Carlo planning. In European Conference on Machine Learning (ECML), pages 282293, 2006.
R. Coulom. Computing elo ratings of move patterns in the game of go. Computer Games Workshop, 2007.
J. R. Finkel, T. Grenager, and C. Manning. Incorporating non-local information into information extraction systems by Gibbs sampling. In Association for Computational Linguistics (ACL), pages 363370, 2005.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In ACL: HLT, pages 142150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics.
R. Socher, A. Perelygin, J. Y. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Empirical Methods in Natural Language Processing (EMNLP), 2013.
N. Kumar, A. C. Berg, P. N. Belhumeur, and S. K. Nayar. Attribute and Simile Classifiers for Face Verification. In ICCV, Oct 2009.
M. S. Bernstein, J. Brandt, R. C. Miller, and D. R. Karger. Crowds in two seconds: Enabling realtime crowd-powered interfaces. In User Interface Software and Technology, pages 3342, 2011.
B. Settles. Active learning literature survey. Technical report, University of Wisconsin, Madison, 2010.
P. Donmez and J. G. Carbonell. Proactive learning: cost-sensitive active learning with multiple imperfect oracles. In Conference on Information and Knowledge Management (CIKM), pages 619628, 2008.
D. Golovin, A. Krause, and D. Ray. Near-optimal Bayesian active learning with noisy observations. In Advances in Neural Information Processing Systems (NIPS), pages 766774, 2010.
R. Greiner, A. J. Grove, and D. Roth. Learning cost-sensitive active classifiers. Artificial Intelligence, 139(2):137174, 2002.
X. Chai, L. Deng, Q. Yang, and C. X. Ling. Test-cost sensitive naive Bayes classification. In International Conference on Data Mining, pages 5158, 2004.
S. Esmeir and S. Markovitch. Anytime induction of cost-sensitive trees. In Advances in Neural Information Processing Systems (NIPS), pages 425432, 2007.
J. Cheng and M. S. Bernstein. Flock: Hybrid Crowd-Machine learning classifiers. In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing, pages 600611, 2015.
P. Dai, Mausam, and D. S. Weld. Decision-theoretic control of crowd-sourced workflows. In Association for the Advancement of Artificial Intelligence (AAAI), 2010.
P. Liang, M. I. Jordan, and D. Klein. Learning from measurements in exponential families. In International Conference on Machine Learning (ICML), 2009.
G. Angeli, J. Tibshirani, J. Y. Wu, and C. D. Manning. Combining distant and partial supervision for relation extraction. In Empirical Methods in Natural Language Processing (EMNLP), 2014.  9
M. Seigel. Confidence Estimation for Automatic Speech Recognition Hypotheses. PhD thesis, University of Cambridge, 2013.
D. E. Heckerman and B. N. Nathwani. Towards normative expert systems: Probability-based representations for efficient knowledge acquisition and inference. Methods Archive, 31(2):106116, 1992.
R. H. Kassel. A comparison of approaches to on-line handwritten character recognition. PhD thesis, Massachusetts Institute of Technology, 1995.
P. Liang, A. Bouchard-Cote, D. Klein, and B. Taskar. An end-to-end discriminative approach to machine translation. In International Conference on Computational Linguistics and Association for Computational Linguistics (COLING/ACL), 2006.
A. Mueller. Methods for Learning Structured Prediction in Semantic Segmentation of Natural Images. PhD thesis, University of Bonn, 2013.
G. W. Brier. Verification of forecasts expressed in terms of probability. Monthly weather review, 78(1):1 3, 1950.
A. H. Murphy. A new vector partition of the probability score. Journal of Applied Meteorology, 12(4):595600, 1973.
D. P. Foster and R. V. Vohra. Asymptotic calibration, 1998.
T. Gneiting, F. Balabdaoui, and A. E. Raftery. Probabilistic forecasts, calibration and sharpness. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69(2):243268, 2007.
J. Brocker. Reliability, sufficiency, and the decomposition of proper scores. Quarterly Journal of the Royal Meteorological Society, 135(643):15121519, 2009.
J. Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in Large Margin Classifiers, 10(3):6174, 1999.
B. Zadrozny and C. Elkan. Transforming classifier scores into accurate multiclass probability estimates. In International Conference on Knowledge Discovery and Data Mining (KDD), pages 694699, 2002.
A. Niculescu-Mizil and R. Caruana. Predicting good probabilities with supervised learning. In Proceedings of the 22nd international conference on Machine learning, pages 625632, 2005.
D. B. Stephenson, C. A. S. Coelho, and I. T. Jolliffe. Two extra components in the brier score decomposition. Weather Forecasting, 23:752757, 2008.
A. Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.
A. Coates and A. Y. Ng. Learning feature representations with K-means. Neural Networks: Tricks of the Trade - Second Edition, 2(1):561580, 2012.
A. Buja, W. Stuetzle, and Y. Shen. Loss functions for binary class probability estimation and classification: Structure and applications, 2005.
D. A. Philip. The well-calibrated Bayesian. Journal of the American Statistical Association (JASA), 77(379):605610, 1982.
A. K. Menon, X. Jiang, S. Vembu, C. Elkan, and L. Ohno-Machado. Predicting accurate probabilities with a ranking loss. In International Conference on Machine Learning (ICML), 2012.
L. W. Zhong and J. Kwok. Accurate probability calibration for multiple classifiers. In International Joint Conference on Artificial Intelligence (IJCAI), pages 19391945, 2013.
D. Yu, J. Li, and L. Deng. Calibration of confidence measures in speech recognition. Trans. Audio, Speech and Lang. Proc., 19(8):24612473, 2011.
X. Jiang, M. Osl, J. Kim, and L. Ohno-Machado. Calibrating predictive model estimates to support personalized medicine. Journal of the American Medical Informatics Association, 19(2):263274, 2012.
K. Nguyen and B. OConnor. Posterior calibration and exploratory analysis for natural language processing models. In Empirical Methods in Natural Language Processing (EMNLP), pages 15871598, 2015.
S. Lichtenstein, B. Fischhoff, and L. D. Phillips. Judgement under Uncertainty: Heuristics and Biases. Cambridge University Press, 1982.  9
G. Andrew, R. Arora, J. Bilmes, and K. Livescu. Deep canonical correlation analysis. In ICML, 2013.
Y. Bengio, E. Thibodeau-Laufer, G. Alain, and J. Yosinski. Deep generative stochastic networks trainable by backprop. In ICML, 2014.
D. Ciresan, A. Giusti, L. M. Gambardella, and J. Schmidhuber. Deep neural networks segment neuronal membranes in electron microscopy images. In NIPS, 2012.
C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Scene parsing with multiscale feature learning, purity trees, and optimal covers. In ICML, 2012.
C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learning hierarchical features for scene labeling. T. PAMI, 35(8):19151929, 2013.
R. Girshick, J. Donahue, T. Darrell, and J. Malik. Region-based convolutional networks for accurate object detection and segmentation. T. PAMI, PP(99):11, 2015.
I. Goodfellow, M. Mirza, A. Courville, and Y. Bengio. Multi-prediction deep Boltzmann machines. In NIPS, 2013.  8
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, 2014.
K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual recognition. In ECCV, 2014.
G. E. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep belief nets. Neural Computation, 18(7):15271554, 2006.
G. B. Huang, M. Narayana, and E. Learned-Miller. Towards unconstrained face recognition. In CVPR Workshop on Perceptual Organization in Computer Vision, 2008.
G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical Report 07-49, University of Massachusetts, Amherst, 2007.
A. Kae, K. Sohn, H. Lee, and E. Learned-Miller. Augmenting CRFs with Boltzmann machine shape priors for image labeling. In CVPR, 2013.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
D. P. Kingma, S. Mohamed, D. J. Rezende, and M. Welling. Semi-supervised learning with deep generative models. In NIPS, 2014.
D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In ICLR, 2013.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, 2012.
H. Larochelle and I. Murray. The neural autoregressive distribution estimator. JMLR, 15:2937, 2011.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, 1998.
H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Unsupervised learning of hierarchical representations with convolutional deep belief networks. Communications of the ACM, 54(10):95103, 2011.
Y. Li, D. Tarlow, and R. Zemel. Exploring compositional high order pattern potentials for structured output learning. In CVPR, 2013.
J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.
P. Pinheiro and R. Collobert. Recurrent convolutional neural networks for scene parsing. In ICML, 2013.
D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In ICML, 2014.
R. Salakhutdinov and G. E. Hinton. Deep Boltzmann machines. In AISTATS, 2009.
P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. OverFeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2013.
K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2014.
K. Sohn, W. Shang, and H. Lee. Improved multimodal deep learning with variation of information. In NIPS, 2014.
N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. JMLR, 15(1):19291958, 2014.
C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In CVPR, 2015.
C. Szegedy, A. Toshev, and D. Erhan. Deep neural networks for object detection. In NIPS, 2013.
Y. Tang and R. Salakhutdinov. Learning stochastic feedforward neural networks. In NIPS, 2013.
A. Vedaldi and K. Lenc. MatConvNet  convolutional neural networks for MATLAB. In ACMMM, 2015.
P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with denoising autoencoders. In ICML, 2008.
N. Wang, H. Ai, and F. Tang. What are good parts for hair shape modeling? In CVPR, 2012.
P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-UCSD Birds 200. Technical Report CNS-TR-2010-001, California Institute of Technology, 2010.
J. Yang, S. Safar, and M.-H. Yang. Max-margin Boltzmann machines for object segmentation. In CVPR, 2014.  9
O. Aalen, O. Borgan, and H. Gjessing. Survival and event history analysis: a process point of view. Springer, 2008.
L. Baltrunas and X. Amatriain. Towards time-dependant recommendation based on implicit feedback, 2009.
E. C. Chi and T. G. Kolda. On tensors, sparsity, and nonnegative factorizations, 2012.
D. Cox and V. Isham. Point processes, volume 12. Chapman & Hall/CRC, 1980.
D. Cox and P. Lewis. Multivariate point processes. Selected Statistical Papers of Sir David Cox: Volume 1, Design of Investigations, Statistical Methods and Applications, 1:159, 2006.
D. Daley and D. Vere-Jones. An introduction to the theory of point processes: volume II: general theory and structure, volume 2. Springer, 2007.
N. Du, M. Farajtabar, A. Ahmed, A. J. Smola, and L. Song. Dirichlet-hawkes processes with applications to clustering continuous-time document streams. In KDD15, 2015.
N. Du, L. Song, A. Smola, and M. Yuan. Learning networks of heterogeneous influence. In Advances in Neural Information Processing Systems 25, pages 27892797, 2012.
N. Du, L. Song, H. Woo, and H. Zha. Uncover topic-sensitive information diffusion networks. In Artificial Intelligence and Statistics (AISTATS), 2013.
A. G. Hawkes. Spectra of some self-exciting and mutually exciting point processes. Biometrika, 58(1):8390, 1971.
K. Kapoor, K. Subbian, J. Srivastava, and P. Schrater. Just in time recommendations: Modeling the dynamics of boredom in activity streams. WSDM, pages 233242, 2015.
K. Kapoor, M. Sun, J. Srivastava, and T. Ye. A hazard based approach to user return time prediction. In KDD14, pages 17191728, 2014.
A. Karatzoglou, X. Amatriain, L. Baltrunas, and N. Oliver. Multiverse recommendation: N-dimensional tensor factorization for context-aware collaborative filtering. In Proceeedings of the 4th ACM Conference on Recommender Systems (RecSys), 2010.
J. Kingman. On doubly stochastic poisson processes. Mathematical Proceedings of the Cambridge Philosophical Society, pages 923930, 1964.
N. Koenigstein, G. Dror, and Y. Koren. Yahoo! music recommendations: Modeling music ratings with temporal dynamics and item taxonomy. In Proceedings of the Fifth ACM Conference on Recommender Systems, RecSys 11, pages 165172, 2011.
Y. Koren. Collaborative filtering with temporal dynamics. In Knowledge discovery and data mining KDD, pages 447456, 2009.
G. Lan. An optimal method for stochastic composite optimization. Mathematical Programming, 2012.
G. Lan. The complexity of large-scale convex programming under a linear optimization oracle. arXiv preprint arxiv:1309.5550v2, 2014.
Y. Ogata. On lewis simulation method for point processes. Information Theory, IEEE Transactions on, 27(1):2331, 1981.
H. Ouyang, N. He, L. Q. Tran, and A. Gray. Stochastic alternating direction method of multipliers. In ICML, 2013.
J. Z. J. L. Preeti Bhargava, Thomas Phan. Who, what, when, and where: Multi-dimensional collaborative recommendations using tensor factorization on sparse user-generated data. In WWW, 2015.
Y. Wang, R. Chen, J. Ghosh, J. Denny, A. Kho, Y. Chen, B. Malin, and J. Sun. Rubik: Knowledge guided tensor factorization and completion for health data analytics. In KDD, 2015.
S. Rendle. Time-Variant Factorization Models Context-Aware Ranking with Factorization Models. volume 330 of Studies in Computational Intelligence, chapter 9, pages 137153. 2011.
S. Sastry. Some np-complete problems in linear algebra. Honors Projects, 1990.
L. Xiong, X. Chen, T.-K. Huang, J. G. Schneider, and J. G. Carbonell. Temporal collaborative filtering with bayesian probabilistic tensor factorization. In SDM, pages 211222. SIAM, 2010.
X. Yi, L. Hong, E. Zhong, N. N. Liu, and S. Rajan. Beyond clicks: Dwell time for personalization. In Proceedings of the 8th ACM Conference on Recommender Systems, RecSys 14, pages 113120, 2014.
A. W. Yu, W. Ma, Y. Yu, J. G. Carbonell, and S. Sra. Efficient structured matrix rank minimization. In NIPS, 2014.
A. N. Zaid Harchaoui, Anatoli Juditsky. Conditional gradient algorithms for norm-regularized smooth convex optimization. Mathematical Programming, 2013.
K. Zhou, H. Zha, and L. Song. Learning social infectivity in sparse low-rank networks using multidimensional hawkes processes. In Artificial Intelligence and Statistics (AISTATS), 2013.
K. Zhou, H. Zha, and L. Song. Learning triggering kernels for multi-dimensional hawkes processes. In International Conference on Machine Learning (ICML), 2013.  9
C. E. Rasmussen and C. K. I. Williams, Gaussian Processes for Machine Learning. The MIT Press, 2006. R in Machine Learning, vol. 2,
Y. Bengio, Learning deep architectures for AI, Foundations and trends no. 1, pp. 1127, 2009.
D. J. C. MacKay, Introduction to Gaussian processes, in Neural Networks and Machine Learning (C. M. Bishop, ed.), NATO ASI Series, pp. 133166, Kluwer Academic Press, 1998.
A. G. Wilson and R. P. Adams, Gaussian process kernels for pattern discovery and extrapolation, in Proc. of International Conference on Machine Learning, 2013.
D. Duvenaud, J. R. Lloyd, R. Grosse, J. B. Tenenbaum, and Z. Ghahramani, Structure discovery in nonparametric regression through compositional kernel search, in Proc. of International Conference on Machine Learning, pp. 11661174, 2013.
D. Duvenaud, H. Nickisch, and C. E. Rasmussen, Additive Gaussian processes, in Advances in Neural Information Processing Systems 24, pp. 226234, 2011.
M. Gonen and E. Alpaydin, Multiple kernel learning algorithms, The Journal of Machine Learning Research, vol. 12, pp. 22112268, 2011.
F. Tobar, S.-Y. Kung, and D. Mandic, Multikernel least mean square algorithm, IEEE Trans. on Neural Networks and Learning Systems, vol. 25, no. 2, pp. 265277, 2014.
R. E. Turner, Statistical Models for Natural Sounds. PhD thesis, Gatsby Computational Neuroscience Unit, UCL, 2010.
R. Turner and M. Sahani, Time-frequency analysis as probabilistic inference, IEEE Trans. on Signal Processing, vol. 62, no. 23, pp. 61716183, 2014.
B. Oksendal, Stochastic Differential Equations. Springer, 2003.
A. V. Oppenheim and A. S. Willsky, Signals and Systems. Prentice-Hall, 1997.
C. Archambeau, D. Cornford, M. Opper, and J. Shawe-Taylor, Gaussian process approximations of stochastic differential equations, Journal of Machine Learning Research Workshop and Conference Proceedings, vol. 1, pp. 116, 2007.
S. F. Gull, Developments in maximum entropy data analysis, in Maximum Entropy and Bayesian Methods (J. Skilling, ed.), vol. 36, pp. 5371, Springer Netherlands, 1989.
B. Scholkopf and A. J. Smola, Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press, 2001.
T. P. Minka, Deriving quadrature rules from Gaussian processes, tech. rep., Statistics Department, Carnegie Mellon University, 2000.
A. H. Jazwinski, Stochastic Processes and Filtering Theory. New York, Academic Press., 1970.
M. K. Titsias, Variational learning of inducing variables in sparse Gaussian processes, in Proc. of International Conference on Artificial Intelligence and Statistics, pp. 567574, 2009.
A. Figueiras-Vidal and M. Lazaro-Gredilla, Inter-domain Gaussian processes for sparse inference using inducing features, in Advances in Neural Information Processing Systems, pp. 10871095, 2009.
A. G. d. G. Matthews, J. Hensman, R. E. Turner, and Z. Ghahramani, On sparse variational methods and the Kullback-Leibler divergence between stochastic processes, arXiv preprint arXiv:1504.07027, 2015.
D. J. C. MacKay, Information Theory, Inference, and Learning Algorithms. Cambridge University Press, 2003.
M. K. Titsias and N. D. Lawrence, Bayesian Gaussian process latent variable model, in Proc. of International Conference on Artificial Intelligence and Statistics, pp. 844851, 2010.
R. E. Turner and M. Sahani, Two problems with variational expectation maximisation for time-series models, in Bayesian time series models (D. Barber, T. Cemgil, and S. Chiappa, eds.), ch. 5, pp. 109130, Cambridge University Press, 2011.
Y. Qi, T. Minka, and R. W. Picara, Bayesian spectrum estimation of unevenly sampled nonstationary data, in Proc. of IEEE ICASSP, vol. 2, pp. II1473II1476, 2002.
D. B. Percival and A. T. Walden, Spectral Analysis for Physical Applications. Cambridge University Press, 1993. Cambridge Books Online.
T. D. Bui and R. E. Turner, Tree-structured Gaussian process approximations, in Advances in Neural Information Processing Systems 27, pp. 22132221, 2014.  9
Jacob Abernethy, Yiling Chen, and Jennifer Wortman Vaughan. Efficient market making via convex optimization, and a connection to online learning. ACM Transactions on Economics and Computation, 1(2), May 2013.
Jacob Abernethy, Sindhu Kutty, Sebastien Lahaie, and Rahul Sami. Information aggregation in exponential family markets. In Proceedings of the fifteenth ACM conference on Economics and computation, pages 395412. ACM, 2014.
Jacob D Abernethy and Rafael M Frongillo. A collaborative mechanism for crowdsourcing prediction problems. In Advances in Neural Information Processing Systems, pages 26002608, 2011.
Stephane Canu and Alex Smola. Kernel methods and the exponential family. Neurocomputing, 69(7):714 720, 2006.
T-H Hubert Chan, Elaine Shi, and Dawn Song. Private and continual release of statistics. ACM Transactions on Information and System Security (TISSEC), 14(3):26, 2011.
Y. Chen and J.W. Vaughan. A new understanding of prediction markets via no-regret learning. In Proceedings of the 11th ACM Conference on Electronic Commerce (EC), pages 189198, 2010.
Yiling Chen, Ian Kash, Mike Ruberry, and Victor Shnayder. Decision markets with good incentives. In Internet and Network Economics, pages 7283. Springer, 2011.
Yiling Chen and David M. Pennock. A utility framework for bounded-loss market makers. In In Proceedings of the 23rd Conference on Uncertainty in Artificial Intelligence (UAI), pages 4956, 2007.
Cynthia Dwork, Moni Naor, Toniann Pitassi, and Guy N Rothblum. Differential privacy under continual observation. In Proceedings of the forty-second ACM symposium on Theory of computing, pages 715724. ACM, 2010.
Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations and Trends in Theoretical Computer Science, 2014.
Rob Hall, Alessandro Rinaldo, and Larry Wasserman. Differential privacy for functions and functional data. The Journal of Machine Learning Research, 14(1):703727, 2013.
R Hanson. Decision markets. Entrepreneurial Economics: Bright Ideas from the Dismal Science, pages 7985, 2002.
R. Hanson. Combinatorial information market design. Information Systems Frontiers, 5(1):105119, 2003.
R. Hanson. Logarithmic market scoring rules for modular combinatorial information aggregation. Journal of Prediction Markets, 1(1):315, 2007.
Abraham Othman and Tuomas Sandholm. Automated market makers that enable new settings: extending constant-utility cost functions. In Proceedings of the Second Conference on Auctions, Market Mechanisms and their Applications (AMMA), pages 1930, 2011.
David M. Pennock and Rahul Sami. Computational aspects of prediction markets. In Noam Nisan, Tim Roughgarden, Eva Tardos, and Vijay V. Vazirani, editors, Algorithmic Game Theory, chapter 26. Cambridge University Press, 2007.
Bernhard Scholkopf and Alexander J Smola. Learning with kernels: Support vector machines, regularization, optimization, and beyond. MIT press, 2002.
Amos J. Storkey. Machine learning markets. In Proceedings of AI and Statistics (AISTATS), pages 716 724, 2011.
J. Wolfers and E. Zitzewitz. Prediction markets. Journal of Economic Perspectives, 18(2):107126, 2004.
Justin Wolfers and Eric Zitzewitz. Interpreting prediction market prices as probabilities. Technical report, National Bureau of Economic Research, 2006.
Erik Zawadzki and Sebastien Lahaie. Nonparametric scoring rules. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.
Lijun Zhang, Rong Jin, Chun Chen, Jiajun Bu, and Xiaofei He. Efficient online learning for large-scale sparse kernel logistic regression. In AAAI, 2012.
Lijun Zhang, Jinfeng Yi, Rong Jin, Ming Lin, and Xiaofei He. Online kernel learning with a near optimal sparsity bound. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages 621629, 2013.  9
Udi Apsel, Kristian Kersting, and Martin Mladenov. Lifting relational map-lps using cluster signatures. In Proc. of AAAI-14, pages 24032409, 2014.
H. Bui, T. Huynh, and S. Riedel. Automorphism groups of graphical models and lifted variational inference. In Proc. of UAI-13, pages 132141, 2013.
R. de Salvo Braz, E. Amir, and D. Roth. Lifted first-order probabilistic inference. In Proc. of IJCAI-05, pages 13191325, 2005.
R. de Salvo Braz, E. Amir, and D. Roth. Lifted first-order probabilistic inference. In L. Getoor and B. Taskar, editors, Introduction to Statistical Relational Learning. MIT Press, 2007.
P. Domingos and D. Lowd. Markov Logic: An Interface Layer for Artificial Intelligence. Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers, 2009.
G. Van den Broeck. On the completeness of first-order knowledge compilation for lifted probabilistic inference. In Proc. of NIPS-11, pages 13861394, 2011.
G. Van den Broeck. Lifted Inference and Learning in Statistical Relational Models. PhD thesis, KU Leuven, 2013.
G. Van den Broeck. On the complexity and approximation of binary evidence in lifted inference. In Proc. of NIPS-13, 2013.
G. Van den Broeck and J. Davis. Conditioning in firsr-order knowledge compilation and lifted probabilistic inference. In Proc. of AAAI-12, 2012.
G. Van den Broeck, N. Taghipour, W. Meert, J. Davis, and L. De Raedt. Lifted probabilistic inference by first-order knowledge compilation. In Proc. of IJCAI-11, 2011.
V. Gogate and P. Domingos. Probabilisitic theorem proving. In Proc. of UAI-11, pages 256265, 2011.
V. Gogate, A. Jha, and D. Venugopal. Advances in lifted importance sampling. In Proc. of AAAI-12, pages 19101916, 2012.
A. K. Jha, V. Gogate, A. Meliou, and D. Suciu. Lifted inference seen from the other side : The tractable features. In Proc. of NIPS-10, pages 973981, 2010.
K. Kersting, B. Ahmadi, and S. Natarajan. Counting belief propagation. In Proc. of UAI-09, pages 277284, 2009.
J. Kisynski and D. Poole. Constraint processing in lifted probabilistic inference. In Proc. of UAI-09, 2009.
L. Mihalkova and R. Mooney. Bottom-up learning of Markov logic network structure. In Proceedings of the Twenty-Forth International Conference on Machine Learning, pages 625632, 2007.
B. Milch, L. S. Zettlemoyer, K. Kersting, M. Haimes, and L. P. Kaebling. Lifted probabilistic inference with counting formulas. In Proc. of AAAI-08, 2008.
H. Mittal, P. Goyal, V. Gogate, and P. Singla. New rules for domain independent lifted MAP inference. In Proc. of NIPS-14, pages 649657, 2014.
M. Mladenov, A. Globerson, and K. Kersting. Lifted message passing as reparametrization of graphical models. In Proc. of UAI-14, pages 603612, 2014.
M. Mladenov and K. Kersting. Equitable partitions of concave free energies. In Proc. of UAI-15, 2015.
D. Poole. First-order probabilistic inference. In Proc. of IJCAI-03, pages 985991, 2003.
S. J. Russell and P. Norvig. Artificial Intelligence - A Modern Approach (3rd edition). Pearson Education, 2010.
P. Singla and P. Domingos. Lifted first-order belief propagation. In Proc. of AAAI-08, pages 10941099, 2008.
P. Singla, A. Nath, and P. Domingos. Approximate lifted belief propagation. In Proc. of AAAI-14, pages 24972504, 2014.
N. Taghipour, D. Fierens, J. Davis, and H. Blockeel. Lifted variable elimination with arbitrary constraints. In Proc. of AISTATS-12, Canary Islands, Spain, 2012.
D. Venugopal and V. Gogate. On lifting the Gibbs sampling algorithm. In Proc. of NIPS-12, pages 16641672, 2012.  9
J. Baxter and P. L. Bartlett. Infinite-horizon policy-gradient estimation. Journal of Artificial Intelligence Research, pages 319350, 2001.
Y. Bengio, N. Leonard, and A. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
M. C. Fu. Gradient estimation. Handbooks in operations research and management science, 13:575616, 2006.
P. Glasserman. Monte Carlo methods in financial engineering, volume 53. Springer Science & Business Media, 2003.
P. W. Glynn. Likelihood ratio gradient estimation for stochastic systems. Communications of the ACM, 33(10):7584, 1990.
E. Greensmith, P. L. Bartlett, and J. Baxter. Variance reduction techniques for gradient estimates in reinforcement learning. The Journal of Machine Learning Research, 5:14711530, 2004.
K. Gregor, I. Danihelka, A. Mnih, C. Blundell, and D. Wierstra. Deep autoregressive networks. arXiv preprint arXiv:1310.8499, 2013.
A. Griewank and A. Walther. Evaluating derivatives: principles and techniques of algorithmic differentiation. Siam, 2008.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):17351780, 1997.
D. P. Kingma and M. Welling. Auto-encoding variational Bayes. arXiv:1312.6114, 2013.
D. P. Kingma and M. Welling. Efficient gradient-based inference through transformations between bayes nets and neural nets. arXiv preprint arXiv:1402.0480, 2014.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, 1998.
J. Martens. Deep learning via Hessian-free optimization. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 735742, 2010.
A. Mnih and K. Gregor. Neural variational inference and learning in belief networks. arXiv:1402.0030, 2014.
V. Mnih, N. Heess, A. Graves, and K. Kavukcuoglu. Recurrent models of visual attention. In Advances in Neural Information Processing Systems, pages 22042212, 2014.
R. Munos. Policy gradient in continuous time. The Journal of Machine Learning Research, 7:771791, 2006.
R. M. Neal. Learning stochastic feedforward networks. Department of Computer Science, University of Toronto, 1990.
R. M. Neal and G. E. Hinton. A view of the em algorithm that justifies incremental, sparse, and other variants. In Learning in graphical models, pages 355368. Springer, 1998.
J. Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Morgan Kaufmann, 2014.
R. Ranganath, S. Gerrish, and D. M. Blei. Black box variational inference. arXiv preprint arXiv:1401.0118, 2013.
D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. arXiv:1401.4082, 2014.
D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller. Deterministic policy gradient algorithms. In ICML, 2014.
R. S. Sutton, D. A. McAllester, S. P. Singh, Y. Mansour, et al. Policy gradient methods for reinforcement learning with function approximation. In NIPS, volume 99, pages 10571063. Citeseer, 1999.
N. Vlassis, M. Toussaint, G. Kontes, and S. Piperidis. Learning model-free robot control by a Monte Carlo EM algorithm. Autonomous Robots, 27(2):123130, 2009.
D. Wierstra, A. Forster, J. Peters, and J. Schmidhuber. Recurrent policy gradients. Logic Journal of IGPL, 18(5):620634, 2010.
R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229256, 1992.
D. Wingate and T. Weber. Automated variational inference in probabilistic programming. arXiv preprint arXiv:1301.1299, 2013.
S. J. Wright and J. Nocedal. Numerical optimization, volume 2. Springer New York, 1999.
W. Zaremba and I. Sutskever. Reinforcement learning neural Turing machines. arXiv preprint arXiv:1505.00521, 2015.  9  A  Proofs  Theorem 1 We will consider the case that all of the random variables are continuous-valued, thus the expectations can be written as integrals. For discrete random variables, the integrals should be changed to sums.  P  Recall that we seek to compute  E cC c . We will differentiate the expectation of a single cost term; summing over these terms yields Equation (6). Z Y p(v | DEPSv )dv c(DEPSc ) (7) EvS,
= vc  vS, vc    EvS,
=  vc  =  Z Y vS, vc  Z Y vS, vc  =  Z Y vS, vc  p(v | DEPSv )dv c(DEPSc ) "  p(v | DEPSv )dv p(v | DEPSv )dv  X vS, vc  (8)    p(v  | DEPSv )  c(DEPSc ) + c(DEPSc ) p(v | DEPSv )   # (9)    X    log p(v | DEPSv ) c(DEPSc ) + c(DEPSc )    vS, vc  (10) = EvS, vc      log p(v | DEPSv )c + c(DEPSc )     (11)  Equation (9) requires that the integrand is differentiable, which is satisfied if all of the PDFs and c(DEPSc ) are differentiable. Equation (6) follows by summing over all costs c  C. Equation (5) follows from rearrangement of the terms in this equation. Theorem 2 It suffices to show that for a particular node v  S, the following expectation (taken over all variables) vanishes     E log p(v | PARENTSv ) b(N ON I NFLUENCED(v)) . (12)  Analogously to N ON I NFLUENCED(v), define I NFLUENCED(v) := {w | w  v}. Note that the nodes can be ordered so that N ON I NFLUENCED(v) all come before v in the ordering. Thus, we can write      log p(v | PARENTSv ) b(N ON I NFLUENCED(v)) (13) EN ON I NFLUENCED(v) EI NFLUENCED(v)       = EN ON I NFLUENCED(v) EI NFLUENCED(v) log p(v | PARENTSv ) b(N ON I NFLUENCED(v))  (14) = EN ON I NFLUENCED(v)
(15) =0  where we used EI NFLUENCED(v)  B  (16)         log p(v | PARENTSv ) = Ev     log p(v | PARENTSv )    = 0.  Surrogate as an Upper Bound, and MM Algorithms  L has additional significance besides allowing us to estimate the gradient of the expected sum of costs. Under certain conditions, L is a upper bound on on the true objective (plus a constant). We shall make two restrictions on the stochastic computation graph: (1) first, that all costs c  C are negative. (2) the the costs are not deterministically influenced by the parameters . First, let 10  us use importance sampling to write down the expectation of a given cost node, when the sampling distribution is different from the distribution we are evaluating: for parameter   ,  = old is used for sampling, but we are evaluating at  = new .    Y Pv (v | DEPSv \, new )   Evc | new
= Evc | old  c P (v | DEPS v \, old )  v vc,  (17)  D v              Y Pv (v | DEPSv \, new )      + 1   Evc | old  clog   P (v | DEPS \,  ) v v old vc,  (18)  D v  where the second line used the inequality x  log x + 1, and the sign is reversed since c is negative. Summing over c  C and rearranging we get " # " # X X X  p(v | DEPSv \, new )  ES | new c  ES | old c + log Qv (19) p(v | DEPSv \, old ) cC cC vS " # X = ES | old log p(v | DEPSv \, new )Qv + const (20) vS  Equation (20) allows for majorization-minimization algorithms (like the EM algorithm) to be used to optimize with respect to . In fact, similar equations have been derived by interpreting rewards (negative costs) as probabilities, and then taking the variational lower bound on log-probability (e.g.,
).  C C.1  Examples Generalized EM Algorithm and Variational Inference.  The generalized EM algorithm maximizes likelihood in a probabilistic model with latent variables
. Suppose the probabilistic model defines a probability distribution p(x, z; ) where x is observed, z is a latent variable, and  is a parameter of the distribution. The generalized EM algorithm maximizes the variational lower bound, which is defined by an expectation over q:    p(x, z; ) . (21) L(, q) = Ezq log q(z) The generalized EM algorithm can take many different forms, leading to different gradient estimation problems. Neural variational inference.
propose a generalized EM algorithm for multi-layered latent variable models such as sigmoidal belief networks that employs an in- x ference network, an explicit parameterization of q as a function of the observed data x, to allow for fast approximate inference. The generative model and inference network take the form X p (x) = p1 (x|h1 )p2 (h1 |h2 )p3 (h2 |h3 )p3 (h3 ) h1 ,h2  h1  h2  h3  r1  r2  r3  1  2  3  2  3  q (h1 , h2 |x) = q1 (h1 |x)q2 (h2 |h1 )q3 (h3 |h2 ), and thus  1       p (x|h1 ) p (h1 |h2 ) p (h2 |h3 )p3 (h3 )    L(, ) = Ehq log 1 + log 2 + log 3 . q1 (h1 |x) q2 (h2 |h1 ) q3 (h3 |h2 )   | {z } | {z } | {z } =r1  =r2  =r3  11  Given a sample h  q an unbiased estimate of the gradient is obtained L     log p1 (x|h1 ) + log p2 (h1 |h2 ) + log p3 (h2 ) (22)     L     log q1 (h1 |x)(Q1  b1 (x)) + log q2 (h2 |h1 )(Q2  b2 (h1 )) + log q3 (h3 |h2 )(Q3  b3 (h2 ))     (23) where Q1 = r1 + r2 + r3 ; Q2 = r2 + r3 ; and Q3 = r3 . Eq. (22) uses the PD estimator to estimate the gradientwith respect to the model parameters ; eq. (23) is an application of the SF estimator to the gradient with respect to the parameters  of the inference network; b1 , b2 , b3 are baselines. Variational Autoencoder, Deep Latent Gaussian Models and Reparameterization.
consider a similar formulation to
but have continuous latent variables and can thus re-parameterize their inference network to enable the use of the PD estimator: x   p (x|h)p (h) (24) Lorig (, ) = Ehq log q (h|x) Lre (, ) = E
(25) where the second term, the entropy of q can be computed analytically for the parametric forms of q considered in the paper (Gaussians). For q being conditionally Gaussian, i.e. q (h|x) = N (h| (x),  (x)) reparameterizing leads e.g. to h = h (; x) =  (x) +  (x).      z  h1    x  h2  Reparameterization        h1  z  h2  Given    an estimate of the gradient is obtained as  L  x  L   Lre 
,    Lre   h   log p (x|h (, x)) + log p (h (, x)) + H
 h h   C.2  x  (26) (27)  Policy Gradients in Reinforcement Learning.  In reinforcement learning, an agent interacts with an environment according to its policy  and receives a reward. The goal is to maximize the expected sum of rewards, the return, under the trajectory distribution that is specified jointly by the environment dynamics and the policy. Policy gradient methods seek to directly estimate the gradient of expected return with respect to the policy parameters
. The RL case is especially interesting as we typically assume that the environment dynamics are not available analytically and can only be sampled, which has implication for gradient estimation. Below we distinguish two important cases: Markov decision processes (MDP) and partially observable Markov decision processes (POMDP). MDPs: In the MDP case, the expectation is taken with respect to the distribution over state (s) and action (a) sequences " T # X L() = E p r(st , at ) , (28)   s1  s2  ...  sT  a1  a2  ...  aT  r1  r2  ...  rT  t=1  where  = (s1 , a1 , s2 , a2 , . . . ) are trajectories and the distribution over trajectories is defined in terms of the environment dynamics Q pE (st+1 |st , at ) and the policy  : p ( ) = pE (s1 ) t  (at |st )pE (st+1 |st , at ). r are rewards (negative costs in the terminology of the rest of 12  the paper). The classic REINFORCE
estimate of the gradient is given by " T !# T X X   L = E p log  (at |st ) r(st0 , at0 )  bt (st ) , (29)   t=1 t0 =t hP i T 0 0 where bt (st ) is an arbitrary baseline which is often chosen to be Vt (st ) = E p t0 =t r(st , at ) , i.e. the well-known state-value function. (Equation (29)) corresponds to an application of the SF estimator at the stochastic nodes at . It is worth noting that a Monte Carlo estimate of (Equation (29)) only requires simulating from the environment by running trajectories forward according to the current policy. This is due to the property of the SF estimator which only requires evaluation (sampling in the stochastic case) of the nodes downstream of the stochastic node at POMDPs. POMDPs differ from MDPs in that the state st of the environment is not observed directly but, as in latent-variable time series models, only through stochastic observations ot , which depend on the latent states st via pE (ot |st ). The policy therefore has to be a function of the history of past observations  (at |o1 . . . ot ). For instance it can take the form of a recurrent neural network (RNN)
. A REINFORCE gradient estimate is then given by   s1  s2  ...  sT  o1  o2  ...  oT  m1  m2  ...  mT  (30)  a1  a2  ...  aT   log  at Note that, at each time step t, the gradient  the stochastic node at is estimated using the SF estimator, and then backpropagated in the RNN via chain-rule in the usual manner. As before, bt is a baseline, which is written here as a function of the observation history up to time t and, as the policy, which can be parameterized through another RNN.  r1  r2  ...  rT  T  hX   L = E p log  (at |o1 . . . ot ))   t=1 ! T i X r(st0 , at0 )  bt (o1 . . . ot ) . t0 =t  13
N. Hansen, S.D. Muller, and P. Koumoutsakos. Reducing the Time Complexity of the Derandomized Evolution Strategy with Covariance Matrix Adaptation (CMA-ES). Evolutionary Computation, 2003.
Y. Sun, D. Wierstra, T. Schaul, and J. Schmidhuber. Efficient Natural Evolution Strategies. In Proceedings of the 11th Annual conference on Genetic and evolutionary computation(GECCO), 2009.
F. Stulp and O. Sigaud. Path Integral Policy Improvement with Covariance Matrix Adaptation. In International Conference on Machine Learning (ICML), 2012.
T. Ruckstie, M. Felder, and J. Schmidhuber. State-dependent Exploration for Policy Gradient Methods. In Proceedings of the European Conference on Machine Learning (ECML), 2008.
T. Furmston and D. Barber. A Unifying Perspective of Parametric Policy Search Methods for Markov Decision Processes. In Neural Information Processing Systems (NIPS), 2012.
I. Loshchilov, M. Schoenauer, and M. Sebag. Intensive Surrogate Model Exploitation in SelfAdaptive Surrogate-Assisted CMA-ES (SAACM-ES). In GECCO, 2013.
S. Mannor, R. Rubinstein, and Y. Gat. The Cross Entropy method for Fast Policy Search. In Proceedings of the 20th International Conference on Machine Learning(ICML), 2003.
E. Theodorou, J. Buchli, and S. Schaal. A Generalized Path Integral Control Approach to Reinforcement Learning. The Journal of Machine Learning Research, 2010.
A. Kupcsik, M. P. Deisenroth, J. Peters, and G. Neumann. Data-Efficient Contextual Policy Search for Robot Movement Skills. In Proceedings of the National Conference on Artificial Intelligence (AAAI), 2013.
J. Peters, K. Mulling, and Y. Altun. Relative Entropy Policy Search. In Proceedings of the 24th National Conference on Artificial Intelligence (AAAI). AAAI Press, 2010.
D. Wierstra, T. Schaul, T. Glasmachers, Y. Sun, J. Peters, and J. Schmidhuber. Natural Evolution Strategies. Journal of Machine Learning Research, 2014.
S. Amari. Natural Gradient Works Efficiently in Learning. Neural Computation, 1998.
M.J.D. Powell. The NEWUOA Software for Unconstrained Optimization without Derivatives. In Report DAMTP 2004/NA05, University of Cambridge, 2004.
M.J.D. Powell. The BOBYQA Algorithm for Bound Constrained Optimization Without Derivatives. In Report DAMTP 2009/NA06, University of Cambridge, 2009.
S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
I.T. Jolliffe. Principal Component Analysis. Springer Verlag, 1986.
G. Mehmet. Bayesian Supervised Dimensionality Reduction. IEEE T. Cybernetics, 2013.
I. Murray, R.P. Adams, and D.J.C. MacKay. Elliptical Slice Sampling. JMLR: W&CP, 9, 2010.
J. Kober and J. Peters. Policy Search for Motor Primitives in Robotics. Machine Learning, pages 133, 2010.
M. Molga and C. Smutnicki. Test Functions for Optimization Needs. http://www.zsd.ict.pwr.wroc.pl/files/docs/functions.pdf, 2005.  In
A. Ijspeert and S. Schaal. Learning Attractor Landscapes for Learning Motor Primitives. In Advances in Neural Information Processing Systems 15(NIPS). 2003.  9
Harri Valpola. From neural PCA to deep unsupervised learning. In Adv. in Independent Component Analysis and Learning Machines, pages 143171. Elsevier, 2015. arXiv:1411.7783.
Steven C Suddarth and YL Kergosien. Rule-injection hints as a means of improving network performance and learning time. In Proceedings of the EURASIP Workshop 1990 on Neural Networks, pages 120129. Springer, 1990.
Marc Aurelio Ranzato and Martin Szummer. Semi-supervised learning of compact document representations with deep networks. In Proc. of ICML 2008, pages 792799. ACM, 2008.
Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with convolutional neural networks. In Advances in Neural Information Processing Systems 27 (NIPS 2014), pages 766774, 2014.
Ian Goodfellow, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Multi-prediction deep Boltzmann machines. In Advances in Neural Information Processing Systems 26 (NIPS 2013), pages 548556, 2013.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504507, 2006.
Antti Rasmus, Tapani Raiko, and Harri Valpola. Denoising autoencoder with modulated lateral connections learns invariant representations of natural images. arXiv:1412.7210, 2015.
Antti Rasmus, Harri Valpola, Mikko Honkala, Mathias Berglund, and Tapani Raiko. Semi-supervised learning with ladder networks. arXiv preprint arXiv:1507.02672, 2015.
Yoshua Bengio, Li Yao, Guillaume Alain, and Pascal Vincent. Generalized denoising auto-encoders as generative models. In Advances in Neural Information Processing Systems 26 (NIPS 2013), pages 899 907. 2013.  8
Jocelyn Sietsma and Robert JF Dow. Creating artificial neural networks that generalize. Neural networks, 4(1):6779, 1991.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. JMLR, 11:33713408, 2010.
Jaakko Sarela and Harri Valpola. Denoising source separation. JMLR, 6:233272, 2005.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning (ICML), pages 448456, 2015.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In the International Conference on Learning Representations (ICLR 2015), San Diego, 2015. arXiv:1412.6980.
Jason Weston, Frederic Ratle, Hossein Mobahi, and Ronan Collobert. Deep learning via semi-supervised embedding. In Neural Networks: Tricks of the Trade, pages 639655. Springer, 2012.
Salah Rifai, Yann N Dauphin, Pascal Vincent, Yoshua Bengio, and Xavier Muller. The manifold tangent classifier. In Advances in Neural Information Processing Systems 24 (NIPS 2011), pages 22942302, 2011.
Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In Workshop on Challenges in Representation Learning, ICML 2013, 2013.
Nikolaos Pitelis, Chris Russell, and Lourdes Agapito. Semi-supervised learning using an unsupervised atlas. In Machine Learning and Knowledge Discovery in Databases (ECML PKDD 2014), pages 565 580. Springer, 2014.
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep generative models. In Advances in Neural Information Processing Systems 27 (NIPS 2014), pages 35813589, 2014.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. JMLR, 15(1):19291958, 2014.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In the International Conference on Learning Representations (ICLR 2015), 2015. arXiv:1412.6572.
Takeru Miyato, Shin ichi Maeda, Masanori Koyama, Ken Nakae, and Shin Ishii. Distributional smoothing by virtual adversarial examples. arXiv:1507.00677, 2015.
Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin A. Riedmiller. Striving for simplicity: The all convolutional net. arxiv:1412.6806, 2014.
Junbo Zhao, Michael Mathieu, Ross Goroshin, and Yann Lecun. Stacked what-where auto-encoders. 2015. arXiv:1506.02351.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv:1502.03167, 2015.
Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout networks. In Proc. of ICML 2013, 2013.
Ian Goodfellow, Yoshua Bengio, and Aaron C Courville. Large-scale feature learning with spike-and-slab sparse coding. In Proc. of ICML 2012, pages 14391446, 2012.
G. McLachlan. Iterative reclassification procedure for constructing an asymptotically optimal rule of allocation in discriminant analysis. J. American Statistical Association, 70:365369, 1975.
D. Titterington, A. Smith, and U. Makov. Statistical analysis of finite mixture distributions. In Wiley Series in Probability and Mathematical Statistics. Wiley, 1985.
Martin Szummer and Tommi Jaakkola. Partially labeled classification with Markov random walks. Advances in Neural Information Processing Systems 15 (NIPS 2002), 14:945952, 2003.
Matthew D Zeiler, Graham W Taylor, and Rob Fergus. Adaptive deconvolutional networks for mid and high level feature learning. In ICCV 2011, pages 20182025. IEEE, 2011.
Frederic Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud Bergeron, Nicolas Bouchard, and Yoshua Bengio. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.
Bart van Merrienboer, Dzmitry Bahdanau, Vincent Dumoulin, Dmitriy Serdyuk, David Warde-Farley, Jan Chorowski, and Yoshua Bengio. Blocks and fuel: Frameworks for deep learning. CoRR, abs/1506.00619, 2015. URL http://arxiv.org/abs/1506.00619.  9
G. Bakir, T. Hofmann, B. Scholkopf, A. Smola, B. Taskar, and S. Vishwanathan. Predicting Structured Data. MIT Press, 2007.
W. Bi and J. Kwok. Mandatory leaf node prediction in hierarchical multilabel classification. In Neural Information Processing Systems (NIPS), 2012.
M. Cisse, N. Usunier, T. Artieres, and P. Gallinari. Robust bloom filters for large multilabel classification tasks. In Proceedings of Advances in Neural Information Processing Systems (NIPS), 2013.
H. Daume and J. Langford. Search-based structured prediction. Machine Learning, 75:297325, 2009.
K. Dembczynski, W. Cheng, and E. Hullermeier. Bayes optimal multilabel classification via probabilistic classifier chains. In Proceedings ICML, 2010.
K. Dembczynski, W. Waegeman, W. Cheng, and E. Hullermeier. On label dependence and loss minimization in multi-label classification. Machine Learning, 88(1):545, 2012.
J. Deng, A. Berg, K. Li, and F. Li. What does classifying more than 10,000 image categories tell us? In Proceedings of the European Conference on Computer Vision (ECCV), 2010.
J. Deng, N. Ding, Y. Jia, A. Frome, K. Murphy, S. Bengio, Y. Li, H. Neven, and H. Adam. Large-scale object classification using label relation graphs. In Proceedings ECCV, 2014.
Y. Guo and D. Schuurmans. Adaptive large margin training for multilabel classification. In AAAI, 2011.
B. Haeffele, R. Vidal, and E. Young. Structured low-rank matrix factorization: Optimality, algorithm, and applications to image processing. In International Conference on Machine Learning (ICML), 2014.
B. Hariharan, S.V.N. Vishwanathan, and M. Varma. Efficient max-margin multi-label classification with applications to zero-shot learning. Machine Learning, 88:127155, 2012.
J. Jancsary, S. Nowozin, and C. Rother. Learning convex QP relaxations for structured prediction. In Proceedings of the International Conference on Machine Learning (ICML), 2013.
T. Joachims. Transductive inference for text classification using support vector machines. In ICML, 1999.
M. Journee, F. Bach, P. Absil, and R. Sepulchre. Low-rank optimization on the cone of positive semidefinite matrices. SIAM Journal on Optimization, 20(5):23272351, 2010.
H. Kadri, M. Ghavamzadeh, and P. Preux. A generalized kernel approach to structured output learning. In Proceedings of the International Conference on Machine Learning (ICML), 2013.
A. Kae, K. Sohn, H. Lee, and E. Learned-Miller. Augmenting CRFs with Boltzmann machine shape priors for image labeling. In Proceedings CVPR, 2013.
A. Kapoor, P. Jain, and R. Vishwanathan. Multilabel classification using Bayesian compressed sensing. In Proceedings of Advances in Neural Information Processing Systems (NIPS), 2012.
B. Klimt and Y. Yang. The Enron corpus: A new dataset for email classification. In ECML, 2004.
J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In International Conference on Machine Learning (ICML), 2001.
D. Lewis, Y. Yang, T. Rose, and F. Li. RCV1: A new benchmark collection for text categorization research. Journal of Machine Learning Research, 5:361397, 2004.
Q. Li, J. Wang, D. Wipf, and Z. Tu. Fixed-point model for structured prediction. In Proceedings of the International Conference on Machine Learning (ICML), 2013.
Z. Lin, G. Ding, M. Hu, and J. Wang. Multi-label classification via feature-aware implicit label space encoding. In Proceedings of the International Conference on Machine Learning (ICML), 2014.
M. Makela. Multiobjective proximal bundle method for nonconvex nonsmooth optimization: Fortran subroutine MPBNGC 2.0. Technical report, U. of Jyvaskyka, 2003.
F. Mirzazadeh, Y. Guo, and D. Schuurmans. Convex co-embedding. In Proceedings AAAI, 2014.
J. Rousu, C. Saunders, S. Szedmak, and J. Shawe-Taylor. Kernel-based learning of hierarchical multilabel classification models. Journal of Machine Learning Research, 7:16011626, 2006.
V. Srikumar and C. Manning. Learning distributed representations for structured output prediction. In Proceedings of Advances in Neural Information Processing Systems (NIPS), 2014.
X. Sun. Structure regularization for structured prediction. In Proceedings NIPS, 2014.
B. Taskar. Learning structured prediction models: A large margin approach. PhD thesis, Stanford, 2004.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Large margin methods for structured and interdependent output variables. Journal of Machine Learning Research, 6:14531484, 2005.
G. Tsoumakas, I. Katakis, and I. Vlahavas. Mining multi-label data. In Data Mining and Knowledge Discovery Handbook, 2nd edition. Springer, 2009.
K. Weinberger and O. Chapelle. Large margin taxonomy embedding for document categorization. In Neural Information Processing Systems (NIPS), 2008.
J. Weston, S. Bengio, and N. Usunier. WSABIE: scaling up to large vocabulary image annotation. In International Joint Conference on Artificial Intelligence (IJCAI), 2011.  9
Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1).
Dissmann, J., Brechmann, E. C., Czado, C., and Kurowicka, D. (2012). Selecting and estimating regular vine copulae and application to financial returns. arXiv preprint arXiv:1202.2002.
Frchet, M. (1960). Les tableaux dont les marges sont donnes. Trabajos de estadstica, 11(1):318.
Genest, C., Gerber, H. U., Goovaerts, M. J., and Laeven, R. (2009). Editorial to the special issue on modeling and measurement of multivariate risk in insurance and finance. Insurance: Mathematics and Economics, 44(2):143145.
Giordano, R., Broderick, T., and Jordan, M. I. (2015). Linear response methods for accurate covariance estimates from mean field variational Bayes. In Neural Information Processing Systems.
Gruber, L. and Czado, C. (2015). Sequential Bayesian model selection of regular vine copulas. International Society for Bayesian Analysis.
Hoff, P. D., Raftery, A. E., and Handcock, M. S. (2001). Latent space approaches to social network analysis. Journal of the American Statistical Association, 97:10901098.
Hoffman, M. D. and Blei, D. M. (2015). Structured stochastic variational inference. In Artificial Intelligence and Statistics.
Hoffman, M. D., Blei, D. M., Wang, C., and Paisley, J. (2013). Stochastic variational inference. Journal of Machine Learning Research, 14:13031347.
Joe, H. (1996). Families of m-variate distributions with given margins and m(m  1)/2 bivariate dependence parameters, pages 120141. Institute of Mathematical Statistics.
Kappen, H. J. and Wiegerinck, W. (2001). Second order approximations for probability models. In Neural Information Processing Systems.
Kingma, D. P. and Ba, J. L. (2015). Adam: A method for stochastic optimization. In International Conference on Learning Representations.
Kurowicka, D. and Cooke, R. M. (2006). Uncertainty Analysis with High Dimensional Dependence Modelling. Wiley, New York.
Nelsen, R. B. (2006). An Introduction to Copulas (Springer Series in Statistics). Springer-Verlag New York, Inc.
Ranganath, R., Gerrish, S., and Blei, D. M. (2014). Black box variational inference. In Artificial Intelligence and Statistics, pages 814822.
Recht, B., Re, C., Wright, S., and Niu, F. (2011). Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 693701.
Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning.
Robbins, H. and Monro, S. (1951). A stochastic approximation method. The Annals of Mathematical Statistics, 22(3):400407.
Saul, L. and Jordan, M. I. (1995). Exploiting tractable substructures in intractable networks. In Neural Information Processing Systems, pages 486492.
Seeger, M. (2010). Gaussian covariance and scalable variational inference. In International Conference on Machine Learning.
Sklar, A. (1959). Fonstions de rpartition  n dimensions et leurs marges. Publications de lInstitut de Statistique de lUniversit de Paris, 8:229231.
Stan Development Team (2015). Stan: A C++ library for probability and sampling, version 2.8.0.
Toulis, P. and Airoldi, E. M. (2014). Implicit stochastic gradient descent. arXiv preprint arXiv:1408.2923.
Tran, D., Toulis, P., and Airoldi, E. M. (2015). Stochastic gradient descent methods for estimation with large data sets. arXiv preprint arXiv:1509.06459.
Wainwright, M. J. and Jordan, M. I. (2008). Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1(1-2):1305.  9
S. Takemura et al. A visual motion detection circuit suggested by Drosophila connectomics. Nature, 500:175-181, 2013.  8
M. Helmstaedter, K. L. Briggman, S. C. Turaga, V. Jain, H. S. Seung, and W. Denk. Connectomic reconstruction of the inner plexiform layer in the mouse retina. Nature, 500:168-174, 2013.
J. S. Kim et al. Space-time wiring specificity supports direction selectivity in the retina. Nature, 509:331336, 2014.
M. Helmstaedter. Cellular-resolution connectomics: challenges of dense neural circuit reconstruction. Nature Methods, 10(6):501-507, 2013.
V. Jain, H. S. Seung, and S. C. Turaga. Machines that learn to segment images: a crucial technology for connectomics. Current Opinion in Neurobiology, 20:653-666, 2010.
T. Tasdizen et al. Image segmentation for connectomics using machine learning. In Computational Intelligence in Biomedical Imaging, pp. 237-278, ed. K. Suzuki, Springer New York, 2014.
K. L. Briggman and D. D. Bock. Volume electron microscopy for neuronal circuit reconstruction. Current Opinion in Neurobiology, 22(1):154-61, 2012.
E. Jurrus et al. Detection of neuron membranes in electron microscopy images using a serial neural network architecture. Medical Image Analysis, 14(6):770-783, 2010.
T. Liu, C. Jones, M. Seyedhosseini, and T. Tasdizen. A modular hierarchical approach to 3D electron microscopy image segmentation. Journal of Neuroscience Methods, 26:88-102, 2014.
Segmentation of neuronal structures in EM stacks challenge - ISBI 2012. http://brainiac2.mit. edu/isbi_challenge/.
D. C. Ciresan, A. Giusti, L. M. Gambardella, and J. Schmidhuber. Deep neural networks segment neuronal membranes in electron microscopy images. In NIPS, 2012.
A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.
K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.
S. C. Turaga et al. Convolutional networks can learn to generate affinity graphs for image segmentation. Neural Computation, 22:511-538, 2010.
G. B. Huang and V. Jain. Deep and wide multiscale recursive networks for robust image labeling. In ICLR, 2014.
M. Seyedhosseini and T. Tasdizen. Multi-class multi-scale series contextual model for image segmentation. Image Processing, IEEE Transactions on, 22(11):4486-4496, 2013.
A. Zlateski, K. Lee, and H. S. Seung. ZNN - A fast and scalable algorithm for training 3D convolutional networks on multi-core and many-core shared memory machines. arXiv:1510.06706, 2015.
D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri. Learning Spatiotemporal Features with 3D Convolutional Networks. arXiv:1412.0767, 2014.
L. Yao, A. Torabi, K. Cho, N. Ballas, C. Pal, H. Larochelle, and A. Courville. Describing Videos by Exploiting Temporal Structure. arXiv:1502.08029, 2015.
P. O. Pinheiro and R. Collobert. Recurrent convolutional neural networks for scene labeling. In ICML, 2014.
Z. Tu. Auto-context and its application to high-level vision tasks. In CVPR, 2008.
M. Mathieu, M. Henaff, and Y. LeCun. Fast training of convolutional networks through FFTs. In ICLR, 2014.
N. Vasilache, J. Johnson, M. Mathieu, S. Chintala, S. Piantino, and Y. LeCun. Fast convolutional nets with fbfft: a GPU performance evaluation. In ICLR, 2015.
J. C. Tapia et al. High-contrast en bloc staining of neuronal tissue for field emission scanning electron microscopy. Nature Protocols, 7(2):193-206, 2012.
N. Kasthuri et al. Saturated reconstruction of a volume of neocortex. Cell 162, 648-61, 2015.
K. J. Hayworth et al. Imaging ATUM ultrathin section libraries with WaferMapper: a multi-scale approach to EM reconstruction of neural circuits. Frontiers in Neural Circuits, 8, 2014.
W. M. Rand. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical Association, 66(336):847-850, 1971.
R. Unnikrishnan, C. Pantofaru, and M. Hebert. Toward objective evaluation of image segmentation algorithms. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 29(6):929-944, 2007.
A. Zlateski and H. S. Seung. Image segmentation by size-dependent single linkage clustering of a watershed basin graph. arXiv:1505.00249, 2015.
J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.
P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. OverFeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014.
A. Giusti, D. C. Ciresan, J. Masci, L. M. Gambardella, and J. Schmidhuber. Fast image scanning with deep max-pooling convolutional neural networks. In ICIP, 2013.
J. Masci, A. Giusti, D. C. Ciresan, G. Fricout, and J. Schmidhuber. A fast learning algorithm for image segmentation with max-pooling convolutional networks. In ICIP, 2013.
M. Chen, Y. Yan, X. Gong, C. D. Gilbert, H. Liang, and W. Li. Incremental integration of global contours through interplay between visual cortical areas. Neuron, 82(3):682-694, 2014.
S. C. Turaga et al. Maximin affinity learning of image segmentation. In NIPS, 2009.  9
S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 2011.
K. Chang and D. Roth. Selective block minimization for faster convergence of limited memory large-scale linear models. In SIGKDD. ACM, 2011.
J. Deng, W. Dong, R. Socher, L. J. Li, K. Li, and L. F. Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.
A. Hoffman. On approximate solutions of systems of linear inequalities. Journal of Research of the National Bureau of Standards, 1952.
M. Hong and Z. Luo. On the linear convergence of the alternating direction method of multipliers, 2012.
C. Hsieh, I. Dhillon, P. Ravikumar, S. Becker, and P. Olsen. Quic & dirty: A quadratic approximation approach for dirty statistical models. In NIPS, 2014.
M. Jaggi, V. Smith, M. Takac, J. Terhorst, S. Krishnan, T. Hofmann, and M. Jordan. Communicationefficient distributed dual coordinate ascent. In NIPS, 2014.
T. Joachims. A support vector method for multivariate performance measures. In ICML, 2005.
S. Kakade, S. Shalev-Shwartz, and A. Tewari. Applications of strong convexitystrong smoothness duality to learning with matrices. CoRR, 2009.
C. Ma, V. Smith, M. Jaggi, M. Jordan, P. Richtarik, and M. Takac. Adding vs. averaging in distributed primal-dual optimization. ICML, 2015.
G. Obozinski, L. Jacob, and J. Vert. Group lasso with overlaps: the latent group lasso approach. arXiv preprint, 2011.
A. Rahimi and B. Recht. Random features for large-scale kernel machines. In NIPS, 2007.
P. Richtarik and M. Takac. Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function. Mathematical Programming, 2014.
S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter. Pegasos: Primal estimated sub-gradient solver for svm. Mathematical programming, 2011.
S. Shalev-Shwartz and A. Tewari. Stochastic methods for l1-regularized loss minimization. JMLR, 2011.
N. Srebro, K. Sridharan, and A. Tewari. On the universality of online mirror descent. In NIPS, 2011.
R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society, 1996.
R. Tomioka, T. Suzuki, and M. Sugiyama. Super-linear convergence of dual augmented lagrangian algorithm for sparsity regularized estimation. JMLR, 2011.
I. Trofimov and A. Genkin. Distributed coordinate descent for l1-regularized logistic regression. arXiv preprint, 2014.
P. Wang and C. Lin. Iteration complexity of feasible descent methods for convex optimization. JMLR, 2014.
I. Yen, C. Chang, T. Lin, S., and S. Lin. Indexed block coordinate descent for large-scale linear classification with limited memory. In SIGKDD. ACM, 2013.
I. Yen, C. Hsieh, P. Ravikumar, and I. Dhillon. Constant nullspace strong convexity and fast convergence of proximal methods under high-dimensional settings. In NIPS, 2014.
I. Yen, T. Lin, S. Lin, P. Ravikumar, and I. Dhillon. Sparse random feature algorithm as coordinate descent in hilbert space. In NIPS, 2014.
I. Yen, X. Lin, K. Zhong, P. Ravikumar, and I. Dhillon. A convex exemplar-based approach to MADBayes dirichlet process mixture models. In ICML, 2015.
I. Yen, K. Zhong, C. Hsieh, P. Ravikumar, and I. Dhillon. Sparse linear programming via primal and dual augmented coordinate descent. In NIPS, 2015.
H. Yu, C. Hsieh, . Chang, and C. Lin. Large linear classification when data cannot fit in memory. SIGKDD, 2010.
G. Yuan, K. Chang, C. Hsieh, and C. Lin. A comparison of optimization methods and software for large-scale L1-regularized linear classification. JMLR, 2010.
K. Zhong, I. Yen, I. Dhillon, and P. Ravikumar. Proximal quasi-Newton for computationally intensive l1-regularized m-estimators. In NIPS, 2014.  9
R. A. Fisher, Statistical Methods for Research Workers. Edinburgh: Oliver and Boyd, 1925. E. Lehmann and J. Romano, Testing statistical hypotheses. Springer Science & Business Media, 2006. E. Fischer, The art of uninformed decisions: A primer to property testing, Science, 2001. R. Rubinfeld, Sublinear-time algorithms, in International Congress of Mathematicians, 2006. C. L. Canonne, A survey on distribution testing: your data is big, but is it blue, ECCC, 2015. T. Batu, R. Kumar, and R. Rubinfeld, Sublinear algorithms for testing monotone and unimodal distributions, in Proceedings of STOC, 2004. A. Bhattacharyya, E. Fischer, R. Rubinfeld, and P. Valiant, Testing monotonicity of distributions over general partial orders, in ICS, 2011, pp. 239252. T. Batu, E. Fischer, L. Fortnow, R. Kumar, R. Rubinfeld, and P. White, Testing random variables for independence and identity, in Proceedings of FOCS, 2001. N. Alon, A. Andoni, T. Kaufman, K. Matulef, R. Rubinfeld, and N. Xie, Testing k-wise and almost k-wise independence, in Proceedings of STOC, 2007. L. Paninski, A coincidence-based test for uniformity given very sparsely sampled discrete data. IEEE Transactions on Information Theory, vol. 54, no. 10, 2008. G. Valiant and P. Valiant, An automatic inequality prover and instance optimal identity testing, in FOCS, 2014. , Estimating the unseen: An n/ log n-sample estimator for entropy and support size, shown optimal via new CLTs, in Proceedings of STOC, 2011. L. Birge, Estimating a density under order restrictions: Nonasymptotic minimax risk, The Annals of Statistics, vol. 15, no. 3, pp. 9951012, September 1987. R. Levi, D. Ron, and R. Rubinfeld, Testing properties of collections of distributions, Theory of Computing, vol. 9, no. 8, pp. 295347, 2013. P. Hall and I. Van Keilegom, Testing for monotone increasing hazard rate, Annals of Statistics, pp. 11091137, 2005. S. O. Chan, I. Diakonikolas, R. A. Servedio, and X. Sun, Learning mixtures of structured distributions over discrete domains, in Proceedings of SODA, 2013. M. Cule and R. Samworth, Theoretical properties of the log-concave maximum likelihood estimator of a multidimensional density, Electronic Journal of Statistics, vol. 4, pp. 254270, 2010. J. Acharya, H. Das, A. Jafarpour, A. Orlitsky, S. Pan, and A. T. Suresh, Competitive classification and closeness testing, in COLT, 2012, pp. 22.122.18. S. Chan, I. Diakonikolas, G. Valiant, and P. Valiant, Optimal algorithms for testing closeness of discrete distributions, in Proceedings of SODA, 2014, pp. 11931203. J. Acharya, A. Jafarpour, A. Orlitsky, and A. Theertha Suresh, A competitive test for uniformity of monotone distributions, in Proceedings of AISTATS, 2013, pp. 5765. R. E. Barlow, D. J. Bartholomew, J. M. Bremner, and H. D. Brunk, Statistical Inference under Order Restrictions. New York: Wiley, 1972. H. K. Jankowski and J. A. Wellner, Estimation of a discrete monotone density, Electronic Journal of Statistics, vol. 3, pp. 15671605, 2009. F. Balabdaoui and J. A. Wellner, Estimation of a k-monotone density: characterizations, consistency and minimax lower bounds, Statistica Neerlandica, vol. 64, no. 1, pp. 4570, 2010. F. Balabdaoui, H. Jankowski, and K. Rufibach, Maximum likelihood estimation and confidence bands for a discrete log-concave distribution, 2011.
. Available: http://arxiv.org/abs/1107.3904v1 A. Saumard and J. A. Wellner, Log-concavity and strong log-concavity: a review, Statistics Surveys, vol. 8, pp. 45114, 2014. M. Adamaszek, A. Czumaj, and C. Sohler, Testing monotone continuous distributions on highdimensional real cubes, in SODA, 2010, pp. 5665. J. N. Rao and A. J. Scott, The analysis of categorical data from complex sample surveys, Journal of the American Statistical Association, vol. 76, no. 374, pp. 221230, 1981. A. Agresti and M. Kateri, Categorical data analysis. Springer, 2011. J. Acharya and C. Daskalakis, Testing Poisson Binomial Distributions, in Proceedings of SODA, 2015, pp. 18291840. C. Canonne, I. Diakonikolas, T. Gouleakis, and R. Rubinfeld, Testing shape restrictions of discrete distributions, arXiv preprint arXiv:1507.03558, 2015. A. L. Gibbs and F. E. Su, On choosing and bounding probability metrics, International Statistical Review, vol. 70, no. 3, pp. 419435, dec 2002. J. Acharya, A. Jafarpour, A. Orlitsky, and A. T. Suresh, Efficient compression of monotone and m-modal distributions, in ISIT, 2014. S. Kamath, A. Orlitsky, D. Pichapati, and A. T. Suresh, On learning distributions from their samples, in COLT, 2015. P. Massart, The tight constant in the Dvoretzky-Kiefer-Wolfowitz inequality, The Annals of Probability, vol. 18, no. 3, pp. 12691283, 07 1990.  9  A  Moments of the Chi-Squared Statistic  We analyze the mean and variance of the statistic Z=  X (Xi  mqi )2  Xi , mqi iA  where each Xi is independently distributed according to Poisson(mpi ). We start with the mean:  (Xi  mqi )2  Xi mqi iA  2 X E Xi  2mqi E
+ m2 qi2  E
= mqi iA  E
=  =  X    E  X m2 p2i + mpi  2m2 qi pi + m2 qi2  mpi mqi iA  =m  X (pi  qi )2 qi iA  = m  2 (pA , qA ) Next, we analyze the variance. Let i = E
= mpi and 0i = mqi . X 1   Var (Xi  i )2 + 2(Xi  i )(i  0i )  (Xi  i ) 02  iA i X 1   = Var (Xi  i )2 + (Xi  i )(2i  20i  1) 02  iA i X 1   = E (Xi  i )4 + 2(Xi  i )3 (2i  20i  1) + (Xi  i )2 (2i  20i  1)2  2i 02  iA i X 1
= 02  i iA X 1 =
02 iA i X 1
= 02 iA i  X  p2i pi  (pi  qi )2 = 2 2 + 4m  (3) qi qi2 iA  Var
=  The third equality is by noting the random variable has expectation i and the fourth equality substitutes the values of centralized moments of the Poisson distribution.  B  Analysis of our 2 -Test Statistic  We first prove the key lemmas in the analysis of our 2 -test. Proof of Lemma 1: The former case is straightforward from (1) and Property 2 of q. We turn to the latter case. Recall that A = {i : qi  2 /50n}, and thus q(A)  2 /50. We first show that 6 dTV (pA , qA )  25 , where pA , qA are defined as above and in our slight abuse of notation we use dTV (pA , qA ) for non-probability vectors to denote 12 kpA  qA k1 . Partitioning the support into A and A, we have dTV (p, q) = dTV (pA , qA ) + dTV (pA , qA ). We consider the following cases separately:  10  (4)   p(A)  /2: In this case,   1X 1 1  2 13 dTV (pA , qA ) = |pi  qi |  (p(A) + q(A))  + . = 2 2 2 2 50 50 iA  Plugging this in (4), and using the fact that dTV (p, q)   shows that dTV (pA , qA )   6 . 25   p(A) > /2: In this case, by the reverse triangle inequality, dTV (pA , qA )   1 1 6 (q(A)  p(A))  ((1  2 /50)  (1  /2)) = . 2 2 25  By the Cauchy-Schwarz inequality, 2 (pA , qA )  4   dTV (pA , qA )2 q(A)  2 . 5  Plugging in (1) proves the result. Proof of Lemma 2: We bound the terms of (1) separately, starting with the first.  2   X p2i X  (pi  qi )2 2pi qi  qi2 =2 + 2 2 2 q qi qi iA i iA   X (pi  qi )2 2qi (pi  qi ) + qi2 + =2 qi2 qi2 iA   X (pi  qi )2 (pi  qi )  2n + 2 + 2 qi2 qi iA  4n + 4  4n +  X (pi  qi )2 qi2 iA  (5) (6)  200n X (pi  qi )2 2 iA qi  (7)  200n E
2 m 1   4n + nE
100 = 4n +  (8) (9)  (5) uses |A|  n, (6) is the AM-GM inequality, the (7) uses that qi   (9)substitutes a value m  20000 2n .   50n  for all i  A, (8) uses (1), and  The second term can be similarly bounded: X pi (pi  qi )2 X p2i 4m  4m 2 qi q2 iA iA i  !1/2  X (pi  qi )4 qi2 iA  !1/2  !1/2 1/2 X (pi  qi )4 1   4m 4n + nE
100 qi2 iA !   X  (pi  qi )2 1 1/4 1/2  4m 2 n + n E
10 qi iA    2 = 8 n + n1/4 E
5   The first inequality is Cauchy-Schwarz, the second inequality uses (9), the third inequality uses the monotonicity of the `p norms, and the equality uses (1).  11  Combining the two terms, we get  2 Var
 4n + 9 nE
3/2 . 5 We now consider the two cases in the statement of our lemma. 1 m2 . Combined with a choice of m   When p  C, we know from Lemma 1 that E
 500  n 20000 2 and the above expression for the variance, this gives:  10 1 4 9 2 4 2 4 m  + m2 4  m2 4 . Var
 m  + 200002 20000  500 12500000 500000    When dTV (p, C)  , Lemma 1 and m  20000  n 2  give:   1 E
 m2  4000 n. 5 Combining this with our expression for variance we get: 4 2 1 9 Var
2 + 40002 4000 100 5 4000  C  Details on Testing Monotonicity  In this section, we prove Lemma 3 necessary for our monotonicity testing result. Our analysis starts with a structural lemma about monotone distributions. In
, Birge showed that any monotone distribution p over
can be obliviously decomposed into O(log(n)/) intervals, such that the flattening p (recall Definition 3) of p over these intervals is -close to p in total variation distance.
extend this result, giving a bound between the 2 -distance of p and p. We strengthen these results by extending them to monotone distributions over
d . In particular, we partition the domain
d of p into O((d log(n)/2 )d ) rectangles, and compare it with p, the flattening over these rectangles. The following result is proved in Section C.1. Lemma 5. Let d  1. There is an oblivious decomposition of
d into O((d log(n)/2 )d ) rectangles such that for any monotone distribution p over
d , its flattening p over these rectangles satisfy 2 (p, p)  2 . This effectively reduces the support size to logarithmic in n. At this point, we can apply the Laplace estimator (along the lines of
) and learn a q such that if p was monotone, then q will be O(2 )-close in 2 -distance. The following result is proved in Section C.2. Lemma 6. Let d  1, and p be a monotone distribution over
d . There is an algorithm which outputs a   2 distribution q such that E 2 (p, q)  500 . The time and sample complexity are both O((d log(n)/2 )d /2 ). Applying Markovs inequality gives the 2 distance guarantee in Lemma 3. The final step before we apply our 2 -tester is to compute the distance between q and Mdn . This subroutine is similar to the one introduced by
. The key idea is to write a linear program, which searches for any distribution f which is close to q in total variation distance. We note that the desired properties of f (i.e., monotonicity, normalization, and -closeness to q) are easy to enforce as linear constraints. Note that the linear program operates over the oblivious decomposition used in our structural result, so the complexity is polynomial in (d log(n)/)d , rather than the naive nd . These results when combined, give precisely the guarantees of Lemma 3.  C.1  A Structural Result for Monotone Distributions on the Hypergrid  Birge
showed that any monotone distribution is estimated to a total variation  with a O(log(n)/)piecewise constant distribution. Moreover, the intervals over which the output is constant is independent of the distribution p. This result, was strengthened to the Kullback-Leibler divergence by
to study the compression of monotone distributions. They upper bound the KL divergence by 2 distance and then bound the 2 distance. We extend this result to
d . We divide
d into bd rectangles as follows. Let {I1 , . . . , Ib } be a partition of
into consecutive intervals defined as: ( 1 for 1  j  2b , |Ij | = jb/2 b2(1 + ) c for 2b < j  b.  12  def  For j = (j1 , . . . , jd ) 
d , let Ij = Ij1  Ij2  . . .  Ijd . The 2 distance between p and p can be bounded as    X X p2 i 1  (p, p) =  pi j
d iIj   X +  pj |Ij |  1 2  j
d  For j = (j1 , . . . , jd )  Slarge , let j = (j1 , . . . , jb ) be ( ji if ji  b/2 + 1  ji = ji  1 otherwise. We bound the expression above as follows. Let T 
be any subset of d. Suppose the size of T is `. Let T be the set of all j that satisfy ji = b/2 + 1 for i  T . In other words, over the dimensions determined by T , the value of the index is equal to d/2 + 1. The map j  j restricted to T is one-to-one, and since at most d  ` of the coordinates drop, |Ij |  |Ij |  (1 + )d` . Since there are ` coordinates that do not change, and each of them have 2(1 + ) coordinates, we obtain X X  pj  pj  |Ij |  (2(1 + ))`  (1 + )d` jT  jT  =  X  ` d p j  |Ij |  2 (1 + ) .  jT  Since the mapping is one-to-one, the probability of observing as element in T is the probability of observing b/2 + 1 in ` coordinates, which is at most (2/(b + 2))` under any monotone distribution. Therefore, `  X 2  2` (1 + )d . pj  b+2 jT  For any ` there are  d `    choices for T . Therefore, ! ` d X d 4 (1 + )d  1 ` b+2 `=0  d 4 = (1 + )d 1 + 1 b+2 d  4 4 = 1++ + 1 b+2 b+2  2 (p, p)   Recall that  = 2 log(n)/b > 1/b, implies that the expression above is at most (1 + 2)d  1. This implies Lemma 5.  C.2  Monotone Learning  Our algorithm requires a distribution q satisfying the properties discussed earlier. We learn a monotone distribution from samples as follows. Before proving this result, we prove a general result for 2 learning of arbitrary discrete distributions, adapting the result from
. For a distribution p, and a partition of the domain into b intervals I1 , . . . , Ib , let pi = p(Ii )/|Ii | be the flattening of p over these intervals. We saw that for monotone distributions there exists a partition of the domain such that p is close to the underlying distribution in 2 distance. Suppose we are given m samples from a distribution p and a partition I1 , . . . , Ib . Let mj be the number of samples that fall in Ij . For i  Ij , let def 1 mj + 1 qi = . |Ij | m + b  13  Let Sj =  P  iIj  p2i . The expected 2 distance between p and q can be bounded as follows.    ! b XX m 2 X  2  m p i 1 E  (p, q) =  (p(Ij ))` (1  p(Ij ))m` ` (` + 1)/(|Ij |(m + b)) j=1 iIj `=0 ! !# " b m X Sj m+1 m+b X (p(Ij ))`+1 (1  p(Ij ))m+1`+1 1 = m + 1 j=1 p(Ij )/|Ij | `+1 `=0 " # b m+b X Sj m+1  = 1 1  (1  p(Ij ) m + 1 j=1 p(Ij )/|Ij | " # b m+b X Sj  1 m + 1 j=1 p(Ij )/|Ij |    m+b 2 =  (p, p) + 1  1 m+1 b m+b  2 (p, p) + . (10) = m+1 m+1 Suppose  = O(log(n)/b), and b = O(d  log(n)/2 ). Then, by Lemma 5, 2 (p, p)  2 .  (11)  Combining this with (10) gives Lemma 3.  D  Details on testing Unimodality  One striking feature of Birges result is that the decomposition of the domain is oblivious to the samples, and therefore to the unknown distribution. However, such an oblivious decomposition will not work for the unimodal distribution, since the mode is unknown. Suppose we know where the mode of the unknown distribution might be, then the problem can be decomposed into monotone functions over two intervals. Therefore, in theory, one can modify the monotonicity testing algorithm by iterating over all the possible n modes. Indeed, by applying a union bound, it then follows that Theorem 6. (Follows from Monotone) For  > 1/n1/4 , there exists an algorithm for testing unimodality over  n log n/2 .
with sample complexity O Our main result for testing unimodality is the following theorem. Theorem 7. Suppose  > n1/4 . Then there exists an algorithm for testing unimodality over
with sample  2 complexity O( n/ ). Recall that to circumvent Birges decomposition, we want to decompose the interval into disjoint intervals such that the probability of each interval is about O(1/b), where b is a parameter, specified later. In particular we consider a decomposition of
with the following properties: 1. For each element i with probability at least 1/b, there is an I` = {i}. 2. There are at most two intervals with p(I)  1/2b.  1 2 3. Every other interval I satisfies p(I)  2b ,b . Let I1 , . . . , IL denote the partition of
corresponding to these intervals. Note that L = O(b). Claim 1. There is an algorithm that takes O(b log b) samples and outputs I1 , . . . , IL satisfying the properties above. The first step in our algorithm is to estimate the total probability within each of these intervals. In particular, Lemma 7. There is an algorithm that takes m0 = O(b log b/2 ) samples from a distribution p, and with probability at least 9/10 outputs a distribution q that is constant on each IL . Moreover, for any j such that p(Ij ) > 1/2b, q(Ij )  (1  )p(Ij ).  14  Proof. Consider any interval Ij with p(Ij )  1/2b. The number of samples NIj that fall in that interval is distributed Binomial(m0 , p(Ij ). Then by Chernoff bounds for m0 > 12b log b/2 ,   (12) Pr |NIj  m0 p(Ij )| > m0 p(Ij ) 2 exp 2 m0 p(Ij )/2 1  2, (13) b where the last inequality uses the fact that p(Ij )  1/2b. The next step is estimate the distance of q from Un . This is possible by a simple dynamic program, similar to the one used for monotonicity. If the estimated distance is more than /2, we output R EJECT. Our next step is to remove certain intervals. This will be to ensure that when the underlying distribution is unimodal, we are able to estimate the distribution multiplicatively over the remaining intervals. In particular, we do the following preprocessing step:  A = .  For interval Ij ,  If q(Ij )  / ((1  )  q(Ij+1 ), (1 + )  q(Ij+1 )) OR q(Ij )  / ((1  )  q(Ij1 ), (1 + )  q(Ij1 )) ,  (14) (15)  add Ij to A.  Add the (at most 2) intervals with mass at most 1/2b to A.  Add all intervals j with q(Ij )/|Ij | < /50n to A If the distribution is unimodal, we can prove the following about the set of intervals Ac . Lemma 8. If p is unimodal then,  p(IAc )  1  /25  1/b  O (log n/(b)) .  Except at most one interval in Ac every other interval Ij satisfies, p+ j p j   (1 + ).  If this holds, then the 2 distance between p and q constrained to Ac , is at most 2 . This lemma follows from the following result. Lemma 9. Let C > 2. For a unimodal distribution over
, there are at most satisfy  p+ j p j  4 log(50n/) C  intervals Ij that  < (1 + /C).  Proof. To the contrary, if there are more than 4 log(50n/) intervals, then at least half of them are on one C side of the mode, however this implies that the ratio of the largest probability and smallest probability is at least (1 + /C)j , and if j > 2 log(50n/) , is at least 50n/, contradicting that we have removed all such C elements. We have one additional pre-processing step here. We compute q(Ac ) and if it is smaller than 1  /25, we output R EJECT. Suppose there are L0 intervals in Ac . Then, except at most one interval in L0 we know that the 2 distance between p and q is at most 2 when p is unimodal, and the TV distance between p and q is at least /2 over Ac . We propose the following simple modification to take into account, the one interval that might introduce a high 2 distance in spite of having a small total variation. If we knew the interval, we can simply remove it and proceed. Since we do not know where the interval lies, we do the following.  1. Let Zj be the 2 statistic over the ith interval in Ac , computed with O( n/2 ) samples. 2. Let Zl be the largest among all Zj s. P 3. If j,j6=l Zj > m2 /10, output R EJECT. 4. Output ACCEPT. The objective of removing the largest 2 statistic is our substitute for not knowing the largest interval. We now prove the correctness of this algorithm.  15  Case 1 p  U Mn : We only concentrate on the final step. The 2 statistic over all but one interval are at most c  m2 , and the variance is bounded as before. Since we remove the largest statistic, the expected value of the new statistic is strictly dominated by that of these intervals. Therefore, the algorithm outputs ACCEPT with at least the same probability as if we removed the spurious interval. Case 2 p  / U Mn : This is the hard case to prove for unimodal distributions. We know that the 2 statistic is large in this case, and we therefore have to prove that it remains large even after removing the largest test statistic Zl . We invoke Kolmogorovs Maximal Inequality to this end. Lemma 10 (Kolmogorovs Maximal Inequality). For independent zero mean random variables X1 , . . . , XL with finite variance, let S` = X1 + . . . X` . Then for any  > 0,   1 Pr max |S` |    2  V ar (SL ) . (16) 1`L  As a corollary, it follows that Pr (max` |X` | > 2)   1 2   V ar (SL ).  In the case we are interested in, we let Xi = Z`  E
. Then, similar to the computations before, and the fact that each interval has a small mass, it follows that that the variance of the summation is at most E
2 /100.  2  Taking  = E SL  m2 /3 /100, it follows that the statistic does not fall below to n. This completes the proof of Theorem 7.  E  Learning product distributions in 2 distance  In this section we prove Lemma 4, thus proving Theorem 3. The proof is analogous to the proof for learning monotone distributions, and hinges on the following result of
. Given m samples from a distribution q over n elements, the add-1 estimator (Laplace estimator) q satisfies:   n . E 2 (p, q)  m+1 To handle the 2 distribution of product distributions, we first bound the 2 -distance between product distributions in terms of the individual coordinates. Lemma 11. Let p = p1  p2 . . .  pd , and q = q 1  q 2 . . .  q d be two distributions in d . Then 2 (p, q) =  d Y  (1 + 2 (p` , q ` ))  1.  `=1  Proof. By the definition of 2 -distance and exchanging the product and summation,    d d  ` 2  2 2 X Y X Y X p (p  q ) p i i i i  1= 2 (p, q) = = 1= 1 + 2 (p` , q ` )  1. ` qi qi qi `=1 `=1 iX iX i
Now, suppose p is a product distribution over X =
    
. We simply perform the add-1 estimation over each coordinate independently, giving a distribution q 1      q d . Since p is a product distribution the estimates in each coordinate is independent. Therefore, a simple application of the previous result and independence of the coordinates implies d  h i   Y E 2 (p, q) = 1 + E 2 (pl , q l )  1 l=1 d  Y  1+   nl 1 m+1 l=1 P  l nl  exp  1, (17) m+1 where (17) follows from ex  1 + x. Using ex  1 + 2x for 0  x  1, we have P  2  nl E  (p, q)  2 l , (18) m+1 P P when m  l nl . Therefore, following an application of Markovs inequality, when m = (( l nl )/2 ), Lemma 4 is proved.  16  F  Details on Testing Log-Concavity  Our main result for testing log-concavity is as follows: Theorem 8. There  exists an algorithm for testing log-concavity over
with sample complexity  O n/2 + 1/5 and time complexity poly(n, 1/). In particular, this implies the following optimal tester for this class: 1/5 Corollary 3. Suppose > 1/n . Then there exists an algorithm for testing log-concavity over
with  n/2 . sample complexity O  Our algorithm will fit into the structure of our general framework. We first perform a very particular type of learning algorithm, whose guarantees are summarized in the following lemma: Lemma 12. Given  > 0 and sample access to p, there exists an algorithm such that: - If p  LCDn , the algorithm outputs a distribution q  LCDn and an O()-effective support S of p such that 2 (pS , qS )  2 /500 with probability at least 5/6; - If dTV (p, LCDn )  , the algorithm either outputs a distribution q  LCDn or R EJECT. The sample complexity is O(1/5 ) and the time complexity is poly(n, 1/). We note that as a corollary, one immediately obtains a O(1/5 ) proper learning algorithm for log-concave distributions. The result is immediate from the first item of Lemma 12 and Proposition 1. We can actually do a bit better  in the proof of Lemma 12, we partition
into intervals of probability mass (3/2 ). If one instead partitions into intervals of probability mass (/ log(1/)) and works directly with total variation distance instead of 2 distance, one can show that O(1/4 ) samples suffice. Corollary 4. Given  > 0 and sample access to a distribution p  LCDn , there exists an algorithm which outputs a distribution q  LCDn such that dTV (p, q)  . The sample complexity is O(1/4 ) and the time complexity is poly(n, 1/). Then, given the guarantees of Lemma 12, Theorem 8 follows from Theorem 14 . The details of these results are presented in Section F. It will suffice to prove Lemma 12. Proof of Lemma 12: We first draw samples from p and obtain a O(1/3/2 )-piecewise constant distribution f by appropriately flattening the empirical distribution. The proof is now in two parts. In the first part, we show that if p  LCDn then f will be close to p in 2 distance over its effective support. The second part involves proper learning of p. We will use a linear program on f to find a distribution q  LCDn . This distribution is such that if p  LCDn , then 2 (p, q) is small, and otherwise the algorithm will either output some q  LCDn (with no other relevant guarantees) or R EJECT. We first construct f . Let p be the empirical distribution obtained by sampling O(1/5 ) samples from p. The Dvoretzky-Kiefer-Wolfowitz (DKW) inequality gives a generic algorithm for learning any distribution with respect to the Kolmogorov distance. Lemma 13. (See
) Suppose we have n i.i.d. samples X1 , . . . Xn from a distribution with CDF F . Let P def 2n2 . In particular, if Fn (x) = n1 n i=1 1{Xi x} be the empirical CDF. Then Pr
 2e 2 n = ((1/ )  log(1/)), then Pr
 . This implies that with probability at least 5/6, dK (p, p)  5/2 /10. In particular, note that |pi  pi |  5/2 /10. Condition on this event in the remainder of the proof. Let a be the minimum i such that pi  3/2 /5, and let b be the maximum i satisfying the same condition. Let M = {a, . . . , b} or  if a and b are undefined. By the guarantee provided by the DKW inequality, pi  3/2 /10 for all i  M . Furthermore, pi  pi  3/2 /10  (1  )  pi . For each i  M , let fi = pi . We note that |M | = O(1/), so this contributes O(1/) constant pieces to f . We now divide the rest of the domain into t intervals, all but constantly many of measure (3/2 ) (under p). This is done via the following iterative procedure. As a base case, set r0 = 0. Define Ij as
, where lj = rj1 + 1 and rj is the largest j 
such that p(Ij )  93/2 /10. The exception is if Ij would intersect M  in this case, we skip M : set rj = a  1 and lj+1 = b + 1. If such a j exists, denote it by 4 To be more precise, we require the modification of Theorem 8 which is described in Section 4, in order to handle the case where the 2 -distance guarantees only hold for a known effective support.  17  j  . We note that p(Ij )  p(Ij ) + 5/2 /10  3/2 . Furthermore, for all j except j  and t, rj + 1 6 M , so 2 p(Ij )  93/2 /10  3/2 /5  5/2 /10  33/2 /5. Observe that this lower bound implies that t  3/2 for  sufficiently small.  Part 1. For this part of the algorithm, we only care about the guarantees when p  LCDn , so we assume this is the case. For the domain
\ M , we let f be the flattening of p over the intervals I1 , . . . It . To analyze f , we need a structural property of log-concave distributions due to Chan, Diakonikolas, Servedio, and Sun
. This essentially states that a log-concave distribution cannot have a sudden increase in probability. Lemma 14 (Lemma 4.1 in
). Let p be a distribution over
that is non-decreasing and log-concave on
be an interval of mass P (I) =  , and suppose that the interval J =
has mass p(J) =  > 0. Then p(y)/p(x)  1 +  /. Recall that any log-concave distribution is unimodal, and suppose the mode of p is at i0 . We will first focus on the intervals I1 , . . . , ItL which lie entirely to the left of i0 and M . We will refer to Ij as Lj for all j  tL . Note that p is non-decreasing over these intervals. The next steps to the analysis are as follows. First we show that the flattening of p over Lj is a multiplicative (1 + O(1/j)) estimate for each pi  Lj . Then, we show that flattening the empirical distribution p over Lj is a multiplicative (1 + O(1/j)) estimate of p(i) for each i  Lj . Finally, we exclude a small number of intervals (those corresponding to O() mass at the left and right side of the domain, as well as j  ) in order to get the 2 approximation we desire on an effective support.  First, recall that p(Lj )  3/2 for all j. Also, letting Jj =
, we have that p(Jj )  (j  1)  33/2 /5. Thus by Lemma 14, p(rj )  p(lj )(1 + 2/(j  1)). Since the distribution is non-decreasing 2 ) for all i  Lj . in Lj , the flattening p of p is such that p(i)  p(i)(1  j1  We have that p(Lj )  33/2 /5, and p(Lj )  p(Lj )  5/2 /10, so p(Lj )  p(Lj )  (1  6 ), and hence p(i)  p(i)  (1  6 ) for all i  Lj . Combining with the previous point, we have that      2  2 11 p(i)  p(i)  1  + +  p(i)  1  . 3(j  1) 6 j1 3(j  1) A symmetric statement holds for the intervals that lie entirely to the right of i0 and M . We will refer to Ij as Rtj for all j > tL . To summarize, we have the following guarantees for the distribution f :  For all i  M , f (i)  p(i)  (1  );   For all i  Lj (except L1 and Lj  ), f (i)  p(i)  1    ;  For all i  Rj (except R1 ), f (i)  p(i)  1  22 3j  22 3j   ;  Note that, in particular, we have multiplicative estimates for all intervals, except those in L1 ,  Lj  , R1 and the interval containing i0 . Let S be the set of all intervals except Lj  , Lj and Rj for j  1/ , and the one  containing i0 Then, since each interval has probability mass at most O(3/2 ) and we are excluding O(1/ ) intervals, p(S) > 1  O(). We now compute the 2 -distance induced by this approximation for elements in S. For an element i  Lj  S, we have (f (i)  p(i))2 60p(i)  . p(i) j2 Summing over all i  Lj  S gives 603/2 j2  since the probability mass of Lj is at most 3/2 . Summing this over all Lj for j  1/  and j 6= j  gives Z  2/3/2 X 1 1 3/2 3/2 60  60 dx 2  x2  j 1/  j=1/     = 603/2 ( ) = O(2 ) as desired.  18  Part 2. To obtain a distribution q  LCDn , we write a linear program. We will work in the log domain, so our variables will be Qi , representing log q(i) for i 
. We will use Fi = log f (i) as parameters in our LP. There will be no objective function, we simply search for a feasible point. Our constraints will be Qi1 + Qi+1  2Qi i 
Qi  0 i 
log(1 + )  |Qi  Fi |  log(1 + ) for i  M      22 22 log 1   |Qi  Fi |  log 1 + for i  Lj , j  1/  and j 6= j  3j 3j      22 22 log 1   |Qi  Fi |  log 1 + for i  Rj , j  1/  3j 3j If we run the linear program, then after a rescaling and summing the error over all the intervals in the LP gives us that the distance between p and q to be O(2 ) 2 -distance in a set S which has measure p(S)  1  4, as desired. If the linear program finds a feasible point, then we obtain a q  LCDn . Furthermore, if p  LCDn , this also 2 tells us that (after a rescaling of ), summing the error over all intervals implies that 2 (pS , qS )  500 for a known set S with p(S)  1  O(), as desired. If M 6= , this algorithm works as described. The issue is if M = , then we dont know when the L intervals end and the R intervals begin. In this case, we run O(1/) LPs, using each interval as the one containing i0 , and thus acting as the barrier between the L intervals (to its left) and the R intervals (to its right). If p truly was log-concave, then one of these guesses will be correct and the corresponding LP will find a feasible point.  G  Details on MHR testing  In this section, we give our main result for testing for monotone hazard rate: Theorem 9. There exists an algorithm for testing monotone hazard rate over
with sample complexity  n/2 + log(n/)/4 and time complexity poly(n, 1/). O This implies the following optimal tester for the class: p Corollary 5. Suppose  > log(n/)/n1/4  . Then there exists an algorithm for testing monotone hazard rate  over
with sample complexity O n/2 . We obey the same framework as before, first applying a 2 -learner with the following guarantees: Lemma 15. Given  > 0 and sample access to p, there exists an algorithm such that: - If p  MHRn , the algorithm outputs a distribution q  MHRn and an O()-effective support S of p such that 2 (pS , qS )  2 /500 with probability at least 5/6; - If dTV (p, MHRn )  , the algorithm either outputs a distribution q  MHRn and a set S 
or R EJECT. The sample complexity is O(log(n/)/4 ) and the time complexity is poly(n, 1/). As with log-concave distributions, this implies the following proper learning result: Corollary 6. Given  > 0 and sample access to a distribution p  MHRn , there exists an algorithm which outputs a distribution q  MHRn such that dTV (p, q)  . The sample complexity is O(log(n/)/4 ) and the time complexity is poly(n, 1/). Proof of Lemma 15: As with log-concave distributions, our method for MHR distributions can be split into two parts. In the first step, if p  MHRn , we obtain a distribution q which is O(2 )-close to p in 2 distance on a set A of intervals such that p(A)  1  O(). q will achieve this by being a multiplicative (1 + O()) approximation for each element within these intervals. This step is very similar to the decomposition used for unimodal distributions (described in Section D), so we sketch the argument and highlight the key differences. The second step will be to find a feasible point in a linear program. If p  MHRn , there should always be a feasible point, indicating that q is close to a distribution in MHRn (leveraging the particular guarantees for our algorithm for generating q). If dTV (p, MHRn )  , there may or may not be a feasible point, but when there is, it should imply the existence of a distribution p  MHRn such that dTV (q, p )  /2. The analysis will rely on the following lemma from
, which roughly states that an MHR distribution is almost non-decreasing. Lemma 16 (Lemma 5.1 in
). Let p be an MHR distribution over
be an interval, 1 and R =
be the elements to the right of I. Let  = p(I)/p(R). Then p(b + 1)  1+ p(a).  19  b Part 1. As before, with unimodal distributions, we start by taking O( b log ) samples, with the goal of par2  titioning the domain into intervals of mass approximately (1/b). First, we will ignore the left and rightmost intervals of mass (). For all heavy elements with mass  (1/b), we consider them as singletons. We note that Lemma 16 implies that there will be at most O(1/) contiguous intervals of such elements. The rest of the domain is greedily divided (from left to right) into intervals of mass (1/b), cutting an interval short if we reach one of the heavy elements. This will result in the guarantee that all but potentially O(1/) intervals have (1/b) mass. Next, similar to unimodal distributions, considering the flattened distribution, we discard all intervals for which the per-element probability is not within a (1  O()) multiplicative factor of the same value for both neighboring intervals. The claim is that all remaining intervals will have the property that the per-element probability is within a (1  O()) multiplicative factor of the true probability. This is implied by Lemma 16. If there were a point in an interval which was above this range, the distribution must decrease slowly, and the next interval would have a much larger per-element weight, thus leading to the removal of this interval. A similar argument forbids us from missing an interval which contains a point that lies outside this range. Relying on the fact that truncating the left and rightmost intervals eliminates elements with low probability mass, similar to the unimodal case, one can show that we will remove at most log(n/)/ intervals, and thus a log(n/)/b probability mass. Choosing b = (2 / log(n/)) limits this to be O(), as desired. At this point, if p is indeed MHR, the multiplicative estimates guarantee that the result is O(2 )-close in 2 -distance among the remaining intervals.  Part 2. We note that an equivalent condition for distribution f being MHR is log-concavity of log(1  F ), where F is the CDF of f . Therefore, our approach for this part will be similar to the approach used for log-concave distributions. Given the output distribution q from the previous part of this algorithm, our goal will be check if there exists an MHR distribution f which is O()-close to q. We will run a linear program with variables fi = log(1  Fi ). First, we ensure that f is a distribution. This can be done with the following constraints: fi  0 fi  fi+1 fn =   i 
To ensure that f is MHR, we use the following constraint: fi1 + fi1  2fi  i 
Now, ideally, we would like to ensure f and q are -close in total variation distance by ensuring they are pointwise within a multiplicative (1  ) factor of each other: (1  )  fi /qi  (1 + ) We note that this is a stronger condition than f and q being -close, but if p  MHRn , the guarantees of the previous step would imply the existence of such an f . We have a separate treatment for the identified singletons (i.e., those with probability  1/b) and the remainder of the support. For each element qi identified to have  1/b mass, we add two constraints: log((1  b/2)(1  Qi ))  fi  log((1 + b/2)(1  Qi )) log((1  b/2)(1  Qi1 ))  fi1  log((1 + b/2)(1  Qi1 )) If we satisfy these constraints, it implies that qi  b  fi  qi + b. Since qi  1/b, this implies (1  )qi  fi  (1 + )qi as desired. Now, the remaining elements each have  1/b mass. For each such element qi , we create a constraint (1  O())  qi qi  fi1  fi  (1 + O()) 1  Qi1 1  Qi1  Note that the middle term is     1  Fi fi fi  log =  log 1   (1  2) , 1  Fi1 1  Fi1 1  Fi1  20  where the second equality uses the Taylor expansion and the facts that fi  1/b and 1  Fi1   (since during the previous part, we ignored the rightmost O() probability mass). If we satisfy the desired constraints, it implies that 1 1  Fi1 fi  (q + O())qi . (1  2) 1  Qi1 Since we are taking (1/4 ) samples and 1Fi1  (), Lemma 13 implies that fi is indeed a multiplicative (1  ) approximation for these points as well. We note that all points which do not fall into these two cases make up a total of O() probability mass. Therefore, f may be arbitrary at these points and only incur O() cost in total variation distance. If we find a feasible point for this linear program, it implies the existence of an MHR distribution within O() total variation distance. In this case, we continue to the testing portion of the algorithm. Furthermore, if p  MHRn , our method for generating q certifies that such a distribution exists, and we continue on to the testing portion of the algorithm.  H  Details of the Lower Bounds  In this section, for the class of distributions Q described in discussion  on lower bounds and a class of interest C, we show that dTV (C, Q)  , thus implying a lower bound of ( n/2 ) for testing C.  H.1  Monotone distributions  We first consider d = 1 and prove that for appropriately chosen c, any monotone distribution over
is -far from all distributions in Q. Consider any q  Q. For this distribution, we say that i 
is a raise-point if qi < qi+1 . Let Rq be the set of raise points of q. For q  Q, (2) implies at least one in every four consecutive integers in
is a raise point, and therefore, |Rq |  n/4. Moreover, note that if i is a raise-point, then i + 1 is not a raise point. For any monotone (decreasing) distribution p, pi  pi+1 . For any raise-point i  Rq , by the triangle inequality, |pi  qi | + |pi+1  qi+1 |  |pi  pi+1 + qi+1  qi |  qi+1  qi = Summing over the set Rq , we obtain dTV (p, q)  12 |Rq |  dTV (Mn , q)  . This proves the lower bound for d = 1.  2c n  2c . n  (19)   c/4. Therefore, if c  4, then  This argument can be extended to
d . Consider the following class of distributions on
d . For each point i = (i1 , . . . , id ) 
d , where i1 is even, generate a random z  {1, 1}, and assign to i a probability of def  (1 + zc)/nd . Let e1 = (1, 0, . . . , 0). Similar to d = 1, assign a probability (1  zc)/nd to the point nd/2  i + e1 = (i1 + 1, i2 , . . . , id ). This class consists of 2 2 distributions, and Paninskis arguments extend to give a lower bound of (nd/2 /2 ) samples to distinguish this class from the uniform distribution over
d . It remains to show that all these distributions are  far from Mdn . Call a point i as a raise point if pi < pi+e1 . For any i, one of the points i, i + e1 , i + 2e1 , i + 3e1 is a raise point, and the number of raise points is at least nd /4. Invoking the triangle inequality (identical to (19)) over the raise-points, in the first dimension shows that any monotone distribution over
d is at a distance c from any distribution in this class. Choosing c = 4 yields a 4 bound of .  H.2  Testing Product Distributions  Our idea for testing independence is similar to the previous section. We sketch the construction of a class of distributions on X =
    
. Then |X | = n1  n2 . . .  nd . For each element in X assign a value (1  c) and then for each such assignment, normalize the values so that they add to 1, giving rise to a distribution. This gives us a class of 2|X | distributions. The key argument is to show that a large fraction of these distributions are far from being a product distribution. This follows since the degrees of freedom of a product distribution is exponentially smaller than the number of possible distributions. The second step is to simply apply Paninskis argument, now over the larger set of distributions, where we show that distinguishing the collectionpof distributions we constructed from the uniform distribution over X (which is a product distribution) requires |X |/2 samples.  H.3  Log-concave and Unimodal distributions  We will show that any log-concave or unimodal distribution is -far from all distributions in Q. Since LCDn  Un , it will suffice to show this for every unimodal distribution. Consider any unimodal distribution p, with  21  mode `. Then, p is monotone non-decreasing over the interval
and non-increasing over {` + 1, . . . , n}. By the argument for monotone distributions, the total variation distance between p and any distribution q over c c elements greater than ` is at least n`1 , and over elements less than ` is at least `1 . Summing these n 4 n 4 two gives the desired bound.  H.4  Monotone Hazard distributions  We will show that any monotone hazard rate distribution is -far from all distributions in Q. Let p be any monotone-hazard distribution. Any distribution q  Q has mass  at least 1/2 over the interval pi I =
. Therefore, by Lemma 16, for any i  I, pi+1 1 + 1/4  pi . As noted before, at least n/8 of the raise-points are in I. For any i  I  Rq , qi = (1 + c)/n, qi+1 = (1  c)/n di = |pi  qi | + |pi+1  qi+1 |.  (20)  If pi  (1 + 2c)/n or pi  1/n, then the first term, and therefore di is at least c/n. If pi  (1/n, (1 + 2c)/n), then for n > 5/(c) 1  c/2 1 1 pi+1    . n 1 + n4 n Therefore the second term of di is at c/2n. Since there are at least n/8 raise points in I, dTV (p, q)   c 1 n c   . 2 8 2n 16  Thus any MHR distribution is -far from Q for c  16.  22  (21)
D. R. Cox and H. D. Miller, The Theory of Stochastic Processes. London: Chapman and Hall, 1965.
C. H. Jackson, Multi-state models for panel data: the msm package for R, Journal of Statistical Software, vol. 38, no. 8, 2011.
N. Bartolomeo, P. Trerotoli, and G. Serio, Progression of liver cirrhosis to HCC: an application of hidden markov model, BMC Med Research Methold., vol. 11, no. 38, 2011.
Y. Liu, H. Ishikawa, M. Chen, and et al., Longitudinal modeling of glaucoma progression using 2-dimensional continuous-time hidden markov model, Med Image Comput Comput Assist Interv, vol. 16, no. 2, pp. 44451, 2013.
X. Wang, D. Sontag, and F. Wang, Unsupervised learning of disease progression models, Proceeding KDD, vol. 4, no. 1, pp. 8594, 2014.
U. Nodelman, C. R. Shelton, and D. Koller, Expectation maximization and complex duration distributions for continuous time bayesian networks, in Proc. Uncertainty in AI (UAI 05), 2005.
J. M. Leiva-Murillo, A. Arts-Rodrguez, and E. Baca-Garca, Visualization and prediction of disease interactions with continuous-time hidden markov models, in NIPS, 2011.
P. Metzner, I. Horenko, and C. Schtte, Generator estimation of markov jump processes based on incomplete observations nonequidistant in time, Physical Review E, vol. 76, no. 066702, 2007.
A. Hobolth and J. L. Jensen, Summary statistics for endpoint-conditioned continuous-time markov chains, Journal of Applied Probability, vol. 48, no. 4, pp. 911924, 2011.
P. Tataru and A. Hobolth, Comparison of methods for calculating conditional expectations of sufficient statistics for continuous time markov chains, BMC Bioinformatics, vol. 12, no. 465, 2011.
F. Medeiros, L. Zangwill, C. Girkin, and et al., Combining structural and functional measurements to improve estimates of rates of glaucomatous progression, Am J Ophthalmol, vol. 153, no. 6, pp. 1197205, 2012.
M. Bladt and M. Srensen, Statistical inference for discretely observed markov jump processes, J. R. Statist. Soc. B, vol. 39, no. 3, p. 395410, 2005.
L. R. Rabinar, A tutorial on hidden markov models and selected applications in speech recognition, Proceedings of the IEEE, vol. 77, no. 2, 1989.
A. Hobolth and J. L.Jensen, Statistical inference in evolutionary models of DNA sequences via the EM algorithm, Statistical Applications in Genetics and Molecular Biology, vol. 4, no. 1, 2005.
C. Van Loan, Computing integrals involving the matrix exponential, IEEE Trans. Automatic Control, vol. 23, pp. 395404, 1978.
N. Higham, Functions of Matrices: Theory and Computation. SIAM, 2008.
P. Metzner, I. Horenko, and C. Schtte, Generator estimation of markov jump processes, Journal of Computational Physics, vol. 227, p. 353375, 2007.
S. Kingman, Glaucoma is second leading cause of blindness globally, Bulletin of the World Health Organization, vol. 82, no. 11, 2004.
G. Wollstein, J. Schuman, L. Price, and et al., Optical coherence tomography longitudinal evaluation of retinal nerve fiber layer thickness in glaucoma, Arch Ophthalmol, vol. 123, no. 4, pp. 46470, 2005.
G. Wollstein, L. Kagemann, R. Bilonick, and et al., Retinal nerve fibre layer and visual function loss in glaucoma: the tipping point, Br J Ophthalmol, vol. 96, no. 1, pp. 4752, 2012.
The Alzheimers Disease Neuroimaging Initiative, http://adni.loni.usc.edu,
A. M. Fagan, D. Head, A. R. Shah, and et. al, Decreased CSF A beta 42 correlates with brain atrophy in cognitively normal elderly, Ann Neurol., vol. 65, no. 2, p. 176183, 2009.  9
Alexander T. Ihler and David A. McAllester. Particle belief propagation. In Proc. 12th AISTATS, pages 256263, 2009.
Martin J. Wainwright and Michael I. Jordan. Graphical models, exponential families, and variational inference. Found. and Tr. in Mach. Learn., 1(12):1305, 2008.
Erik B. Sudderth, Alexander T. Ihler, Michael Isard, William T. Freeman, and Alan S. Willsky. Nonparametric belief propagation. Commun. ACM, 53(10):95102, 2010.
Jeremy Schiff, Erik B. Sudderth, and Ken Goldberg. Nonparametric belief propagation for distributed tracking of robot networks with noisy inter-distance measurements. In IROS 09, pages 13691376, 2009.
Alexander T. Ihler, John W. Fisher, Randolph L. Moses, and Alan S. Willsky. Nonparametric belief propagation for self-localization of sensor networks. In IEEE Sel. Ar. Comm., volume 23, pages 809819, 2005.
Christopher Crick and Avi Pfeffer. Loopy belief propagation as a basis for communication in sensor networks. In Proc. 19th UAI, pages 159166, 2003.
Jian Sun, Nan-Ning Zheng, and Heung-Yeung Shum. Stereo matching using belief propagation. In IEEE Trans. Patt. An. Mach. Int., volume 25, pages 787800, 2003.
Andrea Klaus, Mario Sormann, and Konrad Karner. Segment-based stereo matching using belief propagation and a self-adapting dissimilarity measure. In Proc. 18th ICPR, volume 3, pages 1518, 2006.
Nima Noorshams and Martin J. Wainwright. Belief propagation for continuous state spaces: Stochastic message-passing with quantitative guarantees. JMLR, 14:27992835, 2013.
Judea Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan Kaufman, 1988.
Jonathan S. Yedidia, William T. Freeman, and Yair Weiss. Constructing free energy approximations and generalized belief propagation algorithms. MERL Technical Report, 2002.
Erik B. Sudderth, Alexander T. Ihler, William T. Freeman, and Alan S. Willsky. Nonparametric belief propagation. In Procs. IEEE Comp. Vis. Patt. Rec., volume 1, pages 605612, 2003.
Thomas P. Minka. Expectation propagation for approximate Bayesian inference. In Proc. 17th UAI, pages 362369, 2001.
Kevin P. Murphy, Yair Weiss, and Michael I. Jordan. Loopy belief propagation for approximate inference: an empirical study. In Proc. 15th UAI, pages 467475, 1999.
Mila Nikolova. Thresholding implied by truncated quadratic regularization. IEEE Trans. Sig. Proc., 48(12):34373450, 2000.
Mark Briers, Arnaud Doucet, and Sumeetpal S. Singh. Sequential auxiliary particle belief propagation. In Proc. 8th ICIF, volume 1, pages 705711, 2005.
Leonid I. Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based noise removal algorithms. Physica D, 60(1):259268, 1992.
M. Briers, A. Doucet, and S. Maskell. Smoothing algorithms for state-space models. Ann. Inst. Stat. Math., 62(1):6189, 2010.
Erik B. Sudderth, Michael I. Mandel, William T. Freeman, and Alan S. Willsky. Visual hand tracking using nonparametric belief propagation. In Procs. IEEE Comp. Vis. Patt. Rec., 2004.
Pedro F. Felzenszwalb and Daniel P. Huttenlocher. Efficient graph-based image segmentation. Int. Journ. Comp. Vis., 59(2), 2004.
Thomas P. Minka. Power EP. Technical Report MSR-TR-2004-149, 2004.
Christian A. Naesseth, Fredrik Lindsten, and Thomas B. Schon. Sequential monte carlo for graphical models. In Proc. 27th NIPS, pages 18621870, 2014.  9
Leontine Alkema, Adrian E Raftery, and Samuel J Clark. Probabilistic projections of HIV prevalence using Bayesian melding. The Annals of Applied Statistics, pages 229248, 2007.
MOSEK ApS. The MOSEK optimization toolbox for Python manual. Version 7.1 (Revision 28), 2015.
Albert-Laszlo Barabasi and Reka Albert. 286(5439):509512, 1999.  Emergence of scaling in random networks.  Science,
N. Batra et al. Nilmtk: An open source toolkit for non-intrusive load monitoring. In Proceedings of the 5th International Conference on Future Energy Systems, pages 265276, New York, NY, USA, 2014.
Robert F. Bordley. A multiplicative formula for aggregating probability assessments. Management Science, 28(10):11371148, 1982.
Grace S Chiu and Joshua M Gould. Statistical inference for food webs with emphasis on ecological networks via Bayesian melding. Environmetrics, 21(7-8):728740, 2010.
Luiz Max F de Carvalhoa, Daniel AM Villelaa, Flavio Coelhoc, and Leonardo S Bastosa. On the choice of the weights for the logarithmic pooling of probability distributions. September 24, 2015.
E. Elhamifar and S. Sastry. Energy disaggregation via learning powerlets and sparse coding. In Proceedings of the Twenty-Ninth Conference on Artificial Intelligence (AAAI), pages 629635, 2015.
K. Ganchev, J. Graca, J. Gillenwater, and B. Taskar. Posterior regularization for structured latent variable models. Journal of Machine Learning Research, 11:20012049, 2010.
A. Giffin and A. Caticha. Updating probabilities with data and moments. The 27th Int. Workshop on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, NY, July 8-13,2007.
G.W. Hart. Nonintrusive appliance load monitoring. Proceedings of the IEEE, 80(12):1870 1891, 1992.
Edwin T Jaynes. Information theory and statistical mechanics. Physical review, 106(4):620, 1957.
Jack Kelly and William Knottenbelt. The UK-DALE dataset, domestic appliance-level electricity demand and whole-house demand from five UK homes. 2(150007), 2015.
H. Kim, M. Marwah, M. Arlitt, G. Lyon, and J. Han. Unsupervised disaggregation of low frequency power measurements. In Proceedings of the SIAM Conference on Data Mining, pages 747758, 2011.
J. Z. Kolter and T. Jaakkola. Approximate inference in additive factorial HMMs with application to energy disaggregation. In Proceedings of AISTATS, volume 22, pages 14721482, 2012.
P. Liang, M.I. Jordan, and D. Klein. Learning from measurements in exponential families. In The 26th Annual International Conference on Machine Learning, pages 641648, 2009.
James G MacKinnon and Anthony A Smith. Approximate bias correction in econometrics. Journal of Econometrics, 85(2):205230, 1998.
G. Mann and A. McCallum. Generalized expectation criteria for semi-supervised learning of conditional random fields. In Proceedings of ACL, pages 870878, Columbus, Ohio, June 2008.
Keith Myerscough, Jason Frank, and Benedict Leimkuhler. Least-biased correction of extended dynamical systems using observational data. arXiv preprint arXiv:1411.6011, 2014.
O. Parson, S. Ghosh, M. Weal, and A. Rogers. Non-intrusive load monitoring using prior models of general appliance types. In Proceedings of AAAI, pages 356362, July 2012.
David Poole and Adrian E. Raftery. Inference for deterministic simulation models: The Bayesian melding approach. Journal of the American Statistical Association, pages 12441255, 2000.
MJ Rufo, J Martn, CJ Perez, et al. Log-linear pool to combine prior distributions: A suggestion for a calibration-based approach. Bayesian Analysis, 7(2):411438, 2012.
H. Sevckova, A. Raftery, and P. Waddell. Uncertain benefits: Application of Bayesian melding to the Alaskan way viaduct in Seattle. Transportation Research Part A: Policy and Practice, 45:540553, 2011.
Robert L Wolpert. Comment on Inference from a deterministic population dynamics model for bowhead whales. Journal of the American Statistical Association, 90(430):426427, 1995.
M. Wytock and J. Zico Kolter. Contextually supervised source separation with application to energy disaggregation. In Proceedings of AAAI, pages 486492, 2014.
M. Zhong, N. Goddard, and C. Sutton. Signal aggregate constraints in additive factorial HMMs, with application to energy disaggregation. In NIPS, pages 35903598, 2014.
J.-P. Zimmermann, M. Evans, J. Griggs, N. King, L. Harding, P. Roberts, and C. Evans. Household electricity survey, 2012.  9
Practice Fusion Diabetes Classification. http://www.kaggle.com/c/pf2012-diabetes, 2012. Kaggle competition dataset.  8
P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. Amer. J. of Mathematics, 37:17051732, 2009.
P. Buhlmann. Statistical significance in high-dimensional linear models. arXiv:1202.1377, 2012.
P. Buhlmann and S. van de Geer. Statistics for high-dimensional data. Springer-Verlag, 2011.
E. Candes and Y. Plan. Near-ideal model selection by `1 minimization. The Annals of Statistics, 37(5A):21452177, 2009.
E. J. Candes and T. Tao. Decoding by linear programming. IEEE Trans. on Inform. Theory, 51:4203 4215, 2005.
S. Chen and D. Donoho. Examples of basis pursuit. In Proceedings of Wavelet Applications in Signal and Image Processing III, San Diego, CA, 1995.
E. Greenshtein and Y. Ritov. Persistence in high-dimensional predictor selection and the virtue of overparametrization. Bernoulli, 10:971988, 2004.
A. Javanmard and A. Montanari. Confidence Intervals and Hypothesis Testing for High-Dimensional Regression. arXiv:1306.3171, 2013.
A. Javanmard and A. Montanari. Hypothesis testing in high-dimensional regression under the gaussian random design model: Asymptotic theory. arXiv:1301.4240, 2013.
A. Javanmard and A. Montanari. Nearly Optimal Sample Size in Hypothesis Testing for HighDimensional Regression. arXiv:1311.0274, 2013.
Y. Koren, R. Bell, and C. Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8):3037, August 2009.
E. Lehmann and G. Casella. Theory of point estimation. Springer, 2 edition, 1998.
E. Lehmann and J. Romano. Testing statistical hypotheses. Springer, 2005.
R. Lockhart, J. Taylor, R. Tibshirani, and R. Tibshirani. A significance test for the lasso. arXiv preprint arXiv:1301.7161, 2013.
M. Lustig, D. Donoho, J. Santos, and J. Pauly. Compressed sensing mri. IEEE Signal Processing Magazine, 25:7282, 2008.
N. Meinshausen and P. Buhlmann. High-dimensional graphs and variable selection with the lasso. Ann. Statist., 34:14361462, 2006.
N. Meinshausen and P. Buhlmann. Stability selection. J. R. Statist. Soc. B, 72:417473, 2010.
J. Minnier, L. Tian, and T. Cai. A perturbation method for inference on regularized regression estimates. Journal of the American Statistical Association, 106(496), 2011.
S. N. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers. Statistical Science, 27(4):538557, 2012.
J. Peng, J. Zhu, A. Bergamaschi, W. Han, D.-Y. Noh, J. R. Pollack, and P. Wang. Regularized multivariate regression for identifying master predictors with application to integrative genomics study of breast cancer. The Annals of Applied Statistics, 4(1):5377, 2010.
M. Rudelson and S. Zhou. Reconstruction from anisotropic random measurements. IEEE Transactions on Information Theory, 59(6):34343447, 2013.
T. Sun and C.-H. Zhang. Scaled sparse linear regression. Biometrika, 99(4):879898, 2012.
R. Tibshirani. Regression shrinkage and selection with the Lasso. J. Royal. Statist. Soc B, 58:267288, 1996.
S. van de Geer, P. Buhlmann, and Y. Ritov. On asymptotically optimal confidence regions and tests for high-dimensional models. arXiv:1303.0518, 2013.
A. W. Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.
M. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using `1 -constrained quadratic programming. IEEE Trans. on Inform. Theory, 55:21832202, 2009.
L. Wasserman. All of statistics: a concise course in statistical inference. Springer Verlag, 2004.
L. Wasserman and K. Roeder. High dimensional variable selection. Annals of statistics, 37(5A):2178, 2009.
C.-H. Zhang and S. Zhang. Confidence Intervals for Low-Dimensional Parameters in High-Dimensional Linear Models. arXiv:1110.2563, 2011.
P. Zhao and B. Yu. On model selection consistency of Lasso. The Journal of Machine Learning Research, 7:25412563, 2006.  9
V. Conitzer and T. Sandholm. Computing the optimal strategy to commit to. In Proceedings of the 7th ACM Conference on Electronic Commerce (EC), pages 8290, 2006.
K. Fukuda and A. Prodon. Double description method revisited. In Combinatorics and computer science, pages 91111. Springer, 1996.
P. Gacs and L. Lovasz. Khachiyans algorithm for linear programming. Mathematical Programming Studies, 14:6168, 1981.
M. Grotschel, L. Lovasz, and A. Schrijver. Geometric Algorithms and Combinatorial Optimization. Springer, 2nd edition, 1993.
C. Kiekintveld, J. Marecki, and M. Tambe. Approximation methods for infinite Bayesian Stackelberg games: Modeling distributional payoff uncertainty. In Proceedings of the 10th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pages 10051012, 2011.
D. Korzhyk, V. Conitzer, and R. Parr. Complexity of computing optimal Stackelberg strategies in security resource allocation games. In Proceedings of the 24th AAAI Conference on Artificial Intelligence (AAAI), pages 805810, 2010.
D. Korzhyk, Z. Yin, C. Kiekintveld, V. Conitzer, and M. Tambe. Stackelberg vs. Nash in security games: An extended investigation of interchangeability, equivalence, and uniqueness. Journal of Artificial Intelligence Research, 41:297327, 2011.
J. Letchford, V. Conitzer, and K. Munagala. Learning and approximating the optimal strategy to commit to. In Proceedings of the 2nd International Symposium on Algorithmic Game Theory (SAGT), pages 250262, 2009.
J. Marecki, G. Tesauro, and R. Segal. Playing repeated Stackelberg games with unknown opponents. In Proceedings of the 11th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pages 821828, 2012.
T. S. Motzkin, H. Raiffa, G. L. Thompson, and R. M. Thrall. The double description method. Annals of Mathematics Studies, 2(28):5173, 1953.
P. Paruchuri, J. P. Pearce, J. Marecki, M. Tambe, F. F. Ordonez, and S. Kraus. Playing games for security: An efficient exact algorithm for solving Bayesian Stackelberg games. In Proceedings of the 7th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), pages 895902, 2008.
J. Pita, M. Jain, M. Tambe, F. Ordonez, and S. Kraus. Robust solutions to Stackelberg games: Addressing bounded rationality and limited observations in human cognition. Artificial Intelligence, 174(15):11421171, 2010.
M. Tambe. Security and Game Theory: Algorithms, Deployed Systems, Lessons Learned. Cambridge University Press, 2012.
A. Tauman Kalai and S. Vempala. Simulated annealing for convex optimization. Mathematics of Operations Research, 31(2):253266, 2006.  9
